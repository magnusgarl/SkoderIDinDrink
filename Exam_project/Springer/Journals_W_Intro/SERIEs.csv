Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,SERIEs,20 March 2010,https://link.springer.com/article/10.1007/s13209-010-0022-7,Editorial,March 2010,Víctor Aguirregabiria,,,Male,Unknown,Unknown,Male,,1
1.0,1.0,SERIEs,19 February 2010,https://link.springer.com/article/10.1007/s13209-009-0014-7,The econometrics of DSGE models,March 2010,Jesús Fernández-Villaverde,,,,Unknown,Unknown,Mix,,
1.0,1.0,SERIEs,23 February 2010,https://link.springer.com/article/10.1007/s13209-010-0020-9,DSGE models and their use at the ECB,March 2010,Frank Smets,Kai Christoffel,Massimo Rostagno,Male,Male,Male,Male,,17
1.0,1.0,SERIEs,19 February 2010,https://link.springer.com/article/10.1007/s13209-009-0015-6,Spain in the Euro: a general equilibrium analysis,March 2010,Javier Andrés,Samuel Hurtado,Carlos Thomas,,Male,Male,Mix,,
1.0,1.0,SERIEs,19 February 2010,https://link.springer.com/article/10.1007/s13209-009-0016-5,"Comment on “Spain in the Euro: A General Equilibrium Analysis” by Andrés, Hurtado, Ortega and Thomas",March 2010,Jesús Vázquez,,,,Unknown,Unknown,Mix,,
1.0,1.0,SERIEs,19 February 2010,https://link.springer.com/article/10.1007/s13209-009-0010-y,The drivers of housing cycles in Spain,March 2010,Oriol Aspachs-Bracons,Pau Rabanal,,Male,Male,Unknown,Male,,25
1.0,1.0,SERIEs,19 February 2010,https://link.springer.com/article/10.1007/s13209-010-0019-2,Comments on “The drivers of housing cycles in Spain” by Oriol Aspachs-Bracons and Pau Rabanal,March 2010,Stefano Neri,,,Male,Unknown,Unknown,Male,,1
1.0,1.0,SERIEs,19 February 2010,https://link.springer.com/article/10.1007/s13209-009-0013-8,A rational expectations model for simulation and policy evaluation of the Spanish economy,March 2010,J. E. Boscá,A. Díaz,L. Puch,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,SERIEs,19 February 2010,https://link.springer.com/article/10.1007/s13209-009-0017-4,Discussion of the paper “A rational expectations model for simulation and policy evaluation of the Spanish economy”,March 2010,Javier J. Pérez,,,,Unknown,Unknown,Mix,,
1.0,1.0,SERIEs,23 February 2010,https://link.springer.com/article/10.1007/s13209-009-0011-x,MEDEA: a DSGE model for the Spanish economy,March 2010,Pablo Burriel,Jesús Fernández-Villaverde,Juan F. Rubio-Ramírez,Male,,Male,Mix,,
1.0,1.0,SERIEs,19 February 2010,https://link.springer.com/article/10.1007/s13209-009-0012-9,Commentary on MEDEA: A DSGE model for the Spanish economy,March 2010,Filippo Ferroni,,,Male,Unknown,Unknown,Male,,
2.0,1.0,SERIEs,14 January 2010,https://link.springer.com/article/10.1007/s13209-009-0008-5,Informal care and labour force participation among middle-aged women in Spain,March 2011,David Casado-Marín,Pilar García-Gómez,Ángel López-Nicolás,Male,Female,Male,Mix,,
2.0,1.0,SERIEs,26 May 2010,https://link.springer.com/article/10.1007/s13209-010-0027-2,On the optimal allocation of students when peer effects are at work: tracking vs. mixing,March 2011,Marisa Hidalgo-Hidalgo,,,Female,Unknown,Unknown,Female,,4
2.0,1.0,SERIEs,30 April 2010,https://link.springer.com/article/10.1007/s13209-010-0025-4,Portfolio choice and the effects of liquidity,March 2011,Ana González,Gonzalo Rubio,,Female,Male,Unknown,Mix,,
2.0,1.0,SERIEs,12 October 2010,https://link.springer.com/article/10.1007/s13209-010-0032-5,"Innovation, loyalty and generic competition in pharmaceutical markets",March 2011,Fernando Antoñanzas,Carmelo Juárez-Castelló,Roberto Rodríguez-Ibeas,Male,Male,Male,Male,,3
2.0,1.0,SERIEs,21 January 2010,https://link.springer.com/article/10.1007/s13209-010-0018-3,Short-term monitoring of the Spanish government balance,March 2011,Teresa Leal,Diego J. Pedregal,Javier J. Pérez,Female,Male,,Mix,,
2.0,1.0,SERIEs,18 February 2010,https://link.springer.com/article/10.1007/s13209-009-0003-x,Agency problems with non-smooth decision profiles: the case of monopoly under product quality,March 2011,X. Ruiz del Portal,,,Unknown,Unknown,Unknown,Unknown,,
2.0,2.0,SERIEs,16 June 2011,https://link.springer.com/article/10.1007/s13209-011-0066-3,SEA Presidential address: Group connectivity and cooperation,June 2011,Amparo Urbano,,,Female,Unknown,Unknown,Female,,
2.0,2.0,SERIEs,14 December 2010,https://link.springer.com/article/10.1007/s13209-010-0035-2,With whom to merge? A tale of the Spanish banking deregulation process,June 2011,Ana Lozano-Vivas,Miguel A. Meléndez-Jiménez,Antonio J. Morales,Female,Male,Male,Mix,,
2.0,2.0,SERIEs,01 October 2010,https://link.springer.com/article/10.1007/s13209-010-0031-6,The information content in a volatility index for Spain,June 2011,Maria T. Gonzalez-Perez,Alfonso Novales,,Female,Male,Unknown,Mix,,
2.0,2.0,SERIEs,21 August 2010,https://link.springer.com/article/10.1007/s13209-010-0030-7,Partial price discrimination by an upstream monopolist,June 2011,Lluís Bru,Ramon Faulí-Oller,Joel Sandonís,Unknown,Male,Male,Male,,
2.0,2.0,SERIEs,27 July 2010,https://link.springer.com/article/10.1007/s13209-010-0029-0,"Financial openness, volatility, and the size of productive government",June 2011,Iñaki Erauskin,,,Male,Unknown,Unknown,Male,,6
2.0,2.0,SERIEs,02 November 2010,https://link.springer.com/article/10.1007/s13209-010-0033-4,Specialized advertising and price competition in vertically differentiated markets,June 2011,Lola Esteban,José M. Hernández,,Female,Male,Unknown,Mix,,
2.0,3.0,SERIEs,02 March 2011,https://link.springer.com/article/10.1007/s13209-011-0045-8,Voluntary contributions “vote out” public ones,September 2011,Aleix Calveras,Juan-José Ganuza,Gerard Llobet,Unknown,Unknown,Male,Male,,1
2.0,3.0,SERIEs,27 April 2011,https://link.springer.com/article/10.1007/s13209-011-0058-3,General equilibrium long-run determinants for Spanish FDI: a spatial panel data approach,September 2011,Jaime Martínez-Martín,,,Male,Unknown,Unknown,Male,,10
2.0,3.0,SERIEs,22 April 2011,https://link.springer.com/article/10.1007/s13209-011-0059-2,Introducing managerial attention allocation in incentive contracts,September 2011,Ricard Gil,Jordi Mondria,,Male,Male,Unknown,Male,,1
2.0,3.0,SERIEs,26 November 2010,https://link.springer.com/article/10.1007/s13209-010-0034-3,"Vertical integration, collusion, and tariffs",September 2011,Pedro Mendi,Rafael Moner-Colonques,José J. Sempere-Monerris,Male,Male,Male,Male,,3
2.0,3.0,SERIEs,01 March 2011,https://link.springer.com/article/10.1007/s13209-010-0036-1,Forecasting the Spanish economy with an augmented VAR–DSGE model,September 2011,Gonzalo Fernández-de-Córdoba,José L. Torres,,Male,Male,Unknown,Male,,3
2.0,3.0,SERIEs,02 March 2011,https://link.springer.com/article/10.1007/s13209-011-0037-8,Generalized marginal rate of substitution in multiconstraint consumer’s problems and their reciprocal expenditure problems,September 2011,Manuel Besada,Javier García,Carmen Vázquez,Male,,Female,Mix,,
2.0,4.0,SERIEs,17 November 2011,https://link.springer.com/article/10.1007/s13209-011-0081-4,Introduction to the Special Issues in Honor of Salvador Barberà’s 65th birthday,December 2011,Matthew O. Jackson,Hugo F. Sonnenschein,,Male,Male,Unknown,Male,,
2.0,4.0,SERIEs,15 March 2011,https://link.springer.com/article/10.1007/s13209-011-0050-y,An individual manipulability of positional voting rules,December 2011,Fuad Aleskerov,Daniel Karabekyan,Vyacheslav Yakuba,Male,Male,Male,Male,,15
2.0,4.0,SERIEs,22 February 2011,https://link.springer.com/article/10.1007/s13209-011-0038-7,A characterization of the uniform rule without Pareto-optimality,December 2011,Lars Ehlers,,,Male,Unknown,Unknown,Male,,1
2.0,4.0,SERIEs,23 February 2011,https://link.springer.com/article/10.1007/s13209-011-0046-7,Allocation problems with indivisibilities when preferences are single-peaked,December 2011,Carmen Herrero,Ricardo Martínez,,Female,Male,Unknown,Mix,,
2.0,4.0,SERIEs,23 July 2011,https://link.springer.com/article/10.1007/s13209-011-0075-2,"Euclidean preferences, option sets and strategyproofness",December 2011,Georges Bordes,Gilbert Laffond,Michel Le Breton,Male,Male,Male,Male,,6
2.0,4.0,SERIEs,24 February 2011,https://link.springer.com/article/10.1007/s13209-011-0048-5,Strategy-proof voting rules on a multidimensional policy space for a continuum of voters with elliptic preferences,December 2011,Hans Peters,Souvik Roy,Ton Storcken,Male,Unknown,Male,Male,,
2.0,4.0,SERIEs,21 September 2011,https://link.springer.com/article/10.1007/s13209-011-0077-0,Assessing the extent of strategic manipulation: the average vote example,December 2011,Régis Renault,Alain Trannoy,,Male,Male,Unknown,Male,,6
2.0,4.0,SERIEs,01 March 2011,https://link.springer.com/article/10.1007/s13209-011-0041-z,The Gibbard random dictatorship theorem: a generalization and a new proof,December 2011,Arunava Sen,,,Unknown,Unknown,Unknown,Unknown,,
2.0,4.0,SERIEs,12 May 2011,https://link.springer.com/article/10.1007/s13209-011-0064-5,A unified approach to strategy-proofness for single-peaked preferences,December 2011,John A. Weymark,,,Male,Unknown,Unknown,Male,,16
3.0,1.0,SERIEs,08 February 2011,https://link.springer.com/article/10.1007/s13209-011-0039-6,"Deciding whether a law is constitutional, interpretable, or unconstitutional",March 2012,Pablo Amorós,Ricardo Martínez,M. Socorro Puy,Male,Male,Unknown,Male,,1
3.0,1.0,SERIEs,25 February 2011,https://link.springer.com/article/10.1007/s13209-011-0040-0,"Preferences, actions and voting rules",March 2012,Alaitz Artabe,Annick Laruelle,Federico Valenciano,Female,Female,Male,Mix,,
3.0,1.0,SERIEs,25 March 2011,https://link.springer.com/article/10.1007/s13209-011-0055-6,The division problem with maximal capacity constraints,March 2012,Gustavo Bergantiños,Jordi Massó,Alejandro Neme,Male,Male,Male,Male,,4
3.0,1.0,SERIEs,28 April 2011,https://link.springer.com/article/10.1007/s13209-011-0062-7,Stochastically stable implementation,March 2012,Antonio Cabrales,Roberto Serrano,,Male,Male,Unknown,Male,,2
3.0,1.0,SERIEs,06 April 2011,https://link.springer.com/article/10.1007/s13209-011-0056-5,An agenda-setting model of electoral competition,March 2012,Josep M. Colomer,Humberto Llavador,,Male,Male,Unknown,Male,,7
3.0,1.0,SERIEs,29 March 2011,https://link.springer.com/article/10.1007/s13209-011-0052-9,Majority relation and median representative ordering,March 2012,Gabrielle Demange,,,Female,Unknown,Unknown,Female,,10
3.0,1.0,SERIEs,29 March 2011,https://link.springer.com/article/10.1007/s13209-011-0054-7,Cost effectiveness of a combination of instruments for global warming: a quantitative approach for Spain,March 2012,M. C. Gallastegui,M. González-Eguino,I. Galarraga,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,SERIEs,08 June 2011,https://link.springer.com/article/10.1007/s13209-011-0069-0,Equal opportunity equivalence in land division,March 2012,Antonio Nicolò,Andrés Perea y Monsuwe,Paolo Roberti,Male,Male,Male,Male,,4
3.0,1.0,SERIEs,24 February 2011,https://link.springer.com/article/10.1007/s13209-011-0043-x,On the impact of independence of irrelevant alternatives: the case of two-person NTU games,March 2012,Bezalel Peleg,Peter Sudhölter,José M. Zarzuelo,Unknown,Male,Male,Male,,1
3.0,1.0,SERIEs,15 July 2011,https://link.springer.com/article/10.1007/s13209-011-0073-4,Optimism and commitment: an elementary theory of bargaining and war,March 2012,Clara Ponsati,Santiago Sanchez-Pages,,Female,Male,Unknown,Mix,,
3.0,1.0,SERIEs,02 March 2011,https://link.springer.com/article/10.1007/s13209-011-0047-6,The political economy of income taxation under asymmetric information: the two-type case,March 2012,John E. Roemer,,,Male,Unknown,Unknown,Male,,10
3.0,1.0,SERIEs,19 April 2011,https://link.springer.com/article/10.1007/s13209-011-0057-4,All but one free ride when wealth effects are small,March 2012,Joaquim Silvestre,,,Male,Unknown,Unknown,Male,,1
3.0,1.0,SERIEs,29 March 2011,https://link.springer.com/article/10.1007/s13209-011-0053-8,Freedom of choice: John Stuart Mill and the tree of life,March 2012,Jorge Alcalde-Unzu,Miguel A. Ballester,Jorge Nieto,Male,Male,Male,Male,,
3.0,1.0,SERIEs,10 May 2011,https://link.springer.com/article/10.1007/s13209-011-0061-8,An axiomatic analysis of ranking sets under simple categorization,March 2012,José C. R. Alcantud,Ritxar Arlegi,,Male,Unknown,Unknown,Male,,4
3.0,1.0,SERIEs,24 February 2011,https://link.springer.com/article/10.1007/s13209-011-0044-9,Revealed preference and choice under uncertainty,March 2012,Walter Bossert,Kotaro Suzumura,,Male,Male,Unknown,Male,,8
3.0,1.0,SERIEs,25 March 2011,https://link.springer.com/article/10.1007/s13209-011-0049-4,Nested identification of subjective probabilities,March 2012,Jacques H. Dreze,,,Male,Unknown,Unknown,Male,,
3.0,1.0,SERIEs,25 February 2011,https://link.springer.com/article/10.1007/s13209-011-0042-y,"Anarchism, postmodernism and realism under confirmatory bias",March 2012,Juan Urrutia Elejalde,,,Male,Unknown,Unknown,Male,,
3.0,1.0,SERIEs,10 August 2011,https://link.springer.com/article/10.1007/s13209-011-0074-3,The evaluation of citation distributions,March 2012,Javier Ruiz-Castillo,,,,Unknown,Unknown,Mix,,
4.0,1.0,SERIEs,28 December 2012,https://link.springer.com/article/10.1007/s13209-012-0095-6,"A chronology of turning points in economic activity: Spain, 1850–2011",March 2013,Travis J. Berge,Òscar Jordà,,Male,Unknown,Unknown,Male,"Late in the third quarter of 2007, as the fuse of the Global Financial Recession was being lit across the globe, 20.5 million Spaniards held a job.Footnote 1 4 years later, that number stood at 18.2 million—a loss of over 235,000 jobs at a time when the working age population grew by about 800,000 individuals. Measured by the peak to trough decline in GDP—a 5 % loss—one would have to reach back to the Great Depression (excluding the Spanish Civil War) to find a steeper decline in output. Moreover, employment prospects remain dim in the waning hours of 2011 for many that joined the ranks of the unemployed back in 2007. Given this environment, dating turning points in economic activity may seem the epitome of the academic exercise. Yet the causes, consequences and solutions to the current predicament cannot find their mooring without an accurate chronology of the Spanish business cycle. Not surprisingly, the preoccupation with business cycles saw its origin in the study of crises. Whereas early economic historians found the roots of economic crises in “war or the fiscal embarrassments of governments,”Footnote 2 by the early twentieth century it became clear that economies experienced contractions in economic activity whose origin could not be easily determined. As economies became less dependent on agriculture, more industrialized, more globalized and therefore more financialized, the vagaries of the weather were soon replaced by the vagaries of the whim. Asset price bubbles and financial crises littered the mid-nineteenth and early twentieth centuries (see Schularick and Taylor, 2012). The period from 1870 to 1929 saw no less than four global financial panics, each engulfing a large portion of the industrialized world—and by most accounts upwards of 50 % of global GDP at the time (see Jordà et al. 2011). It was against this backdrop that the National Bureau of Economic Research (NBER) was created in 1920. The NBER now views as its core mission “the aggregate economy, examining in detail the business cycle and long-term economic growth.”Footnote 3 Early exponents of this mission can be found in “Simon Kuznets’ pioneering work on national income accounting, Wesley Mitchell’s influential study of the business cycle, and Milton Friedman’s research on the demand for money and the determinants of consumer spending [\(\ldots \)]”Footnote 4. In fact, it is the work of Wesley C. Mitchell and Arthur F. Burns (1946) which laid the foundations for the study of the business cycle at the NBER. And since 1978 a standing Business Cycle Dating Committee (BCDC) was formed to become the arbiter of the American business cycle, a chronology that now reaches back to 1854. Slowly, other countries have been creating similar committees, such as the Euro Area Business Cycle Dating Committee of the Center for Economic Policy Research, founded in 2002. But to our knowledge, no such independent arrangement has been created in Spain. A chronology of the Spanish business cycle is not only a necessity for the modern study of the origins of macroeconomic fluctuations and the design of optimal policy responses, it is a necessity that as of September 7, 2011 would appear to be a matter of constitutional law. The constitutional reform of article 135 passed by parliament that day states that: “The limits of the structural deficit and public debt volume may be exceeded only in case of natural disasters, economic recession or extraordinary emergency situations that are either beyond the control of the State or significantly impair the financial situation or the economic or social sustainability of the State, as appreciated by an absolute majority of the members of the Congress of Deputies” (emphasis added). It would appear that the whimsy of the business cycle is at the purview of the legislature rather than the economic brain trust. If nothing else, this observation serves to cement the importance that an independent committee, whose job is to determine turning points in economic activity, can play in the economic and political life of a country. But what is a recession? The BCDC offers a clear yet less than operational definition:Footnote 5 A recession is a significant decline in economic activity spread across the economy, lasting more than a few months, normally visible in production, employment, real income, and other indicators. Determination of the December 2007 Peak in Economic Activity, December 2008. Business Cycle Dating Committee of the National Bureau of Economic Research. And most institutions in the business of keeping a chronology of economic cyclical activity use a similarly intuitive yet entirely mathematically imprecise definition of recession. How then would one determine whether or not a business cycle dating committee (or a legislature) is appropriately sorting the historical record into periods of expansion and periods of recession? After all, the true state of the economy (expansion or recession) is inherently unobservable—an infinite sample of data can only improve the precision of the estimated probabilities associated to each state, but it does not reveal the states themselves. Our quest to formalize a chronology of the Spanish business cycle begins with a brief description of the statistical methods that have been used in the literature to achieve a classification of turning points. We begin with the early methods that Gerhard Bry and Charlotte Boschan introduced in 1971 at the NBER. The Bry and Boschan (1971) algorithm comes closest to translating the NBER’s definition into practice: remove seasonals from the data in levels (there is no need to detrend), smooth the data lightly to remove aberrations, then identify local minima and maxima in the series. The local minima and maxima are constrained to ensure that cycles have a certain duration, that they alternate, and that complete cycles last at least 15 months. A local minimum is a trough and the following local maximum a peak, so that the period between trough and peak is an expansion, and from peak to trough a recession. The Bry and Boschan (1971) algorithm was applied to quarterly data by Harding and Pagan (2002a, b), Kose et al. (2003, 2008a, b), among others. As arbitrary as the Bry and Boschan (1971) algorithm may seem, it is simple to implement, reproducible, and perhaps more critically, it does not require that the data be detrended. A more structural view of how fluctuations around trend-growth are determined is to suppose that the data are generated by a mixture process. In the econometrics literature, the pioneering work of Hamilton (1989) was the first to characterize the stochastic process of economic fluctuations as a mixture. The idea is to conceive of the data as being generated by two distributions (one for each state, expansion or recession) and to characterize the transition between states as a hidden-Markov process. In the statistics literature, the problem of identifying the underlying state of the economy closely resembles pattern recognition problems in computational learning, or more briefly decoding. Decoding is most often referred to in information theory as an algorithm for recovering a sequence of code words or messages from a given sequence of noisy output signals (Geman and Kochanek 2001). In fact, almost every cell-phone on earth uses a version of the Viterbi algorithm (Viterbi 1967), itself based on filtering a hidden-Markov process. More recently, an application of these principles with non-parametric computational techniques was introduced by Hsieh et al. (2006) in what they call the hierarchical factor segmentation (HFS) algorithm. An application of HFS to economic data is found in Hsieh et al. (2010a). The basic principle of the HFS algorithm is to use the recurrence time distribution of certain events (say, record each time output grows below a given threshold) to come up with an optimal non-parametric mixture using the maximum entropy principle of Jaynes (1957a, b). Interestingly, the idea of using recurrence times dates back to Poincaré (1890). Each method can be applied to different series, resulting in several different recession chronologies, one for each variable. Alternatively, one could aggregate the data first, perhaps with a factor model, and then use the aggregate series to date the business cycle. The combine-then-date approach appears to be the most commonly used at present (a good example is Stock and Watson 2010), perhaps reflecting the popularity that factor models currently enjoy in other areas of economics. Moreover, a single indicator of economic activity has the advantage of being a succinct tool of communication. From that perspective, our investigation will take us to consider a variety of such indicators that have been proposed to characterize business conditions in Spain. Among these, we will investigate the OECD’s composite leading indicator (CLI) index,Footnote 6 the index of economic activity constructed by the Spanish think tank FEDEA,Footnote 7 and two recent more sophisticated indexes, the MICA-BBVA indexFootnote 8 of Camacho and Doménech (2011), and Spain-STINGFootnote 9 by Camacho and Quirós (2011). Yet as we shall see, variables do not always fluctuate synchronously—consider the behavior of employment versus output across recent business cycles—an observation that favors the date-then-combine approach if interest is tilted toward constructing a single series of turning points. Moreover, the variables in our data set are observed over different spans and at different frequencies, features that complicate the factor model approach. Instead, a simple method of date-combination based on the network connectivity properties of each chronology (see, e.g., Watts and Strogatz 1998), turns out to provide insight into the determinants of economic fluctuations and is a straightforward method to generate a single chronology of turning points. It is not enough to come up with a chronology of turning points, one must also formally assess the quality of any given chronology. A scientific defense of the quality of such a chronology requires formal statistical assessment and to this end we reach back to 1884 and Charles S. Peirce’s “Numerical Measure of the Success of Predictions,” the direct precursor to the Youden index (Youden 1950) for rating medical diagnostic tests, and the receiver operating characteristics (ROC) curve by Peterson and Birdsall (1953) in the field of radar detection theory. Today, the ROC curve is a standard statistical tool in the assessment of medical diagnostic procedures (going back to Lusted 1960), but it is also used routinely in atmospheric sciences (see Mason 1982) and machine learning (Spackman 1989). In economics, early uses appear for the problem of credit scoring, but more recently for the evaluation of zero-cost investments, such as the carry trade (see Jordà and Taylor 2009; Berge et al. 2011). Jordà and Taylor (2011) provide perhaps the most detailed overview of this literature and emphasize the correct classification frontier, a relative of the ROC curve, as the more appropriate tool in economics. Applications of these techniques to the classification of economic data into expansions and recessions in the US is done in Berge and Jordà (2011). Our pursuits end by gazing into the future: What can we say about the problem of predicting future turning points? In another departure from traditional econometric practice, the problem of choosing good predictors for classification purposes does not require that the predictors be accurate in the usual root mean squared error sense. Moreover, we will argue that, unlike conventional time series modelling, it is best to tailor the set of predictors to the forecast horizon under consideration. In our experience, we have found that variables can be good classifiers in the short-run but poor classifiers in the long-run and vice versa. If, as is common practice, one fits a model based on short-run prediction and then iterates forward to longer horizons, the model will tend to put too much weight on the short-run classifiers and generate worse predictions than if a different model is chosen for each horizon—a practice commonly referred to as direct forecasting. Seen through this lens, the outlook for the Spanish economy remains grim.",12
4.0,1.0,SERIEs,25 February 2012,https://link.springer.com/article/10.1007/s13209-012-0086-7,Happiness economics,March 2013,Ada Ferrer-i-Carbonell,,,Female,Unknown,Unknown,Female,,44
4.0,1.0,SERIEs,17 December 2011,https://link.springer.com/article/10.1007/s13209-011-0084-1,Advertising bans,March 2013,Massimo Motta,,,Male,Unknown,Unknown,Male,,3
4.0,1.0,SERIEs,28 October 2011,https://link.springer.com/article/10.1007/s13209-011-0080-5,The effects of surprise political events on quoted firms: the March 2004 election in Spain,March 2013,Pau Castells,Francesc Trillas,,Male,Male,Unknown,Male,,12
4.0,1.0,SERIEs,03 December 2011,https://link.springer.com/article/10.1007/s13209-011-0082-3,Endogenous governance transparency and product market competition,March 2013,Ana Hidalgo-Cabrillana,,,Female,Unknown,Unknown,Female,,1
4.0,2.0,SERIEs,06 December 2011,https://link.springer.com/article/10.1007/s13209-011-0083-2,Sharing a polluted river through environmental taxes,June 2013,María Gómez-Rúa,,,,Unknown,Unknown,Mix,,
4.0,2.0,SERIEs,24 February 2012,https://link.springer.com/article/10.1007/s13209-012-0085-8,Capital structure of small companies in the Spanish footwear sector: relevant factors,June 2013,Miguel A. Acedo-Ramírez,Juan C. Ayala-Calvo,José E. Rodríguez-Osés,Male,Male,Male,Male,,4
4.0,2.0,SERIEs,20 June 2012,https://link.springer.com/article/10.1007/s13209-012-0088-5,Promoting permanent employment: lessons from Spain,June 2013,Ildefonso Mendez,,,Male,Unknown,Unknown,Male,,5
4.0,2.0,SERIEs,07 August 2012,https://link.springer.com/article/10.1007/s13209-012-0089-4,Understanding poverty persistence in Spain,June 2013,Sara Ayllón,,,Female,Unknown,Unknown,Female,,15
4.0,2.0,SERIEs,04 August 2012,https://link.springer.com/article/10.1007/s13209-012-0090-y,Information sharing and lending market competition under strong adverse selection,June 2013,Jorge Fernández-Ruiz,Miguel García-Cestona,,Male,Male,Unknown,Male,,
4.0,3.0,SERIEs,21 November 2012,https://link.springer.com/article/10.1007/s13209-012-0091-x,Who bears labour taxes and social contributions? A meta-analysis approach,August 2013,Ángel Melguizo,José Manuel González-Páramo,,Male,Male,Unknown,Male,"The reduction in labour taxes is a widespread policy recommendation for raising employment (see, for instance, the seminal reports from the European Commission 1994; OECD 1994). In broad terms, labour taxes (i.e. personal income tax and social security contributions) drive a wedge between labour costs and net wages and have a negative effect on labour supply, structural employment and hours worked. From the academia, Prescott (2004) triggered the debate by attributing all the difference in labour utilization between the US and Europe to direct taxes. This author calibrated a labour supply model and found out that the divergence in hours worked per week among the working-age population since the 1970s between the US and France and Germany could be explained by differences in marginal tax rates on labour. In a similar line of research, Coenen et al. (2008) employed a calibrated version of the New Area-Wide Model developed at the ECB to simulate the effects of establishing a ‘US fiscal system’ in the euro area. The analysis showed that the reduction in employer social contributions to the levels prevailing in the US (from 21.9 to 7.1 % of labour costs) may increase output by more than five points, hours worked by more than 6 % and real wages slightly less than 13 %. These results generated a lively debate.Footnote 1 Based on this assumption, since the 1990s many European governments have followed this tax reform path, cutting social security payroll taxes for cyclical and structural reasons. For instance, since 1997 Spain has cut social security payroll taxes for permanent contracts and for population groups affected by long-term unemployment. Recently, the new government has announced a reduction in employer social contributions (1 p.p. in 2013 and another point in 2014), compensated by an increase in value added taxation. Since 2000, France encouraged the transition to the 35-hour week with lower employer social security contributions. In 2007 and 2008 Germany introduced cuts in unemployment insurance contributions, financed by a higher value added tax rate. Finally, in the midst of the recent international crisis, the US Congressional Budget Office (2008), for instance, considered the reduction in social contributions to be one of the most effective measures for responding to short-term economic weakness, albeit subject to lags and uncertainty. Despite these common developments, there remains a significant fiscal gap within OECD countries. As Fig. 1 shows, the direct tax wedge (income tax, employee and employer payroll taxes) for a two-earner household with two children is well over 40 % in France, Italy and Germany, while Anglo-Saxon economies and Japan limit the burden to approximately 25–30 %. Labour taxation in selected OECD countries (total average tax wedge, percentage of labour costs). Source: OECD Taxing Wages. Note: Two-earner married couple one at 100 % of average earnings and the other at 67 % of two children The economic effects of taxation depend ultimately on the long and short-term economic incidence, i.e. on who really bears the burden. In the case of employer social contributions, they can be borne by the employers (ultimately reducing the firm’s profits), they can be shifted backwards to employees (reducing net wages), or they can be shifted forward to consumers (increasing the price level). Most of the previous papers calibrate this effect. Empirical literature shows mixed results. In a classic survey, Hamermesh (1993) analysed 15 seminal studies on the economic incidence of payroll taxes, mainly social security contributions. The author rejected any robust conclusion, not even a consensus interval: results ranged from full to null shifting. By surveying recent studies, Arpaia and Mourre (2005) confirmed that taxation increased unemployment but they also highlighted the complexity of its interactions with other labour market and economic institutions. The European Central Bank (2008) documented the disincentives to work (particularly for low-income workers) stemming from high marginal tax rates. In a similar vein, Keane (2011) reviewed the literature on labour supply by gender. Once again, there appears to be a considerable controversy over their response to changes in wages and taxes. This is especially the case for men, due to differences in the measurement of wages and human capital (for women most studies find a large labour supply elasticity). This is the appropriate field for the meta-analysis approach. In contrast to these narrative surveys, meta-analysis allows revising the relevant empirical literature in a more formal and objective manner. As summarised by Stanley and Jarrel (1989),Footnote 2 meta-analysis starts with the compilation of an exhaustive sample of literature and the choice of the dependent variable (in our case, the degree of backward shifting of labour taxes, proxied by the estimated elasticity of net wages to taxation). A general set of ‘moderators’, i.e. variables that reflect the quantitative and qualitative features of the different studies that could be influencing their results (theoretical model and sample, among others) is then selected and tabulated using dummy variables. The meta-regression of the dependent variable on these moderators can be used to quantifying the‘ true dependent variable‘, that is, the consensus result of the empirical literature on the effect of taxes on wages after controlling for methodological differences. And also, and probably more important, meta-analysis permits to show which aspects of the modelling, data and econometric techniques are important, or not, for the estimates. A sensible starting point is Fuchs et al. (1998). Based on a survey of economics departments at 40 leading US universities, the authors conclude that employers bear 20 % of firm social contributions, while employees bear the remaining 80 % via lower net wages. In other words, the conventional wisdom, in line with public economics textbooks, is that the relatively higher rigidity of labour supply with respect to labour demand determines that the market adjustment is mainly concentrated on wages, and not on employment.Footnote 3 Figure 2 generalises this framework, taking into account wage bargaining and firm market behaviour, as suggested by Layard et al. (1991). In this ‘wage-setting schedule’, real wage accepted by workers (adjusted for trend growth in labour efficiency) varies with the unemployment rate: the higher the unemployment rate, the lower real wages will be. The position of the wage-setting schedule is influenced by a number of structural characteristics, such as the degree of trade union power, the generosity of unemployment benefits, the stringency of employment regulations or the efficiency of the matching process and, related to it, the centralisation of the wage-employment bargaining. The empirical literature confirmed the role of the centralisation of wage bargaining and union presence. Centralised economies with strong unions such as the Nordic counties, or decentralised wage bargaining with weak unions as the Anglo-Saxon exhibit better performance than the Continental and Mediterranean. We will develop this issue afterwards. In this context, an increase in labour taxation (e.g. employer social security taxes, SST in the Fig. 2) would generate a downwards shift of the labour demand curve (‘price-setting schedule’). Market equilibrium would move from A to B, generating a limited negative effect on the employment rate (from N to N’), since labour costs hardly increase (from W to W’). Workers would bear the major part of the tax burden (BD over BC), since net real wages fall from w to w’. Economic incidence of employer payroll taxes The main goal of this paper is to test whether this estimate of the economic incidence of labour taxation is consistent with the empirical literature on the subject. To do so, we quantify the effect of the different methodological approaches, and temporal and geographical coverage. We think we are the first to perform a meta-analysis of the incidence of labour taxes and social contributions. We place more weight on the methodological variables that stem from economic theory or from generally accepted empirical results, such as those related to nominal rigidities, the wage bargaining characteristics or the pension system design. The paper is organized as follows. In Sect. 2, we present a brief description of the empirical literature of taxation and wages/labour costs, with a focus on social contributions. In Sect. 3, we report the basic methodology and the results of the meta-analysis regressions. They are grouped under three headings: ‘basic moderators’ that capture country and temporal fixed effects, ‘economic moderators’ to control the impact of the most relevant economic features and ‘other moderators’, mainly reflecting the econometric techniques. We also perform various robustness checks, based on the sample of estimates, the econometric methods and the procedure of specification selection. Finally, in Sect. 4 we present the main conclusions and general economic policy recommendations.",19
4.0,3.0,SERIEs,16 June 2013,https://link.springer.com/article/10.1007/s13209-013-0096-0,Old age pensions and retirement in Spain,August 2013,Raquel Vegas Sánchez,Isabel Argimón,Clara I. González,Female,Female,Female,Female,"There has been a decline in the labour force participation of older workers, over the last decades. While in the seventies, the participation rates of males over 55 exceeded 50 %, in 2008 it did not reach 30 %. The role of pension benefits rules in this evolution may have been crucial. These rules may be too generous in providing income support, and they may also create incentives to early retirement. Three characteristics are relevant in this context: the amount of the pension that the system provides, the pattern of benefits associated with each age of retirement, and the entitlement rights that define the conditions to be met to be able to claim a pension at each age. While many countries have already implemented large reforms,Footnote 1 Spain has mainly undertaken parametric changes in its Social Security system, which have been agreed under the so-called Toledo Agreement. They have been directed to reducing incentives to early retirement and to increasing incentives to leave the labour market at a later age. Active labour force policies that should stimulate the demand for elderly workers have also been proposed with the aim of contributing to raising labour market participation of the elderly. At the end of the day, the effects of pensions on retirement compound labour supply and demand forces, so that their final impact is an empirical issue. The goal of this paper is to quantify the role that Social Security provisions for old-age pensions play in the retirement decision of Spanish workers and to evaluate the effects of the 2002 reform. We propose to estimate a model for retirement that captures the effects of pension incentives on the timing of the pension claim. In particular, we analyse the probability of claiming a pension at a specific age, given that the worker has not yet retired, as a function, not only of individual demographic characteristics, but also of the economic incentives underlying the retirement legislation. There is already a large body of research that examines the economic incentives created by the pension system in Spain (see, e.g., Boldrin et al. 1999, 2004; Boldrin and Jimenez-Martín 2007; Jimenez-Martín 2006; Moral-Arce et al. 2009). Our contribution to this field is twofold. First, we exploit the longitudinal information contained in the Continuous Sample of Working Histories (CSWH, “Muestra Continua de Vidas Laborales”, MCVL in Spanish), wave 2006, with a duration model.Footnote 2 The detailed information contained in the CSWH, from Spanish Social Security administrative records and that is accompanied by Census data, allows tracking workers’ decisions over time so as to take time dependence into account. Moreover, the CSWH has the advantage over other databases used in previous empirical workFootnote 3 that its sample design is publicly known and, therefore, it allows for a better and broader interpretation of the results. Instead of proposing a point in time estimation, we analyse retirement patterns and the effects of economic incentives using a logistic specification of a duration model. This approach allows the analysis of the role played at each age by incentives stemming from social security rules. The second contribution of this paper is the quantification of the effect on retirement decisions of the reform that was agreed in 2001 in Spain (Acuerdo para la Mejora y el Desarrollo del Sistema de Protección Social, April 2001) and that was fully implemented in 2002 (Law 35/2002, July). In particular, in this paper, we consider the effects of three main changes (a more detailed description of the reform is provided in Annex 1).Footnote 4 On the one hand, the reform gave access to early retirement before the statutory retirement age established at 65, for all workers that were at least 61, had at least 30 years of contributory life (2 of which within the last 15) and their last employment termination had been involuntary. Up to then, only those who had worked before 1967 were entitled to take early retirement from the age of 60. On the other hand, the reform increased the linkages between the contributive effort exerted by workers and the pension received, setting the penalty for early retirement decreasing with age and total years of contribution. Moreover, each full year of employment beyond statutory retirement age (65), implied a 2 % increase in the regulatory base to compute retirement benefits, so that those workers older than 65, with at least 35 years of contributions could receive a pension that exceeded 100 % of the regulatory base value. We selected a sample of men aged from 60 to 70 in 2006, that had contributed to the Social Security General Scheme and that were entitled to receive retirement benefits. The sample period spans 10 years, from 1996 to 2006. We analyse retirement decisions that imply a full withdrawal from the labour market and that give rise to an old-age pension.Footnote 5 We assimilate the claiming of the pension to the decision to retire for good, whether or not the worker was already discouraged and had stopped looking for a job well before claiming the pension. The rest of the paper is organized as follows. Section 2 contains a literature survey of the role played by Social Security in the retirement decision of elderly workers, placing special emphasis on the works addressing the Spanish pension system. Section 3 contains the empirical strategy while Sect. 4 describes the dataset used in the empirical analysis. Section 5 summarises the main results and Sect. 6 contains the conclusions. Additionally, two appendixes are included in the paper. Appendix A provides a short explanation about the main features of the Spanish Social Security pension system. Appendix B contains a brief description of the variables used in the empirical analysis and the detailed results of the estimation.",5
4.0,3.0,SERIEs,04 November 2012,https://link.springer.com/article/10.1007/s13209-012-0092-9,Is informality a barrier to financial development?,August 2013,Ceyhun Elgin,Burak R. Uras,,Male,Male,Unknown,Male,"Financial development is clearly an important indicator of the long-run macroeconomic development. There is a vast empirical literature focusing on the relationship between the level of financial development and long-run economic performance.Footnote 1 Similarly, tax evasion and the extent of the informal activity are among key issues affecting the economic and institutional development of a society (see Johnson et al. 1997, 1998; Torgler and Schneider 2007 and much more recently Elgin 2010 among many others.). Even though informalityFootnote 2 is prevalent and poses serious economic, social and political challenges across the world many issues about the nature and consequences of informality remain largely under-explored or unresolved. For example, the evidence presented in the existing literature, has failed to generate a consensus around the determinants and consequences of the informal sector among researchers. There are also many other open questions including even such basic ones such as whether informal sector size would be larger in low income or high income nations (see Dreher and Schneider 2010); whether taxes are positively correlated with informal sector size or not (see Schneider and Enste 2000; Friedman et al. 2000; Elgin 2010 among many others.) or whether shadow economy and corruption are substitutes or complements (Dreher and Schneider 2010). In this paper, combining these two strands of the literature, we explore the effects of the size of the informal sector on financial sector development. We define the level of financial development as the fraction of external finance over total income in the formal sector and show in a theoretical model that there exists an inverted-U relationship between the level of financial development and the size of informal sector. The effects of informality on financial sector repression in the presence of financial sector capacity constraints is central to our theory. Specifically, for a given state of institutional development we show that high levels of informal activity increases the level of financial repression in the formal financial sector and therefore suppressing the informal sector size benefits the level of financial development in the economy. However, if the initial level of informal sector size is small enough, the losses due to financial repression are negligible in the formal financial sector. In this latter case, if the capacity constraints of the formal financial institutions are binding, lowering the size of the informal sector would retard the level of financial development. We also support our theoretical prediction in a cross-country panel data analysis and present robust empirical evidence for the existence of an inverted-U relationship between financial development and the size of the informal sector. Theoretical studies on financial development are various. Some of the significant contributions in this area are Greenwood and Jovanovic (1990), Bencivenga and Smith (1991), Marcet and Marimon (1992), Banerjee and Newman (1993), Acemoglu and Zilibotti (1997), Aghion and Bolton (1997), Azariadis and Kaas (2007), Antunes et al. (2008), and Aghion et al. (2010). Our work abstracts away from studying the positive effects of financial development on aggregate economy and instead concentrates on the contribution of the size of informal sector in explaining society’s financial development. In this respect our work is closely related with Greenwood and Jovanovic (1990) and Acemoglu and Zilibotti (1997) where the level of financial sector development is to be determined endogenously in the model. Notwithstanding there are other studies that are concerned with the interrelation between finance and informal sector size. We can list Straub (2005), Amaral and Quintin (2006), Antunes and Cavalcanti (2007), Beck et al. (2009) much more recently Massenot and Straub (2011), as important contributions to this literature. The common motivation point of these studies is that the magnitude of credit market imperfections in the formal sector constrains the outside finance for entrepreneurial firms and magnifies the size of the informal activity. The study that gets closest to the central motivation of our paper is Massenot and Straub (2011). In Massenot and Straub (2011) setting, an exogenously given level of credit market frictions (entry costs to be more precise) affects the size of the formal activity and in turn the supply and also demand for credit simultaneously as in Castro et al. (2004). Different from their work, in our model the exogenous variation in the size of the informal activity affects the extent of credit market frictions. Therefore, to the best of our knowledge our paper is the first to analyze the reverse causality between the size of informal sector on the availability of external finance in the formal sector. The theoretical framework we study rests on stylized empirical differences between formal and informal sector production. Specifically, in our model formal sector entrepreneurs pay taxes and borrow and lend in the financial market at an endogenous cost of external finance to insure against idiosyncratic production risk. Informal sector entrepreneurs do not pay taxes and do not have access to external financing. We assume that the government has to collect a given amount of tax revenue from the public. Due to the existence of tax evasion in the economy, the government has to rely on indirect taxation through the financial intermediary. We model the indirect financial sector taxes as financial repression á la Roubini and Sala-i Martin (1992) and Roubini and Sala-i Martin (1995). Roubini and Sala-i Martin (1992) and Roubini and Sala-i Martin (1995), Aruoba (2010), and Cavalcanti and Villamil (2012) show that in the presence of large tax evasion the optimum level of inflation and optimal financial repression is greater than zero. We structure the model in such a way that the endogenous financial repression in the economy is a decreasing function of the size of the informal sector. Furthermore, we assume that the banking sector of the economy functions under capacity constraints. Capacity constraints at the financial sector is a well established characteristic assumption of the Costly-State-Verification (CSV) models of banking pioneered by Townsend (1979), Williamson (1987), and Diamond (1991). In CSV models monitoring costs of the banking sector are represented by a convex-cost function. Following the CSV literature we also adapt a standard convex-cost function for the financial intermediary sector. Our highlighted analytical results are as the following: When the size of the informal sector decreases, the marginal cost of external finance in the formal sector declines as a result of the decline in the level of financial repression whereas at the same time since the demand for external funds in the formal sector rises marginal cost increases due to the presence of convex monitoring costs. If the gains due the decline in financial repression outweighs the losses due capacity constrains, the contraction in the informal sector size stimulates the size of the formal external financing. Otherwise, a contraction in the size of the informal activity crowds outside financing in the formal economy. Empirically, using a cross-country panel data set of 152 countries over the period 1999–2007 we provide a robust support in favor of the presence of an inverted-U relation between the size of the informal sector and financial development. Moreover, using panel system estimations we also present evidence in supporting for the mechanism of our model. Specifically, we show that informal sector is positively related to the proxies of financial repression and financial sector efficiency. The rest of the paper is organized as follows: In the second section of the paper we build the theoretical framework which we want to utilize to account for the proposed relationship between financial development and informality. Next, in the third section we conduct an empirical analysis and establish a robust inverted-U relationship between financial development and the size of the informal sector, even after controlling for various variables that might be associated with the level of financial development. Finally, we provide concluding remarks in the last section.",26
4.0,3.0,SERIEs,19 December 2012,https://link.springer.com/article/10.1007/s13209-012-0094-7,The effectiveness of the audit committee in Spain: implications of its existence on the auditor’s opinion,August 2013,Javier de Andrés Suárez,Elena Cabal García,Camino Rodríguez Gutiérrez,,Female,Unknown,Mix,,
4.0,4.0,SERIEs,11 December 2012,https://link.springer.com/article/10.1007/s13209-012-0093-8,The underreporting of income by self-employed workers in Spain,November 2013,Diego Martinez-Lopez,,,Male,Unknown,Unknown,Male,"One of the most extended ways of tax evasion is that related to the underreporting of income by self-employed workers. As long as their incomes are not subject to third-party reporting, the probability of being detected by the tax authorities in case of hiding earnings is lower than in the case of salary workers, and this leads to higher levels of tax evasion. This acquires more relevance in a country like Spain, where the high levels of unemployment clearly affect the individual’s decisions to work in the underground economy (Ahn and De la Rica 1997), and the lack of flexible enough rules in the labor markets may encourage the informal economy (Bentolila et al. 1994). As other phenomena of tax evasion, the first challenge to approach it lies in the difficulty of measuring the extent of such concealment. The standard method is based on the seminal paper by Pissarides and Weber (1989), which uses the Engel curves for food demand. The underlying idea is simple. Both salary and self-employed workers report accurately their food expenditures in household budget surveys. By contrast, when they are asked about their earnings, only the salary workers say their true income. The estimate of underreporting of income by the self-employed workers is then given by the comparison of food expenditures of both groups in function of declared income, given other economic and demographic characteristics. A detailed explanation of this method is provided in the next section. On this basis, a number of papers have offered estimates of underreporting for different samples. In essence, what is computed is the number by which the reported income of self-employed has to be multiplied to obtain the true income. For the UK economy, Pissarides and Weber (1989) give a central value of 1.55 in 1982. From another point of view, Lyssiotou et al. (2004), using a complete demand system approach and non-parametric estimation methods, suggest that the extent of underreporting by self-employed workers in the UK in 1993 goes from 118 % for households with head in blue collar occupation to 64 % for white collar jobs. With data of Canada, Schuetze (2002) finds, for some years between 1969 and 1992, estimates that go from 11 to 23 % as average values of lower and upper bound estimates, respectively. For the period 1994–1996, Johansson (2005) gives a range of estimates between 16 and 40 % of underreporting in Finland, depending on the definition used for the self-employed household. More recently, Engstrom and Holmlund (2009) conclude that the Swedish households with at least one self-employed member underreport their income by around 30 % in early 2000s. And Hurst et al. (2011), using three data samples for the US in the 80s, 90s and early 2000s, estimate the degree of underreporting by between 25 and 35 %. From the very beginning of this literature, most of papers assume that employees do not hide part of their income and underreporting is exclusively concentrated on self-employed workers. But this simplifying assumption is weak from both theoretical (see, for instance, Kolm and Nielsen (2008), for a model with concealment of income by firms and salary workers) and empirical points of view. In this sense, the 2007 Eurobarometer shows that 5 % of all dependant employees in a representative sample of individuals in the EU admitted having received all or part of their salary as envelope or cash-in-hand wages.Footnote 1 Our own sample shows a number of indications leading to think that also the salary workers partly hide their earnings. For instance, about 12 % of households declaring that the main source of their income is on their own (“cuenta propia”) classify themselves as salary workers. Furthermore, the number of employees that do not inform on their monthly income in the survey is about twice than that of respondents (obviously, this figure is higher in the case of self-employed workers—about three times—but both of them may reveal concealment of income). This paper applies the methodology by Pissarides and Weber (1989) to get an estimation of the extent of underreporting by the Spanish self-employed over the period 2006–2009. Our data come from the Spanish Household Budget Surveys. The robustness of our results has been checked using alternative specifications, testing for non-linearities in the relationship between income and food expenditure, and dealing with potential problems of endogeneity. Different measures of the key variables have been examined as well. In this context, we can summarize the main contributions of the paper as follows. Firstly, we replicate the well-known approach of estimating food demand functions for making explicit a measure of concealment of income in a sample that has never been exploited in this regard, with the particular characteristics of the Spanish case in terms of data, the period considered and others. Secondly, the interpretation of the results considers the possibility that the salary workers also conceal part of their incomes; in fact, this can be seen not only as a realistic assumption but also as a reasonable reading of our findings. After the Introduction, we set up the theoretical framework used to measure the extent of underreporting of income. Section 3 explains the main features of data and the criteria followed to build the sample. Section 4 gives details of estimation procedures and shows the results. Finally, Sect. 5 concludes.",21
4.0,4.0,SERIEs,20 April 2013,https://link.springer.com/article/10.1007/s13209-013-0097-z,Unions’ bargaining coordination in multinational enterprises,November 2013,Domenico Buccella,,,Male,Unknown,Unknown,Male,"Bargaining between multinational enterprises (MNEs) and organized workforce is a subject of key relevance in modern economics. This issue, for instance, takes central position in the European Union (EU) context, where the presence of MNEs characterizes several industries. Furthermore, over 55 % of the added value generated by foreign capital-controlled enterprises comes from other EU Member States enterprises, usually with the affiliates which are active in neighboring countries European Commission (2010). Both the international dimension of MNEs and the EU initiatives such as the institution of the European Works Council (EWC) as well as the practice of opting out from national/sector collective bargaining in favor of company-wide agreements European Foundation (2009) have a substantial impact on labor market outcomes. This framework has caught labor unions’ attention since it could offer the prospect of arranging transnational agreements at company level. Indeed, the figures related to the cross-border company agreements among unions have been increasing. According to the European Trade Union Confederation ETUC (2008), the number of these agreements rose from 92 in 2005 to 147 in 2007, two-thirds of which regarded European MNEs operating within the EU. However, as per the EWC database of the European Trade Union Institute ETUI (2011), these figures relate to a small subset of the MNEs in the EU, approximately one-sixth of those where the EWC is active.  Horn and Wolinsky (1988a) suggest that firms would take strategic advantage of the MNE organizational structure to preclude the formation of encompassing company unions. However, unions may coordinate activities across boundaries to improve their bargaining positions. Under which conditions is coordination advantageous for unions? Why do unions have different attitudes as regards negotiations in MNEs? Could the possibility of unions’ coordination influence the decision about the organization of the wage bargaining in a MNE? This work focuses on these issues. It is crucial to answer the above questions to shed light on recent developments in transnational labor unions’ practices; these developments can lead to the effective coordination of the agendas and outcomes among cross-borders independent negotiations. Workers’ representatives have started exploiting the EWC’s potential to coordinate bargaining across plants and to set the background for negotiations in MNEs (European Commission 2009). In the banking sector, Danish trade unions have the mandate to negotiate on behalf of all employees of the Danske Bank (EIROnline 2009). Furthermore, the European Metalworking’s Federation (EMF), UNI Europa Graphical (UEG), and the European Public Service Union (EPSU), three cross-border industry-level federations, devised a procedure to receive the mandate to represent the workers’ side throughout company-wide transnational agreements. Since its formulation, the EMF has utilized this internal procedure with five MNEs (Areva, Schneider, Daimler-Chrysler, John Deere and ArcelorMittal) (European Foundation 2009; Gennard 2009) while the EPSU has used it with Suez-Lyonnais des Eaux (Papadakis 2010). Finally, transnational campaigns to support wage bargaining, either in targeted industries or MNEs, are additional means labor unions use to move nearer to issues at the core of traditional collective bargaining at international level. The main objective of these unions’ campaigns is to gain better access to (and share of) essential information concerning, for instance, total labor costs and their share in the companies’ total costs as well company profitability, which may be useful in negotiation rounds (Keune and Schmidt 2009). The main results of the paper are as follows. When faced with unions’ coordination costs, the wage bargaining structure in a MNE that produces homogeneous goods is sensitive to the size of these costs and the relative bargaining power of the parties. The coordinated/decentralized choice of the MNE affects the bargaining pattern due to the effects on union rents. Partial coordination and full decentralization of wage negotiations can emerge as equilibrium regimes. Thus, coordination is not always beneficial for the bargaining parties. High transaction costs can more than offset unions’ coordination gains; headquarters agents can make excessive wage offers to the MNE. These effects partly attenuate the conflict of interests among the bargaining parties about the level of centralization the company-wide wage negotiations should be conducted. There is a consistent strand in literature which focuses on various facets of collective bargaining. Authors such as Davidson (1988), Horn and Wolinsky (1988b), Bárcena-Ruiz and Garzón (2002), and Santoni (2009), examine the outcomes of different wage bargaining structures in oligopoly industries.Footnote 1 Davidson (1988) considers a simultaneous bargaining framework in a duopoly with homogeneous final goods. Horn and Wolinsky (1988a, b) extend the analysis to the strategic implications which arise from product differentiation in a model where unions and firms are cast in bilateral monopoly relations. The equilibrium bargaining structures crucially depend on the nature of products. If unions may firstly choose the organization of bargaining, in the presence of complement (substitute) goods, independent (coordinated) negotiations with firms arise as equilibrium. Then, given the unions’ choice, if firms also centralize bargaining, their preference order reverses: unions and firms have a conflict of interest about the negotiation structures. Bárcena-Ruiz and Garzón (2002) likewise develop a model where multiunit firms and unions can choose their wage bargaining structures. If workers organize plant level unions and goods are substitutes (complements), firms centralize (decentralize) negotiations. Conversely, if workers organize a company level union, the firms are indifferent about the bargaining structure. On the other hand, if the firms’ organizational structure is given, workers set up plant (company) unions if goods are substitutes (complements).Footnote 2 Santoni (2009) studies the impact of international market integration on the choice whether to centralize negotiations in the presence of fixed transaction costs for both bargaining parties. If the goods are substitutes (complements), decentralization is a dominant strategy for firms (unions). Hence, either full decentralization or partial centralization arises: the bargaining regimes in equilibrium depend on the characteristics of the product market integration process (one/two-way trade between countries). While these contributions analyze negotiations in national oligopolies, the topics of the collective bargaining structure in MNEs and its transnational dimension are barely explored. Wage formation at company level in the context of an international productive structure is the subject of Borghijs and Du Caju (1999). They analyze unions’ cross-border cooperation vs. plant-specific wage settings within a single MNE producing homogeneous goods in an integrated product market. To coordinate wage bargaining, monopoly unions pay exogenous, symmetric, per member transaction costs. Whether wage coordination is beneficial for unions depends on the size of these costs: up to a threshold value, unions are better off with independent negotiations. Recently, Eckel and Egger (2012) also investigated whether unions can improve their bargaining position in company-level negotiations by cooperating internationally. In a two-country model, a monopolist MNE, producing homogeneous goods, negotiates with local (plant-level) unions over wages and employment (efficient bargaining model). The authors show that cooperation for unions is beneficial if their preferences over wages and employment are similar across countries. Otherwise, the MNE’s threat of relocations makes one union better off and the other worse off; these distributional effects can hamper cooperation. The present work also focuses on bargaining at the company level. The sub-game perfect equilibria bargaining regimes are determined through a sequential game between a MNE and plant-level unions. The paper analyzes a different sequence of moves and number of stages of the game. In the present work, both bargaining parties select their coordination strategy in the first stages. This differs from Horn and Wolinsky (1988b) and Bárcena-Ruiz and Garzón (2002), where each agent chooses the organization of wage negotiations taking as given the bargaining structure of the other agent. Two cases are considered: (1) players decide simultaneously and; (2) the MNE acts as the first mover. Despite different specifications of the first stages, the last two stages of the game are equal. Wage bargaining occurs simultaneously but independently at each production site; then, given the negotiated wages, the MNE allocates the production of goods among its plants. The order of moves matters: the first-mover advantage of the MNE affects the equilibrium bargaining regime. As in Bárcena-Ruiz and Garzón (2002), this paper also finds that, in the presence of homogeneous goods, partial (unions’ or MNE plants’) coordination emerges as the organizational form of the wage bargaining. However, in contrast to those authors, where full coordination arises in equilibrium while full decentralization does not, this paper gets the opposite result due to the presence of the unions’ transaction costs. As in Borghijs and Du Caju (1999) the model considers a single firm with plants in two countries of an integrated product market: this specification reflects in an appropriate way the reality of many well-integrated industries in the EU. However, it extends the model of those authors in several ways. Firstly, by relaxing the monopoly union assumption, this paper provides a general bargaining framework which allows for a wider set of negotiation structures. Then, retaining the idea that coordination is costly for unions, it investigates the bargaining regimes which arise as sub-game perfect equilibria. Unions coordinate bargaining activities by paying some per member fees. Looking at the European context, where (national) unions at plant level delegate European industry-wide unions, such as the EMF and EPSU, to represent workers throughout company-wide negotiations in MNEs, this is the unions’ way to finance coordination activities.Footnote 3 As in Borghijs and Du Caju (1999), this work analyzes the effects of labor market integration (a reduction in coordination costs) on the bargaining regimes arising at company level. Communication technologies have been used in recent years as a tool to structure a permanent information exchange and consultation process. An example is the EMF-EMCEF-ETUF-TCL’s Eucob@n virtual network, which provides a collection of data on collective bargaining for their affiliates. These technologies are likely to decrease unions’ coordination costs. It can be argued that the MNE may also incur transaction costs to coordinate wage negotiations among its subsidiaries. However, throughout the paper, the underlying assumption is that coordination for the MNE is not costly. The reasons are as follows. Firstly, to access, recover, gather, process and to share the relevant information needed during bargaining (e.g., the structure of labor costs) is easier for the management of the MNE than for union delegates. Secondly, the effects of coordination costs on the MNE’s profits may be realistically assumed to be negligible or, at least, to be lower than their effects on the unions’ budgets. The remainder of the article is organized as follows. Section 2 describes the model and derives the sub-game perfect bargaining regime arising in equilibrium, under both simultaneous and sequential games. Finally, Sect. 3 draws some conclusions.",2
4.0,4.0,SERIEs,19 June 2013,https://link.springer.com/article/10.1007/s13209-013-0098-y,An experimental test on dynamic consumption and lump-sum pensions,November 2013,Enrique Fatás,Juan A. Lacomba,Ana I. Moro-Egido,Male,Male,Female,Mix,,
4.0,4.0,SERIEs,02 July 2013,https://link.springer.com/article/10.1007/s13209-013-0099-x,Stockholding in Spain,November 2013,Miguel Ampudia Fraile,,,Male,Unknown,Unknown,Male,"Standard portfolio theory, embodied in the mean-variance expected utility model, predicts that households will always hold part of their portfolio in risky assets, the exact quantity being a function of the equity premium and the volatility of the risky assets’ portfolio (see Markowitz 1952; Tobin 1958). Despite this fact, many empirical studiesFootnote 1 have shown that the majority of the population does not participate in the stock market, resulting in the well documented stockholding puzzle. The goal of this paper is twofold. First, to offer a comprehensive view of stockholding in Spain. Second, to answer the question: why households do not hold stocks? We base our analysis on the mean-variance expected utility model with entry costs. We look at how these entry costs, whether monetary or informational, can explain the lack of participation in the stock market. This study contributes to the existing literature by exploring the stockholding pattern in a country not studied before (Spain), while situating it in an international context and explicitly comparing it with the US, by analysing the effect of entry costs through the use of new proxies, and by testing the extent of these hypotheses by looking at a sample of so-called “sophisticated households”. The decision of whether to participate in the stock market or not has a significant impact on the net worth of a household over the long run. Holding stocks is risky due to price fluctuations and non-guaranteed capital; however, refraining from participation in the stock market entails a risk too. With a 6 % average annual equity premium over the past century (see Kocherlakota 1996), a household investing $100 monthly in the stockmarket for 30 years will retire with $100,452 more in savings than a household who had invested the same amounts in long term risk-free assets (such as government bonds).Footnote 2 While most households are fully aware of the former kind of risk, few of them seem concerned about the latter one, which can be thought of as an opportunity cost. From a macroeconomic point of view, household participation in the stockmarket is also important. A high household participation rate contributes to the liquidity of capital markets, and liquid capital markets allow firms to have a reliable alternate funding channel to traditional banking. This in turn results in faster economic growth. Moreover, by participating in the stockmarket and allocating their funds among industries and sectors, agents are contributing to shape the country’s economic structure. The stockholding puzzle has been widely documented using data from different countries [e.g., Bertaut and Starr-McCluer (2000) analyses the US, Alessie et al. (2000) the Netherlands and Guiso and Jappelli (2000) uses data from Italy] and alternative explanations to it have been proposed such as the combination of high cost of borrowing and uncertain labor earnings (Davis et al. 2006) or the heterogeneity in expectations (Vissing-Jorgensen 2003).Footnote 3 The role of entry costs has been documented for the US by Vissing-Jorgensen (2000) using data from the Panel Study of Income Dynamics. Haliassos and Bertaut (1995) and Bertaut (1998) also identify the role of information costs in deterring stock market participation. Our paper offers a descriptive view of the state of stockownership in Spain and a more formal analysis of the determinants of stockholding. For this we use a Heckman selection model, which allows us to look at both the decision of holding stock and the amount of stock on the portfolios of those who are holding any. We use a number of household characteristics as regressors following Guiso et al. (2003) for comparability reasons, and adding some economically meaningful variables obtained from our data set. We carry out a direct comparison with the US, which can be considered as the reference case in terms of financial markets development. We also explore the stockholding patterns of a subsample of households which are college-educated, work in the financial industry and whose net worth is above the median of the economy. From here on we will refer to these households as “sophisticated households”. Lastly, we use a multivariate probit model to explore how the decision of holding stock is related to the decision of holding other types of assets. The rest of this paper is organized as follows. Section 2 introduces the model of portfolio selection with entry costs. Section 3 explains the contents and methodology of the data (Encuesta Financiera de las Familias). Section 4 presents the results of the descriptive and econometric analysis. Section 5concludes.",6
4.0,4.0,SERIEs,17 July 2013,https://link.springer.com/article/10.1007/s13209-013-0100-8,Reforming the labor market and improving competitiveness: an analysis for Spain using FiMod,November 2013,Tim Schwarzmüller,Nikolai Stähler,,Male,Male,Unknown,Male,"“The global financial crisis triggered an adjustment in the Spanish real estate sector which had serious consequences for the labor market. Since the beginning of the crisis, more than 2 million jobs have been destroyed(...)raising the unemployment rate above 20 %” (see National Reform Programme Spain 2011, p. 15).Footnote 1 Evidently, the current crisis greatly affected the Spanish labor market. But even in “good times” Spain’s unemployment rate was well above the EU average and hardly below around 10 %, which hints at some general structural weaknesses. On the labor market, high employment protection and strong unions, among other things, are said by many to have led to disproportionately increasing wage claims, thereby deteriorating Spain’s competitiveness vis-á-vis the rest of the monetary union member countries (see, for example, IMF 2011). In order now to tackle these problems, the Spanish government has—after consultation with the European Commission and the International Monetary Fund (IMF) as well as remarkable demonstrations by primarily young citizens in basically any major city—chosen job creation and the reformation of the labor market to become a core goal of economic policy. In this paper, we analyze the short and long-run impact of making the Spanish labor market “more flexible” on output, unemployment, international competitiveness and fiscal balances using an extended version of “FiMod—A DSGEModel forFiscal Policy Simulations” developed by Banco de España and Deutsche Bundesbank staff for policy simulations.Footnote 2 The present paper has two objectives. First, we evaluate proposed measures to reform the Spanish labor market—more precisely, a permanent cut in employment protection, constantly weakening trade unions as well as a permanent cut in unemployment benefits, public wages and public-sector employment—in a medium-scale dynamic, stochastic, general equilibrium (DSGE) model. Second, more on the technical side, we propose a way how to simultaneously introduce endogenous dismissal decisions by firms and liquidity-constrained consumers in a medium-scale DSGE framework. DSGE models have recently been more widely used for such analyses as they allow to present arguments in a rather structural way and give some numerical assessment, too. A non-exhaustive overview of papers related to ours are, among others, Zanetti (2011), who finds in a related DSGE model that labor market institutions significantly affect the volatility of output, employment and job flows (negatively for employment protection, positively for unemployment benefits). Thomas and Zanetti (2009) find in a DSGE model with large firms that the effects of such labor market institutions on inflation volatility are rather small. Similarly, Merkl and Schmitz (2011) find that labor market institutions affect inflation volatility to a rather small extent, but they identify significant effects on output volatility. By contrast, Campolmi and Faia (2011) find unemployment benefits to significantly decrease inflation volatility. They, hence, explain inflation differentials in the euro area by differences in the generosity of the unemployment insurance system. Almeida et al. (2010) address the effects of labor and product market reforms on international competitiveness for Portugal in PESSOA, the DSGE model used by the Portuguese National Bank. A similar analysis can be found in Kilponen and Ripatti (2006) using the Finnish model, in Deák et al. (2011) using the LSM (the Luxembourg Structural Model) and in Krause and Uhlig (2012) analyzing Germany’s so-called Hartz IV reforms. These analyses find that, in general, labor market reforms improve competitiveness, foster domestic output and play a part in lowering unemployment. They address labor market reforms only as a cut in the “wage markup” (Almeida et al. 2010; Kilponen and Ripatti 2006) or as a decrease in unemployment benefits (Deák et al. 2011; Krause and Uhlig 2012), however, while we can be somewhat more specific on various measures to be analyzed. Related to this literature, the contribution of the paper at hand is its focus on the effects of specific structural labor market reforms on international competitiveness and fiscal balances. FiMod, the model we use to analyze these questions, is a two-country monetary union DSGE model with a comprehensive fiscal block that includes a wide range of taxes and quite some disaggregation in government spending. Furthermore, it includes the modern theory of unemployment, extending the model of Stähler and Thomas (2012) by endogenizing job destruction along the lines of Mortensen and Pissarides (1994, 1999, 2003) and Zanetti (2011). Our findings from the model simulations can be summarized as follows. A decrease in workers’ outside option through a decrease in unemployment benefits or public-sector wages and a decrease in union’s bargaining power unambiguously decreases private-sector wage claims and makes it more attractive for firms to create jobs. Because of lower labor costs, firms decide to dismiss fewer people, which decreases unemployment. Furthermore, they lower prices. The latter makes Spanish goods cheaper, which fosters exports and improves the terms of trade. Higher production and less unemployment improve fiscal balances. Additionally, they are directly affected by the fact that a cut in the policy-induced expenditure item (unemployment benefits and the public sector wage bill) immediately decreases expenditures. According to our model simulations, these measures have the highest impact on output, employment, international competitiveness and fiscal balances compared to the other measures. In principle, the argumentation also holds for a cut in public employment. However, given that higher private labor demand cannot compensate for the decrease in public employment, unemployment will increase. This, first, diminishes the magnitude of the positive effects resulting from the other two measures just described and, second, may induce firms to dismiss relatively unproductive workers more frequently and to search for more productive ones in the pool of unemployed workers, even though this is costly. As unemployment has increased, search costs may fall to a sufficient extent for such behavior to pay off from the firms’ perspective. Our model simulation suggests that a cut in employment protection is not an adequate measure to tackle problems related to international competitiveness. Indeed, job creation increases as the expected cost of getting rid of a worker falls. However, dismissal probability also increases. On average, workers demand higher wages, partly to compensate for the additional dismissal risk, partly because average productivity of employed workers rises due to a rise in the dismissal productivity threshold. Hence, labor costs rise. In order to tackle this, firms increases prices, which deteriorates the terms of trade and causes exports to fall. In our model simulation, unemployment increases as the dismissal effect dominates the job creation effect. The drop in internal and external demand additionally decreases output, contributing to lower labor demand. To put these findings into perspective, some remarks seem in order. First, while reducing average dismissal costs in Spain may—according to our analysis—not contribute to regaining international competitiveness, a reform may still be in order. Spain is characterized by a dual labor market where some benefit greatly from employment protection while others do not have any. Costain et al. (2010) address this issue in a more adequate and very sophisticated model. Also, the comparatively large informal sector may be an issue here. Second, the bargaining game between the union and the firm may have quite an impact on the results. In our model, we follow the standard approach in the matching literature. However, Stähler (2008) has shown that the bargaining game matters. And last but not least, modelling employment protection itself is quite a complicated issue and it probably deserves a more sophisticated modelling than simply implementing firing costs, the approach we followed in the model at hand. For an overview of the different aspects related to employment protection and a discussion, see, among others, Stähler (2007). The rest of this paper is organized as follows. The model is presented in Sect. 2. Section 3 evaluates the labor market reforms already discussed. We differentiate between short and long-run effects. Section 4 concludes.",2
5.0,1.0,SERIEs,17 December 2013,https://link.springer.com/article/10.1007/s13209-013-0103-5,"Occupational choice, number of entrepreneurs and output: theory and empirical evidence with Spanish data",March 2014,Vicente Salas-Fumás,J. Javier Sanchez-Asin,David Storey,Male,Unknown,Male,Male,"This paper considers the determinants of occupation structure in economies where individuals who differ in skills make occupational choices in response to monetary incentives. We model an economy where heterogeneous individuals choose either to work as salaried employees, to be self-employed and not hire employees (SEWNEs), or to be self-employed and hire employees (SEWEs). The market equilibrium from individual occupational choices determines the relative size of each occupation group, as well as the average productivity of the economy (our indicator of welfare). We examine the sensitivity of the occupational structure and average productivity to changes in certain exogenous parameters of the model and relate the theoretical predictions to empirical regularities observed in the relationship between self-employment rates and economic development, in general, and particularly for the Spanish economy. Our paper is motivated by the observation that existing models of occupational choice ignore the decision of the self-employed to hire employees, or not, and limit occupational choice to employers and employees. The theoretical distinction between SEWEs and SEWNEs is empirically relevant first, because the number of self-employed working on their own is larger than the number of employers, even in a developed country such as Spain; ignoring the SEWNEs, the model is clearly incomplete. Second, because the sign of the association between self-employment rates and productivity in the pool of Spanish cross-region and time-variant data is positive for SEWEs and negative for SEWNEs. Since self-employment rates are a common measure of entrepreneurial activity, this paper presents the different conclusions that can be drawn on the association between entrepreneurship and economic development, depending on the type of self-employment considered and contributes to the literature on entrepreneurial heterogeneity and its implications for economic development. Early models of occupational choice (Lucas 1978; Rosen 1982; Jovanovic 1994) ignore the realistic alternative of working alone as own-account self-employed. Existing models also differ in assumptions about individual skills and production and organization technologies. In certain papers, such as Lucas (1978), and others that followed, individuals differ in entrepreneurial skills and production takes place with labour and capital inputs in organizationally irrelevant firms. Rosen (1982) extended the Lucas model to an economy where individuals differ in general skills that are converted into either entrepreneurial or operational skills. In Rosen model production involves only labour inputs from entrepreneurs and employees (no capital input) and the organization of production within firms is the relevant economic variable that explains the observed increasing and convex relationship between the size of firms and the managers’ compensations. To the best of our knowledge, our paper is the first that, in addition to considering three occupational choices, models and solves market equilibrium from occupational choices in an economy where individuals differ in general skills, the production inputs involve direct labour and capital services, and the organizational structure matters for overall production efficiency. The empirical relevance of the model presented in this paper explains the heterogeneity observed in the population of entrepreneurs and provides theoretical support to the evidence that not all entrepreneur types exhibit the same association with indicators of economic development, such as per capita income. The interest in heterogeneity among entrepreneurs stems from the work of Baumol (1990), who classified entrepreneurship into productive, unproductive, and destructive, with empirical support for the distinction being subsequently provided by van Stel and Storey (2004) and Sobel (2008). The GEM (2006) study also made the distinction between necessity and opportunity entrepreneurs. Our paper separates entrepreneurs into those who work on their own (SEWNEs) and those who hire employees (SEWEs), and provides empirical evidence from Spain (and other countries) of the empirical relevance of this distinction. Over the years, there has arisen a body of literature that seeks to link general indicators of entrepreneurship with economic development (Wennekers et al. 2005; Audretsch et al. 2006, but there is also considerable theoretical and empirical work emphasising that the strength of this link depends on the “type” of entrepreneurship. This has led to the conclusion that not all types of entrepreneurs contribute equally to economic output (Lerner and Schoar 2010). In fact, the evidence leads to contradictory conclusions. On the one hand, there is a strong negative association between self-employment rates and per capita income found in cross-country studies (Kuznets 1971; Iyigun and Owen 1999; Gollin 2008) that raises doubts about the presumed positive link between entrepreneurship and economic development. On the other, there is empirical evidence that productivity growth is greater in economies with a larger share of self-employed (Salas-Fumás and Sanchez-Asin 2013). Our paper aims to provide new insights into the potential causal effects of entrepreneurship on economic development. In this respect, the paper is similar to Gollin (2008) but with certain important differences. First, Gollin assumes, as does Lucas (1978), that individuals in the economy differ in entrepreneurial skills and that the internal organization of firms is irrelevant. Furthermore, Gollin does not solve for equilibrium with three occupational choices, even though the own-account self-employed are included in the theoretical model. In this paper, individuals differ in general skill, the internal organization of firms affects the average productivity of the economy, and the relative size of each occupational group is explicitly calculated. Comparative static analysis clarifies the relationship between entrepreneurship and economic development, showing that there are cases where each type of entrepreneur, SEWEs and SEWNEs, responds differently to changes in the exogenous parameters. The paper is also related to recent empirical work that calibrates the parameters of production technologies and distribution of skills in economies where individuals make occupational choices: Gollin (2008) for Japan; Poschke (2011) for the USA; Garicano et al. (2013) for France, and Braguinsky et al. (2011) for Portugal.Footnote 1 We add Spain to the list of countries, but our interest is in explaining occupational structure and its evolution over time. We observe that the rate of SEWEs increases over time, while the rate of SEWNEs decreases.Footnote 2 Since the average productivity of the Spanish economy also increases during the same time period, Spain is an example of the theoretical result, where the rate of one type of entrepreneur is positively correlated over time with average productivity, the SEWEs, and the other type, the SEWNEs, is negatively correlated. These results continue to hold when we combine time and cross-section data from the Spanish regions. We also find that the time-increasing lower bound in the distribution of skills (due to, for example, improvements in education levels of the population) is an important driver of the cross-regional and over-time evolution of occupational rates and productivity for the Spanish economy in the period (1980–2006).Footnote 3 This result is connected to other recent evidence on the human capital of entrepreneurs as a driver of economic growth (Gennaioli et al. 2013). Another related literature includes work that explains the actual occupational choice using individual level data (Evans and Jovanovic 1989; Evans and Leighton 1989; Carrasco 1999; Dunn and Holtz-Eakin 2000; Dawson et al. 2009). These authors model individual occupation decisions and changes in occupation over time, taking account of individual characteristics such as wealth and wealth constraints, education levels, family background, prior work experience, preferences for independent work, and so on. In this paper, a single attribute, the level of general skill, summarizes all individual characteristics. Our objective is to explain the market equilibrium solutions that result from individual decisions, not the prediction of the occupational choice of a particular individual. The rest of the paper is organised as follows. In Sect. 2, we model the general results on the equilibrium share of entrepreneurs and output when individuals differ in general skills. Section 3 compares the determinants of equilibrium self-employment rates and productivity with both two and three occupational choice models. In Sect. 4, we use the calibrated model to explain the observed self-employment rates and productivity for the Spanish economy over time, and for 18 Spanish Autonomous Communities (AACC). The conclusions summarise our main results and point to areas for further study.",8
5.0,1.0,SERIEs,07 August 2013,https://link.springer.com/article/10.1007/s13209-013-0101-7,Immigration and student achievement in Spain: evidence from PISA,March 2014,Natalia Zinovyeva,Florentino Felgueroso,Pablo Vazquez,Female,Male,Male,Mix,,
5.0,1.0,SERIEs,14 March 2014,https://link.springer.com/article/10.1007/s13209-014-0104-z,Gender gaps in Spain: policies and outcomes over the last three decades,March 2014,Nezih Guner,Ezgi Kaya,Virginia Sánchez-Marcos,Male,Female,Female,Mix,,
5.0,1.0,SERIEs,01 September 2013,https://link.springer.com/article/10.1007/s13209-013-0102-6,The role of platform quality and publicly owned platforms in the free-to-air broadcasting industry,March 2014,Miguel González-Maestre,Francisco Martínez-Sánchez,,Male,Male,Unknown,Male,"This study addresses the role of public intervention in the free-to-air broadcasting industry. In fact, public intervention in broadcasting markets has been particularly justified when advertising is the only method of commercial provision. As suggested by Coase (1966), in the absence of subscription television, public policy can increase social welfare by improving the quality and diversity of the available programming. A basic ingredient of the justification of this public regulation is usually associated with the need of diminishing the nuisance of excessive advertising. The rapid technological advances in the broadcasting and communication industries have enhanced the debate on the role of public intervention in broadcasting industries (see, among others, Armstrong 2005). Moreover, this debate has become particularly relevant as a result of recent controversial policy decisions within the EU. Particularly remarkable is the decision by the public TV platform in France (followed more recently by its counterpart in Spain) to eliminate advertising as a method of financing.Footnote 1 In this study, we consider the role of programme quality and a publicly owned platform in the context of the free-to-air broadcasting industry. Publicly owned platforms are prominent in the broadcasting media industry in many Western countries. The empirical relevance of this presence is observed in Bel and Domènech (2009) (table 1 p. 167), and more recently, in Sanz (2012). In particular, we note the existence of one or more national publicly owned TV platforms in Italy, Germany, France, UK, Switzerland and Spain. Despite the above-mentioned evidence, there is a surprising lack of research into the role of publicly owned platforms in the media industry and its connection to the use of advertising. A remarkable exception is the work by Kind et al. (2007). As noted by those authors, consideration of the role of publicly owned TV as a method of broadcasting market regulation is natural, in view of the problems associated with alternative regulation policies. In particular, lobbying pressures, technological progress and increased globalisation appear to have increased the difficulty of the optimal implementation of the direct regulation of advertising time in private platforms. In a model including horizontal product differentiation, these authors show that a welfare-maximising publicly owned TV channel generates less advertising than private ones if and only if the degree of the TV platforms’ horizontal differentiation is sufficiently large. The intuition of their results is that if the broadcasting products are very close substitutes, then audiences will be very sensitive to advertising, and as a consequence, private advertising incentives will be too small from the social welfare perspective. A mixed duopoly incorporating advertising regulation in the broadcasting industry is considered by Stühmeier and Wenzel (2012) in the context of a Hotelling model including horizontal differentiation. They evaluate the effects of a binding advertising cap on competition for viewers and advertisers in a duopoly framework. They find that the regulation of advertising can increase profits. This result is explained by competing channels’ failure to completely account for the effect that their choice has on rivals’ advertising prices, which is resolved through exogenous regulation. With their empirical approach, Alcock and Docwra (2005) develop a stochastic oligopoly model calibrated for the Australian broadcast TV market. They find that the presence of a public platform can generate positive outcomes for viewers and other market suppliers simultaneously because it increases viewers’ choice and total market size. More recently, Bel and Domènech (2009) have undertaken an empirical analysis in the Spanish broadcasting industry and found that advertisers create a negative externality to viewers that tends to be mitigated by the presence of publicly owned platforms. The analysis of advertising in broadcasting media industries with private platforms has been considered extensively in the recent literature.Footnote 2 In particular, Gabszewicz et al. (2004) consider two private TV-platforms that derive their profits from advertising and show that the platforms’ profiles become closer as advertising aversion becomes stronger. Anderson and Coate (2005) show that advertising levels may be too low or too high with respect to the socially optimal level, depending on the nuisance cost to viewers, the substitutability of programmes and the expected benefits enjoyed by advertisers as a result of contacting viewers. The role of advertising quality is considered by Gantman and Shy (2007). Those authors assume two types of viewers: those whose utility increases in the number of broadcasted advertisements and, those whose utility is unaffected by the number of advertisements they are exposed to. They show that if the improvement of advertising quality is profitable for the advertising firms, it is unprofitable for TV platforms. Peitz and Valletti (2008) analyse the comparisons between the pay-TV setting, in which platforms obtain revenue from advertising and from viewers, and the free-to-air setting, in which platforms obtain all revenues from advertising. They show that if viewers strongly dislike advertising, advertising intensity is greater in a free-to-air setting and that free-to-air platforms tend to provide less differentiated content. In contrast, pay-TV platforms always maximally differentiate their content.  Crampes et al. (2009) consider the effects of advertising on entry into the media industry. They show that under constant or increasing returns to scale in the audience, the entry level is excessive and the advertising level is insufficient. Most of these previous contributions focus on the combination of advertising and horizontal product differentiation among private platforms in two-sided markets. In contrast to these previous contributions, our model considers two relevant aspects of the broadcasting industry simultaneously: First, in addition to horizontal differentiation, we also assume the presence of differences in programme quality, measured in terms of viewers’ utility. In the previous literature, only Armstrong (2005), Crampes et al. (2009), and Lin (2011) analyse the role of program quality in the broadcasting industry. In particular, Armstrong (2005) and Lin (2011) compare the equilibrium quality levels associated with the free-to-air duopoly regime and subscription, whereas Crampes et al. (2009) analyse the effects of endogenous quality improvements on entry. However, these previous contributions assume the existence of competition among symmetric private platforms, whereas we consider the role of a publicly owned platform in the presence of asymmetric quality levels.Footnote 3 Second, our model analyses the combined role of publicly owned television and advertising in the broadcasting markets. As explained above, only Kind et al. (2007) have analysed this issue from a theoretical perspective. However, in contrast to this previous contribution, which is focused on a model of a representative consumer and horizontal differentiation, our approach combines both horizontal and vertical differentiation among platforms in the context of a linear Hotelling model. Specifically, the aim of our paper is twofold: 1) We first analyse the optimal advertising decision of the public platform, considering two welfare effects: (i) a direct effect, measured in terms of advertising revenues and nuisance costs, and (ii) an indirect effect, which works through the distribution of the audience among the broadcasting platforms. As we will show, this indirect effect depends on both the degree of product differentiation and the quality differential between platforms. Interestingly, as in the model created by Kind et al. (2007), in our model, the equilibrium level of advertising by the publicly owned platform can be positive even if advertising has a direct socially harmful effect. However, whereas these authors focus on the effects exerted by the degree of horizontal product differentiation on the advertising comparisons between the private platform and publicly owned platform, our model considers the role of both horizontal and vertical (quality) differentiation explicitly. We show that the quality differential between the platforms is crucial in optimal advertising choice on the part of the public platform. In particular, we find that the greater the public platforms’ quality differential and the higher the relative harmful effect of advertising, the more likely it is that an “advertising-free” public platform is optimal. Intuitively, by decreasing its advertising, the publicly owned platform increases its audience, which is socially beneficial if its quality differential is large. These results show that the explicit consideration of the quality differential in our model provides new insights with respect to the previous models, such as Kind et al. (2007), that have focused on only the role of horizontal differentiation.Footnote 4 2) Second, we compare the equilibrium levels of advertising in two settings: a private duopoly, which includes two private profit-maximising platforms, and a mixed duopoly, in which a welfare-maximising publicly owned platform competes with a private platform.Footnote 5 We identify the conditions under which privatisation is socially desirable and show that the connection between programme quality and advertising incentives differ drastically between the two scenarios. In particular, we show that the presence of a public platform is socially desirable if its quality differential is positive and advertising is harmful or if its quality differential is negative and advertising is beneficial. However, it is also shown that there are conditions under which privatisation is socially better than a mixed duopoly. Intuitively, privatisation might act as a commitment device that allows the government to improve the welfare level through its effect on the response of the rival platform. The main insight provided by our analysis is that the interplay between the social cost of advertising and the quality differential between platforms is crucial in the assessment of both the equilibrium level of advertising and the social desirability of a publicly owned platform. The remainder of the paper is organised as follows: Sect. 2 presents a spatial duopoly market with private platforms, Sect. 3 analyses the model with a mixed duopoly in which one of the competitors is a publicly owned firm that maximises welfare, Sect. 4 considers advertising and welfare comparisons of the two models, and Sect. 5 concludes.",6
5.0,2.0,SERIEs,02 August 2014,https://link.springer.com/article/10.1007/s13209-014-0116-8,The crisis of the Spanish economy,August 2014,Juan F. Jimeno,Tano Santos,,Male,Male,Unknown,Male,"The story of the recent economic crisis in Spain has been told many times and in many different versions. Yet, as time goes by, economic research is showing new elements of its nature and its protractedness. Thus, the editors of SERIEs decided to devote a monographic issue to the analysis of different aspects of the impact of the great recession on the Spanish economy. As guest editors, we had the opportunity to look closely at a new vintage of papers on the performance of the Spanish economy during the last years. In this introductory article we offer our narrative of the crisis, which is, in part, based upon our readings of these papers and of the relevant previous literature, and, in part, on informed conjectures about the main cyclical factors and long-run trends that conditioned such a performance. The Spanish variety of the so-called great recession is indeed a multi-faceted crisis, with the financial system, the public sector, households, and the corporate sector all together at the forefront of its origin and involved in its evolving process. Some of the main developments that eventually led to the crisis are well-known and have been analyzed elsewhere [see, for instance, Estrada et al. (2009), the set of papers collected in FEDEA (2010), Suárez (2010) and Ortega and Peñalosa (2012)]. However, there are still two essential elements that, in our view, are less well understood. The first one has to do with the factors that made the Spanish economy especially prone to embark in the expansionary/speculative path resulting in an over-accumulation of debt and severe macroeconomic imbalances that led eventually to a crisis. The second is about the factors that explain why the crisis materialized with intensity and some characteristics different to the consequences of the great recession in other countries, including the role that economic policies may have played at generating these peculiarities. To answer these questions, it is important to understand three main characteristics of the Spanish economy during the pre-crisis period: A composition of economic activity increasingly biased towards construction, real state, and other non-tradeable sectors. To a large extent, this bias was the result of (1) a labor market regulation that strongly favors the creation of temporary and seasonal jobs, (2) a housing market that, after its liberalization in the early 2000s, provided wide scope for demand and supply to expand significantly, and (3) product and labor markets that, being poorly regulated, do not promote the creation and growth of new firms in the tradeable sector. All these together eventually conformed a “dual economy”, with some large firms highly productive and competitive in international markets, and a large number of small and medium sized firms with low productivity and low growth potential. A banking system capable of satisfying the huge increase in credit demand from households and firms, in a context of very low real interest rates, excessive optimism about growth, and lax facilities for using real assets as loan collateral, thus creating the landscape for financial instability when these real assets lost value. The recourse to external funding to sustain the rise of credit that led to an unprecedented increase in liabilities with respect to the rest of the world, and the lack of anticipation by policy-makers, both with national and international responsibilities, of the consequences of excessive accumulation of external debt. Accordingly, we organize the paper in three main sections, one per each of the factors to be stressed: the weakness of the growth model underlying the performance of the Spanish economy in the pre-crisis period, the consequences of the banking intermediation in a speculative bubble, and the implications of the external disequilibrium. We conclude with some remarks about what has been achieved so far in the correction of these phenomena and what remains to be done.",29
5.0,2.0,SERIEs,25 March 2014,https://link.springer.com/article/10.1007/s13209-014-0105-y,The impact of the great recession on employment polarization in Spain,August 2014,Brindusa Anghel,Sara De la Rica,Aitor Lacuesta,Unknown,Female,Male,Mix,,
5.0,2.0,SERIEs,08 July 2014,https://link.springer.com/article/10.1007/s13209-014-0111-0,Wage cyclicality: Evidence from Spain using social security data,August 2014,Jorge De la Roca,,,Male,Unknown,Unknown,Male,"Recent evidence on real wage cyclicality using worker-level data shows wages are much more cyclical than previously thought. Following the lead of Bils (1985), many studies have found that since the early 1970s individual wages respond to changes in the unemployment rate [see Pissarides (2009) for a summary of the evidence]. However, most of this evidence is available for countries with flexible labour markets, mainly the United States. Even more, in contrast to many countries in Continental Europe, the European countries for which estimations are available (the United Kingdom, Germany, Portugal and Italy) do not exhibit a large incidence of wage indexation policies or centralized collective agreements. Spain is a suitable scenario to evaluate real wage cyclicality within a much rigid labour market. The Spanish system of collective bargaining is based on two principles that deter firms from adjusting wages along the business cycle. The first principle automatically extends any collective agreement beyond the scope of a firm to all workers in the same sector and province, even if they had not participated in the bargaining process. The second principle secures the validity of collective agreements after their expiration. Likewise, a large share of agreements (more than 60 %) include indexation clauses which trigger high inertia in firms’ wage-setting decisions. Lastly, duality in the labour market insulates workers under permanent contracts (around 67–70 % of the workforce with high levels of employment protection) from business cycle fluctuations. I find weak procyclicality of real wages in Spain over the period 1988–2011. My baseline estimate is a 0.4 % increase in wages in response to a 1 % decline in the (lagged) unemployment rate. This estimate is the lowest among available estimates, which usually vary between 1.3 and 1.5 % increase in wages in the United States—again for a 1 % drop in the unemployment rate—and an even larger increase in wages between 2.0 and 2.2 % in European countries. When I use total salaries instead of base salaries for a restricted period in which the former are available, I still obtain a low level of procyclicality at 0.6 %. Thus, as expected, I find real wage cyclicality is lower in a country with institutions that hinder firms to respond to business cycle fluctuations. In this line, this finding indicates that for some European countries with high-wage indexation and employment protection policies (e.g., Belgium, Austria, France and Scandinavian countries), wage cyclicality is presumably much lower than the available European estimates. To obtain these cyclicality estimates I use a rich social security data set—Muestra Continua de Vidas Laborales (mcvl). This is an administrative data set that tracks career histories for a 4 % sample of individuals who in a calendar year have any relationship with social security. For each individual, all employment and most unemployment spells are available at the daily level since 1981 or entry in social security, whichever is more recent. Thus, I can construct a monthly panel recording labour market status, some individual traits, job characteristics and wages. This unique data set has strong advantages relative to other data sets that have been used to estimate wage cyclicality. By exploiting the high frequency in the data I can identify most labour market transitions, specially those that are of particular interest in the wage cyclicality literature (e.g. estimating cyclicality levels for job movers, for workers who start jobs from periods of unemployment or inactivity, or for workers who remain within an employer–employee match). Such transitions are not available in surveys with high attrition or are difficult to detect in administrative data sets with longer periodicity. Moreover, I can estimate the cyclicality of the net present values of wages in new matches over their job duration, which constitutes a key piece of information for the Mortensen-Pissarides search and matching model. One drawback in the data set is the intermediate level of censoring in wages. I propose a simple approach to simulate wages using information on individual and job characteristics, uncensored wage observations and wage persistence estimated by exploiting the longitudinal dimension in mcvl. This is by itself one empirical contribution of the study. In the line of Haider and Solon (2006), I assume uncensored wages for a worker follow a multivariate log-normal distribution. I estimate using Tobit regressions the mean and variance of wages in each period. To approximate wage correlation coefficients between any two periods I develop an indirect inference approach. Lastly, I simulate wages only for censored observations and evaluate the fit of the simulation by comparing these simulated wages to total salaries available from income tax return data for a restricted period. Overall, the fit of the simulation is quite satisfactory. This study contributes to the wage cyclicality literature in several aspects. First, as already mentioned, unlike wage cyclicality estimates for countries with flexible labour markets, this study provides one estimate for a rigid labour market scenario. Second, the study shows how wage cyclicality responds in a setting with high duality in employment protection. I find cyclicality for workers under temporary contracts is twice as large as for workers under permanent contracts. Thus, temporary workers carry most of the burden of wage adjustments along the cycle, while permanent workers are much less affected. Third, I present evidence of wage cyclicality decreasing consistently with the level of job tenure. I find cyclicality is much higher for newly-hired workers (those who start jobs from periods of unemployment or inactivity) than for job stayers with high levels of tenure. The availability of employer identifiers allows to calculate tenure levels with high precision and to estimate cyclicality within an employer–employee match as in Devereux (2001). The estimated difference in wage cyclicality between newly-hired workers and job stayers is relevant for the empirical validity of the Mortensen-Pissarides search and matching model (Mortensen and Pissarides 1994; Pissarides 2000). The model has been challenged on its ability to match the observed cyclicality on vacancies and unemployment. Some studies have suggested wage rigidity as a potential solution to this so called unemployment-volatility puzzle (Hall 2005; Shimer 2005). In this model, the cyclicality of the net present value of wages in new matches is a key statistic to determine job creation (Pissarides 2009). I estimate such cyclicality for the net present value of wages in new matches and obtain a similar estimate for wages of newly-hired workers. This result, the first using actual data on monthly wages and job durations, is encouraging since the net present values of wages is rarely observed and impossible to calculate in most data sets. Overall, this finding does not give support to wage rigidity as a solution to the unemployment-volatility puzzle. The rest of the paper is structured as follows. Section 2 reviews some institutional aspects of the Spanish labour market. Section 3 describes the data and the approach developed to simulate wages for censored observations. Section 4 explains the estimation methodology. Section 5 presents the results. Finally, Sect. 6 concludes.",16
5.0,2.0,SERIEs,21 March 2014,https://link.springer.com/article/10.1007/s13209-014-0106-x,Labour market dynamics in Spanish regions: evaluating asymmetries in troublesome times,August 2014,Hector Sala,Pedro Trivín,,Male,Male,Unknown,Male,"The Great Recession has severely hit Spain in many dimensions. No more than other economies in most of them (economic (de)growth, sovereign debt crisis, banking system collapse), but disproportionately hard on unemployment. After more than a decade trending downwards and converging to the European average, the rate of unemployment reached 8.0 % in 2007—falling from a peak of 24.5 % in 1994 and values above 20 % still in 1996. In 2012, however, after five years of steep rise, the historical maximum was surpassed reaching a massive 26.0 % (Fig. 1). Quarterly unemployment rate in Spain. 1996–2012. Source Spanish Labour Force Survey (EPA) The intense progress first, and deterioration afterwards, of the Spanish labour market goes in parallel with an extreme degree of regional persistence in labour outcomes. This is illustrated in Fig. 2. Figure 2a shows two groups of regions. The first one with employment growth rates around \(-\)2.5 % in 2008–2012 (with the Balearic Islands and Madrid close to \(-\)2.0 %), and a second one between \(-\)3.3 and \(-\)4.1 %.Footnote 1 The difference between the two groups points to the existence of less responsive regions in the North and North-West of Spain (Galicia, Asturias, Castile and Leon, Basque Country, Navarre, Aragon), and more volatile ones in the South and East part of Spain. Madrid (also the Balearic Islands and to some extent Cantabria) would be a salient exception with top employment performance simultaneously in good and bad times. Figure 2b, in contrast, gives a much homogeneous picture in terms of unemployment rates, with a regression slope of 0.83 and a \(R^{2}\) of 0.43. When combined, the information supplied by Fig. 2a, b discloses two main stylised facts: (i) changes in employment provide just a partial explanation of the evolution of unemployment, and (ii) there is a great persistence in regional unemployment over the years. Labour market performance of Spanish regions. 1996–2012. a Employment growth, b unemployment rates. Source Spanish Labour Force Survey (EPA). AND Andalusia, ARA Aragon, AST Asturias, BAL Balearic Islands, CAN Canary Islands, CNT Cantabria, CLE Castile and Leon, CMA Castile-La Mancha, CAT Catalonia, VAL Valencian Community, EXT Extremadura, GAL Galicia, MAD Community of Madrid, MUR Region of Murcia, NAV Navarre, PVA Basque Country, LRJ La Rioja These facts and the regional specificity of the Spanish labour market may be studied from a variety of perspectives, taking into account, along the lines of Marston (1985), that changes in regional (un)employment may be the outcome of both national and regional driving forces. Elhorst (2003) distinguishes four types of approaches including single-equation models, implicit models (where he places the Blanchard and Katz model), accounting identity models, and simultaneous-equation models dealing with interactions. The strength of the implicit models are their solid theoretical basis, while simultaneous equation models should be chosen from an empirical viewpoint (Elhorst 2003, p. 741). Multi-equation structural models have been used in Bande and Karanassou (2009, 2013a, b) to assess to what extent the evolution of differences in Spanish regional unemployment can be attributed to disparities in the respective regional equilibrium unemployment rates or to the evolution of other key variables such as, for example, capital accumulation.Footnote 2 Our aim, however, is to analyse the regional labour market from a regional specific point of view. It would be too demanding, in our context, to conduct a detailed analysis using their Chain Reaction Theory methodology. The reason is that we consider small sample periods of study, as deserved by the unprecedented specificities of the recent economic developments, at the same time that we need information highly disaggregated by regions. On one side, this causes severe restrictions in terms of degrees of freedom. On the other side, it constrains the analysis to a relatively small number of variables quarterly available for all Spanish regions, and with up-to-date coverage.Footnote 3 We are interested in answering questions related to the most recent evolution of the Spanish labour market. What has happened regarding the specific regional responses to labour market shocks? Have they changed relative to previous responses, studied for the period up to the mid 1990s? Are these responses similar in good and bad times? What role do prices play? The framework of analysis developed in Blanchard and Katz (1992) allows us to provide answers to these questions. It yields the possibility of evaluating the impact of employment shocks through the responses they cause, not only in terms of the unemployment rate, but also through changes in participation rates and regional mobility.Footnote 4 Such analysis will enhance our understanding, from a regional perspective, of the labour market adjustment mechanisms in the different scenarios studied. The model of Blanchard and Katz has been used to investigate the dynamics of regional labour markets in the US (Blanchard and Katz 1992), Europe (Decressin and Fatás 1995), Sweden (Fredriksson 1999), the Netherlands (Broersma and van Dijk 2002), Finland (Mäki-Arvelaa 2003) and, more recently, for the German East-West disparities (Alecke et al. 2010). It has also been used to analyse the Spanish labour market by uncovering its regional persistence in 1976–1994 (Jimeno and Bentolila 1998), and to provide specific analyses on the Southern regions (Murillo et al. 2006) and by level of education (Mauro and Spilimbergo 1999). Notwithstanding its wide use, it is important to discuss two of its prominent features since it is a model that relies upon (1) the assumption of regional mobility of workers and firms; and (2) the measurement of regional variables as deviations from the national average, which implies that shocks are regionally idiosyncratic. Regarding the first feature, it is important to show that Blanchard and Katz’s model can be safely applied to study the behavior of the Spanish regions in a context of relative low interregional mobility (see Figure 1 in Bover and Arellano (2002)), and relative large international flows, regionally heterogeneous, since the end of the 1990s. In particular, it is important to ensure that our analysis of migration responses to specific shocks are not mixed with demographic changes also affecting the patterns of regional population. Figure 3 plots the regional relative evolution of interregional and international migration in Spanish regions since 1998.Footnote 5 Relative regional migration in Spanish regions (%). The sample period is 1998–2012 according to available data. a Relative interregional migration (%), b relative international migration (%). Note the complete name of each region is provided in the note below Fig. 2. Source National Statistics Institute (Variaciones Residenciales Interiores y Exteriores) Figure 3a shows the average relative internal residential migration rate for each region computed as the ratio of each region’s residential migration (across Spanish Autonomous Communities) over total interregional migration. Note that, although there is in general low mobility, regions with a larger relative interregional migration in expansion are also the ones with larger mobility in recession. Figure 3b shows the relative international migration rate computed as the total flows of international migration flowing into and out of each Spanish region over the total flows of international migration in Spain. Note that it is not the relative net inflows what are computed, but the addition of inflows and outflows of international migrants in each region over total international migration flows in Spain. In this way, the computed ratios on relative interregional and international migration are directly comparable. Once again, it can be observed that each region’s ratio has remained stable across both periods of analysis. This analysis, in relative terms, is of course compatible with fluctuations in the absolute values characterized by large net inflows of international migrants during the boom years, especially in the regions with the largest employment growth rates, and the subsequent brake in these flows during the crisis. It is important to note that these ratios have remained roughly constant between 1998–2007 and 2008–2012 which correspond, broadly, to the two periods of analysis in this work. As there are no important changes in the ratio for each region from one period to another, the regional migration response (both interregional and international) that we estimate and compare across periods is not subject to biases stemming from variations in the regional migration behaviour.Footnote 6 Regarding the fact that we are only evaluating region-specific shocks, we acknowledge that nation-wide shocks may also be relevant, as argued by Bande and Karanassou (2009, 2013b) for other periods. As it will be shown below, however, regional characteristics still play an important role in the determination of the labour market variables. Given that the effects of nation-wide shocks cannot be examined within our framework, this study should be interpreted as complementary to the existing ones conducted through the estimation of multi-equation models. In any case, the novelty of our analysis neither lies in the use of Blanchard and Katz’s methodology nor is a mere time extension of the work by Jimeno and Bentolila (1998). The paper contributes to the literature in three main dimensions. One contribution is the specific evaluation of the effects of average regional employment shocks when hitting in expansion and when hitting in recession. For this, we use quarterly data (as Jimeno and Bentolila 1998) and consider two subsample periods: 1996–2007, covering the expansion; and 2008–2012, covering the crisis. This disaggregation allows an evaluation of the asymmetries in shock responses across business cycle phases (upward and downward). Another key contribution consists in extending the labour market model to include prices. This extension was already present in Blanchard and Katz (1992), but it has generally been disregarded in subsequent literature. Consideration of prices in Spain is a relevant issue both in 1996–2007 and in 2008–2012. In expansion, it allows us to assess the response of wages to the improved economic conditions of the workers. In recession, it allows us to examine to what extent price adjustments have followed the intense quantity adjustments characterising the Spanish economy in recent years. Summing up, we offer new information on how prices respond regionally to labour demand shocks, and the potential asymmetries of these responses in good and bad times. A third key contribution, finally, is the additional disaggregation by groups of regions based on a cluster analysis. The two resulting groups (one including Catalonia, Madrid, Navarre, and the Basque Country, and the other one grouping the rest of the regions) are used to re-estimate the models and conduct the analysis for the two groups. Our findings are diverse. First, we identify asymmetric labour market responses across business cycle phases. We find that changes in participation rates are the main adjustment mechanism in expansion, while unemployment becomes the central one in recession. Moreover, the long-run employment impact is larger when the shock hits in a recessive period than when it hits in expansion. This result is an indication that net migration—spatial mobility—is more relevant in troublesome than in good times. We also provide evidence of real wage rigidities in both periods along the lines of Jimeno and Bentolila (1998).Footnote 7 And there is, finally, evidence of similar labour market dynamics across high and low unemployment regions generated by the one-off employment shocks. This is consistent with the large degree of aggregate (un)employment persistence characterizing Spain, and is to some extent reassuring in the sense that consideration of an average Spanish region is not flawing the results. Nevertheless, we still find differences in the relative long-run regional employment impact and unemployment persistence, resulting on larger spatial adjustments in high than in low unemployment regions, which appear as more resilient to the shock. On this account, it seems safe to conclude that people in a region are more willing to migrate (relative to the national average) not just when regional shocks take place in a recessive period, but also when they impact in places with larger relative unemployment rates. The remaining of the paper is structured as follows. In Sect. 2, we outline the analytical framework and its empirical implementation. In Sect. 3 we present the data used and the econometric methodology. In the following four sections we show our findings related, respectively, to the aggregate analysis, the disaggregation by business cycle phases, the inclusion of price responses, and the consideration of two groups of regions. Section 8 concludes.",18
5.0,2.0,SERIEs,14 August 2014,https://link.springer.com/article/10.1007/s13209-014-0115-9,The real estate and credit bubble: evidence from Spain,August 2014,Ozlem Akin,José García Montalvo,Josep Maria Raya,Unknown,Male,Male,Male,"In the summer of 2007 the economies of the United States and Western Europe starting suffering bank liquidity and real estate problems, which were followed by a severe banking crisis, with a strong credit reduction and economic recession that still last in Spain. This sequence of events was not unique: Banking crises are recurrent phenomena, often-triggering deep and long-lasting recessions (Reinhart and Rogoff 2009; Schularick and Taylor 2012). A weakening in banks’ balance-sheets may lead to a contraction in the supply of credit and to a slowdown in real activity (Bernanke 1983). Moreover, highly leverage non-financial borrowers may face tightened lending conditions in crises due to a debt overhang problem (Myers 1977). Importantly, banking crises are not random exogenous phenomena, but regularly come on the heels of periods of strong credit growth and asset-price bubbles (Kindleberger 1978; Schularick and Taylor 2012; Gourinchas and Obstfeld 2012). As in other important banking crises in history, the boom and the bust in the housing market, and the associated credit cycle, seem the main drivers of the crises that hit USA, Ireland, UK and Spain, among other countries, in 2007–2008. It is therefore crucial to answer the following questions: Were the lending conditions and standards to housing loans too soft in the boom? Did all banks behave similarly or were there differences? Were pervasive bank incentives present in the boom? Did they contribute to the real estate and credit bubble? If so, how did banks circumvent the regulatory constraints? Did the lending standards tighten in the crisis? Spain offers an excellent setting to analyze these questions. Spain, a bank dominated economy, suffered one of the highest boom and bust in the housing and credit market over the last 10 years. Household mortgages were at the peak 65 % of the GDP and loans to real estate developers and construction firms accounted for another 45 % of the GDP. Therefore, the size of the loans’ pool related directly with real estate activities (production and transactions) amounted to more than 100 % of GDP. Moreover, household debt in Spain (loans to households for mortgages and consumer credit) was 91 % of GDP in 2010, just below 106 % in UK and 95 % in USA, but substantially higher than France and Germany, with 69 and 64 %, respectively.Footnote 1 Despite the importance of this concentration of banking risk on the real estate sector, in particular on household mortgage credit, there is scant evidence identifying the channels that explain the real estate and credit booms. The main reason is the lack of individual (loan) borrower-lender matched level data.Footnote 2 We have access to a unique dataset obtained from a housing market intermediary on mortgage loans from 2005 to 2010 matched with borrower and lender identity for 30,262 mortgage loans concentrated somewhat in the middle-low distribution part of the price distribution. The loan-level dataset contain the loan price (spread), principal and appraisal value of the house. Moreover, we know the identity of the lender and, therefore, we can classify the lender as a commercial bank, a savings bank (caja de ahorros) and a non-bank financial institution (financiera), and also exploit the variation across banks in the ex-post (revealed) risks that they took (whether banks were rescued or intervened). In addition, we know crucial characteristics of the borrower such as labor contract (temporal vs. permanent), status (employed in the private or public sector or not employed), income, education, age, location, etc. Finally, we have been able to match part of this sample (10.92 %) with a dataset also from the housing market intermediary containing the market price and some characteristics of the dwelling. For this subsample, we have the appraisal, the transaction (market), and the officially registered price for each house. We first analyze—at the loan level—credit conditions (both loan to values and loan spreads) in both good (2005:Q1–2007:Q2) and bad (2007:Q3–2010:Q4) times, where the turning point in bank liquidity, credit and real estate dynamics starts in the summer of 2007.Footnote 3 We find robust evidence that lending conditions and standards were softer in the boom than in the bust. For example, household income and labor contract/status matter more (statistically and economically speaking) for LTV (loan to value) and loan pricing in the bust than in the boom. Despite some adjustment in lending conditions in the good times depending on household risk, the results suggest too soft lending standards (excessive bank pro-cyclically/risk-taking) in some loan margins in the boom. Controlling for other key borrower variables such as income, workers with temporal contracts get in the boom the same LTVs (both statistically and economically speaking) as workers with permanent contracts in the crisis, instead, temporal workers obtain less LTVs and pay substantially more spreads. As temporal workers are the ones that mainly went unemployed in the crisis period, these results suggest not only ex-ante high risk, but also ex-post. Even more important for credit supply and excessive bank risk-taking, we find that borrowers who are not employed pay identical loan spreads than employed ones in the boom. However, in the crisis period, the difference in spreads is substantially different. Furthermore, we also find that higher LTV did not impact higher loan spreads (and the same for spreads on LTVs) in the boom.Footnote 4 All in all, the results in Spain suggest too soft lending standards and excessive risk-taking in the boom. Finally, controlling for borrowers’ fundamentals and other characteristics, we find that rescued banks increase even more the LTVs in the boom than other banks, where banks were rescued either individually at the beginning of the crisis or later in the crisis with the Spanish banking rescue fund for bank restructuring called FROB (Fondo de Reestructuración Ordenada Bancaria). As all these rescued banks were savings banks (cajas), the results suggest that banks with worse corporate governance problems soften even more the lending standards, thereby taking excessive risks in the boom.Footnote 5 Our previous results suggest that high LTVs were used for risky borrowers (e.g. temporal workers) and by risky banks (the rescued ones). However, there were regulations and restrictions on LTVs in Spain. How could banks overpass the tough regulation in terms of LTVs? How did banks contribute to the real estate bubble, both in terms of real estate pricing and mortgage loan principals? What were key moral hazard (conflicts of interest) problems that explain the build-up of the credit and real estate bubble in Spain? The specific agency mechanism that inflated the bubble in the US was quite different from the forces at work in the Spanish case. In both cases, lax standards and excessive credit were the ultimate causes of the house price inflation. However, in the case of the US, those lax standards for mortgage granting were the result of perverse incentives in the housing finance sector related with the securitization process, and the possibility of taking out of the banks’ balance sheets the securitized mortgages.Footnote 6 We show evidence consistent with an alternative channel for Spain: real estate appraisal firms were encouraged to introduce an upward bias in appraisal prices to satisfy their owners or most important clients (banks). The Bank of Spain recommends not to grant credits over 80 % of the value of the property and covered bonds, a crucial source of financing for banks, was possible only if collateralized with residential mortgages with LTVs lower than 80 %. Moreover, it was considered a bad practice in lending if LTVs were higher than 100 %. In fact, the requirements of information of the Bank of Spain ask banks to report the mortgages classified in three categories: 80 % or below; between 80 and 100 %; and above 100 %. Appraisal firms’ incentives in Spain were distorted, as financial institutions own most of the appraisal firms (or are the crucial clients for these firms).Footnote 7 During the boom period, appraisers had the incentive to approve even an inflated transaction price in order to satisfy its client (financial institutions) by allowing the borrower to obtain a high mortgage principal by adjusting the actual loan to value of the mortgage when the borrower did not have enough resources for the down payment or did have the resources but preferred to borrow more, thereby circumventing regulatory restrictions. As the appraisal price was the price used by financial institutions to determine the loan to value ratio, this artificial increase in appraisal prices permit to draw larger mortgages. All in all, our results suggest that banks encourage real estate appraisal firms to introduce an upward bias in appraisal prices, to meet loan-to-value regulatory thresholds (40 % of loan mortgages are just bunched in these regulatory limits), thus building-up the credit and the real estate bubble.Footnote 8 Our paper contributes to the literature in two ways. There are several papers analyzing the lending conditions and standards in the Spanish market (Jiménez et al. 2012, 2013a, b), but these papers only analyze business loans and do not analyze loan prices. Moreover, there are papers analyzing mortgage loans in other countries; in particular, Besley et al. (2013) analyze UK mortgages, but do not compare lending standards in boom versus bust nor analyze key borrowers’ risk characteristics such as temporal or inactive workers (see Peydró 2012), which is our first main contribution to the literature. Finally, we uncover a key mechanism by which banks drive the housing and credit bubble, which is our second main contribution. We identify the specific mechanism that led to the creation of a large credit and housing bubble in Spain. Opposite to the US case, the Spanish banking regulation has a much larger regulatory perimeter for banks than the US, and therefore it did not foster the creation of shadow banking activities.Footnote 9 In addition, Spain was pioneer on the use of countercyclical buffers. Despite these regulatory constraints, Spain could not avoid a credit and housing bubble. The mechanism did not rely on the lack of incentive for monitoring the quality of mortgages but the ability of financial institutions to influence the valuations of properties by the appraisal companies. The paper proceeds as follows. Section 2 discusses the Spain’s housing and credit boom and bust. Section 3 introduces the characteristics of the unique dataset we use. Section 4 presents and discusses the results. Section 5 concludes.",52
5.0,2.0,SERIEs,15 August 2014,https://link.springer.com/article/10.1007/s13209-014-0117-7,The recent slowdown in bank lending in Spain: are supply-side factors relevant?,August 2014,Ignacio Hernando,Ernesto Villanueva,,Male,Male,Unknown,Male,"The role of bank balance sheets in shaping credit growth received substantial attention during the 2008 recession. On the one hand, exposure to “toxic” assets affected some banks’ ability to lend (see Puri et al. 2011). On the other hand, even in banking systems without structured off-balance sheet products, but with a high exposure to real estate, drops in house prices have impaired financial intermediaries’ capital positions and, possibly, the overall supply of credit. This paper analyses the case of Spain, an economy that experienced a housing boom until 2007 and a drop in business lending in 2009, to gauge the impact of (current and anticipated) changes in bank capital on business lending.Footnote 1 The Spanish financial system was not severely hit by the first wave of the global financial crisis, as a result of a number of factors: the negligible presence of toxic assets on bank balance sheets; regulations that prevented the creation of off-balance sheet investment vehicles; and the large weight of long-term instruments in their funding structure. However, as the initial financial turmoil turned into a deep recession, Spanish banks were increasingly affected as their high exposure to the real estate sector and the sharp increase in unemployment led to a major deterioration in the quality of their loan portfolios. The existence of provisioning buffers required by the Spanish prudential regulatory system initially mitigated the impact on profits of the surge in non-performing loans. Nevertheless, the progressive reduction in these buffers together with increases in the target capital ratios considered acceptable by regulators and markets have put additional pressure on banks’ capital. All these developments may have constrained the credit supply of some institutions. Disentangling the specific contribution of bank capital from demand-related factors is a difficult task. In a time series context, firms revise downward their investment plans and, consequently, their demand for loans. At the same time, bank capital and borrower creditworthiness deteriorate in a recession, so stricter lending standards are applied by financial institutions. Alternatively, in a cross-section context, banks and firms do not match randomly and it may be the case that firms whose demand for credit is most sensitive to recessions end up borrowing from the smallest and least capitalized banks. Our strategy is the following. Firstly, guided by a simple model of bank behavior that predicts that lending growth is conditioned by capital growth only when the level of capital is near the amount required either by the regulator or the funding markets, we study two different channels through which the capital position of a bank may affect its lending behavior (see Van den Heuvel 2001). The first relates changes in lending to firms to current changes in the capital position of a bank. The second is highlighted in the literature on bank lending under uncertainty and captures cuts in lending associated with anticipated future reductions in capital stemming from expected losses. Secondly, to identify the role of supply factors in business lending, we use financial statements (balance sheets and profit and loss accounts) reported by the Spanish depository institutions to the supervisor between 1995 and 2010. We match this information to data on business lending at the bank-industry level and, as a robustness check, to data at the bank-loan level. We then use an instrumental variable procedure along with industry-year and firm-year fixed effects to address the simultaneity between lending and capital. Our instrument is based on Watanabe (2007) and relies on bank-specific exposure to real estate development as of 1995. The underlying idea is that, unlike banks that enter the real estate development industry during a housing boom, banks with a traditionally high exposure to real estate development are likely to better know the industry than the rest, but possibly due to relationship lending, they are more likely to be exposed to the ups and downs of real estate prices. That fact, combined with whether or not these banks operate in areas that experienced house price declines, permits us to track the changes in banks’ capital position for reasons unlikely to be related to the quality of firms’ demand for loans as of 2007. Our findings can be summarized as follows. Firstly, lagged exposure to real estate development and its interaction with local house prices in the provinces where banks operate are good predictors of capital growth only after 2008, when house prices had accumulated two years of negative growth at the national level. Secondly, those same instruments predict changes in the overall non-performing loans (NPL) ratio already in 2008. We find that the exposure to real estate and its interaction with house prices predicts the NPL ratio only in the real estate development sector and for banks that operated in provinces where housing prices fell. Finally, we find that banks that were more exposed to real estate developers cut their lending to a larger extent during the recession, and that the drop in business lending was mitigated if the banks operated in provinces where house prices fell less. These results hold in analyses performed at matched bank-industry level, but also in specifications using matched bank-loan data that control for the quality of the borrower by means of firm-year fixed effects. Interestingly, using firm level data, we find rather similar cuts in lending across all firms, independently of their size. Finally, we estimate two stage least squares (TSLS) models of credit growth on current and (proxies of) anticipated capital growth. We find a statistically significant elasticity of business lending to actual capital growth of about 0.7–0.8. Nevertheless, back-of-the-envelope calculations suggest a limited role for the capital channel during the sample period we consider (accounting for 18–21 % of the actual credit reduction). Overall, our preferred estimates suggest that the deterioration of banks’ capital position has had a negative but limited effect on the supply of loans to firms. We make three distinct contributions. Firstly, we contribute to the debate on the role of house price dynamics in bank balance sheets and business credit. Gan (2007) finds that Japanese banks that were more exposed to real estate development cut business lending after a sizable fall in aggregate real estate prices. However, Chaney et al. (2012) or Cuñat et al. (2014) advocate the use of local house prices to study balance sheet effects, as aggregate prices may correlate with macro factors such as exchange rates. Chaney et al. (2012) find that shocks to the value of firms’ collateral diminish investment while Cuñat et al. (2014) document that banks cut their lending in response to local house price drops. We estimate how local house prices and banks’ exposure to real estate development affect capital growth, NPLs and loans granted by banks. We also document that house price increases did not affect capital or business lending growth, but house price reductions actually did. Secondly, we find that house prices in the provinces where a bank operates affect business lending even when the prices in the locality where firms operate hold constant. That finding suggests that changes in local house prices spread to other regions through banks’ balance sheets. Secondly, and due to Spanish regulations, the dynamic provisioning system allows us to separate the role of contemporary and anticipated capital growth in determining the changes in business credit. The stylized model we use delivers quantitative predictions on the impact of capital growth on business credit; it should be bounded between zero and one. That is not necessarily the case when anticipated capital growth is not held constant. Finally, our results cast light on possible reasons for the discrepancies among existing estimates of the role of bank capital in determining business credit. Berrospide and Edge (2010) use aggregate data and find a very limited role for bank capital asset ratios in business lending growth. Using loan level data, Albertazzi and Marchetti (2010) and Jiménez et al. (2012) find larger impacts made by banks’ balance sheets. The discrepancy among those results could either be due to cross-country differences in the dependence of firms on bank lending or, alternatively, to differences in the level of data disaggregation. The availability of comprehensive loan-level and of aggregate data allows the response of lending growth to balance sheet shocks to be estimated using data on business loans granted both at the bank-loan and at the bank-industry level. The response of credit growth to banks’ exposure to real estate development and its interaction with local house prices does not depend greatly either on the degree of the data aggregation or on whether or not we control for the quality of borrowers by means of firm-year fixed effects. Hence, the heterogeneity in responses across studies may depend more on the extent to which firms depend on banks for their funding or on the nature of the shocks that affect bank balance sheets. The rest of the paper is organized as follows. The next section contains a brief review of the recent literature analyzing the influence of a bank’s capital position on its lending provision. Section 3 describes the theoretical framework and our empirical strategy. Section 4 presents the procedure used to address the potential simultaneity of capital and lending, and describes the data sources and the variables included in the empirical specifications. Section 5 discusses the results obtained while Sect. 6 provides some robustness checks at the bank level. Section 7 examines loan level outcomes. Finally, Sect. 8 offers some concluding remarks.",6
5.0,2.0,SERIEs,20 May 2014,https://link.springer.com/article/10.1007/s13209-014-0109-7,Are there alternatives to bankruptcy? A study of small business distress in Spain,August 2014,Miguel García-Posada,Juan S. Mora-Sanguinetti,,Male,Male,Unknown,Male,"Business bankruptcy rates (ratio of the number of business bankruptcy filings to the number of business exits) in Spain are among the lowest in the world, which means that Spanish firms rarely enter a formal bankruptcy procedure. The goal of this paper is to explain this empirical observation, which may imply that economic agents regard the system as inefficient and try to deal with financial distress in alternative ways.Footnote 1 For that purpose we employ a large sample of Spanish, French and UK firms, finding that small businesses in Spain, unlike their European counterparts, rely on mortgage foreclosuresFootnote 2 as the main alternative to bankruptcy proceedings. According to Table 1 Spain had the second lowest bankruptcy rate out of 26 countries, including both high-income and emerging economies, in 2006. An even more striking observation is the difference in the orders of magnitude between Spain and other developed economies: for instance, while there were around 29 bankruptcies per 100 firm exits in France and 16 in the UK, there were 0.3 in Spain. Only the deep economic crisis that Spain is currently experiencing has modestly increased the number of bankruptcy filings, but the Spanish bankruptcy rate was still one of the lowest in the world in 2010 (see Table 1). In contrast with the low incidence of business bankruptcies, business mortgage foreclosures have soared during the crisis. While around 8,000 firms filed for bankruptcy in 2012, there were nearly 26,000 business mortgage foreclosuresFootnote 3 in the same year. Moreover, the latter figure must be considered a lower bound, since small business owners may finance their firms with loans secured on their homes (Berkowitz and White 2004) but, if lenders repossess the collateral, they will be reflected as residential foreclosures in the official statistics. However, the use of bankruptcy procedures by Spanish businesses varies widely depending on the size of the distressed firms, as shown in Fig. 1. While the rates of micro firms (businesses with less than 10 employees) were around 0.15 % in 2006 and they have just reached 1.3 % during the economic crisis, those of non-micro firms were 10.4 % in 2006 and they have increased up to 90 % during the crisis, in line with the aggregate rates of developed countries. Since micro firms account for more than the 95 % of firms in Spain,Footnote 4 they are the key drivers of the low bankruptcy rate of Spanish companies. They are also very important in terms of economic activity: they accounted for 51 % of total employment and 28 % of total value added before the economic crisis and they currently account for 39 and 25 %, respectively.Footnote 5 Finally, although the available evidence is rather limited, Spanish micro firms seem to file for bankruptcy much less than some of their European counterparts: in 2006, the bankruptcy rates for self-employed and micro enterprises were 0.01 and 0.15 %, respectively, in Spain, while those in France were 11.1 and 23 %,Footnote 6 and the bankruptcy rate for self-employed in the UK exceeded 16.2 %.Footnote 7 Bankruptcy rates by size in Spain. Data are quaterly except for the first period 04C3 (last 4 months of 2004). Rates are annualized. Source: authors’ calculations on data from the Spanish National Statistics Institute. Size is measured in terms of employees. Micro: [0,9], small: [10,49], medium and large: \(>\)50. Non-micro: \(>\)9 Spanish micro firms also have other distinct characteristics. They hold, by far, the largest proportion of mortgage loans over financial debt, as shown in Fig. 2. Filing for bankruptcy is especially unattractive for them because a significant proportion of the bankruptcy costs are fixed (Van Hemmen 2011).Footnote 8 Personal bankruptcy may apply to many of those firms regardless of their legal form, because the distinction between limited and unlimited liability may be blurred for them, partly because lenders require personal guarantees or security in the form of a mortgage on the owner’s home (Berkowitz and White 2004). % Mortgage loans over bank debt by business size in Spain. Source: Authors’ elaboration with data from the Central Credit Register and the Central Balance Sheet Data Office, Banco de España Consistent with those stylized facts, our hypothesis on the low business bankruptcy rates in Spain is the following. Filing for bankruptcy in Spain is very costly for both small firms and their creditors. Due to this, the capital structure of micro firms is biased towards mortgage loans (i.e., loans secured on land and buildings). Having this capital structure allows them to avoid bankruptcy by carrying out debt enforcement via mortgage foreclosures,Footnote 9 which are cheaper procedures than bankruptcy, in case of financial distress. In order to test this hypothesis the optimal identification strategy would be to analyse the impact of substantial changes in the Spanish bankruptcy law in both bankruptcy rates and firms’ capital structure. The current bankruptcy code entered into force in 2004 after a major legislative reform. But it seems that the de facto insolvency framework barely changed because the performance of bankruptcy proceedings did not seem to substantially improve (Gutiérrez 2005; Van Hemmen 2004), bankruptcy rates did not increase after the introduction of the new code and it seems that firms’ capital and asset structures have not changed either (Celentani et al. 2010). By contrast, our identification strategy relies on cross-country comparisons. Specifically, we compare the observed choices (choice of capital structure, choice between bankruptcy and mortgage) of Spanish firms with those of firms from countries where their bankruptcy systems are more efficient and their laws do not incentivise them to bias their capital structure towards mortgage loans. France and the UK are chosen as the comparison group because their bankruptcy rates are much higher than the Spanish ones and because of the specific features of their insolvency frameworks.Footnote 10 Our findings corroborate the proposed hypothesis. First, there is a positive and strong correlation between the ex-ante probability of default and the ratio of tangible fixed assets (the assets that can be pledged as mortgage collateral) to financial debt in the case of Spanish micro firms, suggesting that firms with risky business models bias their capital structure towards mortgage loans to avoid filing for bankruptcy in the event of default. Second, a higher proportion of tangible fixed assets over financial debt significantly decrease the probability of being in bankruptcy among Spanish micro firms in financial distress. By contrast, these two relations do not hold either for Spanish larger businesses or for firms from the other two countries. Finally, we must stress the importance of the research question. The model of García-Posada (2013) predicts that, in the context of the Spanish insolvency framework, there is a positive relation between bankruptcy rates and welfare. The intuition is that low bankruptcy rates and low welfare are the outcome of an institutional design characterised by the low efficiency and low creditor protection of the bankruptcy system relative to those of an alternative insolvency institution, the mortgage system. In that context, firms and their creditors avoid filing for bankruptcy by heavily relying on mortgage collateral, which can be repossessed and liquidated in the event of default. The problem is that the mortgage system is not well suited for some firms, which need to bias their asset structure to have enough collateral, with the ensuing productive inefficiencies. Those firms would be better off if they had access to a bankruptcy system that worked relatively well. In other words, as the bankruptcy and mortgage systems are imperfect substitutes, the equilibrium in which only mortgage is widely used (reflected in low bankruptcy rates) is Pareto dominated by the equilibrium in which agents can choose between the two insolvency institutions (reflected in higher bankruptcy rates). His analysis also predicts that bankruptcy will be unfeasible for the smallest firms in the economy as long as some of the bankruptcy costs are fixed. As some of those firms will have to overinvest in capital assets to sign their contracts under mortgage, they will incur in productive inefficiencies. If the absence of a well-functioning bankruptcy system for those firms also reduces their growth opportunities—e.g., by hampering access to unsecured lending such as venture capital—then the current insolvency framework may help explain the firm size distribution and the low aggregate productivity of the Spanish economy. This is consistent with the evidence of Fabbri (2010) in Spain, who finds that lengthy bankruptcy procedures decrease firm size and raise funding costs and with that of Ponticelli (2012) in Brazil, who shows that congestion in bankruptcy courts substantially reduces firm-level investment and productivity. The rest of the paper is structured as follows. Section 2 provides a brief literature overview and discusses the paper’s main contributions. Section 3 discusses some key features of the insolvency framework of Spain, France and the UK Sect. 4 focuses on data sources and sample selection criteria. Section 5 explains the empirical testing of the hypothesis. Section 6 concludes. Appendix A provides a description of the main legal concepts used in this paper and Appendix B contains some robustness analyses.",14
5.0,2.0,SERIEs,22 July 2014,https://link.springer.com/article/10.1007/s13209-014-0112-z,Growth and imbalances in Spain: a reassessment of the output gap,August 2014,Enrique Alberola,Ángel Estrada,Daniel Santabárbara,Male,Male,Male,Male,"The global financial crisis of 2008 was preceded by a protracted phase of economic expansion coupled with low and stable inflation. This period came to be known as the ‘Great Moderation’ (see, for example, Stock and Watson 2002; Bernanke 2004) and it was widely considered that the observed growth was underpinned by solid economic foundations. However, during this period, domestic and external imbalances, many of them closely related to the exuberance of the financial sector, were accumulating. These eventually brought about the worst crisis in decades, which has become known, in contrast to the previous label, as the ‘Great Recession’. To be fair, there were warnings from different quarters, that imbalances building up meant that the observed growth rates were unsustainable. For instance, the IMF alerted to the global imbalances (the buildup of increasing current account deficits and surpluses) and the BIS emphasized the risks deriving from asset bubbles and excessive credit growth. In the case of Spain, some of these imbalances were rather evident. For instance, it was ex-ante recognized that the observed current account deficits required quite high long-term growth expectations to be coherent with the intertemporal budget constrain of households (Campa and Gavilán 2006) or that by the mid 2000s the prices of housing were significantly overvalued (Ayuso and Restoy 2006). However, the dominant perception was that the high growth rates—along with mild cyclical oscillations were here to stay. The progressive increase in the estimated potential growth rate contributed to and was a reflection of this perception. The concept of potential growth plays a key role in the design of the macroeconomic policies. Monetary, fiscal and, more recently, macroprudential policies take into account the output gap estimates—the difference between potential and actual output—to adapt their stance in order to reduce possible macroeconomic imbalances/disequilibria and dampen aggregate fluctuations. This role is even more important for a country like Spain, which cannot rely on a specific monetary policy. Indeed, potential growth has been subject to extensive analysis in the theoretical and empirical literature, but the relevance and usefulness of these concepts for economic policy will depend on two factors. First, the ability of the output gap to reflect and summarise the disequilibria of the economy. And second, the degree of uncertainty surrounding the estimates of the output gap and their robustness to new information. In that respect, the experience of the crisis reveals the weaknesses of standard potential growth estimates as a tool to capture the sustainable rate of growth of the economy. The main limitation of the potential growth estimates is the consideration of just one indicator to sum up the imbalances of the economy: the inflation rate, which is supposed to capture the deviations of observed from structural unemployment. This approach involves the estimation of potential growth through the Phillips curve, which allows the NAIRU, that is the “potential” unemployment rate, to be calculated. However, inflation rates, represented by the consumer prices (CPI), seem not to have been a sufficient indicator of the macroeconomic imbalances of the economy during the last decade or so. As can be seen in Fig. 1, advanced economies displayed a statistically significant positive correlation between growth and changes in inflation before 2001. This association mostly disappeared after 2001. Various reasons have been put forward for explaining this result: the success of central banks in controlling inflation and anchoring inflation expectations, reforms in the labor and product markets, or the globalization process, among others (see, for example, WEO, 2013 for a detailed analysis). GDP vs. inflation. Advanced economies (simple averages). Source: IMF (WEO) At a time when inflation had stabilized, other indicators of imbalances showed a significant widening in many countries. This was especially the case in Spain. For instance, the current account deficits increased significantly, while at the same time, potential growth estimates remained quite strong or even increased (Fig. 2). The same happened with the private and public balance, or with private investment, which was mainly driven by residential investment. It could be argued that part of observed growth in Spain was fostered by the launch of the Monetary Union and the implied gains in terms of macroeconomic stability, financial integration, lowered costs of funding and credibility of monetary policy. This structural change could result in large potential growth and limited inflationary pressures, identified with a process of real convergence. However, the crisis in Spain has shown that in that period observed growth was excessive and its nature pernicious for the stability of the system. Spain GDP vs. selected imbalance indicators (%). Sources: European Commission and AMECO This paper applies a new methodology to obtain estimates of sustainable growth rates for Spain (see also Alberola et al. 2013). The sustainable growth rate is defined as the output growth that does not widen macroeconomic imbalances, which are identified through a wide set of domestic and external indicators (for alternative definitions, see, for example, Basu and Fernald 2009). The methodology is analogous to that used to estimate standard potential growth, with two major modifications. First, several refinements to the components of the production are made in order to obtain a more precise framework to assess cyclical fluctuations related to imbalances. Second, we consider a much richer set of economic and financial variables which may reflect economic imbalances, in order to identify which imbalances drive the business cycle. On the basis of these elements, we estimate the sustainable growth rate for Spain over the period 1970–2011. As it will be seen, this paper has strong links to the literature related to early warning indicators (Frenkel and Saravelos 2012), which have recently been incorporated to the multilateral supervision mechanisms of Europe (Scoreboard, MIB) and the G-20 (Indicative Guidelines, SA). Insofar as most of the imbalances indicators considered have a strong financial component, it also has close links to the literature relating financial and business cycles (Claessens et al. 2011 or Borio and Disyatat 2011). Especially relevant is the recently published working paper by Borio et al. (2013), which reaches very similar conclusions using a somewhat different methodology to refine the traditional output gap estimates. Finally, in the Spanish case, the papers of both Campa and Gavilán (2006) and Estrada et al. (2010) have stressed the relevance of the external imbalances to account for sustainable growth. The document is organised as follows. In Sect. 2, macroeconomic imbalance indicators are discussed and some stylised facts on their interaction with standard estimates of potential growth are presented. This will show, first, that although potential growth estimates over time (real-time) are not correlated with inflation, they are correlated with some of the indicators of imbalances considered; and second, after the crisis, as imbalances have been corrected, there have been important revisions to the estimates of potential growth. Section 3 presents the methodology used to estimate sustainable growth rates. The overview of the results is presented in Sect. 4, detailing the imbalances indicators relevant for Spain, the contribution of production factors to sustainable growth and the reassessment of the output gap. A brief Sect. 5 presents a preliminary comparison of the revisions in potential and sustainable growth estimates, which shows the lower revisions associated to our methodology. The final section concludes.",2
6.0,1.0,SERIEs,13 December 2014,https://link.springer.com/article/10.1007/s13209-014-0120-z,Are unemployment benefits harmful to the stability of working careers? The case of Spain,March 2015,Yolanda F. Rebollo-Sanz,J. Ignacio García-Pérez,,Female,Unknown,Unknown,Female,"The impact of the unemployment insurance system on labour markets is currently at the heart of political debate (see, for example, OECD 2013). Multiple aspects of the unemployment benefit system are at issue, including the short- and long-term effects of the system and the appropriate balance of efficiency and equity considerations.Footnote 1 In particular, the efficiency properties of unemployment benefits, in terms of employment stability, are theoretically ambiguous. Accordingly, they must be tested empirically. Empirical research on the effect of benefit duration on the exit rate from unemployment is extensive, both in the US and in Europe. However, empirical literature that describes the effects of the Unemployment Insurance System (UIS, hereafter) on unemployment and employment durations and that controls for selection effects is rather limited due to the scarcity of large micro datasets with complete information on labour market histories. The main conclusions that can be drawn from the existing literature are that changes to benefit duration produce substantial effects on unemployment durationFootnote 2 and that those benefits tend to be exhausted before individuals return to employment. For example, Meyer (1990) finds that the unemployment exit rate in the US doubles one month before benefits expire. Card and Levine (2000) conclude in the same vein that 13 extra weeks of benefits would raise the average duration of regular claims by approximately one week. The evidence for Europe is rather similar. Roed and Zhang (2003), Van Ours and Vodopivec (2006) and Lalive (2007) all find that benefits strongly affect the duration of unemployment. Furthermore, they also find a large spike in the re-employment hazard at the point of UI exhaustion for job seekers. More close to our empirical strategy, Boone and Van Ours (2009) report that the job-finding rate for permanent jobs in the month of benefit expiration is about three times as high for males and 3.7 times as high for females than it is in months without benefit exhaustion. In the case of transitions to temporary contracts, they find spikes that are approximately 50 % (males) and 75 % (females) higher than regular job-finding rates. Some previous empirical papers have estimated the exhaustion effect of benefits for Spain, being (Bover et al. 2002) the first to quantitatively measure the effect of receiving unemployment benefits on the exit from unemployment. Arranz and Muro (2004) show that, for UI recipients, the hazard rate rises dramatically \(-\)170 % two months prior to exhaustion-, when UI benefits lapse approaches. Alba-Ramirez et al. (2007) investigate benefit recipients’ exits from unemployment but they do not analyse the probability of exiting unemployment near the time that benefits expire. More recently, Rebollo-Sanz (2012) differentiates between recalls and entry into new jobs and get that the spike in the unemployment hazard rate at benefit exhaustion differs between recalls and new job entries. Hence, there is almost no doubt that lengthier benefit periods favour longer episodes of unemployment. However, the impact of unemployment benefits on the duration of subsequent employment is more scarce and mixed. On the one hand, it is found that the length of the benefit entitlement period is positively correlated with the duration of unemployment, which might decrease subsequent job duration (i.e., the unemployment ‘scarring’ or ‘signalling’ effect).Footnote 3 On the other hand, previously insured workers might have lengthier subsequent employment spells because benefits finance the search for good (and therefore durable) matches. Given these competing effects, the overall impact of benefits on employment stability is not obvious. In this sense, Tatsiramos (2009) find that the effect of benefits on employment stability is more pronounced in countries with relatively more generous benefit systems, such as Denmark, Germany, France and Spain, than in countries like Greece and Italy, in which the UI system is underdeveloped. Finally, Caliendo et al. (2012) find that the overall effect of extended benefit duration on exit rates from subsequent employment is negative, but small, and not significantly different than zero. In any case, however, the literature on these issues takes a short-term perspective and focuses on the effect of unemployment benefits on the duration of single employment and unemployment episodes. However, a medium-term perspective is also highly relevant, as medium- and long-term impacts of the benefit system can differ markedly from the short-term impact if current employment duration depends on previous labour market history. Our paper try to shed light on these issues by proposing an analysis of the effects of benefit entitlement duration on job turnover and workers’ labour market stability, taking into account the endogeneity of benefits, dynamic selection and occurrence dependence issues. This paper also contributes to the empirical literature by analysing the potential interaction of the benefit system with a segmented labour market characterised by two types of workers: stable and unstable. In this regard, we make our assessment for Spain given that its extremely dual labour market responds differently to economic shocks than labour markets in other European economies (see Bentolila et al. 2010). In particular, we wonder whether the UIS might favour stable labour markets paths, not only through the pure matching effect (i.e., by increasing job match quality by allowing individuals to wait for better job offers) but also by favouring the transition from unstable to stable jobs. The alternative to be empirically tested is whether this system is effectively trapping workers in a vicious cycle of unemployment and unstable jobs.Footnote 4 Our empirical analysis is based on duration models and is closely related to the timing-of-events approach developed in Abbring and van den Berg (2004). Formally, the model has two states: employment and unemployment. We model each state following a competing risk approach to consider the potential endogeneity of each transition and its relation to unemployment insurance benefit parameters. For the employment equation, as illustrated in Fig. 1, we separately model layoffs, quits, job-to-job transitions into unstable or fixed-term contracts and job-to-job transitions into stable or open-ended contracts. Similarly, for the unemployment equation (see Fig. 2), we separately model exits into jobs with fixed-term contracts and exits into jobs with open-ended contracts. Additionally, our model accounts for the heterogeneous effects of the benefit system based on contract type since we interact our benefit system covariates with the type of contract held by the worker. Differentiating by the type of contract offers a new dimension to the analysis that is crucial to understanding the interaction between the benefit system and dual labour markets,Footnote 5 particularly because the arrival rate of job offers and job duration in this type of labour market vary substantially based on previous and current contract types. Empirical employment hazard rates (1995–2007) Empirical unemployment hazard rates for UIS receivers and non-receivers (1995–2007) Given the design of the Spanish UIS, all employees who involuntarily become unemployed are entitled to Unemployment Benefits, provided that they have been employed for at least 12 months over the previous 72-month period. Individuals who receive full-time disability benefits, people who leave their jobs voluntarily and anyone over the age of 64 are excluded from these benefits. Benefits end when individuals cease to be unemployed or reach the maximum entitlement period. During the analysed period, the amount of income provided to each unemployed individual is determined by multiplying the gross replacement rate by the individual’s average basic pay over the twelve months preceding unemployment. This replacement rate is 70 % of previous wages for the first six months and 60 % of previous wages from the seventh month onward.Footnote 6 The length of the benefit entitlement period depends on previous employment duration. Specifically, the initial benefit period is at least 4 months, which may be extended in 2-month increments up to a maximum of 2 years, depending on the worker’s employment record. Finally, Unemployment Assistance benefits are available for people who have not been working long enough to qualify for the previously described unemployment benefits and for people who have exhausted them and have family responsibilities. In this paper, we use the information provided by a Spanish administrative database extracted from Social Security records. The analysis will focus on labour market transitions during 1995–2007 for a sample of male workers. We made this decision because, in this time span,Footnote 7 the arrival rate of job offers is more likely to be close to one (or, at least, very large for temporary job offers). Hence, in terms of exits from unemployment, we can better interpret our results as depending primarily on reservation and offered wages. The major recession of 2008–2012 was the most severe recession in developed countries since World War II, and we think it deserves special analysis.Footnote 8 As summarized above, the previous empirical evidence on post-unemployment outcomes suggests that, on average, there is no effect of unemployment benefits on the quality of the post-unemployment job. However, there is some evidence of heterogeneous effects, which lead to zero net effects when this heterogeneity is ignored and which indicate that some individuals might be facing liquidity constraints. Except for Tatsiramos (2009), the existing literature bases its analysis on reforms which is a good identification strategy but it is an alternative not possible in our case. Nevertheless, these papers only look at post-unemployment job characteristics and do not take into account “dynamic” effects as the ones considered in this paper. This is, from our point of view, the main advantage of our approach, which enhance us in order to look for the medium or long-term impact of unemployment benefits on workers’ labour market careers. Furthermore, this paper extends the existing literature by analysing the effect of UIS on labour market transitions when the following factors are considered separately: quits, layoffs, and job-to-job transitions as the means of exiting employment, and whether exits from unemployment are to temporary or permanent contracts. Thus, it takes into account a major singularity of the Spanish labour market, i.e., strong segmentation due to the existence of temporary and permanent contracts and the high job-turnover rate. In accordance with previous literature, particularly with the results in Boone and Van Ours (2009), our findings show that non-insured unemployed workers experience a greater rate of transition to employment than insured workers, except at the time of benefit expiration. Incidentally, we find that benefits encourage job stability for temporary workers not only by increasing subsequent job tenure—the pure matching effect—but also by increasing the probability of entering into a permanent contract. Finally, our simulation exercises demonstrate that reducing (or increasing) the benefit entitlement period does not lead to substantial changes in employment stability because, although unemployment duration is being affected in the predicted direction, job turnover also increases (decreases) when unemployment benefits are cut (raised), especially for temporary workers. In sum, job stability hardly varies both when reducing or increasing benefit entitlements, being this result quite novel in the literature.Footnote 9 We show that the reason for this is because the effects on the overall time spent employed and on job turnover tend to cancel each other out making total time employed, our measure of job stability, almost unchanged under any of the two alternative measures. On the contrary, when we simulate a situation where the transition to permanent contracts is more likely, the long-term impact on job stability is much larger. This illustrates that, in terms of enhancing job stability, it might be more important to cope with the strong dual character of the Spanish labour market than to reshape the UI system. Our paper starts with a brief description of the data we use and main characteristics of the sample. Section 3 describes our empirical model, and Sects. 4 and 5 provide our main results. The last section presents our basic conclusions.",15
6.0,1.0,SERIEs,23 December 2014,https://link.springer.com/article/10.1007/s13209-014-0119-5,"Income tax progressivity, growth, income inequality and welfare",March 2015,Cruz A. Echevarría,,,,Unknown,Unknown,Mix,,
6.0,1.0,SERIEs,27 July 2014,https://link.springer.com/article/10.1007/s13209-014-0114-x,Public goods and decay in networks,March 2015,Enrique Fatas,Miguel A. Meléndez-Jiménez,Hector Solaz,Male,Male,Male,Male,"A public goods game is an example of an economic situation in which individual and collective interests are not aligned. Therefore, the Nash equilibrium prediction leads to a suboptimal collective outcome. The experimental evidence strongly suggests that subjects act on a reciprocal basis, following the lines first suggested by Sugden (1984). In her experiment, Croson (2007) finds a simple reciprocity pattern: subjects try to match the average contribution, rather than the minimum or the maximum (see also Fischbacher et al. 2001). In a recent study, Fischbacher and Gächter (2010) find that conditional cooperation is imperfect; participants systematically fall short of matching others’ contributions by a relatively small, but significant margin. Conditional cooperation requires information about others’ contributions. Recent experimental literature analyzed how limited information about other subjects’ contributions shapes their decisions (Fatas et al. 2010, 2014; Leibbrandt et al. 2014; and Carpenter et al. 2012). These papers manipulate the level to which agents observe the contributions of others modeling groups as informational networks. The whole group still provides the public good, but you may get different information about the decisions of the other group members. Note that this is a major difference with respect to the standard theoretical literature on public goods in networks (as in Bramoullé and Kranton 2007, where agents only benefit from the contributions of her neighbors). The main finding in the experimental analysis of network structures is that contributions are lower in incomplete networks, and some incomplete networks (the star) significantly outperform others (the circle or the line). Both the Nash prediction and the conditional cooperation logic described above are unable to explain these differences across networks. In this paper we propose a simple behavioral model that assumes that (1) agents are imperfect conditional cooperators and that (2) agents predict unobserved contributions assuming imperfect conditional cooperation by others. By introducing two additional behavioral assumptions (we consider that players only react to the most recent experience and that they have a limited level of rationality) we keep the model purposely simple and stop its recursive nature.Footnote 1 This simple formulation is enough to reveal a connection between imperfect conditional cooperation and the presence of decay, one of the main ingredients in some influential models of network formation (e.g., Jackson and Wolinsky 1996, or Bala and Goyal 2000).Footnote 2 In these models, the decay represents frictions that determine the value of the externalities generated by individual links. In our formulation, the decay factor will coincide with the parameter capturing the imperfect conditional cooperation. Besides this theoretical connection, our model captures the differences in contributions across networks. Interestingly, it also predicts well the decisions of players with the same numbers of neighbors (i.e., nodes with the same degree) in different networks. We simulate our model and compare the results to the evidence derived from the experimental results by Fatas et al. (2010) in which groups of four subjects repeatedly play a public goods game within different network structures (the complete network, the circle, the line and the star). In our simulations, we estimate the imperfect conditional cooperation parameter and fit the model to predict behavior in every incomplete network, relative to a quite natural benchmark: the complete network. Our results show that the model offers a good approximation to the dynamic evolution of contributions in different incomplete networks. Moreover, it also fits well with a range of different players’ network positions (defined by their connectivity to other players). Rather than describing precise behavioral rules, the model provides a simple but powerful analytical basis for the deterioration of information, and cooperation, in networks. Our results suggest that the incompleteness of the network erodes conditional cooperation by making more difficult for subjects to get accurate information about the decisions of the other group members. The remainder of the paper is organized as follows. Section 2 presents the model. In Sect. 3 we describe the experiment. Section 4 describes our simulations procedures. Section 5 compares the results. Finally, Sect. 6 concludes.",2
6.0,1.0,SERIEs,27 August 2014,https://link.springer.com/article/10.1007/s13209-014-0118-6,Appointing high-court judges by political parties,March 2015,Nicolás Porteiro,Antonio Villar,,Male,Male,Unknown,Male,"There are a number of relevant instances when political parties have to agree on the choice of candidates for some public institutions. This is the case in many countries for regulatory committees, central bank councils or High Courts. A potential conflict arises when candidates are nominated for a fixed period of office and the norm requires a qualified majority for their nomination. In other words, we may find that the period of office of a member of such an institution expires but the parties do not reach an agreement on the person that should replace him/her. The origin of this paper lies in a real life problem of this sort: the difficulties involved in replacing Spanish Constitutional Court judges over the last few years. This High Court consists of twelve magistrates, eight of whom are chosen in Parliament (four in each of the two Chambers).Footnote 1 The Law establishes that 3/5 favourable votes in Parliament are needed for their nomination. In practice, this requires the consensus of the two largest parties, as neither of them has ever had such a large share of Parliamentary seats, and no other party has enough seats to enable them to reach the necessary share. Each magistrate is appointed for a 9-year period, with a renewal rule that prescribes that one third of the magistrates must be replaced every 3 years. The lack of consensus arose as a major problem in 2007 (the Constitutional Court was established in 1980). Three magistrates stayed in office more than 3 years beyond their duty (and one seat was empty due to the death of another magistrate who was not replaced). Legislators never considered the possibility of such a long delay in the renewal of the Constitutional Court. Indeed, it broke the established renewal pattern, as keeping to the 9-year mandate entailed replacing two thirds of the magistrates on one occasion, one third on next occasion, and none of them the third time. To prevent inconsistency, the law that regulates the Constitutional Court had to be amended, introducing the caveat that the extra period that a magistrate stayed in office was to be discounted from the period of office of his/her substitute. That amendment somehow preserved the renewal norm, even though now some magistrates will stay in office for twice as long as others. Nonetheless, the main issue behind the undue extension of the magistrates’ mandate is one of legitimacy. The Spanish Constitutional Court has made very important decisions in such anomalous circumstances, in particular sentences regarding the nature of some Estatutos (the equivalent of the “constitutions” of the Spanish Autonomous Regions). The result is a lack of confidence both in political parties and legislators, the deterioration of the relationships between legislators and the judiciary, and the dissemination of the idea that parties are more concerned about their relative power than about serving citizens. The situation deteriorated so much that some of the magistrates resigned, something unheard of in an institution of this kind. This problemFootnote 2 derives from the collision of two sensible principles that inspire the norm, or rather from the way in which those principles are implemented. On the one hand, each judge is designated for a fixed term, in order to prevent disproportionate influence of specific individuals on the legal system. On the other, the rule establishes that substituting a judge whose mandate expires requires a qualified majority in Parliament, in order to diminish the effects of the political cycle on the interpretation of the fundamental norms. The deadlock at which the system can arrive when a judge’s mandate expires and Parliament does not have the majority required to approve a substitute, is directly related to the choice of the status quo as the default option (i.e. the judge whose mandate has expired stays in office until an agreement is reached). Such a default option does not provide the right incentives to reach an agreement. The reason is clear: if the ideology of the judge to be replaced is close to a given party, then this party has no incentive to agree on a different candidate, unless he/she belongs to the same ideological group. When this party is needed for the nomination but cannot ensure a replacement that satisfies its interests, it will block the process. Here we propose a simple mechanism that is respectful of Parliament and ensures a judge’s timely replacement, by simply changing the default option. Once the judge’s appointment expires, Parliament is required to make a proposal that satisfies the established quota (3/5 in the case of the Spanish Constitutional Court). If there is agreement, then the candidate is chosen, substitutes the exiting judge, and the process ends. This part is consistent with the current procedure. If there is no agreement, then a weighted lottery applies. Each party presents a candidate who enters a random choice mechanism with a probability weight equal to the proposing party’s share in Parliament. The weighted lottery is played and the chosen candidate will be determined by the lottery. This mechanism is fair because it respects the distribution of power among parties. Firstly, it is consistent with the current system provided that parties are able to reach an agreement in due time. Secondly, when no agreement is reached, on average each party will get its preferred candidate a fraction of times equal to the fraction of its Parliamentary seats. We show that, under reasonable conditions (single peaked preferences on the ideological traits of the candidates) and a rich variety of candidates over the ideological spectrum, the following holds: When status quo is the default option, disagreement is the natural outcome; When parties are risk averse (or even risk neutral), the weighted lottery mechanism induces agreement between parties before actually recurring to the lottery; This two-stage mechanism implements the politically balanced alternative as a Nash equilibrium; and The results are robust to the case of bi-dimensional preferences (i.e. when parties care about both the candidates’ ideology and their capabilities). All those results are simple derivations of the non-cooperative theory of bargaining, following the seminal work of Rubinstein (1982), and are closely related to legislative bargaining and arbitration models (Baron and Ferejohn 1989; Banks and Duggan 2006; Cardona and Ponsati 2011; Mylovanov and Zapechelnyuk 2013). Yet they may be useful in suggesting practical ways of solving a relevant problem. The model aims at highlighting the essential ingredients of the problem and the proposed solution, at the cost of depicting a highly simplified world. In particular: (a) The model focuses on the replacement of one judge at a time; (b) There are only two (relevant) political parties; (c) The utility function of political parties only depends on the ideology of the candidate (a single dimensional variable) and, possibly, on his/her ability (an observable variable that all parties perceive equally). We shall briefly refer to those aspects later on (see Sect. 5). The rest of the paper goes as follows. The next section sets up the reference model. Section 3 presents the weighted lottery mechanism and characterizes its implications. Section 4 briefly elaborates on the consequences of taking not only judges’ ideology into account, but also their competence. Finally, Sect. 5 concludes by discussing some other aspects of the problem.",
6.0,1.0,SERIEs,20 December 2014,https://link.springer.com/article/10.1007/s13209-014-0121-y,Equilibrium mergers in a composite good industry with efficiencies,March 2015,Cristina Pardo-Garcia,Jose J. Sempere-Monerris,,Female,Male,Unknown,Mix,,
6.0,2.0,SERIEs,06 February 2015,https://link.springer.com/article/10.1007/s13209-015-0122-5,Rental housing discrimination and the persistence of ethnic enclaves,June 2015,Mariano Bosch,M. Angeles Carnero,Lídia Farré,Male,Unknown,Female,Mix,,
6.0,2.0,SERIEs,16 May 2015,https://link.springer.com/article/10.1007/s13209-015-0125-2,"Split or straight? Evidence of the effects of work schedules on workers’ well-being, time use, and productivity",June 2015,Jorge González Chapela,,,Male,Unknown,Unknown,Male,"A prominent feature of the Spanish labor market is that many individuals work split shifts, consisting typically of 5 h work in the morning, a 2-h break at lunch time, and another 3 h work in the afternoon/evening. According to the Spanish Survey of Working Conditions, 52.2 % of workers were on a daytime split work schedule in 2003, and 40.2 % in 2011 (INSHT 2003, 2011). As a result, compared to other OECD countries, the distribution of working hours in Spain is quite wide and features a sharper dip in the middle of the day (see, e.g., Amuedo-Dorantes and de la Rica 2009). The way the working day is organized can have far-reaching implications. For example, while there is compelling evidence that parental time is important for a child’s cognitive development (e.g., see Del Boca et al. 2014), results in Rapoport and Le Bourdais (2008) indicate that working between 5 pm and 7 pm substantially reduces the time that parents spend with their children. More generally, working split shifts complicates the scheduling of family activities, which might have a bearing on the fact that the Spanish employment gender gap is one of the highest among OECD economies (Guner et al. 2014). Furthermore, ARHOE (2013) suggests that split shift workers may sleep substantially less than comparable straight-shifters, and insufficient sleep can impair the worker’s productivity and psychological well-being (Akerstedt et al. 2009). Productivity, in turn, is one of the key determinants of wages. The purpose of this paper is to examine empirically the effects that working split shifts have on Spanish workers’ well-being, time use, and productivity (hourly wages). The study adds to the literature in several regards. For the US and Canada, Presser (e.g., 1988, 1994), Kostiuk (1990), and Williams (2008), among others, examine the prevalence and consequences of shift work, which they define as anything other than a regular daytime schedule. But since very few US and Canadian workers work split shifts, they do not specifically study split schedules.Footnote 1 In contrast, this paper focuses on the split schedule, which is the normal daytime work schedule in Spain. Amuedo-Dorantes and de la Rica (2009) look at the effect of working split shifts on wages, and show that full-time workers are not compensated with higher wages for having such a schedule. But wage gaps may also result from productive characteristics, which offer an avenue for exploring the existence of differences in productivity across work schedules. ARHOE (2013) does contain a theoretical analysis of the consequences of the split work schedule for workers’ time use and productivity, while the current paper studies these issues empirically. One important limitation of this study is that the work schedule may not be entirely the result of a “random assignment”. For example, individuals with a strong preference for having free time in the early evening could select themselves into sectors of employment, occupations, or even companies with widespread straight-shift jobs. This would be also the case of more able individuals if working straight shifts were considered to be more convenient (Amuedo-Dorantes and de la Rica 2009). Not taking into account the circumstances underlying the “assignment” of shift type could lead to biased effects of the work schedule. Fortunately, the data set used in this paper, the 2002–2003 Spanish time use survey, does contain information on the worker’s sector of employment, industry, and occupation, so that these characteristics can be held fixed in the analyses. However, the degree to which the worker’s company allows them to choose their schedule is unknown.Footnote 2 Although Amuedo-Dorantes and de la Rica (2009) instrument an employee’s schedule by his/her partner’s work schedule, the presence of assortative mating on unobserved preferences for leisure or on unobserved ability (e.g., Hamermesh 2002; Goux et al. 2014; Greenwood et al. 2014) would invalidate the proposed instrument. The same would occur with the size of the worker’s firm if workers selected themselves into firms based upon the prevalence of shift types. These concerns limit the usefulness of an instrumental variable approach, which is not used in the current analysis. The results suggest the existence of an increased feeling of being at least sometimes overwhelmed by tasks and not having enough time to complete them among female split-shifters. Holding other factors fixed, working split shifts increases average incidence of being overwhelmed for female full-time employees by 12 %. On working days, and for both men and women, a split work schedule is positively related to time spent working, eating and drinking, and sleeping, and negatively associated with time spent on housework, parental child care, and leisure activities. Again controlling for other factors, workers on split shift work about 37 min more per day (about 3 h per week) than workers in straight shift. The productivity of workers on a split schedule appears to be lower than that of comparable straight-shifters: split shift is associated with a 5.3 and 7.4 % hourly wage penalty for females and males, respectively. The remainder of the paper is organized as follows. Section 2 describes the data and methods used. Section 3 presents the results. Section 4 provides some concluding observations. The Appendix at the end of the paper contains additional descriptive statistics, whereas the complete estimation output is presented in the Online Appendix.",3
6.0,2.0,SERIEs,10 May 2015,https://link.springer.com/article/10.1007/s13209-015-0124-3,"Migration, health knowledge and teenage fertility: evidence from Mexico",June 2015,Marianna Battaglia,,,Female,Unknown,Unknown,Female,"Far from being a new phenomenon, Mexican migration to the United States has seen increasing flows of migrants over time. In both absolute and relative terms, their number has been larger than any other immigrant influx in the past century, corresponding nowadays to approximately seven million people and therefore 31 % of the foreign born population (U.S. Census Bureau 2006).Footnote 1 According to the 2000 Mexican Census, from 1995 to 2000, 4.1 % of all Mexican households saw at least one family member migrating to live in the United States, while an additional 1.8 % of households had family members migrating back and forth between the two countries or returning to Mexico (INEGI 2000). Overall, almost 10 % of the population born in Mexico now resides in the United States (UNDP 2007).Footnote 2 This paper aims at investigating the impact of this international migration on occurrence of pregnancy and child health care of women, especially teenagers, remaining in Mexico. My results show that having at least one household member who migrated to the United States decreases the occurrence of pregnancy among teenagers by 0.339 probability points. The impact observed is very large, representing a decline to 0.09 from the sample mean. This finding can be partially explained by the fact that teenagers in migrant households have a higher knowledge of contraceptive methods and likely practice active birth control. Being in a migrant household increases the knowledge of at least one contraceptive method among teenagers by 0.307 probability points on average. Less significant results are obtained for child care outcomes: being in a migrant household only raises the likelihood of the child to be delivered by a doctor in the sample of all women, especially in rural areas. This probability is estimated to increase by 0.569 probability points on average. I focus on teenage women because delaying onset of fertility is still a policy priority in the country. Although fertility rates have decreased considerably in the last decades—the number of children per women of childbearing age decreased from 6.8 in 1970 to 2.2 in 2006Footnote 3—adolescent fertility rates have declined much more slowly. In 2005, the pregnancy rate for women between 12 and 19 years old was still 79 out of 1000, in public institutions 21 % of children were born to women under the age of 20 (CONAPO 2007), and the proportion of teenage mothers was around 17 % (INEGI 2005).Footnote 4 Adolescent pregnancy and childbearing are associated with a range of adverse health outcomes, including high risk of pregnancy-related diseases, unsafe abortion practices, and maternal mortality.Footnote 5 Adolescent mothers are also less likely than women aged 19–23 to use either antenatal or delivery care and have their infants immunized (Reynolds et al. 2006).Footnote 6 Moreover, especially among the poor, adolescent childbearing is linked with lower future monthly earnings for mothers, and contributes to the persistence of poverty from one generation to the next, thus affecting long-term human capital accumulation and growth. Adolescent child bearers are themselves often born to adolescent mothers: in Mexico two-thirds of adolescent mothers have mothers who gave birth in their teens (Buvinic 1998). The major concern in this study is the endogeneity of migrant status: family planning decisions can be related to the characteristics of migrant households themselves. The direction of the potential selection of individual into migration is a priori unclear. It is difficult to disentangle the extent to which the impacts on occurrence of pregnancy and child health care reflect the unobserved characteristics of migrants, their households or their communities as opposed to the migration experience itself. I tackle the problem by using an instrumental variable method. Potential migration—measured as historic migration levels interacted with the proportion of adult males in the household—is used as instrument for current migration. Historically, migrants have mainly come from the central-western region of the country. The states of Jalisco, Michoacán, and Guanajuato, which together accounted for about 30 % of all Mexico-U.S. migrants throughout the twentieth century, are still among the principal sending states. In order to obtain variations at the household level, 1924 migration levels are interacted with the proportion of adult males in the household: in the Mexican context, the likelihood that at least one household member migrates increases with the proportion of males in the household. The principal channels through which migration is expected to affect occurrence of pregnancy and child health care are wealth, exposure to the host country’s norms and knowledge transmission to the sending country, and cost of migration. First, remittances sent back from migrants likely change household income and allow families to spend resources on different categories of expenditure, such as education and child health care. Higher levels of educational attainment and health investment are expected in households experiencing migration and receiving remittances, with consequences on human capital accumulation and family planning decisions (Cox et al. 2004). Access to education by adolescent girls may delay the onset of fertility (Breierova and Duflo 2004) and increase the opportunity cost of women’s time, thus leading to changes in preferences regarding the quality and quantity of children (Becker 1960) and endowing girls with a better ability to process information, which, in turn, potentially increases knowledge of contraception options (Rosenzweig and Schultz 1987; Duncan et al. 1991).Footnote 7
 Second, through exposure to the host country’s practices, migrants acquire health knowledge and become more familiar with the use of contraceptive methods. In turn, they provide examples of behaviors that may be considered and copied by other family members at home. As much of the literature suggests (Fargues 2006; Beine et al. 2013; Lindstrom and Munoz-Franco 2005), migrants’ behavioral norms tend to converge to those of their host country. Through the process of living and working in the receiving country, migrants acquire information and become aware of alternative models of gender roles and family relationships that they may accept and adopt. This assimilation process—a change in individual preferences—is gradual. The impact of the host country’s values and norms on migrants’ behavior increases with the length of migration: the longer a migrant has been in the receiving country, the more her outcomes are similar to those of a native. The adjustment in fertility behavior, however, also occurs in response to the economic opportunities and constraints present in the country of destination, and can therefore take place in the short term. Both processes happen simultaneously so it is challenging to disentangle the effects of assimilation—change in preferences—from those of adaptation—adjustment in fertility behavior due to economic opportunities and constraints (Lindstrom and Saucedo 2002). What is observed is that migrants share similar economic and institutional environments with natives and acquire information and behavioral norms of the country of residence. Fertility rate of the Mexican-born population in the United States is converging towards the non-Hispanic U.S. fertility rate (IPUMS 2010). Information and behaviors that flow from receiving to sending country communities can represent an additional pathway through which migration affects health outcomes and fertility behaviors. Kovsted et al. (2002), for instance, suggested that although maternal education is important in determining child health and mortality, its effect diminishes or disappears when health knowledge is introduced as an explanatory variable. Health knowledge crowds out the effects of mother’s education and is thus not necessarily associated with the income or education of parents. The information and behaviors obtained through the experience in the receiving country therefore have the potential to result in less risky sexual behaviors and—in the long run—in better health outcomes also in the sending country.Footnote 8 The strong family ties of Mexican society and the recurrence of Mexican migration to the United States make the role of migration networks as a transfer of behavioral norms from the host to the migrants’ home country highly plausible. Third, the migration of a household member decreases the cost of migration for other family members and women, especially teenagers, can decide to delay pregnancy in anticipation of future migration. Having at least one household member who migrated to the United States increases the likelihood that another member who remained in the home country will try to migrate. Thus, women who do not want to get pregnant in Mexico, as this would increase their cost of migration, will acquire more knowledge of contraceptive methods and likely practice active birth control. My data do not allow to provide a complete decomposition of the channels through which migration matters for the occurrence of pregnancy and child health care. I show some evidence of the increased knowledge of contraceptive methods among teenagers in households that experienced migration to U.S. and I argue that these girls may be better endowed to process information (thanks to higher access to education-wealth effect), decide to delay pregnancy and actively acquire more information (due to the reduced cost of migration) and are more exposed to behavioral norms of their household members’ host country (knowledge transmission to the sending country). Knowledge of contraceptive methods is therefore used as a measure of more general health knowledge. So far the literature has not largely investigated the interaction between migration and health outcomes or, even more specifically, sexual behavior. When it has, it mainly focuses on the potential effects in the country of destination, such as access to welfare benefits (Bertrand et al. 2000), health services (Menjivar 2002; Deri 2005; Devillanova 2008), and fertility (Chou 2011), but little is said about the impact of migration on these outcomes in the sending region. There are some recent contributions in this direction. Beine et al. (2013) examined the relationship between international migration and source country fertility and provided evidence of a strong transfer of fertility norms from migrants to their country of origin. Analogously, in their study on rural Guatemala, Lindstrom and Munoz-Franco (2005) found that contraceptive use increases and fertility falls with variables such as having family members in urban or international destinations and living in a community where urban migration is common. De (2013) showed that women in Mexico belonging to migrant families have a higher propensity to use modern contraceptives. Kanaiaupuni and Donato (1999), Frank and Hummer (2002), Hildebrandt and McKenzie (2005) used the same Mexican data to investigate the effects of migration on infant mortality and birth weight, finding positive effects for children living in households with at least one migrant. In Pakistan, Mansuri (2006) found that migration has a positive impact on early child growth outcomes, especially for young girls. My paper is related to this strand of the literature and attempts to provide a first, in-depth investigation of the impact of migration on occurrence of pregnancy in the country of origin, especially for adolescent women for whom delaying onset of fertility is still a policy priority in many countries. The paper is structured as follows. Section  2 provides information on the data used in the study. The empirical strategy is presented in Sect.  3. Section  4 illustrates and interprets the results. Conclusions are drawn in the last section.",4
6.0,2.0,SERIEs,20 May 2015,https://link.springer.com/article/10.1007/s13209-015-0123-4,Fixed income strategies based on the prediction of parameters in the NS model for the Spanish public debt market,June 2015,Julián Andrada-Félix,Adrian Fernandez-Perez,Fernando Fernández-Rodríguez,Male,Male,Male,Male,"The study of the Spanish public debt is of special interest in the panorama of the recent sovereign debt crisis in May 2010 that troubled European economies and threatened stability and unity in the Eurozone. In 1997 the Spanish treasury prioritized the achievement of a more liquid and efficient public debt market, undertaking a set of initiatives aimed at attracting investor savings within the new capital market, providing greater depth and liquidity, decreasing bond yield volatility and increasing pricing efficiency. This was done through two channels: first, an appropriate exchange policy ensured an adequate tradable supply of bonds priced near par (at the expense of premium bonds, which some classes of investors avoid). Second, debt exchanges increased the outstanding amounts of strippable bonds that were critical in supporting bond dealer stripping and reconstitution operations in the new strips market (see Díaz et al. 2006 for details). However, since summer 2010 the Spanish debt market, as in other Eurozone periphery countries, suffered a sharp escalation of risk premia. During the recent sovereign debt crisis in the Eurozone, Spain reflected investors’ perceptions of risks or uncertainties about the Spanish economy and raised important concerns about the possibilities of contagion to the global financial system, due to the size and importance of its economy (see Gómez-Puig and Sosvilla-Rivero 2014). Although prior to the crisis Spain had a low level of debt, in comparison with other developed economies, from late 2009 fears of a sovereign default developed among investors in Spanish public debt, in view of the growing volume of private debt, arising in turn from a property bubble. In a disruption scenario within the European interbank market, this situation deteriorated banking system balance sheets and provoked a downgrading of government debt by the international rating agencies. In these circumstances, the escalating yields paid on Spanish public debt increased the interest margin of these securities relative to the interest cost of bank deposits, making them a very attractive fixed income asset and providing additional option value for Spanish banks before and after the beginning of the financial crisis (Pérez-Montes 2013). Thus, although term structure literature mainly focuses on the government bond markets of the most highly developed European economies, the analysis and better comprehension of a peripheral country of the Economic and Monetary Union, such as Spain, may be of considerable interest for policymakers, academic researchers and, especially, for international investors, who need to be aware of potential profit opportunities in the Spanish debt market. The yield curve or term structure of interest rates is the relation between the (level of) interest rate (or cost of borrowing) and the time to maturity of the debt for a given borrower in a given currency. The yield curve forms the basis for the valuation of all fixed income instruments, because the price of a fixed income security can be calculated as the net present value of the stream of cash flows, and each cash flow has to be discounted using the zero coupon interest rate for the associated term to maturity. The term structure of interest rates as the interception of macroeconomics and finance has increasingly been employed as a means of explaining the upward slope of the yield curve and the bond premium puzzle [see Gürkaynak and Wright (2012) for a survey on macroeconomics and the term structure or Lange (2013) and Pericoli and Taboga (2012) for recent specific examples]. Accordingly, fixed income portfolio managers, central bankers and market participants apply econometric models to achieve a better representation of the evolution of interest rates, in the view that these models are useful decision-orienting tools for their purposes. The Nelson and Siegel (1987) model (NS hereafter) is an exponential component framework with four parameters by which the yield curve can be estimated parsimoniously; these parameters have the economic interpretation of level, slope, curvature and speed of convergence to long term rates. The NS model provides parametric curves that are flexible enough to describe a whole family of observed term structure shapes and is consistent with a factor interpretation of the term structure (Litterman and Scheinkman 1991). In addition to the factors present in the NS model, the Svensson (1994, (1996) model contains a second hump/trough factor which allows for an even broader and more complicated range of term structure shapes. The NS model has been extensively used by central banks and monetary policy makers (Bank for International Settlements 2005; European Central Bank 2008) for more than two decades. It is also used by fixed-income portfolio managers that wish to immunize their portfolios (Barrett et al. 1995; Hodges and Parekh 2006). 
Diebold and Li (2006), taking an explicit out-of-sample forecasting perspective of the term structure of interest rates, showed that the three-factor NS model, where the factor measuring the speed of convergence is fixed beforehand, can also be used to construct accurate term structure forecasts by considering it as a dynamic yield curve model that is capable of capturing its time-varying shape. These authors considered the practical problem of forecasting the yield curve by studying variations on the NS framework to model the entire yield curve, period by period, as a three-dimensional parameter evolving dynamically. By using a straightforward two-step estimation procedure, they generated term-structure forecasts for both the short and the long term, observing that their forecasts appear to be much more accurate for long horizons than are several standard benchmark forecasts. The present study has two main goals: on the one hand, to examine the predictive possibilities of the yield curve for the Spanish public debt market, using the methodology proposed by Diebold and Li (2006). On the other hand, to consider the capability of generating profits from yield curve predictions, transforming them into technical trading strategies. The main contribution of our paper is that the trading strategies presented outperform benchmark hedging strategies for long (1 year) horizons in our prediction period (2000–2010) and specifically during the current crisis period (2008–2010). Nevertheless, these strategies do not outperform the benchmarks for short (1 month) horizons. A further point of interest is that the introduction of non-parametric models improves the profitability of the level, slope and curvature strategies in terms of Sharpe’s ratio, especially in 1-year-ahead predictions. This finding is in line with Diebold and Li (2006), whose forecasts are much more accurate for long horizons than are those of several standard benchmark models.",
6.0,3.0,SERIEs,07 May 2015,https://link.springer.com/article/10.1007/s13209-015-0126-1,Minimum coverage regulation in insurance markets,August 2015,Daniel McFadden,Carlos Noton,Pau Olivella,Male,Male,Male,Male,"A widespread regulation in the private health insurance industry is the existence of a minimum standard, which puts a lower bound on the coverage that can be offered to agents in different services. Most states in the US consider legal mandates for health insurance in the individual and small group markets.Footnote 1 Importantly, there are large differences in both the number of mandates across states and their estimated cost. Figure 1 depicts these facts and is based on Keating (2011) and Bunce and Wieske (2008).Footnote 2 How stringent the legislation should be is of a great importance since US authorities have to establish the minimum standards nationwide as signed in the federal legislation. Mandates and cost heterogeneity across States in the US As it turns out, this regulation comes at the expense of low risks. Hence it is often accompanied by mandatory enrollment laws, whereby all individuals are forced to pick one of the outstanding contracts in the market. This indeed is the case in Patient Protection and Accountable Care Act of 2010 (PPACA)Footnote 3 that is in the process of implementation at the time of writing this paper. One of the arguments for such regulation is the underprovision of coverage for a large segment of the population. This phenomenon can be caused by several reasons,Footnote 4 but here we focus on the presence of asymmetric information between insurers and insurees, an issue that has attracted a great deal of attention for more than thirty years. Since the seminal work of Rothschild and Stiglitz (1976) (henceforth RS), it is well known that when individuals have privileged information on their own health risks, the market will respond by providing a set of contracts, one intended for low risks with low coverage and low premium, and the other contract intended for high risks with full coverage and high premium. The fact that high risks are forced to pay a high premium is seen as unfair to many analysts and regulators. Hence, many researchers have been devoted to find ways to regulate this market in order to implement some degree of cross-subsidization. An extreme form of such cross-subsidization is present in a pooling equilibrium, where all risks obtain the same coverage at the same premium, regardless of their risk.Footnote 5 RS also showed that no such pooling equilibrium can exist in the absence of regulation, since one of the insurers can profitably deviate by offering a contract with a slightly cheaper premium and lower coverage, which will only attract the low risks. This action is labeled as “cream skimming” (also known as “cherry picking”). Our aim is to determine whether a minimum coverage legislation (henceforth MCL) can implement some degree of cross-subsidization among different risks. The idea is that undesirable cream skimming deviations might be ruled out through such legislation. As mentioned above, since cross-subsidization comes at the expense of low risks, such legislation is often accompanied by mandatory enrollment laws. Using the model of RS as a benchmark, we show that the effects of MCL drastically depend on how demanding this regulation is. In a nutshell, our main result is that a weak MCL could bring an unexpected result. Namely, insurers might increase their profits while all types of individuals might be worse off. In other words, a weak MCL may result in individuals subsidizing the insurers rather than low-risks subsidizing high-risks. In contrast, a sufficiently stringent MCL can indeed restore the desired cross-subsidization from low risks to high risks while all insurers make zero profits. We focus on single contract competition as evidence suggests that it may be difficult for a single insurer to implement a perfect screening menu by itself. We do not claim that single contract competition holds in every market, but that there is evidence for this being an accurate description in some markets. We provide empirical support for this assumption in Sect. 5. Regarding the relevant literature, the closest paper to ours is by Neudeck and Podczeck (1996) (henceforth NP). They were the first authors to point out that a weak MCL could have perverse effects. However, our analysis and results differ from theirs in several respects. First and foremost, their results are less dramatic than ours. Namely, they focused on an equilibrium where only insurers attracting low risks make positive profits (p. 400), whereas we show existence of an equilibrium where all insurers make positive profits and all individuals are worse-off –even the high risks– as compared to the laissez faire. Second, their result is based on the use of a non-Nash equilibrium notion, namely, Grossman Equilibrium, a point that we return to below, whereas we stick to the Nash concept.Footnote 6 Finally, their prediction on the equilibrium market structure is quite imprecise. Except from exhibiting a separating equilibrium, there is no prediction regarding how many insurers are offering each contract in the separating set. In contrast, we are able to predict a unique market structure that is fully spelled out below.Footnote 7
 The comment on NP by Encinosa (2001) also focuses on MCL. Instead of Grossman’s equilibrium notion, he takes two independent routes to restore equilibrium. The first one is to use the so-called Wilson–Miyazaki–Spence (WMS) equilibrium notion.Footnote 8 The second one is to stick to the Nash equilibrium notion but assume that (i) insurers offer contracts in a limited amount (or “capacity”, in his terminology) and that (ii) there is a sufficiently large proportion of high risks in the population.Footnote 9 He concludes that, under both alternatives, there is a menu equilibrium that is second best and where insurers make zero profits. We prefer to stick to the Nash equilibrium notion and not assume any capacity constraints. Let us now present our results in detail. We make two working assumptions. First, an exogenous number of insurers strictly larger than 2 serve this market.Footnote 10 Second, the proportion of low risks is below the threshold for existence in the RS model, which we refer to as the RS threshold. This ensures existence of a laissez faire equilibrium. As in RS, we have a two stage game. In the first stage, insurers offer their contracts simultaneously. In the second stage, individuals choose one of the outstanding contracts in the market. By backward induction, once the optimal choice by individuals among any possible profile of contract offers has been determined, we use the Nash equilibrium notion to find the equilibrium set of contracts. Hence, we do not restrict deviations by an insurer to be robust to further deviations by other insurers.Footnote 11 Also, unlike Grossman’s notion, we do not allow insurers to withdraw contracts that were previously offered.Footnote 12
 In the absence of a binding MCL, the standard separating equilibrium of RS arises. As soon as the MCL becomes binding, there exists an open set of parameter values such that there exists an equilibrium where all insurers make identical strictly positive profits. This equilibrium is still separating and coexists with the equilibrium studied by NP for any given vector of parameter values in the aforementioned open set. In both equilibria, a single insurer, which we name “the scapegoat” for reasons that will become clear below, attracts all the high risks in the population while the rest of insurers “free ride” on the scapegoat to obtain profits at least as large as the scapegoat’s. In the NP equilibrium, the high risks enjoy the same contract as under laissez faire, which in turn coincides with the one that obtains under symmetric information. In the new equilibrium that we find, the high risks enjoy the same coverage as under laissez faire but pay a higher premium. Hence the scapegoat obtains positive profits as well. In contrast, if the MCL is sufficiently demanding, then a pooling equilibrium, where all insurers offer the same contract, is the only possible equilibrium. In this equilibrium all insurers make zero profits and the desired cross-subsidization from high to low risks is attained. Obviously, as compared to the laissez faire, low risks are worse off and high risks are better off. Notice however that the low risks are always worse-off no matter how stringent the MCL is. The intuition for the result arising under a weak MCL is the following. The same legislation that impedes cream skimming deviations also has a severe anti-competitive effect. Suppose all free riders make positive profits. A free rider trying to undercut his free-rider rivals can only do so by decreasing his premium, due to the MCL. This breaks separation and the deviation becomes unprofitable. What is new in our analysis is the following additional intuition. Suppose that the scapegoat also enjoys positive profits. Obviously, he is not going to undercut himself. If he tries to undercut the free riders then again separation is broken and the deviation becomes unprofitable. Lastly, we need to ensure that a free rider does not want to undercut the scapegoat. Hence our result that in equilibrium, each free rider obtains no less profits than the scapegoat. This (perhaps) justifies our terminology. To sum up, our contribution is two fold. First, we are able to sustain the equilibrium studied by NP without having to resort to non-Nash equilibrium notions. Interestingly, this allows us to be much more precise in our prediction of the market structure that will arise. Second, we show that this market structure is compatible with other equilibria where also the insurer serving the high risks makes positive profits.Footnote 13
 We have also analyzed a variation of the game described above where a large set of potential insurers, in the first stage of the game, not only choose their contract but also whether to offer a contract at all. If they do offer a contract they must bear some fixed (entry) cost.Footnote 14 Hence the number of insurers becomes endogenous. Unfortunately, in such a model, Nash Equilibria in the entry stage never exist under laissez faire. Interestingly, however, introducing a MCL may allow for the existence of such equilibria. The idea is that, as mentioned above, there exists a middle range of MCL where a finite number of insurers obtain positive (variable) profits. This allows these insurers to recover the entry costs and it is possible to sustain an equilibrium that is separating with the structure described above (one scapegoat and at least two free riders). The paper is organized as follows. In Sect. 2 we introduce the game and the equilibrium notion and we present the benchmark case of RS. In Sect. 3 we solve the game. In Sect. 4 we introduce the game where insurers choose whether to enter or not and sustain equilibria for a range of minimum coverage levels. Section 5 presents empirical evidence supporting our model. Section 6 concludes. Proof of all lemmas and propositions are relegated to the appendix.",2
6.0,3.0,SERIEs,26 July 2015,https://link.springer.com/article/10.1007/s13209-015-0129-y,Distortions and the size distribution of plants: evidence from cross-country data,August 2015,Manuel García-Santana,Roberto Ramos,,Male,Male,Unknown,Male,"Why do some countries have so low levels of income per capita? Why, for instance, income per capita in Nepal is only 2.5 % that of the United States? A common view is that a high proportion of income variation across countries can be attributed to differences in total factor productivity (TFP).Footnote 1 Moreover, a recent strand of literature has started to emphasize misallocation of resources across plants as a source of these differences in aggregate productivity.Footnote 2 This literature emphasizes that the existence of distortions favoring small low productivity firms and hindering large high productivity firms makes the economy deviate from its first best. Any distortion that leads to too many resources being allocated to relatively small unproductive firms makes aggregate productivity fall. As a result, the size distribution of firms becomes a crucial object in understanding the aggregate productivity of a country. Although these recent works on misallocation have provided valuable insights on the impact of distortions on how resources are allocated across firms, our understanding of the underlying factors driving the variation across countries in the firm size distribution remains somehow limited. In this paper, we empirically investigate the cross-country relationship between the amount of labor allocated to small plants and a number of economic distortions using plant-level data: the Enterprise Surveys of the World Bank 2006–2010 (ESWB). This dataset has three main advantages. First, it is standardized. This means that every plant in every country answers the same questions, allowing for comparability across countries. Second, coverage is very broad (more than 100 countries), which gives us power to validate the statistical significance of our findings. And third, the sample of surveyed plants is representative of the population of formal private non-agricultural plants. This allows us to establish some facts about the allocation of resources beyond manufacturing. Figure 1 shows that there exists substantial variation across countries in the share of labor allocated to small plants. To motivate our analysis, we first show that there exists a strong relationship between the size distribution and productivity at the aggregate level: countries with a higher amount of labor allocated to small plants have lower levels of income per worker and TFP. We then show that high economic distortions, as measured by the Doing Business Index, are systematically associated to a higher amount of labor allocated to small plants. This is so after conditioning for other determinants of the size distribution, such as the size of the informal sector, amount of FDI, and presence of export firms. We address endogeneity concerns by instrumenting the economic distortions with variables argued in the literature to provide exogenous variation in institutions. We also explore the different components of the business environment driving the previous result. We decompose the Doing Business Index into measures of access to credit, tax rates, costs of entry, rule of law, trading easiness, and corruption. We find that, when we include all these components together, access to credit is the one driving our results. Share of labor accounted by small plants. The share of employment accounted by plants of less than 20 employees, computed from the Enterprise Surveys of the World Bank (2006–2010). The cutoffs are obtained by classifying the sample in three groups of the same size From a theoretical point of view, the sign of the relationship between TFP and the share of employment accounted for by small plants is ambiguous, since it depends on the type of model and the nature of the distortions studied. This comes from the fact that the amount of misallocation and its relationship with the prevalence of small plants is also model specific. However, in the type of models and distortions that have been analyzed in the recent misallocation literature, this latter relationship is generally positive, implying a negative association between TFP and the amount of resources allocated to small plants. This is the case, for instance, in models of occupational choice à la Lucas (1978) under the presence of distortions that impose restrictions on firms’ size, see for instance Guner et al. (2008) and García-Santana and Pijoan-Mas (2014). This is also the case in the model used by Hsieh and Klenow (2009) when firms’ idiosyncratic taxes are positively correlated to productivity. In their model, this positive correlation implies that more productive firms are inefficiently small, whereas small unproductive firms are inefficiently large. This has actually been found to be the case when carrying out the Hsieh and Klenow (2009) exercise for a large set of developing countries, see Busso et al. (2013). Finally, Bartelsman et al. (2013) find substantial cross-country variation in the covariance between size and productivity. In particular, they find that this covariance is generally lower in poor countries. This suggests that the higher presence of distortions in low-income countries biases the size distribution of plants towards small unproductive units.
 The rest of the paper is organized as follows. Section 2 discusses the related literature and places the paper within it. Section 3 explains in detail the characteristics of our dataset. Section 4 illustrates the relationship between size and productivity at the aggregate level. Section 5 shows that economic distortions significantly explain part of the cross-country variation in the amount of labor allocated to small plants. Section 6 analyzes the role of particular distortions. Finally, Sect. 7 gives concluding remarks.",11
6.0,3.0,SERIEs,15 July 2015,https://link.springer.com/article/10.1007/s13209-015-0127-0,External investigations and disciplinary sanctions against auditors: the impact on audit quality,August 2015,Cristina De Fuentes,Manuel Illueca,Maria Consuelo Pucheta-Martinez,Female,Male,Female,Mix,,
6.0,3.0,SERIEs,24 July 2015,https://link.springer.com/article/10.1007/s13209-015-0128-z,Voting over law enforcement: mission impossible,August 2015,Hakan İnal,,,Male,Unknown,Unknown,Male,"Law enforcement is among the key elements of a civil society that ensures the achievement of a higher social welfare. Since Becker (1968) has introduced an economic analysis of law enforcement, there has been a vast literature on economics of crime and punishment.Footnote 1 In public enforcement of law models, the state determines the level of enforcement expenditures, hence the probability of detecting violators, and fines for harmful acts. Then, the law enforcement monitors the activities of agents, detects and fines the violators. As the enforcement expenditures increase, the likelihood of violators being detected and fined increases. There are various ways the state can determine the enforcement expenditures as well as fines. One recent approach is to use majority voting. In the majority voting, policy x is socially preferred to policy y if there is a majority of voters who prefer x to y. The median voter theorem states that if preferences satisfy single-peakedness, then the most preferred alternative of the resulting social preference coincides with the most preferred alternative of the median voter (Black 1948; Moulin 1980). An alternative characterization of the aforementioned median voter theorem is given in Gans and Smart (1996) using the single-crossing property on preference profiles. Traxler (2009, 2012), Besfamille et al. (2013), and Bethencourt and Kunze (2015) apply the median voter theorem to the tax enforcement problem to determine the enforcement level. In the context of enforcement of illegal immigration, Garca (2006), and Facchini and Testa (2014) use the median voter theorem to determine the enforcement level. These aforementioned authors either use single-peaked preferences assumptionFootnote 2 or single-crossing propertyFootnote 3 of preference profiles to be able to apply the median voter theorem. In this paper, I question the applicability of the median voter theorem to the law enforcement problem to determine either one of the two dimensions of the enforcement level. I show that the use of preferences over enforcement levels (enforcement expenditures as well as fines) has important limitations. I derive individual equilibrium preferences over enforcement levels (enforcement expenditures and fines) in a specific enforcement environment (see Polinsky and Shavell 1984) and over enforcement expenditures in a general enforcement environment (see Polinsky and Shavell 1992). In the specific enforcement model, there is one harmful act agents can take, and the enforcement agency monitors agents, detects violators with some probability, and fines them. On the other hand, in the general enforcement model, there are various acts, and each agent commits one of these acts. The enforcement agency monitors all of these acts, detects violators of all kinds with some probability, and fines them at levels depending on harms caused by their acts. In an enforcement equilibrium (of both of these models), given the enforcement level (fines and the enforcement expenditure, which determines the level of monitoring/detection), each agent decides whether to engage in the harmful act. Given the level of enforcement, an agent’s expected utility depends not only on his own action but also on actions of other agents. This is because other agents’ actions determine the harm he faces, and affect fine revenues used to finance public enforcement along with taxes collected. Example 1 in Sect. 2 gives a range of agents’ benefits (from the harmful act) who have double-peaked preferences, violating the single-peakedness condition sufficient for the existence of a majority voting equilibrium in the median voter theorem. Moreover, single-crossing property of preference profiles, which is also sufficient for the existence of a majority voting equilibrium, is also violated. The example is quite general and hence it provides insight to problems with these assumptions in law enforcement models. Theorem 1 shows that in the specific enforcement model, in any society it is impossible to have agents with opposite equilibrium preferences over enforcement expenditure or fine level, i.e., in equilibrium, if there is an agent who prefers higher enforcement expenditure (fine level) to less, then there cannot be another agent in the same society who prefers less enforcement expenditure (fine level) to a higher one. In the general enforcement model, Theorem 2 shows that in any society it is impossible to have agents with opposite equilibrium preferences over enforcement expenditure, i.e., in equilibrium, if there is an agent who prefers higher enforcement expenditure to less, then there cannot be another agent in the same society who prefers less enforcement expenditure to a higher one. In order to determine the enforcement levels (enforcement expenditures and fines), alternative objectives for the state have been proposed in the economics of law enforcement literature. For example, in Becker (1968) the objective of social planner is to minimize social cost, which consists of harms caused by agents, costs of detection, and the costs of punishment to the criminals less gains of criminals. Agents and their choices are implicitly explained, and they get utility from consumption, and face uncertainty because of the possibility of getting caught. Cooter and Ulen (2008) (p.510) suggest that the aim of the law maker is to minimize social cost but it consists of costs of protection and the net harm caused, i.e. social loss, while the crime is committed. In Polinsky and Shavell (1984),Footnote 4 on the other hand, the objective of the law maker is to maximize total expected utility of agents in the society. The expected utility of an agent consists of his gain from engaging in the harmful activity, expected loss due to the possibility of getting caught, expected loss due to the possibility of being a victim of a crime, and the per capita cost of enforcement. If the agent does not engage in the harmful activity, then the expected utility will not include the gain from engaging in harmful the activity, and the expected punishment. In Sect. 2, the specific enforcement model, including the formal definitions of enforcement equilibrium, equilibrium expected utility, and the results in this environment are given. Example 1 in Sect. 2 shows violations of single-peakedness and single-crossing property. General enforcement model and the main result in this model are given in Sect. 3.",
6.0,4.0,SERIEs,26 November 2015,https://link.springer.com/article/10.1007/s13209-015-0136-z,Measuring expectations from household surveys: new results on subjective probabilities of future house prices,November 2015,Olympia Bover,,,Female,Unknown,Unknown,Female,"This lecture is concerned with household subjective expectations. Its central theme is the analysis of new data on subjective probabilistic expectations on house prices collected in the Spanish Survey of Household Finances (EFF). As a front-end, I first provide a review of the methodology of expectation measurement and of some recent work that use household subjective probabilities. Finally, as a back-end I provide some results on how subjective expectations matter for predicting consumption behavior. Despite widespread agreement on the fundamental role of expectations in explaining behavior, direct measurement of individual expectations is a relatively recent activity. The standard practice in the economics of the last century was to infer the individuals’ decision process from their observed choices. Following this revealed preference analysis, both preferences and the uncertainty about the future are identified from data on choices and market outcomes alone. Such strategy requires strong assumptions. For example, assuming individuals have rational expectations as well as knowledge of the model may be needed despite that this has often not been credible. In his seminal paper Manski (2004) strongly advocated for collecting self reported expectation data and using those jointly with observed choice data. The hope is this would improve economists’ credibility and ability to predict behavior. But are household expectations collected through surveys trustworthy? Do subjective household survey expectations really improve the ability to predict behavior? To help put these questions in context, I begin by reviewing basic concepts of the methodology of expectation measurement as well as recent work on the elicitation and use of household subjective expectations. The EFF is a representative survey of the Spanish population that contains detailed information on household assets, debts, income and consumption. Data have been collected every three years since 2002. Starting in 2011, the EFF introduced a new question to elicit household house price probabilistic expectations. Households were asked to distribute ten points among five different scenarios concerning the price change of their homes over the next 12 months. In this way respondents provide information not only about point expectations but also about the probabilities they assign to different future outcomes. One motivation for introducing this question in the EFF is the importance of real estate assets in the wealth of Spanish households (80 % of the value of household assets) all along the wealth distribution (88 % for the bottom quartile and 67.5 % for the top decile). Aside from a high proportion of owner occupier households (83 %), 36 % of Spanish households hold some other real estate property. It is also a timely question due to the housing market collapse that shattered house price expectations after 2007 in Spain. The number of households buying housing dropped dramatically from an overall annual average rate of 2.3 % between 2002 and 2005 to 1.1 % in 2011. According to the data I analyze in this paper, in 2011 over 23 % of households expected a large drop (of over 6 %) in the future price of their homes. Moreover, among households expecting such large drops, the fraction who bought a car was half the fraction in the total population (4.5 instead of 9.4 %). This paper is one of the first empirical studies to document the beliefs of households about the future value of their homes, and the first one that uses a representative sample of households. Questions on probabilistic house price expectations have only recently been introduced in household surveys, as detailed in Sect. 3. Niu and van Soest (2014) have independently obtained results that are complementary to ours using newly collected house price expectations data from the Rand American Life Panel. I start by analyzing patterns of the answers provided by the EFF2011 respondents to the house price probabilistic expectation question to assess the coherency of responses. These include bunching, number of intervals used, and their association with the extent of non-response. Next I model individual probability densities and analyze how the heterogeneity in the individual distributions relates to differences in housing properties and in the characteristics of households. An important result of the paper is that women are more optimistic about the evolution of house prices than men. Being a woman is associated with a positive shift in the median and the quartiles of the subjective distributions. I further examined potential differences in asset valuations by gender by considering self-assessed values of other assets reported in the EFF. I find that women tend to provide higher estimates for the value of their home compared to men but lower ones when it comes to value their financial assets. Location at the postal code level accounts for a large fraction of the variation in the subjective distributions across households. Importantly, in the absence of postal code fixed effects the estimated effects of demographics on house price expectations would be biased. For example, the result on gender would not be found. Moreover, the location effects that emerge from the subjective probability data are meaningful and respond to economic fundamentals. In particular, estimated location fixed effects respond to past local house prices and unemployment rates. Finally, I study whether reported household expectations predict household expenditure decisions. This is of substantive interest to understand household behavior and also a further step in the validation of the house price expectation responses. I exploit the availability in the EFF of information about purchases of secondary housing, cars, other big ticket items, and food. These data allow me to uncover some novel findings about correlations of house price expectations and their uncertainty with those purchases and expenditures. I find that housing investment and car purchases are negatively associated with pessimistic expectations about future house price changes and with uncertainty about those expectations. Moreover, these effects depend on household wealth. Specifically, the negative effects of holding very pessimistic house price expectations on secondary housing purchases are more pronounced at the top of the wealth distribution than at the median, while the opposite is true for car purchases. The paper is structured as follows. In Sect. 2 the work on elicitation and use of household expectations is reviewed. I discuss the specificities in implementing expectation questions in household surveys and the validation of such questions. I also discuss some specific uses of subjective expectations, work on expectation formation, and some enlightening experiments conducted within expectation surveys. Section 3 contains the analysis of the house price expectations data in the EFF. First I describe the formulation of the question and I examine the quality of the responses. Next I estimate a probability density for each respondent, which I use to document the extent of heterogeneity in beliefs. Based on these individual densities I compute various quantiles and measures of dispersion, and study their association with respondent and house characteristics. Finally, Sect. 4 reports the results on the relation between house price expectations and expenditure decisions. I present predictive results for the probabilities of purchasing secondary housing, an automobile, and other big ticket items.",29
6.0,4.0,SERIEs,25 November 2015,https://link.springer.com/article/10.1007/s13209-015-0135-0,Coverage of infertility treatment and fertility outcomes,November 2015,Matilde P. Machado,Anna Sanz-de-Galdeano,,Female,Female,Unknown,Female,"The average age at first birth in the United States has been rising steadily over the past decades, from 21.49 in 1968 to 23.72 in 1985 and 25.26 in 2004. As shown in Fig. 1, this increase has been accompanied by remarkable changes in the age distribution of first-time mothers, which has become less skewed with a substantially higher density of first-time mothers older than 25 and an extension of first-time motherhood beyond the age of 40. Distributions of maternal age at first birth in 1968 and 2004 Women, however, face a biological time constraint on bearing children because fecundity decreases with age. The introduction of and the subsequent increase in the use of assisted reproductive therapies (ARTs) have helped women in extending their reproductive lives (CDC 2007). ARTs, particularly in-vitro fertilisation (IVF), are very expensive procedures. For example, in 1992, a birth from an IVF procedure cost between 44,000 and 211,942 USD (Neumann et al. 1994). Over time, however, ART patients have faced substantially lower costs due to increased competition (Hamilton and McManus 2012), a reduced number of cycles due to better technology,Footnote 1
\(^{,}\)
Footnote 2 and most importantly, the availability of insurance in both the United States and in Europe.Footnote 3 In this paper, we analyse whether easier access to ARTs induces women to delay motherhood and whether, in the long term, it affects women’s completed fertility by the end of their reproductive lives. The perception that ARTs increase fertility has led the European Parliament to call on member states to insure ‘the right to universal access to infertility treatment’ (Ziebe and Devroey 2008). This movement’s incarnation in the United States has sponsored several attempts at approving the ‘Family Building Act of 2009’, which would extend coverage for infertility treatments, and the enactment by several states of infertility insurance coverage laws, which are referred to as infertility treatment mandates.Footnote 4 Considering the high cost of infertility treatments (Bitler and Schmidt 2012; Collins 2001), policy interventions that grant insurance coverage for infertility treatments may affect fertility trends and ultimately, population age structures. The mid- to long-term consequences of ARTs are central to the European debate on possible solutions to an ageing population—i.e., can ARTs be part of a package of policies intended to increase fertility rates in Europe? (Grant et al. 2006; Ziebe and Devroey 2008).Footnote 5
 The answer is complex because the short-term effect of an increase in coverage for infertility treatments may be very different from the long-term effect. In the short term, an increase in the aggregate fertility rate is expected due to an increase in fertility amongst the least fertile women (a compositional effect). Typically, these are relatively old women who delayed motherhood and would be unlikely to conceive otherwise (Buckles 2005; Schmidt 2005a, 2007). Moreover, increased access to ARTs increases the frequency of multiple births in the population (Bundorf et al. 2007). These two effects are short-term and non-strategic and may be referred to as ex-post moral hazard. In the long term, however, easier access to infertility treatments and the possibility of extending reproductive life may induce women to further delay motherhood, possibly because of overly optimistic perceptions about the effectiveness of infertility treatments (Lampi 2006; Benyamini 2003). This response by relatively young women, which may be referred to as ex-ante moral hazard, is strategic and would increase the average age at first birth for several years after the policy was implemented.Footnote 6 Such a response would be consistent, for example, with the delay in marriage due to increased infertility coverage documented in Abramowitz (2014). Therefore, it is possible that an increase in insurance coverage for infertility treatment may have negative effects on total fertility in the mid- to long-term. This paper examines these issues in the United States, where, by 2001, more than 1 % of live births were due to IVF (CDC 2007). Our objective in this paper is twofold. First, we analyse the impact of an increase in infertility insurance on the timing of first births. Although this question was first explored by Buckles (2005), we believe that our paper contributes in a substantial way to the few existing manuscripts that address this topic by using more adequate data and methodology. Moreover, we go a step further by looking into the long term effects of increasing infertility insurance. Second, we ask whether the increase in infertility insurance affects completed fertility, i.e., fertility by the end of a woman’s reproductive life. This study represents, to the best of our knowledge, the first to address this issue.Footnote 7 Both objectives are analysed using data from the United States. To assess whether infertility insurance induces a delay in motherhood, one needs to combine evidence about reduced fertility of young women with information on when women become mothers, i.e., when (if at all) they stop delaying motherhood. This precisely describes the approach we adopt in the first part of this paper; we not only offer similar evidence as Buckles (2005) on the reduced probability that relatively young women in mandated states have children, but we also demonstrate that the average age of first-time mothers continues to increase in the medium to long term after the enactments of infertility mandates.Footnote 8 Our long-term estimate (10–16 years after the first and the last mandates were passed) ranges from 3 to 5 months. These effects are substantial insofar as they represent between 15.7 and 18.8 % of the total increase in the age of first-time mothers during the period considered for the group of six states that enacted infertility treatment mandatesFootnote 9 and between 24.8 and 34.3 % for the three states with the most generous coverage (Illinois, Massachusetts, and Rhode-Island).Footnote 10
 The ageing of first-time mothers may impact women’s completed fertility in the long term. Hence, our second goal is to determine whether infertility insurance indeed increases women’s completed fertility by the end of their reproductive lives, a question that has not been addressed in the existing literature. In principle, any potential negative effects on fertility induced by a delay of motherhood may eventually be offset by a higher prevalence of multiple births,Footnote 11 so the impact of infertility insurance on completed fertility is ultimately an empirical question. Overall, our estimates, based on data on the number of biological children from the June CPS, show no statistically significant effect of either the strong or the comprehensive mandates on completed fertility. In sum, our paper shows that, despite being associated with higher birth rates among relatively older women and with a higher prevalence of multiple births, infertility insurance does not have a statistically significant effect on women’s fertility at the end of their reproductive lives. The reason lies, as we further show, in the fact that infertility insurance mandates also appear to delay motherhood among relatively younger women and, hence, make conception more difficult because fecundity decreases with age. The rest of the paper is structured as follows: Sect. 2 describes the characteristics of infertility treatment mandates including where and when they were enacted; Sect. 3 describes the data sources used in this paper; Sect. 4 presents our evidence on the delay of motherhood; Sect. 5 presents an analysis of the impact of the mandates on women’s completed fertility; Sect. 6 presents conclusions; Sect. 7 contains figures and tables; and Sect. 8 is the “Appendix”.",11
6.0,4.0,SERIEs,19 September 2015,https://link.springer.com/article/10.1007/s13209-015-0130-5,Echoes of the crises in Spain and US in the Colombian labor market: a differences-in-differences approach,November 2015,Luis E. Arango,Dolores de la Mata,Nataly Obando,Male,Female,Female,Mix,,
6.0,4.0,SERIEs,17 November 2015,https://link.springer.com/article/10.1007/s13209-015-0133-2,"Welfare and inequality effects of debt consolidation processes: the case of Spain, 1996–2007",November 2015,Miguel Viegas,Ana Paula Ribeiro,,Male,Female,Unknown,Mix,,
7.0,1.0,SERIEs,14 March 2016,https://link.springer.com/article/10.1007/s13209-016-0141-x,Introduction to the special issue in honor of Agustín Maravall,March 2016,Gabriele Fiorentini,Gabriel Perez Quiros,,Female,Male,Unknown,Mix,,
7.0,1.0,SERIEs,25 February 2016,https://link.springer.com/article/10.1007/s13209-016-0139-4,Illuminating ARIMA model-based seasonal adjustment with three fundamental seasonal models,March 2016,David F. Findley,Demetra P. Lytras,Agustin Maravall,Male,Female,Male,Mix,,
7.0,1.0,SERIEs,21 December 2015,https://link.springer.com/article/10.1007/s13209-015-0137-y,On some remarks about SEATS signal extraction,March 2016,Guy Mélard,,,Male,Unknown,Unknown,Male,"Seasonal adjustment is fundamental for the analysis and interpretation of macro-economic time series. The series is considered as a juxtaposition of several components: the trend-cycle, the seasonal component and the irregular component. A seasonally adjusted series is obtained by removing the seasonal component. During the second half of the 20th century, the Bureau of the Census X-11 method (Shiskin and Eisenpress 1957) has dominated. It is based on a mixture of statistical techniques mainly moving averages, treatment of atypical observations and trading day adjustments. For a nice illustrated example showing the internals of X-11, see Ladiray and Quenneville (2002). An alternative approach under the form of an Excel file is available (Online Resource 1). Statistics Canada X-11-ARIMA (Dagum 1980) has introduced autoregressive integrated moving average (ARIMA) modelling in particular to extend the series in the past and in the future in order to avoid end-adjustments in the moving averages. Census X-12-ARIMA (Findley et al. 1998) has improved on this by including a better outlier detection procedure and trading day corrections by regression with ARIMA errors, within a regARIMA module, while leaving nearly unchanged the old extraction of components by moving averages. Bank of Spain TRAMO-SEATS (Gómez and Maravall 1994, 2001a, b) has used a more powerful automatic model selection (AMS) procedure called TRAMO (Time series Regression with ARIMA noise, Missing values and Outliers), see below, before a very different signal extraction-based procedure called SEATS (Signal Extraction in ARIMA Time Series). While keeping regARIMA and the available model selection procedures, versions of X-12-ARIMA after version 0.3 have added the automdl spec based on TRAMO. Now (Time Series Research Staff 2013), the Bureau of the Census X-13ARIMA-SEATS has appeared, with few changes on regARIMA and automdl, but offering a choice between SEATS and the old X-11 decomposition procedure, with some improvements for the latter. Therefore, the paper is valid for TRAMO-SEATS but also for the SEATS part in X-13ARIMA-SEATS. It should also be valid for JDemetra+ which intends to be a re-implementation of TRAMO-SEATS and X-13ARIMA-SEATS using the same concepts and algorithms. Before going into the details, let us mention the basic ingredients of these two modules, TRAMO (or regARIMA), on the one hand, and SEATS, on the other hand. Although TRAMO is complex and contains features which are still not always standard in statistical software packages for time series, the use of ARIMA models is now well mastered. Besides the standard ARIMA modelling stage by maximum likelihood, generally in an automated way, the following techniques are involved: regression with autocorrelated errors; treatment of extreme observations using corrections for outliers (additive outliers, level shift, and transitory change); Easter and mobile holiday’s effect and calendar effect adjustments; and treatment of missing observations. The ARIMA model will be directly used by SEATS but it serves also to extend the finite series, by computing as many forecasts and backcasts (i.e. forecasts performed backwards, also called backforecasts by Box and Jenkins in 1970, e.g. Box et al. 2008) as needed. It should be stressed that these techniques are integrated in the ARIMA modelling, although the use of a concentrated likelihood approach allows to estimate the parameters more or less separately. The seasonal adjustment procedure derived by SEATS on the basis of the prolonged series and the model fitted is more difficult to explain, except the basic objective: a decomposition of the series a little bit like in elementary seasonal decomposition methods. However, the signal extraction procedure in SEATS is based on engineering techniques which are generally much less mastered by economists and even statisticians, with some exceptions. The underlying theory of the SEATS program is studied in many papers but there is no complete and systematic description of its output. There are a few tutorial papers, like Kaiser and Maravall (2001), but they are perhaps still too complex for the interested audience. There does not appear to be a paper showing the main concepts in a simple way. Unfortunately, even for the airline model (or an \(\hbox {ARIMA}(0,1,1){(0,1,1)}_s\) model on logarithms of the data, with s \(=\) 12 for monthly observations), the simplest realistic ARIMA model at the TRAMO or regARIMA stage, things are still too complex. The main purpose of this paper is therefore to examine the details of SEATS text output and to explain the results in simple words and formulas. It is done on the basis of an example, with a step-by-step description of a typical output from SEATS. To keep the explanation as simple as possible, the example will not use a seasonal decomposition but will use the simpler signal extraction of a trend. The example is for a time series with a non-seasonal model so that the computations can be easily verified. Therefore the models are much simpler and fewer numbers need to be interpreted while preserving the essential. We have designed a Microsoft Excel Workbook (Online Resource 2) which shows most of the output and a document with some instructions (Online Resource 3). Using Microsoft Excel for doing statistics is not generally recommended (see the references in Mélard 2014), but it is the right tool to describe simple computations. It would be difficult to use Excel to demonstrate SEATS in a more realistic example with a seasonal component. The regARIMA or TRAMO part of the treatment are not discussed, nor the graphical output. We have found a monthly series called TICD (also used in Mélard 2007), the interest rates of U.S. certificates of deposit, between December 1974 and December 1979, which is a non-seasonal series. We have obtained a non-seasonal but otherwise very interesting decomposition, being able to compare the results for the filter weights and most of the output with those obtained using SEATS, see Tables and Online Resource 4. The principles behind SEATS are described in Sect. 2, including the admissible decompositions and the canonical decomposition, and a procedure to implement the derivation of the Wiener-Kolmogorov filter. In Sect. 3 the example is introduced: the time series and the text output from SEATS is presented in edited form in several tables. Finally, in Sect. 4, the main results are checked on the example by means of a Microsoft Excel workbook and direct computations. In particular, the forecasts and backcasts are obtained; the admissible and canonical decompositions with two components are discussed; the filters are first derived using autocorrelations of two auxiliary ARMA processes, then applied on the prolonged time series; and the characteristics of the estimates, the revisions and the growth rates are analyzed. We will conclude in Sect. 5. Appendix 1 will serve to introduce spectral analysis in a general approach. Appendix 2 will introduce the spectral analysis used in SEATS for the derivation of a canonical decomposition.",
7.0,1.0,SERIEs,10 December 2015,https://link.springer.com/article/10.1007/s13209-015-0134-1,Robust time series models with trend and seasonal components,March 2016,Michele Caivano,Andrew Harvey,Alessandra Luati,Female,Male,Female,Mix,,
7.0,1.0,SERIEs,23 October 2015,https://link.springer.com/article/10.1007/s13209-015-0132-3,Neglected serial correlation tests in UCARIMA models,March 2016,Gabriele Fiorentini,Enrique Sentana,,Female,Male,Unknown,Mix,,
7.0,1.0,SERIEs,12 October 2015,https://link.springer.com/article/10.1007/s13209-015-0131-4,Identification of asymmetric conditional heteroscedasticity in the presence of outliers,March 2016,M. Angeles Carnero,Ana Pérez,Esther Ruiz,Unknown,Female,Female,Female,"One of the main topics that has focused the research of Agustín over a long period of time is seasonality. However, this is not his only topic of interest. Agustín’s contributions to the Econometric Time Series literature are much broader and include, among others, the treatment of outliers in time series; see, for example, Maravall and Peña (1986), Peña and Maravall (1991), Gómez et al. (1999) and Kaiser and Maravall (2003). In these papers, Agustín and his coauthors consider the effects and treatment of outliers in macroeconomic data and, consequently, deal primarily with linear time series models. However, outliers are also present in the context of financial time series mainly when they are observed over long periods of time. It is important to note that, in this framework, the interest shifts from conditional means to conditional variances and, consequently, to non-linear models. Agustín has also contributions in this area; see Fiorentini and Maravall (1996) for an analysis of the dynamic dependence of second order moments. When dealing with financial data, many series of returns are conditionally heteroscedastic with volatilities responding asymmetrically to negative and positive past returns. In particular, the volatility is higher in response to past negative shocks (‘bad’ news) than to positive shocks (‘good’ news) of the same magnitude. Following Black (1976) this feature is commonly referred to as leverage effect. Incorporating the leverage effect into conditionally heteroscedastic models is important to better capture the dynamic behaviour of financial returns and improve the forecasts of future volatility; see Bollerslev et al. (2006) for an extensive list of references and Hibbert et al. (2008) for a behavioral explanation of the negative asymmetric return–volatility relation. The identification of conditional heteroscedasticity is often based on the sample autocorrelations of squared returns. Carnero et al. (2007) show that the presence of outliers biases these autocorrelations with misleading effects on the identification of time-varying volatilities. On the other hand, the identification of leverage effect is often based on the sample cross-correlations between past and squared returns. Negative values of these cross-correlations indicate potential asymmetries in the volatility; see, for example, Bollerslev et al. (2006), Zivot (2009), Rodríguez and Ruiz (2012) and Tauchen et al. (2012). In this paper, we analyse how the identification of asymmetries, when based on the sample cross-correlations, can also be affected by the presence of outliers. This paper has two main contributions. First, we derive the asymptotic biases caused by large outliers on the sample cross-correlation of order h between past and squared observations generated by uncorrelated stationary processes. We show that k large consecutive outliers bias such correlations towards zero for \(h\ge k\), rendering the detection of genuine leverage effect difficult. In particular, one isolated large outlier biases all the sample cross-correlations towards zero and so it could hide true leverage effect. Moreover, the presence of two big consecutive outliers biases the first-order sample cross-correlation towards 0.5 (\(-0.5\)) if the first outlier is positive (negative) and so it could lead to identify either spurious asymmetries or asymmetries of the wrong sign. The second contribution of this paper is to address the problem of robust estimation of serial cross-correlations by extending several popular robust estimators of pairwise correlations and autocorrelations. In the context of bivariate Gaussian variables, there are several proposals to robustify the pairwise sample correlation; see Shevlyakov and Smirnov (2011) for a review of the most popular ones. However, the literature on robust estimation of correlations for time series is scarce and mainly focused on autocovariances and autocorrelations. For example, Hallin and Puri (1994) propose to estimate the autocovariances using rank-based methods. Ma and Genton (2000) introduce a robust estimator of the autocovariances based on the robust scale estimator of Rousseeuw and Croux (1992, 1993). More recently, Lévy-Leduc et al. (2011) establish its asymptotic and finite sample properties for Gaussian processes. Ma and Genton (2000) also suggest a possible robust estimator of the autocorrelation function but they do not further discuss its properties neither apply it in their empirical application. Finally, Teräsvirta and Zhao (2011) propose two robust estimators of the autocorrelations of squares based on the Huber’s and Ramsay’s weighting schemes. The theoretical and empirical evidence from all these papers strongly suggests using robust estimators to measure the dependence structure of time series. We analyse and compare the finite sample properties of the proposed robust estimators of the cross-correlations between past and squared observations of stationary uncorrelated series. As expected, these estimators are resistant against outliers remaining the same regardless of the size and the number of outliers. Moreover, even in the presence of consecutive large outliers, the robust estimators considered estimate the true sign of the cross-correlations although they underestimate their magnitudes. Among the robust cross-correlations considered, the modified version of the Ramsay-weighted serial autocorrelation suggested by Teräsvirta and Zhao (2011) provides the best resistance against outliers and the lowest bias. To illustrate the results, we compute the sample cross-correlations and their robust counterparts of a real series of daily financial returns. We show how consecutive extreme observations bias the usual sample cross-correlations and could lead to wrongly identifying potential leverage effect. These empirical results enhance the importance of using robust measures of serial correlation to identify both conditional heteroscedasticity and leverage effect. The rest of the paper is organized as follows. Section 2 is devoted to the analysis of the effects of additive outliers on the sample cross-correlations between past and squared observations of stationary uncorrelated time series that could be either homoscedastic or heteroscedastic. Section 3 considers four robust measures of cross-correlation and compares their finite sample properties in the presence of outliers. The difficulty of extending the Ma and Genton (2000) proposal to the estimation of serial cross-correlation is discussed in Sect. 4. The empirical analysis of a time series of daily Dow Jones Industrial Average index is carried out in Sect. 5. Section 6 concludes the paper with a summary of the main results and proposals for further research.",4
7.0,2.0,SERIEs,05 March 2016,https://link.springer.com/article/10.1007/s13209-016-0140-y,Gorman revisited: nonparametric conditions for exact linear aggregation,June 2016,Laurens Cherchye,Ian Crawford,Frederic Vermeulen,Male,Male,Male,Male,"Many models in the theoretical and empirical literature on macroeconomics, international trade and industrial organisation assume, at least implicitly, that aggregate demand is invariant to changes in the income distribution over individual consumers and, hence, that aggregate demand depends only on prices and aggregate income. The best known theoretical results on this topic are probably those of Gorman (1953, 1961), who made explicit the conditions on microeconomic consumer behaviour under which aggregate demand can be written as a function of prices and aggregate income alone.Footnote 1 Specifically, Gorman showed that exact linear aggregation is possible if and only if consumers have preferences of the Gorman Polar Form such that the corresponding linear Engel curves have common slopes. The empirical literature on consumer behaviour, however, has consistently tended to show that these conditions do not hold in practice. Lewbel and Pendakur (2009), for example, provide strong parametric evidence of nonlinear Engel curve behaviour whilst Blundell et al. (2007) consider semi- and nonparametric evidence for this non-linearity. In this paper, we revisit the problem that Gorman addressed. We too seek necessary and sufficient conditions for exact linear aggregation. However, we do this from a rather different perspective, that of the nonparametric revealed preference tradition of Samuelson (1938, 1948), Afriat (1967), Diewert (1973) and Varian (1982). Instead of describing the restrictions on behaviour in terms of the derivatives of certain functions (the slopes of Engel curves, for example), this approach works by characterising them in terms of a finite system of inequalities involving the consumers’ observed choices only. Finding a nonparametric equivalent to Gorman’s aggregation theorems is, of course, of a certain amount of theoretical interest, but this is not our only motivation: we are also interested in empirical implementation. In particular we are interested in whether it may be possible empirically to analyse microdata for its aggregation properties without resorting to regression analysis. Regression analysis, in the words of Daniel McFadden in his presidential address to the Econometric Society, “interposes an untidy veil between econometric analysis and the propositions of economic theory”. In fact McFadden was discussing parametric regression but nonparametric regression is not immune from the same observation. If the implications of economic theory are described in terms of the shapes of functions implied by the theory (e.g. Engel curves) then any empirical investigation of the theory requires those functions to be estimated from data. As a result, the conclusions from such an exercise necessarily rest jointly on the validity of the hypothesis at stake plus a number of crucial auxiliary statistical assumptions necessary to deliver consistent estimates of the functions of interest. This is the case whether the estimates are parametric, semi-parametric or nonparametric. For example both Lewbel and Pendakur (2009) and Blundell et al. (2007) provide evidence based on pooled cross-section data—they therefore need to make a number of carefully chosen auxiliary assumptions about the form of unobserved heterogeneity and how it enters the model in order to deliver their estimates. Both studies also need to follow an instrumental variables strategy which also brings with it a set of important identifying assumptions. Revealed preference methods do not require the identification or estimation of structural functions. Instead they involve only inequality restrictions on the observables alone. As a result they are, to a great extent, free of the need for auxiliary hypotheses. They therefore allow researchers to focus with much greater clarity on the hypothesis at the core. Furthermore, they are applicable when there are only very few observations and, hence, when statistical methods would be infeasible or uninformative. The main contribution of this paper is twofold. Firstly, we establish the nonparametric counterparts of Gorman’s aggregation conditions. We start by providing a revealed preference characterisation of Gorman Polar Form preferences for an individual consumer. We then propose an easy-to-implement necessary and sufficient test for Gorman’s conditions for exact linear aggregation. Secondly, we demonstrate the practical usefulness of our results through an empirical investigation using a balanced microdata panel of Spanish households. Our first main conclusion here will be that we strongly reject exact linear aggregation when focusing on the set of all rational households in our sample. Our second main result is that this rejection is primarily due to heterogeneity in the marginal utility of income. To investigate this heterogeneity, we also considered the possible partitioning of our sample of households into subsets for which exact linear aggregation holds. We conducted two exercises. Firstly, we partition our sample on a standard set of observable household characteristics. Again, however, we find that exact linear aggregation is rejected for each thus defined subset of households. Secondly, we use a slight adaptation of a method introduced by Crawford and Pendakur (2013) to define a partitioning that accounts for possibly unobserved household characteristics. Essentially, this method identifies the minimal number of subsets of households such that each individual subset is consistent with exact linear aggregation. We conclude that we need a substantial number of groups (revealing unobserved heterogeneity) for the observed household consumption to be exactly aggregable. Summarizing, given the nonparametric nature of our tests, our empirical results provide robust evidence against the existence of a representative agent. This complements the already existing empirical evidence (see, for example, Kirman 1992 and Carroll 2000), but now from a revealed preference perspective. Moreover, we also show that the existence of a limited set of representative agents seems to be a very unrealistic hypothesis. As we will discuss more in detail in Sect.  3, we interpret all this as providing empirical support to macroeconomic models working with a continuum of heterogeneous agents, such as the so-called standard incomplete markets models (see, for example, Heathcote et al. 2009). As a final point, we remark that the revealed preference approach that we follow in this paper is completely deterministic and static in nature. First, in its pure form, it defines testable conditions that ignore any source of randomness in the data, which excludes formal statistical hypothesis testing. Importantly, however, it is possible to formally account for statistical issues by combining the exact aggregation conditions that we present below with methodological tools that have been presented in alternative revealed preference contexts. For example, it is fairly straightforward to account for measurement error by combining our results with an original proposal of Varian (1985). Next, we could account for random utility considerations by integrating our analysis with the one of McFadden and Richter (1991). To focus our discussion, we will not explicitly discuss these extensions in the current paper. Finally, as in Gorman (1953, 1961), we focus on a static framework, which makes it easier to define the concept of a representative agent. However, this of course implies that we ignore intertemporal aspects such as habit formation and/or saving decisions.Footnote 2 To address these issues, we should for instance integrate the revealed preference characterizations on the life-cycle rational expectations hypothesis (see Browning 1989) and/or habit formation (see Crawford 2010) into our framework. We see all these points as interesting developments for follow-up research. The remainder of this paper is structured as follows. Section 2 contains our main theoretical results, which provide a revealed preference characterisation of individual Gorman Polar Form preferences and an easy-to-implement necessary and sufficient nonparametric test for exact linear aggregation. Section 3 presents our empirical application. Section 4 concludes.",3
7.0,2.0,SERIEs,13 February 2016,https://link.springer.com/article/10.1007/s13209-016-0138-5,Understanding international migration: evidence from a new dataset of bilateral stocks (1960–2000),June 2016,Joan Llull,,,Female,Unknown,Unknown,Female,"International migration has increased dramatically in recent decades. Understanding the determinants of the movement of workers across international borders is crucial for immigration policy design. This paper aims to enhance our knowledge about these determinants by presenting new data on bilateral migrant stocks, a new treatment of those data in the empirical analysis, and new empirical evidence on the determinants of international migration. To create the new database, I collected data on international migrant stocks by country of origin from National Statistical Offices of the 24 richest OECD countries. This dataset includes bilateral stocks of immigrants from 188 countries of origin into these 24 destination countries over the period 1960 to 2000. The data come from Census records at these destination countries. Given this, it covers the total amount of immigrants living in the country. Importantly, because the data sometimes appear in grouped categories, I keep track of these groups in a raw manner, without making imputations to specific countries of origin.Footnote 1 This is important because imputations, dropping grouped observations, and/or counting grouped observations as zeros may lead to important biases in the estimates. Empirically, this paper makes two contributions. First, it gives explicit treatment to these grouped data in standard gravity regressions. Second, it presents evidence on the existence of heterogeneous effects of income gains on migration prospects depending on distance. According to a static model—the approach which mostly followed by the literature—when individuals decide whether to migrate to another country, they base their decision on net income gains from migration, i.e. the differential in expected wages between the two countries net of (one time) moving costs.Footnote 2 From a dynamic point of view, however, individuals may care about moving costs (distance in particular) even after having migrated. Large moving costs may reduce their flexibility to move back and forth to their home country as a consequence of income shocks;Footnote 3 and, if individuals dislike living far away from home, they may require a compensating wage differential for living abroad that might be increasing in distance. Forward looking individuals will take these two factors into account when deciding whether to migrate in the first place. As a result, the effect of income gains on moving prospects (net of the initial moving cost) may be heterogeneous depending on distance: individuals from countries away from home would be less reactive to income fluctuations compared to individuals from closer countries. Results suggest that these heterogeneities are indeed very important. For example, a 1000$ increase in US income per capita increases the stock of Mexican immigrants in the US by a percentage that is 2.6 times larger than the percentage increase in the stock of Chinese immigrants. In other words, the effect of income on log migrant stocks is 2.6 times larger for Mexico compared to China (8 vs. 3.1 %), given that Beijing is around 2.6 times as far from Washington DC as Mexico City is. This differs from the standard gravity equation, which would predict linear effects of income gains on log migrant stocks (Beine et al. 2015). This result is relevant for immigration policy design. For example, a pull-driven immigration shock (i.e. positive income shock) may imply significant changes in the composition of immigrant population in terms of nationalities. Similarly, a negative shock to a developing country may have a much larger effect for neighboring countries than previous estimates in the literature suggest; this larger effect suggests that destination countries may want to favor neighboring countries in development assistance policies if they are interested in reducing immigrant inflows. Collecting data on bilateral migration is, in general, a difficult task. Reliability of statistics from origin countries is low because it is difficult to keep track of the people who leave the country. Data from destination countries is more accurate. The lack of comparable cross-destination country bilateral data led many papers in the literature to follow a single destination country over time (e.g. Borjas and Bratsberg 1996; Karemera et al. 2000; Clark et al. 2007; Bertoli and Fernández-Huertas Moraga 2013). More recently, researchers and institutions have put some effort in gathering comparable bilateral migration data across destination countries. Pedersen et al. (2008) and Mayda (2010) are the first papers using cross-destination country panel data on bilateral inflows to analyze the effect of income gains and moving costs on international migration. Mayda (2010) uses a database from OECD on annual legal inflows of workers by country of origin; she uses these data to investigate the determinants of migration inflows into 14 OECD countries between 1980 and 1995. Pedersen et al. (2008) produce a similar database collecting data on issues of residence and work permits from National Statistical Offices from 1989 to 2000. They use these data to look at the effects of networks and welfare benefits on international migration. These two databases have recently been expanded by Ortega and Peri (2013) and Adserà and Pytliková (2012) respectively.Footnote 4 The four databases contain information on inflows of immigrants and, with a lower accuracy, net flows. They are based on the number of issues of residence and work permits, which is likely to produce a severe underestimation the real numbers due to illegal migration. And, acknowledged by the authors, they have an important amount of missing data and incorrect zero values (for countries with relatively small flows), covering, as a result, a limited fraction of total inflows (Mayda 2010, pp. 1258–1259). Similarly to what I do in this paper, Docquier and Marfouk (2006) and Docquier et al. (2009) collect Census-based data. The aim of their databases is to gather information on stocks of immigrants by educational level, and, for this reason, they only cover two census dates, 1990 and 2000. Two papers use these data to analyze the determinants of international migration. Grogger and Hanson (2011) use them to the analyze the determinants of scale and composition of migration flows. Ortega and Peri (2014b) combine these two years of data on stocks with the OECD database on annual legal inflows used in Mayda (2010) to extrapolate stocks back to 1980 and analyze the determinants of migration flows. Contemporaneously to this paper, a few additional datasets appeared. Özden et al. (2011) is the most similar. These authors collect bilateral stock data for the same period and from similar sources. The key difference with the current dataset is the treatment of data when bilateral information is not available. When this happens, which is often the result of grouping of data (residual categories, aggregations of countries,...), these authors try to recover the bilateral information by means of an array of different imputations. Conversely, I keep these grouping in a raw manner, giving it a specific treatment in the empirical analysis. Given the similarity, I draw some comparisons with this dataset below. The other three datasets are: United Nations (2013), which provides similar information for years 1990, 2000, 2010, and 2013; Brücker et al. (2013), who add the educational and gender dimension for the period 1980 to 2010; and, Abel and Sander (2014), who estimate inflows and outflows out of the stock data for 1990–2010. The rest of the paper is organized as follows. Section 2 presents the database. Section 3 introduces the econometric model and explains the implications of grouped data in terms of identification of fixed effects. Section 4 shows estimation results. And Sect. 5 concludes.",11
7.0,2.0,SERIEs,02 May 2016,https://link.springer.com/article/10.1007/s13209-016-0142-9,From bilateral two-way to unilateral one-way flow link-formation,June 2016,Norma Olaizola,Federico Valenciano,,Female,Male,Unknown,Mix,,
7.0,2.0,SERIEs,28 May 2016,https://link.springer.com/article/10.1007/s13209-016-0145-6,Erratum to: Measuring expectations from household surveys: new results on subjective probabilities of future house prices,June 2016,Olympia Bover,,,Female,Unknown,Unknown,Female,"In the original publication of the article, the author group and the affiliation were incorrect. Olympia Bover is the author for this article and the correct affiliation is “Banco de España, Madrid, Spain”. The original article has been updated accordingly.",
7.0,3.0,SERIEs,18 June 2016,https://link.springer.com/article/10.1007/s13209-016-0146-5,An analysis of the cost of disability across Europe using the standard of living approach,August 2016,José-Ignacio Antón,Francisco-Javier Braña,Rafael Muñoz de Bustillo,Unknown,Unknown,Male,Male,"Disability is far from being a marginal phenomenon in developed countries.Footnote 1 According to Eurostat, 27.8 % of European Union (EU) citizens above 16 years old suffered from a long-standing illness or health problem in 2014, whereas 8.6 % reported experiencing strong limitations in their daily activities.Footnote 2 Both in the Organization for Economic Co-operation and Development (OECD) and in the EU, there is strong concern about the issue and a mandate to promote and attain the full economic and social participation of people with disabilities.Footnote 3 As a reflection of this concern, in the OECD and in the EU there is a variety of disability benefit systems, regulations and coverage. Some of these policies address the reintegration of disabled people into the labour market, while others aim to compensate individuals with disabilities.Footnote 4 According to Eurostat, public social spending in this area reached a sizable 2 % of GDP in the EU-28 in 2012, fluctuating from 0.7 % in Cyprus to 4.4 % in Denmark. The aim of this paper is to offer an estimate of the “extra” costs of severe disability for households in 31 European countries (the 28 member states of the current European Union and Iceland, Norway and Switzerland), where the term “extra” refers to the over-cost faced by households with members with disabilities to reach a given level of well-being compared to similar households with non-disabled members. As far as we know, this is the first attempt to offer such an estimation using homogeneous data and the same methodology for a wide set of European countries. On top, we try to outline several plausible explanations for the differences in estimated costs across countries. The estimation of the cost of disability in a large number of countries is relevant for both substantive and methodological reasons. Starting with the former, the estimation of the cost of disability for households will contribute to better evaluate the sufficiency of public compensatory disability policies. Furthermore, this type of analysis will allow better gauging the economic welfare implications of the future expected increase in disability rates related to the demographic change and the rise in life expectancy at older ages.Footnote 5 On the methodological realm, the large number of countries included in the analysis will facilitate a better assessment of the appropriateness of the method of analysis used to estimate the cost of disability by looking at the similarity or dissimilarities of the estimates across countries, as well as the possible reasons behind them. We characterise disability as a functional limitation that results not only from a medical condition of the body or the whole person but also from the relation of a person with the environment, which involves dysfunction at one or more of three levels: impairments, activity limitations and participation restrictions.Footnote 6 The resulting loss of autonomy, physical or mental, prevents the performance of some of the activities of daily living, increasing the cost of reaching a given level of well-being. Our approach to the cost of disability draws from the work of Sen (2004), who makes a distinction between two types of handicaps that tend to be associated with disability. On one hand, a disabled person may find it harder to get a job or to keep it, and he or she may receive a lower wage. Disability can even affect the acquisition or accumulation of human capital.Footnote 7 On the other hand, because persons with disabilities have special needs, they face more difficulties than non-disabled people in achieving well-being from their resources or may need more income for the same activity. Sen calls the first one an “earning handicap” and the second one a “conversion handicap”. The latter handicap is recognised in social protection systems in many countries, which provide through direct expenditure or tax expenditures and in other areas such as preferred parking or employment subsidies aimed to offset the higher costs associated with disability. The starting point of our work is Sen’s (1985, 1987) concept of distributive justice, based on equalising people’s basic capabilities. For this author, the ultimate reference in redistributive policy is the standard of living, not the utility or the mere possession of goods. The issue is to establish an objective minimum standard that represents a good approximation to the real income level, considering that the standard of living is primarily an issue concerning lifestyle, rather than the means for its development. For Sen, the standard of living is a matter of functionings and capabilities. As it is well known, Sen’s point of departure is the modern theory of the consumer (Lancaster 1966), according to which goods are not relevant in themselves, but because they incorporate features and properties that make them desirable. What matters is the use that each person can get from these characteristics, which depends on his or her capability to perform the functions to take advantage of the characteristics of each good. Therefore, given a set of goods, each individual, according to her/his capabilities, can convert its characteristics into different combinations of functionings, from which she/he obtains a certain level of welfare. The standard of living approach aims to determine the cost of disability by comparing households with disabled and non-disabled members with the same level of welfare and allowing the difference in income to determine the cost of disability. Researchers have devoted some attention to the study of the costs of disabilities, though almost all the literature focuses on Anglo-Saxon countries. This body of research is also based on very different methodologies (discussed in the third section) and it relies exclusively on national studies, so the comparability of the different results found in the literature is far from ideal. Apart from the surveys of Indecon (2004), Tibble (2005) and Stapleton et al. (2008), one should highlight the works of Martin and White (1988), Matthews and Truscott (1990), Berthoud et al. (1993), Jones and O’Donnell (1995), Zaidi and Burchardt (2005), Kuklys (2005) and Wood and Grant (2010) for the United Kingdom, Indecon (2004) and Cullinan et al. (2011a) for Ireland, Saunders (2007) for Australia, She and Livermore (2007) and Mitra et al. (2009) for the United States, Wilkinson-Meyers et al. (2010) for New Zealand and Braña and Antón (2011) for Spain. In addition, Braithwaite and Mont (2009) estimate the cost of disability for two developing countries, Bosnia and Herzegovina and Vietnam. Although a significant share of the mentioned studies are based on the standard of living approach, the overall results of this literature, discussed in more detail in the methodological section, are extremely difficult to summarise. This is because the authors rely on different definitions of disability and use different variables and econometric specifications to estimate the cost of disability, making their outcomes difficult to compare. The absence of comparative studies might also cast some additional doubts on the methodology followed, which is the reason why cross-country studies using a common methodology can contribute to test the appropriateness of the standard of living approach. Are the results for different countries roughly similar, or are the differences obtained consistent with economic theory and the idiosyncratic features of these countries? In this respect, the present work also aims to fill this gap. Our study estimates the cost of disability—understood as suffering longstanding strong limitation in daily activities—for 31 European countries using two different strategies, one based on a subjective question about the household’s ability to make ends meet and another related to the ownership of several assets. Overall, we find a pattern of diversity, with Scandinavian countries at the top and Eastern Europe nations at the bottom. Our results suggest that the cost of disability is positively correlated with per capita Gross Domestic Product (GDP) per capita, in line with an interpretation where disability implies a larger opportunity cost limiting the possibilities of making the best of participating in market activities of people with disabilities or their relatives or income transfers from relatives in other households. Also, we find that, on average, those countries with a larger cost of disability devote more social spending to this area, which could be a partial response of countries to the mentioned costs. Finally, this correlation is also found when looking at in-kind social spending, which might lead to an overestimation of the costs of disability in those countries that makes a higher budget effort in this sector. The article unfolds in four additional sections as follows. In Sect. 2, we describe the database used to estimate the cost of disability, pointing out its strengths and shortcomings. The third section presents and discusses the methodology followed in an estimation of the cost of disability. In Sect. 4, the results obtained in terms of the cost of disability in the 31 European countries are presented and discussed. Finally, Sect. 5 summarises the main conclusions obtained in the paper and outlines further lines of research.",17
7.0,3.0,SERIEs,27 May 2016,https://link.springer.com/article/10.1007/s13209-016-0143-8,Information in elections: Do third inflexible candidates always promote truthful behavior?,August 2016,Ascensión Andina-Díaz,,,Unknown,Unknown,Unknown,Unknown,,
7.0,3.0,SERIEs,02 June 2016,https://link.springer.com/article/10.1007/s13209-016-0144-7,Modelling cross-dependencies between Spain’s regional tourism markets with an extension of the Gaussian process regression model,August 2016,Oscar Claveria,Enric Monte,Salvador Torra,Male,Male,Male,Male,"In recent years there has been a growing interest in machine learning (ML) techniques for economic forecasting (Weron 2014; Gharleghi et al. 2014; Kock and Teräsvirta 2014; Ben Taieb et al. 2012; Crone et al. 2011; Andrawis et al. 2011; Carbonneau et al. 2008). ML is based on the construction of algorithms that learn through experience. The main ML forecasting methods are support vector regression (SVR) and artificial neural network (ANN) models. Plakandaras et al. (2015) propose a hybrid forecasting methodology that combines an ensemble empirical mode decomposition algorithm with a SVR model to forecast the US real house price index. Lin et al. (2012) also combine an algorithm for time series decomposition with a SVR model for foreign exchange rate forecasting. Kao et al. (2013) and Kim (2003) use different SVR models for stock index forecasting. Tay and Kao (2001, (2002) apply support vector machines in financial time series forecasting. 
Stasinakis et al. (2015) use a radial basis function ANN to forecast US unemployment. Feng and Zhang (2014) and Aminian et al. (2006) use ANN models in forecasting of economic growth. Sermpinis et al. (2012) and Lisi and Schiavo (1999) make exchange rates predictions by means of several ANNs. Sarlin and Marghescu (2011) generate visual predictions of currency crisis by means of a self-organizing map ANN model. Adya and Collopy (1998) evaluate the effectiveness of ANN models at forecasting and prediction. A complete summary on the use of ANNs with forecasting purposes can be found in Zhang et al. (1998). Whilst SVR and ANN models have been widely used in economic modelling and forecasting, other ML techniques such as Gaussian process regression (GPR) have been barely applied for forecasting purposes (Andrawis et al. 2011; Ahmed et al. 2010; Banerjee et al. 2008; Chapados and Bengio 2007; Brahim-Belhouari and Bermak 2004; Girard et al. 2003). GPR was originally devised for interpolation. The works of Smola and Bartlett (2001), MacKay (2003), and Williams and Rasmussen (2006) have been key in the development of GPR models. By expressing the model in a Bayesian framework, the authors extend GPR applications beyond spatial interpolation to regression problems. GPR models are supervised learning methods based on a generalized linear regression that locally estimates forecasts by the combination of values in a kernel (Rasmussen 1996). Thus, GPR models can be regarded as a non-parametric tool for regression in high dimensional spaces. One of the limitations of the current methods for GPR is that the framework is inherently one dimensional, i.e. the framework is designed for multiple inputs and a single output. GPR models present one fundamental advantage over other ML techniques: they provide full probabilistic predictive distributions, including estimations of the uncertainty of the predictions. These features make GPR an ideal tool for forecasting purposes. This paper presents an extension of the GPR model for MIMO forecasting. This approach allows to preserve the stochastic properties of the training series in multiple-step ahead prediction (Ben Taieb et al. 2010). By extending conventional local modelling approaches we are able to model the cross-dependencies between a given set of time series, returning a vectorial forecast. The structure of the proposed model, consists of a batch of univariate forecasting modules based on Gaussian regression, followed by a linear regression that takes into account the cross-influences between the different forecast. ML methods are particularly suitable to model phenomena that presents nonlinear interactions between the input and the output. The complex nature behind the data generating process of economic variables such as tourism demand, explains the increasing use of ML methods in this area. There is wide evidence in favour of ML methods when compared to time series models for tourism demand forecasting (Akin 2015; Claveria and Torra 2014; Wu et al. 2012, Hong et al. 2011; Chen and Wang 2007; Giordano et al. 2007; Cho 2003; Law 2000 and Law and Au 1999). Tsaur and Kuo (2011) and Yu and Schwartz (2006) use fuzzy time series models to predict tourism demand. Celotto et al. (2012) and Goh et al. (2008) apply rough sets algorithms. Other authors combine different ML techniques in order to refine forecasts of tourism demand (Cang 2014a; Cang and Yu 2014b; Pai et al. 2014; Shahrabi et al. 2013). Peng et al. (2014) use a meta-analysis to examine the relationships between the accuracy of different forecasting models and the data characteristics in tourism forecasting studies. Athanasopoulos et al. (2011) carry a thorough evaluation of various methods for forecasting tourism data. In spite of the desirable properties of GPR models, there is only one previous study that uses GPR for tourism demand forecasting (Wu et al. 2012). The authors use a sparse GPR model to predict tourism demand to Hong Kong and find that its forecasting capability outperforms those of the autoregressive moving average (ARMA) and SVR models. We attempt to cover this deficit, and to break new ground by proposing an extension of the GPR model for MIMO modelling, and assessing its forecasting performance. We make use of international tourist arrivals to all seventeen regions of Spain. By incorporating the connections in tourism demand to all regions, we generate forecasts to all markets simultaneously. We finally compare the forecasting performance of the GPR model to that of a multi-layer perceptron (MLP) ANN in a MIMO setting. This strategy is cost-effective in computational terms, and seems particularly indicated for regional forecasting. Several regional studies have been published in recent years (Lehmann and Wohlrabe 2013), but only a few regarding tourism demand forecasting. Gil-Alana et al. (2008) use different time-series models to models international monthly arrivals in the Canary Islands. Bermúdez et al. (2009) generate prediction intervals for hotel occupancy in three provinces of Spain by means of a multivariate exponential smoothing model. The first attempt to use ML methods for tourism demand forecasting in Spain is that of Palmer et al. (2006), who design a MLP ANN to forecast tourism expenditure in the Balearic Islands. Medeiros et al. (2008) develop an ANN-GARCH model to estimate demand for international tourism also in the Balearic Islands. Claveria et al. (2015) compare the forecasting performance of three ANN architectures to forecast tourist arrivals to Catalonia. The main aim of this study is to provide researchers with a novel approach for MIMO forecasting, and a method for modelling cross-dependencies. The proposed extension of the GPR model to the MIMO framework allows incorporating the relationships between the different response variables in order to generate a vector of predictions. The study is organized as follows. The next section presents the proposed extension of the GPR model to the MIMO case. In Sect. 3 we briefly describe the data. Section 4 reports the results of the multiple-step ahead forecasting comparison carried out to test the effectiveness of the model. The last section provides a summary of the theoretical and practical implications, and potential lines for future research.",8
7.0,3.0,SERIEs,15 July 2016,https://link.springer.com/article/10.1007/s13209-016-0147-4,The effect of taxes on the debt policy of spanish listed companies,August 2016,José A. Clemente-Almendros,Francisco Sogorb-Mira,,Male,Male,Unknown,Male,"A large body of research has examined the effects of corporate taxation. Although the results of empirical models vary significantly, the majority of this research does find that, to some degree, taxes influence a broad range of corporate financial decisions such as financing policy, investment policy or corporate reorganization and hedging.Footnote 1 The magnitude of these effects and their overall impact on the economy are still under debate. Notwithstanding, the most significant obstacle a policy maker confronts in deciding on the tax treatment of corporate debt and equity financing is that the impact of taxation on corporate financial policy is not entirely understood. In addition, Graham (2013) reviews a number of studies that suggest that taxes influence financing decisions; however, this effect is not always strong. Likewise, he concludes that more research is needed for a better understanding of the influence of taxes on capital structure, particularly related to time-series effects. Therefore, whether and to what extent taxation affects the choice of capital structure is still an unsettled topic, deserving further study. The meta-study of the existing empirical studies conducted by Feld et al. (2013) concludes that capital structure choices are indeed positively affected by taxes, an effect which is also quantitatively relevant. Tax rates are shown to be correlated with corporate capital structure choices, which suggests that firms may increase value through optimal debt choice. The trade-off theory of capital structure offers a theoretical explanation to the relationship between corporate debt policy and taxes. Specifically, this theory argues that firms determine their optimal debt ratio by comparing the present value of additional tax savings and of the additional expected cost of financial distress caused by a marginal increase in debt. There has been relatively limited empirical research into the effects of marginal corporate tax on debt policy, despite its clear significance. In this regard, Graham (1996a), as well as the subsequent studies, found that marginal corporate tax rate does influence the debt policies of US firms.Footnote 2 In countries other than the US, Alworth and Arachi (2001) conducted a similar analysis using a data panel on Italian firms and found a positive relationship between firm-specific marginal tax rates and Italian firms’ debt policy. In addition, Kunieda et al. (2011), Hartmann-Wendels et al. (2012) and Sinha and Bansal (2013) obtained analogous results for Japanese, German and Indian firms, respectively. To the best of our knowledge, there have been no empirical studies to date on the effects of simulated marginal tax rates on debt policy in Spain. In the area of public finance, recent debate about corporate tax reform has focused on the consequences of asymmetric tax treatment of equity and debt financing. US and European fiscal authorities have considered limiting the ability of companies to deduct interest payments from taxable income, as well as calling for equal treatment of equity and debt. Some examples are the Comprehensive Business Income Tax (CBIT) proposal by the US Treasury, the Mirrlees Review proposals for the UK tax system or the Resolution of the ECOFIN Council Meeting of June 8, 2010, which recommended to European Union member States the adoption of thin-capitalization rules.Footnote 3 The reason for this is that the tax-favoured status of debt has reduced tax revenue collection and supposedly encouraged a “debt bias” whereby tax incentives encourage companies to use extra debt. In this regard, it is believed that excessive use of debt financing increases firms’ probability of becoming financially distressed and thereby exacerbates or perhaps even causes economic downturns. According to Mooij (2011), although the existence of debt in the capital structure did not cause the financial crisis, excessive leverage makes firms more vulnerable to economic shocks and therefore debt bias might have contributed to the extent of the crisis. The main objective of this study is to analyse the relationship between taxes and debt financing using panel data on Spanish listed companies. More specifically, we focus on how the deductibility of debt interest affects the capital structure of firms. Our empirical analysis is based on a sample of Spanish listed firms for the period 2007–2013. We test the hypothesis that companies have a tax incentive to use debt financing rather than equity financing because interest paid is tax-deductible while dividends paid to shareholders are not. Besides, we use the Shevlin (1990) and Graham (1996a) expected marginal tax rate approach to examine the effects of tax on the debt policies of Spanish firms. In addition, we test the non-debt tax shields hypothesis which considers other tax shelters different from the interest allowances. In the time period analysed, the Corporate Tax Income Law was reformed and this fact might have influenced the debt policy of Spanish listed companies. For that reason, we test for a tax reform effect and consider this shock as a quasi natural experiment for our research. This paper contributes to the existing literature on the impact of corporate taxation on firms’ capital structure, further developing the contributions of previous literature in different ways. Firstly, we provide additional empirical evidence on the relationship between taxes and debt financing. In contrast with other papers, our measure of leverage includes only financial debt and directly excludes other liabilities such as trade payables, which mainly depend on business transactions and not on the effect of corporate taxation. Secondly, our findings shed some light on this issue in the European Union, which has received little attention to date in the literature. Moreover, International Financial Reporting Standards were adopted in Spain on January 1st 2007, which allows meaningful comparison between our results and those from other economies that have also implemented these international standards. Thirdly, we take into account the Spanish corporate tax reform in 2012, as an exogenous shock, which enacted a new thin-capitalization rule limiting the tax deductibility of financing expenses. Besides, and as similar tax reforms have been conducted in many OECD countries, we believe that our conclusions might be portable to other settings. Applying a difference in differences approach, we analyse the potential impact of the abovementioned reform. Finally, we study a special period partially characterized by a severe economic and financial crisis that has dramatically affected European Mediterranean countries such as Spain. Our findings show that marginal tax rates significantly affect the debt policy of Spanish listed firms. The results are consistent with the significance of corporate taxes in company financing decisions considering the uniqueness of the Spanish tax provisions. As expected, there is a stronger relationship between taxes and debt policy in less levered companies. Furthermore, the existence of non-debt tax shields constitutes an alternative to the use of debt as a tax shelter. Regarding the corporate tax income reform approved by the Spanish Government in 2012, we found that the new thin-capitalization rule potentially affected 28 % of the companies in our sample. On average, these companies had higher debt ratios than their non-affected counterparts, and after the reform, the former group on average displayed stronger declining debt ratios as compared to the latter one. Our analysis provides empirical evidence consistent with a tax reform effect. The remainder of the paper is structured as follows. The next section analyses the theoretical framework of the study and presents the hypotheses to be tested. Subsequently, the Spanish corporate tax legislation is described in Sect. 3, including the new thin-capitalization rule. In Sect. 4, we discuss the variables definitions, and explain the estimation of companies’ marginal corporate tax rates. Thereafter, Sect. 5 provides a description of our sample and analyses descriptively the tax data. The empirical model specification, econometric methodology and the results are discussed in Sect. 6. Several robustness checks are presented in Sect. 7, and the final section draws some concluding remarks.",6
7.0,4.0,SERIEs,17 September 2016,https://link.springer.com/article/10.1007/s13209-016-0149-2,Intergenerational mobility: measurement and the role of borrowing constraints and inherited tastes,November 2016,Jordi Caballé,,,Male,Unknown,Unknown,Male,"The analysis of intergenerational mobility refers to the study of the lack of persistence in outcomes across generations within the same family. From an Economics viewpoint, the outcomes that are subject of study range from quantitative variables like consumption, wealth, income or earnings, to qualitative variables like occupation or education attainment. The degree of intergenerational mobility in these economic variables tells us how easily families move between different parts of the associated distribution and, thus, it has been typically used to measure equality of opportunity within an economy. Intergenerational mobility is a topic that has drawn the attention to economists since its presence tends to be associated with an inefficient allocation of resources. For instance, consider an environment where the optimal investment in education for each individual were a function of his learning ability (or innate talent) and that the learning ability of individuals were independent of their predecessors’ ability. In this case, the optimal allocation of education expenditures should be incompatible with any type of correlation in educational attainment between parents and children. The intergenerational persistence in human capital, possibly arising from the correlation between parental wealth and education investment or from other environmental factors acting during the early periods of children’s life, would call for public intervention to restore ex-ante optimality. One of the first issues that the research on intergenerational mobility deals with refers to its measurement. In Sect. 2, I will discuss some of the traditional techniques of measurement. These techniques are based on single numerical values, like the intergenerational correlation and the intergenerational elasticity, or provide more exhaustive information, like the transition matrices. Finally, I will present a more recent measure aimed at capturing upward mobility. While some of the factors underlying the phenomenon of intergenerational persistence of economic variables across the different generations of the same dynasty are purely biological (like genetic inheritance) or cultural (like transmission of tastes, aspirations, or neighborhood effects), others have a more economic component, like the existence of borrowing constraints, which condemn poor families to remain poor across generations since they cannot afford the investment in one of the more efficient social elevators, namely, education. Section 3 reviews briefly these potential drivers of socioeconomic immobility. Sections 4 and 5 pay attention to two of the most popular mechanisms undermining intergenerational mobility. The first mechanism is not necessarily connected to any kind of inefficiency as it has to do with the way preferences of individuals are shaped in the early periods of their lives. If children end up having the same preferences as their parents, then they will tend to take the same economic decisions concerning the consumption profile along their life cycle, occupation or attitude towards work effort. Therefore, intergenerational persistence in consumption and earnings should be observed as a result of this process of preference formation. The second mechanism has however clear welfare implications since it is linked to the existence of financially constrained individuals who cannot implement their optimal decisions like, for instance, those concerning the optimal investment in either their own or their children’s education. For this second mechanism, I will also discuss the positive implications of some popular policy measures aimed at remedying the immobility brought about by financial constraints. Section 6 analyzes the possibility of dramatic reversals of fortune within a family and reviews some of the driving factors for this extreme form of intergenerational mobility. Those factors are typically associated with the conjecture formulated by the famous steel businessman Andrew Carnegie, according to which the sons that receive a large inheritance from their parents tend to allocate less effort to work. Section 7 concludes the paper.",3
7.0,4.0,SERIEs,15 November 2016,https://link.springer.com/article/10.1007/s13209-016-0150-9,Does dual employment protection affect TFP? Evidence from Spanish manufacturing firms,November 2016,Juan J. Dolado,Salvador Ortigueira,Rodolfo Stucchi,Male,Male,Male,Male,"Different conclusions emerge from the available literature on how Employment Protection Legislation (EPL) affects total factor productivity (TFP) growth. On the one hand, there is the view that more stringent EPL reduces TFP. For example, Hopenhayn and Rogerson (1993) and Autor et al. (2007) argue that the distortion and the lower reallocation flows induced by stringent EPL lead firms to use resources less efficiently. Likewise, Saint-Paul (2002) shows that stricter EPL induces firms to adopt mature technologies to improve existing products rather than to experiment with riskier primary innovations. Wasmer (2006), in turn, emphasizes that, by inducing substitution of specific for general skills, stringent EPL hinders worker relocation across industries in the presence of sectorial shocks. Finally, Ichino and Riphahn (2005) claim that layoff protection may reduce workers’ effort (a component of TFP) by inducing higher absenteeism. On the other hand, there is an alternative strand of the literature which argues in favour of favourable effects of stringent EPL on TFP. For instance, since EPL raises reservation wages, Lagos (2006) argues that firms become more selective and only realize more productive matches, while MacLeod and Navakachara (2007) and Belot et al. (2007) claim that EPL induces firms and workers to invest more in match-specific training, therefore improving TFP growth. It should be noticed, however, that the setup considered in most this literature is one where all workers get hired under the same (open-ended) labour contracts, so that a single EPLregime is assumed to operate in the labour market. Yet, this assumption leaves out the case of dual EPL regimes which have been prevalent in several southern European countries, where there are substantial differences between the dismissal regulations pertaining to workers under permanent (open-ended) and temporary (fixed-term) contracts. As a result, the issue of how a  dual EPL regime affects TFP has received much less attention and remains an open issue (see Sect. 2 below). Our goal here is to help fill this gap in two different ways. First, by providing some novel theoretical underpinnings of how labour market dualism could affect technical efficiency through two specific components of TFP, namely, workers’ effort and occupational training. Second, by empirically testing the predictions of our model using longitudinal firm-level data for Spain, which has been often considered as an epitome of dual EPL in southern Europe. Dualism is the Spanish labour market dates back to the mid-eighties. To fight high unemployment, a radical “two-tier” labour reform was passed in 1984 allowing firms to use very flexible fixed-term contracts (entailing low or no severance pay) not only for seasonal/replacement jobs but also for regular activities. Yet, previous stringent EPL rules for open-ended contracts (entailing high redundancy pay) remained effectively unchanged (see, e.g., Dolado et al. 2002 and Bentolila et al. 2008). The share of temporary workers in dependent employment shot from 15% at the time of the reform to 35.4% during the mid-1990s. Since then, temporary contracts have represented more than 90% of all new contracts signed each year. Subsequently, following several EPL reforms at the margin, a plateau of 30% was reached. More recently, despite a massive destruction of temporary jobs during the Great Recession, and a more radical EPL reform approved in 2012 to fight dualism, the rate of temporary work has only dropped to 25%, which still remains one of the highest rates among OECD countries. On top of labour market dualism, another salient feature of the Spanish economy is the large slowdown in labour productivity experienced during the decade preceding the global financial crisis, when both employment and hours worked soared. This fall in productivity growth was not due to lower capital accumulation per worker in the aftermath of rapid employment growth, but rather to a drastic reduction in TFP growth, from 1.5% in 1980–1994 to −0.5% in 1995–2005. Although part of this productivity drop was due to the strong dependence of the Spanish economy on several low value-added industries (like residential construction, tourism and personal services), there is ample evidence documenting that TFP also performed very poorly in tradable sectors, including manufacturing (see e.g., Escribá and Murgui 2009, and Garcia-Santana et al. 2015). This negative outcome at a time where the use of IT technologies was very intense worldwide contrasts sharply not only with TFP growth in the US, which accelerated since the 1990s, but also with the rest of the EU-15, where the productivity slowdown was less acute than in Spain.Footnote 1 As Jimeno and Santos (2014) have argued in their narrative of the crisis in the Spanish economy, the increasing bias in its sectorial composition towards labour-intensive and low- productive industries has been due to two intertwined channels: (i) a labour market regulation which favoured the intensive use of temporary contracts, and (ii) a banking system capable of feeding the huge increase in credit demand by consumers and firms through recourse to external funding and lax facilities for the use of real assets a loan collateral in a context of very low real interest rates. In view of these considerations, our focus here is on issues related to channel (i) above. In particular, we analyze how exogenous variations in the large differential between the firing costs of permanent and temporary workers (dubbed the firing-costs or EPL gap hereafter) impinges on TFP growth through changes in the relative job performance and on-the-job training of these two types of workers. The specific mechanism we highlight here is one where regulatory changes in the EPL gap influence firms’ decisions to upgrade contracts from temporary to permanent status (in short, temp-to-perm conversion rate). These decisions, in turn, affect both workers’ incentives to exert effort and the amount of paid-for training that firms invest on their employees. To the extent that effort and training are relevant unobserved components of TFP, this channel may have played a relevant role in linking changes in dual EPL and firms’ productivity. To make this argument transparent, we propose a stylized model of how employers’ and workers’ decisions interact in a prototypical dual labour market, akin to the Spanish one. Our setup is one where firms find optimal to open jobs under fixed-term contracts which last for one period and cannot be renewed with the same worker in sequence.Footnote 2 At their termination, the employer faces the decision to dismiss the worker (paying a small severance compensation for contract termination) or to upgrade her temporary contract into a permanent one (subject to much higher severance pay). Temporary workers supply effort by trading off its disutility against a combination of a higher wage and promotion prospects. Firms bargain wages with these workers and design contracts (in terms of conversion rates and paid-for-training provision) to elicit that level of non-contractible effort that maximizes their expected profits, subject to participation and incentive compatibility constraints. Insofar as severance pay cannot be fully neutralized in the wage bargaining (see Lazear 1990), our main theoretical prediction is that, unless permanent workers respond to a high EPL gap by exerting much more effort (thus making their jobs much more attractive for firms and temporary workers), a rise in the EPL gap is likely to reduce firms’  temp-to-perm conversion rates. The basic insight is as follows: absent a large increase in effort by permanent workers, a higher EPL gap reduces the profitability of permanent jobs, decreasing firms’ conversion rates and their provision of paid-for-training for temporary workers since their job tenures are short. As a result, the latter opt for lower effort which, ceteris paribus, hinders firm productivity. Conversely, reductions of the EPL gap would lead to opposite results. To corroborate this intuition, we use longitudinal firm-level data from the Survey on Business Strategies (Encuesta sobre Estrategias Empresariales, ESEE) which offers detailed annual information on a representative sample of Spanish manufacturing firms over the period 1991–2005. Although ESEE lacks information on workers’ effort and paid-for training, a unique feature of this dataset is that it provides all the key variables required to compute TFP and temp-to-perm conversion rates for each firm level in each year of the sample. This allows us not only to use TFP as a composite embedding unobserved worker’s effort and training, but also to have a precise measure of the other key variable in the mechanism explored here. Further, ESEE includes information on other relevant variables which could affect TFP, such as R&D expenditure and the proportions of public and foreign capital. Since it reasonable to assume that these variables affect TFP in a more sluggish way than effort and training, they are used as predetermined covariates in the empirical exercise, isolating in this way the relationship between conversion rates and the two specific components of TFP we focus on here. Using TFP as a composite of the two outcome variables of interest raises the issue of how to disentangle the responses of temporary and permanent workers’ performance to changes in the EPL gap. Our identifying strategy relies upon testing the above-mentioned mechanism separately for firms with very high and very low rates of temporary work. We expect the decisions concerning temporary workers to be much more prevalent in the first group of firms. Our main finding is that the response of permanent workers to changes in the EPL gap has been rather minor in comparison to the response of temporary workers. This could be due to two specific features of the two-tier EPL reforms in Spain that we analyze in the empirical section. The first one is that none of these reforms changed the EPL rights of permanent workers in a retroactive fashion (i.e., legal changes applied exclusively to new workers) while those affecting the EPL of temporary workers had immediate effects on them (see Dolado et al. 2002). The second one is that the share of newly hired permanent workers after the reforms was fairly small (less than 5% per year) and evolved very slowly over time. Fragmentary evidence supporting this finding is also provided using data from the European Community Household Panel (ECHP) which, unlike ESEE allows us to examine differences in training practices between permanent and temporary workers. Some suggestive motivation for our empirical approach is provided in Fig. 1 where the (employment weighted) annual averages of the temp-to-perm  conversion rates (left axis) and TFP growth rates (right axis) are jointly displayed for our sample of manufacturing firms.Footnote 3 As can be inspected, both variables exhibit strikingly similar patterns over the sample period, including a common declining trend from the late 1990s to the mid-2000s. Weighted averages of conversion rates and firms’ TFP growth rates (ESEE, 1992–2005) In view of this preliminary evidence, we evaluate the impact of changes in the firing-cost gap on firms’ TFP using three dual EPL reforms that took place during the sample period. Since these were nationwide reforms, variation of their effects across firms is obtained by assuming that those firms (or industries) with a higher share of temporary workers prior to the implementation of the reforms were more strongly affected than other firms with lower shares of temporary work. Indeed, we show that, in those instances when the EPL gap went down (up), conversion rates and TFP move up (down) together much more closely in firms with high shares of temporary workers than in those firms with lower shares. Hence, we take this empirical finding as supportive of our main prediction. However, on its own, this direct mechanism cannot explain the collapse in TFP growth and in conversion rates from the late 1990s to the mid-2000s. This is so since the only reform raising the firing-costs gap during this period took place in 2002, i.e. in the middle of that period. We present some additional empirical evidence indicating that the declining patterns in both variables could be related to the expansion of ancillary industries (with high rates of temporary work) to the real estate sector, where a bubble started to grow at the turn of the century. Insofar this specialization pattern into low-TFP industries responds to the possibility of using flexible fixed-term contracts in an otherwise rigid labour market, we conjecture that this channel may have provided an indirect detrimental effect of dualism on productivity growth. Using Hsieh and Klenow’s 2009 well-known methodology, Garcia-Santana et al. (2015) have recently argued that the poor productivity performance of the Spanish economy between 1995 and 2007 was due to a pervasive misallocation of resources across all sectors. This was especially relevant in those industries more prone to cronyism (e.g., construction and real estate), where the role of connections with public officials is key for business success. Admittedly, our approach abstracts from this channel and instead focuses on the impact of changes in dual EPL on average TFP growth through the mechanism described before. Incorporating their effects on the allocation of resources across firms remains an open issue to be more deeply analyzed in future research. The rest of the paper is organized as follows. Section 2 offers a brief overview of the related literature. Section 3 lays out a model of the relevant decisions taken by workers and firms in a dual labour market, and draws relevant predictions for the subsequent empirical analysis. Section 4 describes the EESE dataset and provides descriptive statistics on the main variables in our mechanism. Section 5 presents empirical evidence about the impact of three dual EPL reforms on firms’ TFP, via its effects on conversion rates, as well as some scant evidence based on a different dataset about its effect on training . Finally, Sect. 6 concludes. An Appendix with three parts contains some algebraic derivations and more detailed definitions of the variables.",25
7.0,4.0,SERIEs,30 August 2016,https://link.springer.com/article/10.1007/s13209-016-0148-3,From Bismarck to Beveridge: the other pension reform in Spain,November 2016,J. Ignacio Conde-Ruiz,Clara I. González,,Unknown,Female,Unknown,Female,"There is no doubt that the population ageing is threatening the financial sustainability of Pay-As-You-Go pension systems. In the last decades, many countries have undertaken major reforms and others are now undergoing similar processes. Spain finds itself in this latter case, and several studies have shown that, in the absence of reforms, pension expenditure would increase in the next four decades, accelerating from 2035 onwards (European Commission 2009a; MTIN 2008; Jimeno et al. 2008; Diaz-Saavedra 2005; de la Fuente and Doménech 2009; Rojas 2005; Sanchez-Martin and Sanchez-Marcos 2010; Alonso and Herce 2003; Herce et al. 2009). The changes approved in Spain in 2011, which included increasing the retirement age from 65 to 67, among others, together with the introduction of the Sustainability Factor in 2013, are the most significant reforms that have been made in decades. Despite the last two major reforms of the Spanish pension system, the ageing process is so intense that it will be impossible to prevent the replacement rate from decreasing at least 20 to 25 percentage points moving from the current 74 to 50 % in the next decades.Footnote 1 As a result, a public debate has arisen, in which it is argued that it would be fairer if the inevitable fall in the replacement rate were mainly focused on pensioners with the highest pensions. Therefore, if this type of reform were to be implemented, it would change the basis of the Spanish pension system from a contributory system (or Bismarckian) to a universal pension system (or Beveridgean). Currently, both types of pension models exist in Europe. Bismarckian systems are designed to provide a sufficient retirement income for all workers: from the low skilled to the highly skilled. In contrast, the Beveridgean pension system aims to ensure a minimum pension and, as a result, requires lower contributions, leaving room for the middle classes to add to their pension pot with private savings. Indeed, countries with a Beveridgean system have an average pension expenditure of 6 % of GDP, while countries with a Bismarckian pension system have an average expenditure of more than 10 % of GDP. In view of the above, how could this type of reform be implemented? In the case of Spain, this would be feasible if some key parameters of the pension system were modified—in particular the value of the maximum and minimum pension, as well as the upper and lower limit of the contribution base. This would result in major changes in the redistribution and the overall generosity of the system, which is defined as the ratio between average pension and productivity. In fact, a low intensity reform of this type has been carried out in Spain. Some experts have named it the ‘silent reform’ of the pension system due to the fact that it is almost imperceptible to voters in the initial phase. More specifically, the key measures in this kind of reform are: (1) adjusting pensions in line with inflation instead of wage growth, and (2) setting a cap to the maximum retirement benefit an individual may receive (maximum pension) and indexing it to inflation. In a period of economic growth, these measures would imply an increase in the number of retired individuals whose pensions would be limited due to the maximum pension limit. For this reason, any of these two measures could reduce future pension expenditure because they would break the link between benefits and wage growth. The aim of this article is to quantify the potential consequences of this hypothetical reform, which has been widely discussed and studied in theoretical terms in Spain [amongst others Boldrin et al. (2000), Jimeno (2002), Alonso and Herce (2003), Conde-Ruiz and Alonso (2004) and Conde-Ruiz and Jimeno (2004)]. To date, there are no studies quantifying its implications, and this paper is the first to analyse its hypothetical impact on the Spanish pension system. The main results of the paper show that if this type of reform were to be fully implemented, it would change the basis of the Spanish pension system by transforming it into a universal pension system (or Beveridgean). We quantified how the intra-generational redistribution element in the pension system would increase. Moreover, we show the important implications that this sort of reform would have on reducing future expenditure, as well as on the overall generosity of the system. In order to evaluate the potential of the reform in the sustainability of the Spanish pension system we compared its effects with those of the 2011 reform, which, among other changes, increased the retirement age from 65 to 67. The idea of converting the Bismarckian Spanish pension system into a Beveridge one is starting to enter public debate. The financial crisis in Spain has exacerbated income inequality, not only due to significant wage devaluation, but also to a high increase in the long-term unemployment rate. In this respect, Conde-Ruiz and Profeta (2007) have provided a positive theory on why a Bismarckian or a Beveridgean system may arise. They show that income inequality represents the key determinant in social security design and suggest that Beveridgean systems may be supported by a voting coalition between low-income individuals, who favour its redistributive aspect, and high-income individuals, who support the reduced size of the Beveridgean system, which allows them to make more use of private provisions. This paper is set up as follows: Sect. 2 discusses the role of the upper limits to contribution bases and pension benefits and the institutional aspects of the Spanish pension system. The methodology used in our simulations is presented in Sect. 3.1. Next, the results related to individual pensions are collected in Sect. 3.2, regarding the percentage of new pensions that are affected by both the upper cap and the amount to which the pension is limited. The implications regarding the sustainability of the pension system are discussed in Sect. 3.3. In Sect. 3.4, the consequences for the nature of the system are examined. Finally, we draw our conclusions. Additional results are presented in the Appendix, together with the rules for calculating retirement pensions in Spain.",9
8.0,1.0,SERIEs,24 March 2017,https://link.springer.com/article/10.1007/s13209-017-0155-z,Are the Spanish long-term unemployed unemployable?,March 2017,Samuel Bentolila,J. Ignacio García-Pérez,Marcel Jansen,Male,Unknown,Male,Male,"In the aftermath of the Great Recession, long-term unemployment reached unprecedented levels in Spain. At the worst moment during the crisis, 16% of the labor force and nearly two out of three unemployed persons (64%) had been searching for a job for over a year. Since that time, the situation in the labor market has improved considerably, with employment growing at a rate around 2.3% per year since 2014, but the share of long-term unemployed is still 57%, while the share of unemployed with spells lasting more than 2 years is 42%. The high incidence of long-term unemployment (LTU) entails a risk for social cohesion and it poses enormous challenges for policymakers. The probability of exit from unemployment tends to fall with duration due to factors such as skill depreciation, loss of motivation, and discrimination on the part of employers. Moreover, health problems due to mental stress, the accumulation of unsustainable levels of debt or housing problems also tend to increase over time.Footnote 1 Reenfranchising the long-term unemployed therefore becomes progressively harder,Footnote 2 exposing individuals to a risk of social exclusion and society at large to high levels of structural unemployment. This state of affairs motivates our work. We start by documenting the factors that have contributed to the buildup of LTU and we subsequently perform an econometric analysis to quantify the impact of individual and aggregate determinants on the probability that an individual enters and exits LTU. The main objectives of our econometric analysis are to isolate the impact of unemployment duration on job-finding rates and to identify the population groups that are most vulnerable. Our empirical approach delivers reduced-form estimates, resulting from the interplay of labor demand and supply decisions, and hence do not have a causal interpretation. For this reason, we also analyze the response of self-reported reservation wages to both unemployment duration and changes in aggregate labor market conditions. The factors that account for the dismal performance of the Spanish labor market are well-known. Like many other European countries, Spain suffered two consecutive recessions, the 2008 international financial crisis and the Eurozone crisis, but in the Spanish case these external shocks were compounded by the bursting of a housing bubble and a large-scale banking crisis that led to a severe reduction in bank lending to firms (Jimeno and Santos 2014; Bentolila et al. 2016). The collapse of the construction sector alone added 1.7 million workers, mostly low-educated, to the ranks of the unemployed. Furthermore, all these shocks interacted with institutional factors, such as the dual nature of the Spanish labor market and a rigid system of collective bargaining, that resulted in high worker-turnover due to the massive destruction of temporary jobs and a slow adjustment of negotiated wages.Footnote 3 Here we take the shocks to the Spanish economy as given and pursue an analysis of the mechanisms that generate the high degree of unemployment persistence that has fueled the buildup of LTU. Our descriptive analysis in Sect. 3 is based on data from the Spanish Labor Force Survey (LFS). It reveals that LTU especially affects older and less educated workers. Nonetheless, even for prime-age male workers we obtain an LTU share close to 60%. Moreover, inspection of longitudinal data indicates a strong negative relationship between the average transition rate from unemployment to employment and the duration of unemployment spells. A key objective of our empirical analysis is to assess to what degree this negative correlation reflects genuine duration dependence. This question is important because such a negative correlation could also be the outcome of dynamic selection. Over time the composition of the pool of unemployed tends to worsen, as the most employable workers leave first. This process generates a negative relationship between the average job-finding rate and duration that is entirely driven by composition effects rather than by a decrease in the exit rate of individual unemployed workers. In other words, duration dependence is mainly a reflection of the lack of demand, while the alternative explanation of dynamic selection would point at the importance of supply-side factors, such as inadequate skills, that result in low job-finding rates from the very start of the unemployment spell. In order to avoid the potentially confounding role of changes in worker characteristics, in Sect. 4 we estimate state-of-the-art duration models. To account for selection, we estimate similar models for the outflow from both employment and unemployment, and we allow for worker unobserved heterogeneity. To take into account the dual nature of the Spanish labor market, we model flows to and from temporary and open-ended, or permanent, jobs. And, lastly, to properly capture the impact of the unemployment benefit system, we use workers’ full employment history to measure their entitlement to contributory benefits. Most of these refinements are absent from the recent literature on long-term unemployment, as described in Sect. 2. The estimation is performed using the Continuous Sample of Working Lives, which is a large sample of Social Security records. This data set allows us to capture short-term worker flows which are mostly missed by the LFS. On the other hand, this source does not allow us to distinguish between unemployment and nonparticipation, though we show that this is probably not an important shortcoming. In view of the fact that in Spain—like in other countries—the Great Recession reduced male employment significantly more than female employment, we limit our analysis to the population of native prime-age males.Footnote 4
 Our empirical results indicate that in Spain the conditional probability of entering LTU is very large and is significantly raised by receipt of unemployment benefits, mature age, low experience, and—especially once we mitigate the relevance of spells below 1 month—low education and low skill. In agreement with the recent literature on LTU in the US, we also find that duration dependence and not dynamic selection is the primary source of the low job-finding rates of the long-term unemployed. Temporary contracts help to reduce the risk of LTU conditional on unemployment, but they also cause huge inflows into unemployment. For this reason, it is natural to ask whether temporary contracts are indeed a useful work-sharing arrangement during a crisis. A related question is whether short-duration contracts or placements have a noticeable impact on the subsequent employment prospects of the long-term unemployed or whether employers ignore these spells when assessing prospective employees. Here we take a first step towards answering the latter question by considering a model in which exits to spells below 30 days are treated as censored. Our estimates indicate that temporary jobs especially help specific groups of workers to leave unemployment, namely the least educated and experienced ones. In Sect. 5 we conduct an exploratory analysis of reservation wages during the Great Recession with a different data source, the Spanish Survey of Family Finances. We find that self-reported reservation wages strongly adjust with the aggregate business cycle and also, though much less, with individual unemployment duration. In Sect. 6 we summarize our findings and argue that higher aggregate demand alone will not solve the LTU problem. Expanding and, especially, improving active labor market policies, linking them to the receipt of unemployment benefits, and intensifying early activation would all be helpful measures. The Appendix includes further empirical results.",23
8.0,1.0,SERIEs,03 March 2017,https://link.springer.com/article/10.1007/s13209-017-0154-0,The continuous sample of working lives: improving its representativeness,March 2017,Juan Manuel Pérez-Salamero González,Marta Regúlez-Castillo,Carlos Vidal-Meliá,Male,Female,Male,Mix,,
8.0,1.0,SERIEs,19 November 2016,https://link.springer.com/article/10.1007/s13209-016-0152-7,Influence networks and public goods,March 2017,Dunia López-Pintado,,,Female,Unknown,Unknown,Female,"The study of networks has been of significant importance in diverse academic fields such as sociology, physics and computer science (see, e.g., Wasserman and Faust 1994; Newman 2003, and the long list of references cited therein). The last two decades have witnessed how numerous phenomena of economic relevance have also been studied using the paradigm of networks. Instances are network formation (e.g., Jackson and Wolinsky 1996; Bala and Goyal 2000), diffusion of behaviors (Morris 2000; López-Pintado 2008a), labor markets (Calvó-Armengol 2004; Calvó-Armengol and Jackson 2007), peer effects in education (Calvó-Armengol et al. 2009), crime activities (Ballester et al. 2006; Goyal and Virgier 2014), financial markets (Elliot et al. 2014) and microfinance credits (Benerjee et al. 2013).Footnote 1
 We consider a model of local public goods in a random network context. There are many socioeconomic situations which can be described as a local public good, that is, a good that is non-rival and non-excludible among neighbors in a relevant network. Some examples are innovations among collaborating firms, complementarity of skills within social contacts, the provision of an open source product (e.g., software) or information in the Internet (e.g., websites or blogs). We shall be concerned with directed networks, i.e., networks in which the benefits from interacting with an agent providing the public good is only one way. We define the out-degree (observability) of an agents as the number of agents she observes, whereas her in-degree (visibility) indicates how many agents observe this agent. The correlation between agent’s out-degree and in-degree might depend on the application. For instance, in friendship networks, this correlation tends to be high because friendship is mostly bidirectional, whereas for other types of networks such as the WWW this correlation could potentially be lower. As standard in this literature, we assume that the network is exogenously given in order to isolate the decision to contribute to the local public good from the network formation issue. We depart, however, from the fixed network approach and consider random networks instead (see e.g., Pastor-Satorrás and Vespignani 2001; Jackson and Rogers 2007; López-Pintado 2008b, 2012, 2013; Galeotti and Goyal 2009, among others). That is, the network is characterized by its large-scale properties which are fixed, although the connections evolve randomly over time. This paper focuses on two of such properties: the (out-)degree distribution and the in/out-degree correlation. Agents have to decide whether to invest, or not, in the public good. If they enjoy the public good -either because they have invested in it or because they free ride on some other agent in the population that has done so- they obtain some benefits. This determines a game known as the best-shot game in which an agent has incentives to invest in the public good only if no other agent observed by her has already done so. The static version of this model on a fixed (directed) underlying network structure has some limitations. On the one hand, for most networks structures there will be a large set of equilibrium outcomes and therefore we would incur in multiplicity problems. On the other hand, for some simple networks an equilibrium, in pure strategies, will not exist. We therefore propose and focus on an alternative approach based on a dynamics influence process which leads to a unique equilibrium prediction. In particular, there exists a unique globally
stable outcome (fraction of individuals investing in the public good) of this dynamics characterized by the (out-)degree distribution and the in/out-degree correlation. Thus, some comparative statics results can be provided. There are two outcomes of interest: (1) the fraction of public good providers in equilibrium, and (2) the fraction of links that reach a public good provider in equilibrium. The comparative statics results lead to the following conclusions. On the one hand, for any given out-degree distribution, an increase in the in/out-degree correlation increases measure (1) and decreases measure (2). On the other hand, for any in/out-degree correlation, if the network is sufficiently dense, a First Order Stochastic Dominance shift (Mean Preserving Spread) of the out-degree distribution decreases (increases) measure (2). We partially compare the efficient and equilibrium outcomes. We find that in the case of an homogeneous population in equilibrium there is underprovision of the public good. We also show that if we allow for some heterogeneity regarding out-degrees the efficient and equilibrium outcomes provide opposite results; the probability of contributing to the public good increases with respect to out-degree in equilibrium, whereas it decreases in the efficient state. Public goods in a network context was first analyzed in the seminar paper by Bramoullé and Kranton (2007) who characterized the (multiple) Nash equilibria of a public good game in a static network framework. Boticinelli and Pin (2012) addressed the issue of equilibrium selection by introducing a dynamic model. Galeotti et al. (2010) also shrink considerably the potentially large set of equilibria that arise under complete information by assuming incomplete knowledge by part of the consumers with respect to the network. Bramoullé et al. (2014) focus on games of strategic substitutes on networks with linear best-reply functions which has recently been extended to non-linear settings by Allouch (2015). We contribute to this vast literature by analyzing local public goods in a random network context. The rest of the paper is organized as follows. In Sect. 2 we describe the model. In Sect. 3 we characterize the equilibrium. The comparative statics of the equilibrium outcome are discussed in Sect. 4. In Sect. 5 we provide some results on efficiency. Finally, in Sect. 6 we conclude.",3
8.0,2.0,SERIEs,23 November 2016,https://link.springer.com/article/10.1007/s13209-016-0151-8,Burden sharing in deficit countries: a questionnaire-experimental investigation,June 2017,Wulf Gaertner,Lars Schwettmann,,Male,Male,Unknown,Male,"During the last decades public debt continually increased in almost all member states of the European Union with only a few short-lived exceptions. The financial crisis in 2008, starting with the collapse of Lehman Brothers, dramatically aggravated this situation. The construction industry, small-scale manufacturing, middle-sized firms and the export sector were hard hit in many countries which led to a sharp decline of tax revenues for the public authorities. While several countries from the Northern “hemisphere” of the EU have been faring relatively well in terms of growth rate and unemployment ratio, most of the countries in the Southern “hemisphere” have undergone a phase of negative growth, rising unemployment and, as a reaction to this, general political discontent. This has led to outbreaks of violence but also to an atmosphere of disappointment and frustration on the part of those who were and still are seriously hurt by the economic downturn. The countries most affected started to take, more or less successfully, measures to cut public spending. At the same time, higher taxes were introduced in order to prevent that public deficits reach intolerable heights. Both the cuts within the welfare system and the increase in taxes were also required by other members of the currency union as a precondition for financial assistance to prevent an even more severe financial deterioration. These measures raise a general issue, namely, who should carry the burden of all this? There are at least two aspects to consider, a within-country perspective and an outside view held by other members of the currency union. In this paper, we shall focus on the former perspective, the inner-country situation. Should all income groups of society in the crisis-ridden countries contribute to a reduction and consolidation of the public budget? And even if there is agreement on this question, how should the corresponding shares be determined? Greece probably is the most prominent example of a programme of austerity that immediately affected its population via cuts in unemployment benefits, pensions, tax increases and the cut-back of other financial transfers. These cuts were, however, carried out at a relatively high level, at least when compared with the levels in various other countries within the monetary union which were also forced to turn to a policy of severe cuts. People in Greece and in other South-European countries are stakeholders whose position is, understandably, tainted by self-interest. One should, however, keep in mind that the consequences of the various policy measures taken are much more perceptible to the population in these countries than to citizens in better-off economies who at this point mainly suffer from interest rates on savings that are close to zero per cent. The views of the people in the latter countries, of course, matter as well. Many of them fear that at some point, they will be the ones who, with their own tax money, have to foot the bill when public debt can no longer be expected to be paid back by the high-deficit countries. Important as this aspect is in relation to the question of viability of the Euro currency union, we will not consider it any further. Again, our focus will be on the problem of burden-sharing in the ailing countries, viz. in Southern Europe and Ireland. Theoretical and empirical studies on economic inequality have been the topic of several scholars during the last decades (see e.g. Amiel and Cowell 1999; Engelmann and Strobel 2004; Carlsson et al. 2005). In the social choice literature situations of allocating burdens are called “claims problems”. This is somewhat different to the typical scenario of a bargaining problem where agents come together in order to jointly bring about an outcome which is supposed to be superior to their individual initial position, the cooperative surplus, and which each of them would not have been able to achieve just by his or her own productive effort alone. Most generally a claims problem is completely described by the vector of relevant claims and the amount of resources to be allocated among claimants. The theoretical literature aims at identifying well-behaved rules, which determine for every claims problem a division of the amount available among the individuals (see Thomson 2003 for an overview). Different division schemes proposed in the literature include the proportional rule, which aims at proportionality of awards and losses, the constrained equal awards rule (CEA), which distributes the available resource evenly as long as no individual receives more than what she claims, and the constrained equal losses rule (CEL), which equalizes losses under the constraint that nobody ends up with a negative amount. These rules are sometimes called the “three musketeers” (Herrero and Villar 2001). A fourth classical solution is the Talmud rule proposed by Aumann and Maschler (1985) which rationalizes allocation situations found in the Talmud. All these rules, and several more, have been characterized axiomatically (see e.g. Bosmans and Schokkaert 2009; Thomson 2011, 2013). Since our focus is on loss sharing, a reference to contributions which explicitly deal with lower bounds is warranted (Thomson 2013, 2015). Moreno-Ternero and Villar (2004) introduce a property called securement which guarantees a minimal share to each person and they use this notion to characterize the Talmud rule. This property can be considered as a weakening of the concept of lower bound introduced by Moulin (2002). In this context, one should also refer to so-called equal-sacrifice rules characterized by Young (1988) and generalizations of this notion, namely generalized equal-sacrifice rules, introduced by Chambers and Moreno-Ternero (2016). With respect to the claims problem in general, three interesting empirical investigations have recently been published. First, in questionnaire experiments conducted by Bosmans and Schokkaert (2009), students in Belgium and Germany faced allocation problems in the context of the division of revenues and the distribution of pensions. The authors varied both the amount to be divided and the degree of inequality of the claims vector. Allocations offered in the questionnaire were in accordance with various different rules known from the literature and, depending on the specific situation, have been classified by the authors as egalitarian (for example, constrained equal awards), neutral (e.g. proportionality rule) and anti-egalitarian (e.g. constrained equal losses). Their results showed strong support for the proportional rule in both contexts. In contrast, the constrained equal awards rule and the constrained equal losses rule received much less support, while other progressive solutions garnered some attention. Second, Herrero et al. (2010) used both laboratory experiments and questionnaire investigations to gain further insights into the acceptance of the three prominent rules described. In the study four hypothetical situations were presented. Participants had to imagine a bank going bankrupt. Correspondingly, claimants were shareholders, depositors, or non-governmental organizations. In a fourth context the problem was framed in a neutral way. Again, the proportional rule served as a convincing coordination device in the lab and was also accepted by a larger majority of questionnaire respondents. Furthermore, the authors could detect a remarkable sensitivity towards the context within which the situations were presented to participants. Third, Gächter and Riedl (2006) also considered the attractiveness of the “three musketeers” and a simple rule called “equal-awards”. They conducted laboratory experiments and questionnaire studies. In the experiments participants had to negotiate with another individual about the division of a given amount of money. The authors constructed two different decision problems with distinct degrees of inequality with respect to the claims. In each situation, individuals were assigned to either group of claims on the basis of their results in a knowledge quiz. The bargaining process was “free-form”: participants communicated with the help of computers and had to reach agreement within a given time limit. Without an agreement they received nothing except their show-up fee. The same decision problem was described in a questionnaire. Participants from different samples had to state which allocation they viewed as “fair”. Yet another group of subjects faced the same problem but received descriptions of different rules and corresponding outcome in the specific situation. These participants were asked to rank the rules according to their attractiveness. Gächter and Riedl report several results. Most importantly for the present study, they found the proportional solution to be the most attractive in the questionnaire studies, and this result held irrespective of whether respondents were asked to state the fair solution or to rank different rules that were described, while negotiation agreements were closer to the constrained equal awards rule. The authors explain these disparities by pointing out that the proportional solution is an attractive focal point without negotiation where the normative aspect comes to the fore, whereas the symmetric disagreement payoff in the bargaining experiment is zero to each person. This might have highlighted the attractiveness of more equal solutions. Furthermore, negotiators were forced to find compromise solutions in order to avoid a zero payoff. Nevertheless, it should be pointed out that especially in the experiments by Gächter and Riedl participants bargained over some additional money, even though it turns out to be less than expected, while in our problem of burden sharing different income groups are thought to claim to be exempted from further tax payments. The importance of this difference was already pointed out in a study by Schokkaert and Overleat (1989) who conducted questionnaire experiments on the perception of justice in micro-level production contexts. Based on the observation that respondents reacted differently towards gains and losses, their results suggest that surplus sharing and cost sharing models have to be distinguished. Furthermore, the authors argue that acquired rights are important in the way that subsequently imposed income reductions are perceived differently compared to immediate losses. In the following sections, we shall report our findings on burden-sharing in relation to questionnaire investigations that were run in countries that were particularly hard hit both economically and financially. It was our expectation that respondents in those countries would be well acquainted with the economic and social problems surrounding them and would therefore take our questionnaire study seriously. The countries from which we were able to gather empirical data were Cyprus, Greece, Ireland, Italy, Portugal, and Spain. The structure of the rest of this paper is as follows. In the next section, we shall explain our questionnaire experiment (“rules” for the proceeding of the experiment and the full text of our questionnaire are relegated to Appendices A and B, respectively). In this section, we shall also briefly report findings from pilot studies, the genesis of our investigation so to speak, that we did at universities in Germany, Italy, and Spain (corresponding results are summarized in Appendix C). In the third section, we report the descriptive statistics for all six countries included in the final sample, statistically verify several hypotheses and present some regression results. In the fourth section we offer a general discussion of our findings and a few concluding remarks.",8
8.0,2.0,SERIEs,10 March 2017,https://link.springer.com/article/10.1007/s13209-017-0156-y,On the effect of taxation in the online sports betting market,June 2017,Juan Vidal-Puga,,,Male,Unknown,Unknown,Male,"A remarkable feature of online betting (which includes sports, casino and card games such as poker) is that their operators require little more than an internet web page to enter a new market. As opposed to offline operators, they do not need to open physical selling points. Under an unregulated market, the cost of offering a bet is inelastic with respect to the bet volume, i.e. the total sum of betting stakes. This is because online betting users can bet from any internet terminal, even at home. As opposed, offline betting users need to be physically at a selling point. Things may be different in a regulated market. Over the last years, many European countries have been regulating their online betting and gaming sector. However, this regulation has not been done in a uniform way throughout the different countries. In general, the basic taxation scheme is based on two types of taxes: the General Betting Duty (GBD) is levied as a proportion of betting stakes; whereas the Gross Profits Tax (GPT) is levied as a proportion of the net revenue of the operators. Some examples: the United Kingdom applied a 6.75% tax on GBD until October 2001, when it was replaced by a 15% tax in GPT (National Audit Office 2005). Italy applies a 2–5% tax on GBD (Ficom Leisure 2011) for general sport betting and a 20% tax on GPT for spread bets (PwC 2011). France applies a 8.5% tax on GBD (Global Betting and Gaming Consultancy 2011) since 2010. In Germany, tax rates largely depend on the respective federal state, and they vary between 20 and 80% on GPT plus a 5% federal tax on GBD (Hofmann and Spitz 2015). In 2011, Spanish authorities approved a lawFootnote 1 that applies a 25% tax on GPT for some types of sports bets and a 22% tax on GBD for others, plus a 0.1% tax on GBD. In Table 1 we summarize the data. In the Spanish case, GBD has been the taxation scheme in the most traditional offline sport betting (la quiniela), which takes a parimutuel structure. In a parimutuel market, a winning bet pays off a proportional share of the total stake on all outcomes. However, the most popular online sport operators are specialized in another two markets: Fixed-odds and spread. In a fixed-odd market, the operator sets the odds for each possible outcome of the match, and the bettors decide whether they accept or not these odds. In a spread market, the operator acts as an intermediator among the users, who bargain the odds. For sport matches, a bet of 1 monetary unit on a particular team yields a return of \(\frac{1}{\pi }\) monetary units in case the team wins the match, and 0 otherwise. In this context, an odd \(\pi \in \left( 0,1\right) \) is defined as the probability assigned by the market. Notice that any risk-neutral bettor would find it profitable to bet at odd \(\pi \) when her private probability estimation is higher than \(\pi \). In parimutuel and spread bets, the operator’s profit comes from a commission on either the amount at risk or the winning amount (typically around 5% in online spread operators). In fixed-odds bets, the operator’s profit comes from the odds, which should sum up more than \(100\%\)
Footnote 2 for all the possible outcomes of the sport match.Footnote 3
 In this paper, we model the three types of market in a general setting. The regulator decides on the general taxing scheme (either GBD or GPT) and the operators decide on their commission (parimutuel and spread operators) or odds (fixed-odds operators). We assume that the spread betting commission is applied to the winning bets (as it is typical in online spread operators), whereas commission in parimutuel betting applies to the total amount (as in the Spanish regulation). We show that, from the online bettor’s point of view, it is preferable a GPT scheme, in the following sense: In equilibrium, the odds are not affected by the taxation under GPT; whereas a GBD scheme would reduce the odds and hence the bettors’ utility. These results agree with the ones presented by Smith (2000) and Paton et al. (2001, 2002), whom analyse the effect of the different taxation schemes in Australian, UK and USA betting markets. These results, however, are more focused on offline betting operators and government revenue. Moreover, they take into account the marginal cost of each bet. As opposed, we assume that these marginal costs are negligible. There are other works that focus on parimutuel markets: Ottaviani and Sørensen (2009) provide a model that explains the empirical evidence of underdogs overbet. These authors argue that this bias may be due to privately informed bettors. As opposed, we prove (Corollary 1) that the spread bet operator would get a higher profit if the underdog wins the game. Other works concentrate on fixed-odd markets. For example, Bag and Saha (2011, 2016) study the externalities due to bribery in sports; and Levitt (2004) argues that the operators may achieve higher profits by an accurate prediction of the match outcome. As far as we know, no similar research has been addressed for spread markets. The rest of the paper is organized as follows. In Sect. 2 we describe the model. In Sect. 3, we characterize the equilibrium payoffs in each of the markets. In Sect. 4 we provide the main results. In Sect. 5, we study the symmetric case. In Sect. 6, we present some concluding remarks. Technical proofs are deferred to the “Appendix”.",1
8.0,2.0,SERIEs,26 December 2016,https://link.springer.com/article/10.1007/s13209-016-0153-6,"Health, responsibility and taxation with a fresh start",June 2017,Aitor Calo-Blanco,,,Male,Unknown,Unknown,Male,"The idea of a fresh start defends compensating those who have changed their preferences and hence regret their past choices. Although the issues of health and the provision of health care have attracted a lot attention recently, they have been barely studied in relation to such an idea. In this paper we study how the interaction between the ethical concept of fresh starts and basic notions of fairness affects the optimal design of an incentive-compatible tax scheme which aims to provide extra health care to those who may need it. The forgiving ethical view that advocates giving those who regret their previous choices a fresh start is a controversial one (see Arneson 1989; Dworkin 2000, 2002; Fleurbaey 2002, 2005a, 2008). Some authors are reluctant to endorse this view as they consider that it may entail both incentive and moral issues (e.g., Arneson 1989; Dworkin 2002). Their argument is twofold. On the one hand, these authors claim that helping those who have mismanaged their resources generates incentive problems, as some individuals may fake regret in order to receive extra resources. On the other hand, such authors also consider that it is unfair to help regretful individuals for a frugality they have never practised. This would allow the spendthrift to ‘have the proverbial cake and eat it too’ (see Dworkin 2002). All these arguments against fresh starts have been disputed by Fleurbaey (2005a, 2008). First of all, he argues that if providing individuals with a fresh start were completely free, basically no-one would be against this principle. Consequently, the moral criticisms seem to be only valid in cases in which fresh starts entail a cost to others. Moreover, the idea of charging costs to other individuals is not only defended by this ideal, but by any redistribution policy as well. Second, in terms of efficiency Fleurbaey (2005a, 2008) also shows that a properly designed incentive-compatible fresh start policy limits any possible ‘undeserved’ compensation that individuals might receive from misreporting their real preferences. Additionally, he argues that this policy would increase freedom as individuals would no longer be forced to bear the consequences of their early choices. Together with this idea of forgiveness and fresh starts, in this paper we also consider the issue of compensating individuals who are endowed with different traits. The most relevant theories of fairness and responsibility argue that inequalities in agents’ outcomes may contain elements for which those agents are responsible, but also other elements they should not be held responsible for. The aim of such theories is to reduce only the outcome differences that originate in factors for which individuals cannot be held responsible (e.g., Rawls 1971; Dworkin 1981a, b; Arneson 1989; Cohen 1989; and Roemer 1998). The objective of this paper is to study the optimal distribution of medical and non-medical consumption that results from the implementation of a public policy that endorses these two ethical principles, forgiveness and fair compensation. The reason to focus our analysis on the issue of health is that there is a large consensus that it is one of the most crucial dimensions of the individual well-being, and hence its allocation cannot be analysed as the distribution of other alternative goods such as consumption (e.g., Fleurbaey and Schokkaert 2011). Moreover, as regards forgiveness it is not infrequent to observe the implementation of the idea of a fresh start to real-life situations in which health is involved. An example of this kind of implementation is any public health system, which usually treats all those individuals who are in a bad health condition, regardless of their lifestyle. Interestingly enough, some countries are recently starting to defend the opposite view in order to reduce the cost of health care. Specifically, they propose limiting the right to health care of those who do not stop leading an unhealthy lifestyle. These two ethical principles that we endorse have been separately analysed by previous works (e.g., Fleurbaey 2005a, b). In a recent paper, Calo-Blanco (2014) proposes a way of combining forgiveness and fair compensation in a model in which individuals differ in both their health care needs and their preferences over health and consumption. Specifically, he derives a social ordering function that gives the highest priority to that agent with the largest level of a specific measure of individual well-being loss. Such a measure is defined as the difference that exists, in terms of a hypothetical consumption, between the individual’s current situation and the ideal choice she would have made with her true ex post preferences if she had the most favourable health disposition possible. Given the way in which this value is defined it is possible to reach a compromise between compensating individuals endowed with a poor medical disposition, and granting those who regret their initial choice a fresh start. Nevertheless, an important question that yet remains to be addressed is the implication that the adoption of a particular social ordering function accounting for a fresh start has for the design of the optimal taxation policy. Following Mirrlees’ (1971) seminal contribution, many papers have studied the issue of social welfare and optimal taxation (e.g., Atkinson 1995; Diamond 1998). However, many of these papers have resorted to a specific choice of both social evaluation and individual utility functions, and hence their results are only robust to the particular specification they have assumed. By contrast, a recent branch of the literature has focused on the theory of optimal taxation from a more general viewpoint. More precisely, such a branch resorts to social value judgments that are defined by means of fairness conditions, which consider only individual non-comparable ordinal preferences. The most representative models of this last approach are due to Fleurbaey and Maniquet (2006, 2007). Assuming that individuals differ in both their preferences and their labour skills, the main objective of these papers is to provide criteria for the characterisation of the optimal fair tax scheme over observable income levels. They conclude that, in order to maximise social welfare, the optimal scheme should focus on a particular region of the budget set that is attainable by a specific type of agent. An extension of this framework is proposed by Valletta (2014), who analyses the joint taxation of income and health expenditure in a model in which agents have heterogeneous preferences over consumption, labour and health. By assuming this recent approach to optimal taxation, in this paper we study the overall distribution of consumption and medical expenditure that results from a fresh start policy that is implemented via a particular tax scheme. This policy is designed to satisfy the social preferences that minimise the individual well-being loss as defined in Calo-Blanco (2014). However, such social preferences are derived in a first-best framework in which health states and medical dispositions are observable, something that is usually ruled out in taxation models (see Valletta 2014). Therefore, we adapt our analysis to a second-best scenario in which only the distribution of the total expenditure on health and consumption is observable, considering this way that the tax scheme has to be defined as monetary transfers that depend on this distribution alone. Specifically, agents are taxed as a function of their consumption, and as a consequence they may or may not receive a public subsidy that can be exclusively devoted to get additional health treatment. Although the analysis of any policy for a population which is heterogeneous in several dimensions is a difficult task, in this paper we present a characterisation of the optimal (tax-treatment) fresh start policy and the allocation that it generates. The first conclusion we reach is that the policy defines an optimal balance between paying additional health treatments and putting protective constraints on early individual non-medical decisions (by means of a consumption tax) to limit the possibility of a future regret. This result can be used as a solution to the recent public discussion about how to deal with those who refuse to lead a healthy lifestyle. Specifically, to avoid having an ‘unfair’ society that limits the right to health care, which is a pivotal element of the individual well-being, our fresh-start policy advocates limiting instead the individuals’ ability to fully enjoy their preferences. This solution is in line with the arguments proposed by Fleurbaey (2005a) that defend the idea of a fresh start. The second outcome of our characterisation results is that the scope of the fresh start policy is limited by the informational constraints of the model. As a result of this limitation the planner cannot guarantee the goal of perfect equality in terms of individual well-being. This is so because such informational constraints allow those agents endowed with a good medical disposition to apply for low tax rates and extra health treatments that are originally intended to help those with higher health care needs. The third main conclusion we obtain is that the largest well-being loss is defined by the specific shape of the set of the indifference curves associated with each type of preferences, and not by the ‘size’ of the change between ex ante and ex post preferences. This is so because the equivalent measure that we use to compare well-being losses is specifically defined for each type. Finally, under an additional assumption about individual preferences, we show that the agent who pays the highest tax is someone who makes her choice with the preferences that show the largest concern for health, that is, someone who does not regret her choice. The rest of the paper is organised as follows. Section 2 introduces the basic elements of the model, including the social preferences that society endorses. Section 3 develops the characterisation of the optimal tax scheme. Additionally, it displays the numerical computation for a particular parameter configuration of the model. Section 4 offers the conclusions of this study. All proofs are contained in the appendix.",1
8.0,3.0,SERIEs,21 July 2017,https://link.springer.com/article/10.1007/s13209-017-0158-9,"Productivity, taxes, and hours worked in Spain: 1970–2015",August 2017,Juan C. Conesa,Timothy J. Kehoe,,Male,Male,Unknown,Male,"What forces have driven output growth and fluctuations in Spain over the last three decades? What has been the impact of the evolution of taxes and total factor productivity (TFP) on aggregate hours worked and output? We study these questions through the lens of growth accounting and the neoclassical growth model. Our results show that the evolution of aggregate hours worked in Spain has been consistent with the evolution of taxes, whereas lack of TFP growth has had a minor impact on hours worked. A shortcoming of our model is that it fails to account for about 20% of the movements in hours worked in booms and recessions. The model, however, does account for the declining trend in hours worked over the period 1975–2015. The methodology used is that introduced in Kehoe and Prescott (2002), following the methodology proposed in Cole and Ohanian (1999) in their study of the U.S. Great Depression. See Conesa et al. (2007) for an exposition of this methodology and an explanation of how to extend it to different model environments. As the first step in studying growth and hours worked in Spain over the period 1970–2015, we use growth accounting to quantify the contribution of TFP, capital deepening, and aggregate hours worked for the dynamics of output per working-age person. Next, we construct a standard neoclassical growth model in which a stand-in household chooses hours worked, consumption, and capital holdings, taking as given the deterministic evolution of working-age population, TFP, and tax rates. This methodology provides us with a quantitative tool for identifying the relevant margins for potential candidate explanations for changes in such variables as hours worked. A striking feature of the Spanish growth experience is the lack of TFP growth since 1994. Our exercise is silent about the reasons behind this observation. Diaz and Franjo (2016) argue that excessive investment in structures—rather than in capital equipment—accounts for much of the stagnation of TFP. Garcia-Santana et al. (2016) argue that misallocation of resources because of “crony capitalism” is responsible for this feature. Regardless of the reason for this stagnation, however, our quantitative exercise indicates that the lack of TFP growth has had only a minor impact on the evolution of aggregate hours worked. Prior to 1975, TFP and output per working-age person moved together in Spain. (Because of data availability, we define working age to be ages 15–64; our results are not sensitive to minor changes in this definition.) After 1975, however, this has not been the case. The reason is that 1975 marked the beginning of a trend of decreasing aggregate hours worked. This trend of decreasing hours worked sharply contrasts with the U.S. experience, where hours worked per working-age person have been roughly constant. Over the period 1970–1974, hours worked per working-age person in Spain, 23.7 hours per week, were higher than those in the United States, 23.4 hours per week. Spain provides an extreme example of a general trend in European labor market dynamics. In France, for example, hours worked per working-age person have been systematically falling since the 1960s, although the decline in France has not been as steep as in Spain. The differences in the labor market experiences in the United States and Europe have been extensively studied. Most of the literature in this research area has focused on the impact of differences in labor market institutions. Bentolila and Bertola (1990), Blanchard and Summers (1986) and Alesina et al. (2005), among others, focus on the role of institutions and labor market restrictions. Ljungqvist and Sargent (1998) focus on the interaction between shocks and institutions. Prescott (2004), however, argues that differential taxation alone can account for the differences in the current level of aggregate hours worked between Europe and the United States. 
Ohanian et al. (2008) provide a comparison across countries that are members of the Organisation for Economic Co-operation and Development (OECD) and focus mostly on the correlation between distortionary wedges on labor supply and hours worked. While we work with a theoretical framework that is similar to that of Ohanian et al. (2008), we pay more attention to the details of the aggregate growth process for the Spanish economy. In particular, our calibration is specific to the Spanish case (instead of matching some OECD average), and we compare model outcomes to data along all the relevant dimensions at every point in time to identify the specific episodes in which there are departures between data and model predictions. Besides the fact that we work with a longer time horizon, it is hard to compare our results to those in Ohanian et al. (2008) since they do not report a specific comparison of model outcomes and data for the case of Spain. Our analysis shows that the evolution of the taxation of consumption and factor earnings can account for the secular trend decrease in hours worked observed in Spain. Of course, our exercise is silent about the distribution of aggregate hours worked within the working-age population. The quantitative exercise that we perform allows us to identify years in which data deviate from theory in a quantitatively important way. We want to identify these episodes because they suggest avenues for future research. In particular, the comparison between the model outcomes and the data reveals that in periods of rapid changes in aggregate hours worked, the model systematically underestimates the magnitude of such changes. In particular, the model fails to account for much of the large decrease in hours worked during 1975–1986 and for much of the large increase in hours worked during 1994–2007. Our model also predicts that hours worked and output start to fall after 2009, whereas the recession had already started in 2008. Hours fall in the model because both consumption taxes and labor income taxes increase. The discrepancy between model outcomes and the data is consistent with the critique of our model that it predicts smoother movements in hours than those observed in the data. The observation that Spanish labor markets react more to shocks than in other countries is well established in the literature. Bentolila et al. (2012) attribute the differential labor market response between Spain and France during the 2008–2009 recession to the duality of Spanish labor market institutions. In particular, a large fraction of employment in Spain is covered by temporary contracts with very low hiring and firing costs. For a more detailed treatment of the nature and consequences of the Great Recession in Spain, see Jimeno and Santos (2014) and the other papers in the special issue of SERIEs on the Great Recession in Spain.",5
8.0,3.0,SERIEs,05 July 2017,https://link.springer.com/article/10.1007/s13209-017-0157-x,The granularity of Spanish exports,August 2017,Juan de Lucio,Raúl Mínguez,Francisco Requena,Male,Male,Male,Male,"Exports are dominated by only a few firms. For example, the top 1% of exporters in Spain accounted for 72% of exports in 2015. As stated by Gabaix (2011), these large firms, also known as superstars, are the incompressible grains of economic activity. If only a few firms account for most of the exports, they are likely to play an important role in shaping the countries’ export specialization and dynamics. In this paper, we use Spanish firm-level export data to investigate this pivotal role. We first show that exports are highly concentrated by firm, and this concentration has not changed substantially over the 1997–2015 period. We also document heterogeneity in export concentration across products. Second, superstars contribute substantially to trade specialization in many industries. If we removed the top 10 firms in each industry, Spain would lose its revealed comparative advantage in 60% of industries, which account for 45% of total exports. This result suggests that superstars, along with country-level fundamentals, play a very important role in determining Spain’s trade pattern. Finally, we show that superstars can explain around one-third of the variation in aggregate exports in Spain. This paper is related to the recent empirical literature analyzing the granularity of exports. In particular, it is related to the work by Freund and Pierola (2015), who analyze the export concentration in 32 developing countries, investigating whether the countries’ trade patterns are defined by superstars.Footnote 1 Our research adds to the literature analyzing the weight and role of top exporters in Spain, a major world exporter. Our analysis is relevant, since there are few studies that analyze the concentration of exports with a sample that includes all exporters. In addition to this, our analysis allows us to compare the role of top exporters when defining export specialization in developing countries, along with their role in developed countries. We also add to the literature showing that export concentration is stable over time and documenting that export flows, where a flow is defined as a particular 8-digit product shipped to a particular destination, are also concentrated by firm. We also report heterogeneity in export granularity across products, especially in more disaggregated classifications. Finally, we add to the literature measuring the idiosyncratic contribution of top firms to the variation of exports. In a broader sense, our research also relates to the literature that has introduced granularity into trade models (Eaton et al. 2012; Bernard et al. 2016), and to the literature that has applied these models to estimate the role of fundamental and granular forces in shaping the comparative advantage (Gaubert and Itskhoki 2016). Our research provides empirical support for this class of models. Finally, our paper is also related to the literature that has investigated how granularity might affect important economic phenomena, such as aggregate volatility (Gabaix 2011; Giovanni and Levchenko 2012; Giovanni et al. 2014), welfare (Giovanni and Levchenko 2013), and trade balance (Canals et al. 2007). We add to this literature investigating the idiosyncratic contribution of superstars to the variation of exports. The rest of the paper is split into four different sections. Section 2 describes the concentration of exports by firm. Section 3 analyzes whether superstars define the specialization of Spanish exports. Section 4 investigates the contribution of top firms to the growth in Spanish exports. Section 5 presents the main conclusions of the paper.",3
8.0,3.0,SERIEs,01 September 2017,https://link.springer.com/article/10.1007/s13209-017-0160-2,The short-term debt choice under asymmetric information,August 2017,David Abad,Juan Pedro Sánchez-Ballesta,José Yagüe,Male,Male,Male,Male,"The information environment of a firm, in general, and the level of information asymmetry between better-informed insiders and less-informed investors, in particular, plays a fundamental role in the financing decisions of firms. From the seminal work of Myers and Majluf (1984), there is an extensive theoretical and empirical literature showing that information asymmetry has important implications in firms’ financial decisions, such as financing choices between debt and equity (Myers and Majluf 1984), public and private debt (e.g. Leland and Pyle 1977; Diamond 1984; Fama 1985), debt maturity choice (e.g. Flannery 1986; Barclay and Smith 1995; Berger et al. 2005), and the choice between bank and non-bank debt (e.g. Johnson 1997; Anderson and Makhija 1999). Most papers on corporate finance that examine financing decisions focus on the agency conflict between insiders and creditors, and they commonly use information asymmetry proxies based on financial and accounting characteristics of the firm such as size, growth options, or earnings opacity. More recently, some researchers have been addressing financing questions by looking at information asymmetry proxies from the trading process in the stock market (Bharath et al. 2009; Gao and Zhu 2015; Shen 2014; Petacchi 2015). Market microstructure is the branch of literature providing these measures which proxy the market perception of the information advantage held by insiders and the adverse selection risk faced by outsiders (Bharath et al. 2009). In this paper, we follow the market microstructure approximation to examine an empirical question that, to the best of our knowledge, has not been previously addressed: whether the information asymmetry concerning firms’ market valuation affects their short-term financing structure. To do so, we focus on the Spanish case, a market with high levels of short-term debt combined with a bank-oriented financial system. We analyze whether the degree of information asymmetry associated with a firm affects the choice between the different alternative sources of short-term private debt financing, i.e., bank debt and trade credit. In addition, since the choice of short-term financing may be determined by restrictions in the public debt market and/or in the long-term private debt market, we also analyze the influence of information asymmetries on debt maturity, public debt and long-term bank debt. Previous financial research shows the significance of adverse selection costs in debt maturity choices by firms. A great number of studies find that firms with higher information asymmetries show shorter debt maturities (Barclay and Smith 1995; Stohs and Mauer 1996; Scherr and Hulburt 2001; Berger et al. 2005). These findings validate the empirical predictions of Flannery’s (1986) and Diamond’s (1991) theoretical models: (1) firms with larger levels of information asymmetries and favorable private information are more likely to issue short-term debt because they can obtain this debt at a low interest rate and roll it over (Berger et al. 2005). (2) Under asymmetric information, short-term debt facilitates monitoring, because lenders can refuse or modify the terms of the loans, which is particularly important in financing supplied by banks (Diamond 1991; Magri 2010). Focusing on the different types of short-term financing, extensive examples in the literature show that one of its main determinants is the adverse selection and moral hazard problems due to the information asymmetry faced by lenders. Demirgüç-Kunt and Maksimovic (2002) and De Andrés Alonso et al. (2005) hypothesize that companies’ access to bank financing differs in market-based and bank-based financial systems. In market-based financial systems, firms with higher information asymmetry use more bank debt than public debt (e.g. Johnson 1997; Anderson and Makhija 1999; Denis and Mihov 2003). Due to their closer relationship with the firms, banks and other private lenders are more effective at monitoring borrowers, and consequently have an informational advantage over lenders in the public debt market (Leland and Pyle 1977; Diamond 1984; Fama 1985). Hence, firms with a higher degree of information asymmetry will borrow privately, while firms with lower information asymmetry will prefer public debt (Denis and Mihov 2003). On the contrary, in a bank-based financial system, most firms deal almost exclusively with financial intermediaries and the possibility of substituting market debt for bank debt is limited to a very few large firms. In this context, an increase in the information asymmetry can impair the access to bank debt (De Andrés Alonso et al. 2005; García-Teruel et al. 2014b) and create bank credit rationing (Stiglitz and Weiss 1981). Consequently, there is an incentive for constrained firms to use more trade credit as a short-term financing alternative, since suppliers might be better skilled at overcoming informational asymmetries and enforcement problems than financial institutions (Love 2011). Several theoretical papers support the relationship between bank credit rationing and trade credit driven by information asymmetries (e.g. Smith 1987; Biais and Gollier 1997; Jain 2001; Burkart and Ellingsen 2004). According to the implications of these papers, suppliers can provide trade credit to firms when institutional credit is not available since suppliers present advantages over banks and other lenders: (1) suppliers can obtain better information on the borrower’s creditworthiness than banks, (2) they face lower monitoring costs, (3) they can control the debtor better through the delivery of goods supplied, and (4) they can liquidate assets more efficiently (Petersen and Rajan 1997). Consistently, the majority of the empirical evidence finds that firms increase their demand for trade credit to overcome financial constraints and that suppliers may act as liquidity providers in situations of credit rationing (e.g. Petersen and Rajan 1997; Nilsen 2002; Danielson and Scott 2004; Cuñat 2007; Garcia-Appendini and Montoriol-Garriga 2013). Accordingly, we hypothesize that information asymmetry among investors is positively (negatively) related to the level of trade credit (short-term bank debt). In corporate finance, information asymmetry refers to the notion that insiders, typically the managers, know more about the quality and value of their firm’s assets than outsiders do. This asymmetry creates the possibility that the market will not price the firm’s claims correctly (Klein et al. 2002) and the insiders take advantage of the firm’s misvaluation. Since measuring information asymmetry is a difficult task, studies on financial decisions have used different proxies such as the amount of growth options in a firm’s investment opportunity set (Barclay and Smith 1995), firm size (Stohs and Mauer 1996; Scherr and Hulburt 2001), risk ratings (Berger et al. 2005), and accounting quality (Bharath et al. 2008; García-Teruel et al. 2014a, b). However, stock markets play an important role in the transmission of useful information to creditors (Demirgüç-Kunt and Maksimovic 1999) since stock prices partially reveal information that is in the possession of informed traders. In the market microstructure literature it is well recognized that, when people trade, there are some participants with better information (informed traders) than others (uninformed traders) about the fundamental value of the asset. In fact, firm managers and insiders are typically considered as informed agents. Market microstructure literature has proposed different measures to capture the adverse selection faced by an uninformed investor when trading a stock, and thus proxy for the information asymmetry level of the firm. As Bharath et al. (2009) point out, market microstructure measures of information asymmetry are proxies for the financial markets’ perception of the information advantage held by firm insiders and the adverse selection risk borne by outsiders.Footnote 1 On this basis, recent empirical research has used market microstructure proxies for adverse selection to analyze whether the capital structure of firms is driven by asymmetric information (i.e., Agarwal and O’Hara 2007; Bharath et al. 2009; Shen 2014; Gao and Zhu 2015). Thus, Bharath et al. (2009) document a positive relationship between information asymmetry and leverage, and Shen (2014) finds that firms have more difficulties to access public debt as the level of information asymmetry increases. However, whereas these papers have focused mainly on capital structure to test the pecking order theory, in this paper we use this approximation to deal with a fundamental question in bank-based financial countries and which has scarcely been examined by the prior literature on listed companies: the effect of information asymmetries on debt structure, and particularly, on the short-term financing decision. In order to capture the extent of information asymmetry, we use a composite index based on market microstructure measures estimated from high frequency data: the bid-ask spread, the illiquidity measure developed in Amihud (2002), the price impact introduced by Huang and Stoll (1996), the Probability of Informed Trading (PIN) of Easley et al. (1996), and the new Volume-Synchronized Probability of Informed Trading (VPIN) of Easley et al. (2012). With the index of information asymmetry, we extract the common variation in these information asymmetry proxies, so minimizing the possibility that these measures are driven by factors other than adverse selection (such as inventory costs, transactions costs, monopoly rents, etc.). To test our hypothesis, we use a panel of non-financial firms listed in the Spanish Stock Exchange (SIBE) for the period 2001–2008.Footnote 2 Spain is classified as a code law country characterized by weak investor protection, a high concentration of ownership and a less developed capital market than Anglo-Saxon countries (La Porta et al. 1998; Faccio and Lang 2002). Moreover, Spain is a clear example of a country with a banking-oriented financial system in which few firms issue public debt (basically listed companies) and where banks and suppliers are the main sources for firm financing (Schmidt and Tyrell 1997; Demirgüç-Kunt and Maksimovic 2002). In contrast to US firms, Spanish companies present a very different debt maturity structure showing higher levels of short-term debt. According to the data from Central de Balances del Banco de España (CBBE), the mean value of short-term debt over total debt was 57.6% in 2008 for Spanish firms, much higher than the 21% presented by US companies (Datta et al. 2005).Footnote 3 Moreover, short-term debt financing and trade credit have significant relevance in the Spanish financial system. In particular, trade credit in Spanish firms is one of the highest in Europe (Marotta 2005) representing 23.7% of total debt and 41.2% of total short-term debt in 2008 (according to the data provided by CBBE). Our findings show that market microstructure measures of information asymmetry explain debt structure and, in particular, short-term debt financing decisions of listed firms. Our results reveal that information asymmetry in the stock market reduces firms’ debt maturity, i.e., firms face more restrictions to obtain long term debt. In addition, firms with higher informational asymmetries face more difficulties in accessing public debt and bank debt, in particular short-term bank debt. Thus, in their choice between short-term financing sources, they have to rely on account payables as an alternative source of financing to overcome the financial constraints induced by information asymmetry problems. Since firm size is a commonly considered proxy for the quantity and quality of information available about a firm (e.g., Barry and Brown 1984; Atiase 1985; Bhushan 1989; Bhattacharya et al. 2013), we further investigate the effects of information asymmetry on short-term financing by splitting our sample into small and large firms. We find that information asymmetry is a significant determinant both of the level of short-term debt and the types of financing sources for small firms (characterized by a poorer information environment). Hence, for smaller listed firms, we find a negative association between information asymmetry and financial funds obtained in the credit markets (public and bank debt), and that the relevance of trade credit enhances as information asymmetries increases. For larger listed firms, we also find a positive relationship between information asymmetry and the weight of trade credit within the short-term debt. However, our findings suggest that financing decisions are less sensitive to information asymmetry in larger listed firms, probably because the information asymmetry problem is not as severe as in smaller firms. Our paper contributes to the literature by examining the implications for corporate decisions of information asymmetry in the equity market. Specifically, our paper extends the strand of empirical papers that use measures from market microstructure literature to explain firm’s capital structure decisions (Agarwal and O’Hara 2007; Bharath et al. 2009; Shen 2014; Gao and Zhu 2015). Unlike these studies, we do not examine the firm’s decision to issue debt or equity when raising capital, but we focus on the debt structure, and particularly on the short-term financing decision. Thus, we analyze how information asymmetry affects debt maturity, the choice between public and private debt, and the firm’s short term debt financing decision between bank debt and trade credit. As a consequence, from an informational perspective, our paper contributes to the financial literature concerning the firm’s choice of short-term financial sources. Our findings suggest a substitutive relationship between trade credit and bank credit, which is consistent with the information asymmetry theories of trade credit. The rest of the paper proceeds as follows. Section 2 describes the measure of information asymmetry, model specifications, and the sample. Section 3 reports the empirical results and the final section concludes the paper.",2
8.0,3.0,SERIEs,27 September 2017,https://link.springer.com/article/10.1007/s13209-017-0161-1,Determinants of bank’s financing choices under capital regulation,August 2017,Vanesa Llorens,Alfredo Martin-Oliver,,Female,Male,Unknown,Mix,,
8.0,4.0,SERIEs,04 December 2017,https://link.springer.com/article/10.1007/s13209-017-0169-6,Introduction to the special issue on the Survey of Adult Skills (PIAAC),November 2017,Nezih Guner,,,Male,Unknown,Unknown,Male,,
8.0,4.0,SERIEs,25 October 2017,https://link.springer.com/article/10.1007/s13209-017-0163-z,"The effects of vocational education on adult skills, employment and wages: What can we learn from PIAAC?",November 2017,Giorgio Brunello,Lorenzo Rocco,,Male,Male,Unknown,Male,"Vocational education and training (VET) are highly valued by many. For instance, VET is expected to play an important role in achieving two Europe 2020 headline targets set in the education field: (a) reduce the rate of early school leavers from education to <10%; (b) increase the share of 30–40 years old having completed tertiary or equivalent education to at least 40%. In this paper we investigate the effects of VET on adult skills and labour market outcomes by using the PIAAC survey. Data comparability across countries, the breath of countries involved, and the almost unique presence of information on assessed skills, training, earnings and employment makes this survey especially valuable to study the different facets of VET as compared to more academic education. Our approach is to consider the education careers available to individuals as alternative treatments in a multivalued treatment framework. Focusing mainly but not exclusively on upper secondary, post-secondary and tertiary education, we assume that individuals are exposed to four alternative treatments: (1) vocational education at the upper secondary or post-secondary level; (2) academic education at the upper secondary or post-secondary level; (3) vocational education at the tertiary level; (4) academic education at the tertiary level. Our key assumption is that, after controlling for individual differences in parental education, country of birth and the number of books in the house, the assignment of individuals to these treatments can be considered as good as random. This assumption implies that our selection on observables is capable of controlling for the sources of self-selection into educational tracks. We discuss its plausibility in the context of the data being used. This is important for the interpretation of our results. Only when this assumption holds we can interpret our estimates of the effects of alternative treatments as causal effects. When it does not, a more modest interpretation is in order, that views our findings as interesting correlations at best. In particular, if there are factors affecting selection into different curricula that we cannot control for with the data at hand, our estimates are affected by selection bias, which could amplify the estimate gap in labour market outcomes associated to alternative curricula. For instance, failure to properly control for unobserved ability could increase the estimated premium associated to academic curricula if individuals with higher ability are less likely to select into vocational tracks. When we compare the labour market payoffs accruing to vocational and academic education at ISCED 3 and 4 (upper secondary and post-secondary education), we find that a vocational curriculum is associated to slightly lower hourly earnings, a higher probability of being currently employed, and a higher share of the completed working life spent in paid employment. The estimated differences are small: for earnings, the negative gap ranges between \(-\,1.3\%\) for males and \(-\,4.8\%\) for females; for the probability of employment and the share of time spent in paid employment, the estimated positive gaps are 2.2 and 3.3% points for males and 1.9 and 0.6% points for females. The contrast between vocational and academic education is much sharper when we consider tertiary education (ISCED 5). In this case, the earnings gap between vocational and academic education at the time of the interview is as big as \(-\,19\%\) for males and \(-\,21.7\%\) for females. There is also a small negative gap in the probability of being currently employed. This gap should however be contrasted with the positive gap in the share of the working life spent in paid jobs, estimated at 6.9% points in the case of males and at 3.7% points in the case of females. Overall, the evidence we have on different ISCED levels is that vocational education does not perform as well as academic education when earnings are concerned, and performs slightly better than academic education when employability measures are considered. Independently of the ISCED level, we find that individuals with vocational education have a higher likelihood of being NET (not employed and with no education or training in the past 12 months), report poorer health and have poorer civic behaviour than comparable individuals with academic education. There is also evidence that vocational education is associated to poorer labour market returns among older than younger cohorts. Whether these differences simply reflect cohort effects or also indicate the presence of age effects is impossible to tell with the data at hand, which are a cross section of individuals. This is an important issue that must be left to better data and further research. When we consider the proficiency in foundation skills we find that those with a vocational education tend to be less effective than those with an academic education, for any ISCED level. This is true for both genders and, in spite of some heterogeneity, for all countries. The negative gap is larger for those with tertiary education and increases with the country-specific share of vocational students. In particular, we estimate that the negative percentage gap associated to vocational education at the secondary or post-secondary level ranges from \(-\,2.0\) to \(-\,2.2\%\) for literacy, from \(-\,1.9\) to \(-\,2.9\%\) for numeracy and from \(-\,1.8\) to \(-\,2.3\%\) for problem solving skills. In the case of tertiary education, the negative gap is larger and ranges from \(-\,5.7\) to \(-\,5.9\%\) for literacy, from \(-\,6.7\) to \(-\,7\%\) for numeracy and from \(-\,4.4\) to \(-\,4.7\%\) for problem solving skills. There is evidence that the wage and employment returns to VET are higher in countries where the relative supply of VET graduates is lower. In these countries, skill performance by VET graduates is also better. However, in spite of the growing interest attracted by dual systems, which alternate school and work, we do not find systematic evidence that returns to VET are higher in the countries where vocational education systematically combine school and work. The paper is organised as follows. Section 2 provides a review of the literature. Our empirical approach is discussed in Sect. 3. The data are briefly introduced in Sect. 4. Results for skills and competencies are presented in Sect. 5, and results for employment and earnings are discussed in Sect. 6. Conclusions follow.",24
8.0,4.0,SERIEs,16 November 2017,https://link.springer.com/article/10.1007/s13209-017-0166-9,Dual employment protection and (lack of) on-the-job training: PIAAC evidence for Spain and other European countries,November 2017,Antonio Cabrales,Juan J. Dolado,Ricardo Mora,Male,Male,Male,Male,"This paper looks at whether the gap between the amount of on-the-job training (OJT hereafter) employers provide to permanent workers (those holding an open-ended/indefinite contract) and temporary workers (those holding a fixed-term contract) is larger in labor markets with highly discontinuous severance pay schemes than in less segmented ones. One plausible mechanism leading to this differential in training relies on the larger turnover rate experienced by temporary workers because of the much less stringent employment protection legislation (EPL henceforth; in particular mandated severance pay) they enjoy relative to permanent workers. Whenever wage rigidity prevents firms neutralizing severance payments (i.e., a transfer from employers to workers), this gap in EPL rules makes firms more prone to use temporary contracts in sequence and less so to convert these contracts into permanent contracts. As a result of the greater job churn of temporary workers, employers do not find it profitable to invest in their OJT. Conversely, the higher job stability implied by the much higher EPL enjoyed by permanent workers makes firms more eager to invest in their training. As a byproduct of exploring the previous channel, we are also interested in documenting whether gaps in OJT due to dual EPL are positively correlated with differences in workers’ cognitive skills. To the extent that training at the workplace helps accumulating these skills, dualism in the labor market may not only hinder temporary workers’ specific human capital but may also lead to a negative “value-added” effect (i.e., in addition to the skills achieved through education and other observable characteristics) on their accumulation of general human capital.Footnote 1
 Both are issues of considerable importance for policy in Europe, especially in Southern Mediterranean countries which have been badly hit by the crisis. A well-known example is Spain, often considered as an epitome of a highly segmented labor market (OECD 2014). For this reason we devote the first part of the paper to carefully illustrate how the mechanism at play works for this country. Yet, because the explored channel is general in nature, further empirical evidence is provided in the second part of the paper on, first, how differences in the degree of EPL gap affect OJT in a large set of European countries and, second, how differences in OJT lead to differences in general human capital skills. Regarding our case study, the rate of temporary work (i.e., the share of employees under temporary contracts) soared in Spain from 15% just before the radical labor market reform of 1984 to 35.4% in the mid-nineties (see, e.g., Dolado et al. 2002; De la Rica et al. 2008).Footnote 2 Since then, around 90% of newly signed contracts each year have been temporary ones, out of which one quarter last for less than one week and their average duration hardly reaches around two months. The average temp-to-perm conversion rate has fallen from 12% in the nineties and first half of the 2000s to about 7% in 2014 (see Amuedo-Dorantes 2001; Güell and Petrongolo 2007). Following a long sequence of partial labor market reforms after 1984, the rate of temporary work stabilized around 30% at the turn of the new century (see Bentolila et al. 2012). More recently, the mass destruction of temporary jobs during the Great Recession and the sovereign debt crisis lowered the rate to 25%, which has gone up to 27% over the subsequent recovery and still remains as one of the highest rates in Europe and among OECD member countries. As regards workers’ involvement in OJT schemes, the participation rate in Spain increased from 10% in the early nineties to 24% in 2010, but still remains at the bottom of the EU, together with Italy. It is 8 percentage points below the EU average, and between 20 to 30 points lower than in Scandinavian countries and the UK (see The European Commission 2014). As further motivation of our argument, Fig. 1 displays the transition rates from temporary to permanent contracts (vertical axis, in percent) in 2011, as a measure of segmentation in labor markets (see Eichhorst 2013), and the fraction of employees participating in OJT schemes (horizontal axis, in percent) in the five largest EU countries and the EU average as of 2010. This evidence suggests that in countries where temporary contracts become a springboard to more stable jobs, workers’ participation in OJT schemes tends to be higher. Temp-to-perm transition rates and on-the-job training in several European countries. Note: The sources for transition and participation rates are Eichhorst (2013) and Fifth European Working Conditions Survey (2010), respectively Returning to our argument above, lower OJT attached to temporary contracts might be one of the main reasons for why these contracts are often considered to be dead ends in dual labor markets while they become stepping stones in other labor markets where the EPL gap is smaller or even absent (see Booth et al. 2002; Autor 2001). By contrast, stringent EPL for permanent contracts implies that OJT is higher for this type of workers. Thus, both features put together imply that a dual labor market structure has distributional consequences for OJT. To document the role of OJT in the mechanism described above, we use detailed information for Spain and other twelve European countries drawn from the first wave (2013) of the Survey of Adult Skills, the main output of the Programme for the International Assessment of Adult Competencies, PIAAC. This survey provides a harmonized dataset for a large number of countries on the availability of OJT at individual level (both at the extensive and intensive margins), as well as a wide array of individual-level demographics and job characteristics, including type of labor contract. Further, the availability of PIAAC scores on workers’ literacy and numeracy competencies allows us also to explore whether OJT affects workers’ proficiency conditional on predictors of previously acquired skills, including educational attainment or family background. A note of caution is due before summarizing our main empirical findings. Our approach relies on comparing workers with similar observable characteristics, except type of labor contract (our treatment). We argue that selection on un-observables is not likely to play a major role in our setup. There are two reasons to think so. First, it is the case that PIAAC contains a much wider array of individual and job characteristics than other datasets which have been used in previous studies about the effects of temporary contracts on labor market outcomes. For example, PIAAC includes variables like the worker’s motivation and her/his use of reading and numerical skills at the workplace, as well as detailed educational achievements and family background covariates; these are often considered to be among the standard confounding factors leading to biases in the estimation of causal effects by standard regression analysis.Footnote 3 Hence, in a rich dataset as ours, the set of potentially confounding unobservables is bound to be small. Secondly, as argued in the next section, it is also the case that, at least for Spain, a comparison of non-experimental and quasi-experimental estimates of the returns to temporary work is bound to yield minor differences. This may possibly reflect that, in a country where more than 90% of entry jobs are temporary, selection issues into type of contract are not so crucial. Nevertheless, we refrain throughout the paper from readily admitting a causal interpretation of our results. Despite the limitations imposed by the use of our cross-sectional dataset, our interpretation of the main empirical findings obtained from the Spanish PIAAC sample is that they are rather supportive of the mechanism stressed above. First, we find a substantially negative and statistically significant relationship between holding a temporary contract and the amount of workplace training. Secondly, we document that the less OJT individuals received amount, the worse their test scores (especially as regards numeracy), on top of those predicted by their individual characteristics. Foremost, we provide similar evidence for a pool of 13 European countries for which a novel proxy of dual EPL is constructed. We find that the above-mentioned results are stronger for countries with segmented labor markets than for those with more unified labor markets. The rest of the paper is organized as follows. Section 2 provides a brief overview of the related literature on this topic. Section 3 describes the PIAAC database, provides descriptive statistics of main variables of interest, and motivates the empirical strategy. Section 4 presents the main empirical results for our illustrative case study, Spain. Section 5 reports further cross-country evidence for our sample of European countries. Finally, Sect. 6 draws some brief conclusions. An “Appendix” develops a simple theoretical model that guides our empirical approach.",15
8.0,4.0,SERIEs,23 September 2017,https://link.springer.com/article/10.1007/s13209-017-0162-0,Development accounting using PIAAC data,November 2017,Ana Hidalgo-Cabrillana,Zoë Kuehn,Cristina Lopez-Mayan,Female,Female,Female,Female,"At least since Smith’s (1776) “The Wealth of Nations,” economists have tried to understand why some countries are richer than others. One of the most widely used approaches to address this question—development accounting—combines measured inputs according to an aggregate production function and compares the estimated outputs to countries’ observed gross domestic products (GDPs). Current consensus is that differences in physical capital only account for a limited fraction of differences in GDP. On the other hand, the relative importance of total factor productivity (TFP) and human capital for explaining income differences remains an unsettled question. For instance,  Manuelli and Seshadri (2014) and Jones (2014) find that differences in human capital account for four-fifth of cross-country income differences while earlier work by Hall and Jones (1999) concluded that they explain only one-fifth. There are two main reasons for such pronounced differences in findings: (1) how human capital is measured and (2) how inputs to human capital are combined; i.e. functional forms for the production function (or composite) of human capital. The current paper mainly focuses on the first aspect: measurement of human capital. Accurately measuring a country’s stock of human capital is challenging. Literature has pointed out various shortcomings that lie with the traditionally used measure of average years of schooling and has highlighted the importance of taking into account qualitative measures of human capital. One can envision that a broad notion of human capital would furthermore include aspects related to accumulation and investments beyond formal schooling such as job experience, and that for experience to translate into more skills, lifelong learning and training could potentially be important. Last but not least, individuals’ health also has a clear impact on human capital. However, until recently data limitations made it impossible to construct such a broad measure for different countries. This changed when data from the “Programme for the International Assessment of Adult Competencies” (PIAAC) became available. In the current paper we construct a comprehensive measure of aggregate human capital using many different variables related to individual human capital from PIAAC. In particular, PIAAC data is available for a sample of 30 upper-middle and high-income countries.Footnote 1 GDP per worker among these countries ranges from 48,325 USPPP$ in Estonia to 151,909 USPPP$ in Norway. Given that all ingredients for our measure of human capital—schooling, cognitive skills, job experience, on-the-job-training, and health—thus come from a common source, we are able to circumvent measurement problems that arise when using different data bases. To obtain parameters for the weight of each dimension of human capital in the human capital composite, we use individual level PIAAC data which also include information on wages, and we estimate Mincerian wage regressions for the US. We then combine these so-constructed measures of human capital with data on the stock of physical capital from the Penn World Tables, and we carry out a classical development accounting exercise. Our results show that differences in physical capital together with our multidimensional measure of human capital can account for 42% of the variance in income, compared to 27% when using years of formal schooling only. Differences in cognitive skills play the largest role while experience and health are of lesser importance. The current paper contributes to the development accounting literature which is extensively reviewed in Caselli (2005) and Hsieh and Klenow (2010). First, while some purely quantitative measures of human capital such as average years of schooling are readily available across countries (see Barro and Lee (2013)), finer measures that take into account aspects of quality are harder to come by. Unlike previous literature that relies on student test scores as a proxy for the quality of human capital (e.g. Caselli (2005) or Hanushek and Woessman (2008)), we are able to approximate the quality of human capital actually used in production with test scores on cognitive skills of the working-age population. Alternative approaches by Schoellman (2012) or Hendricks (2002) who use returns to schooling or average earnings of foreign-educated individuals in the US to proxy for differences in the quality of education across countries are potentially affected by the selection of migrants or wage discrimination. Second, unlike previous papers, we consider a broad measure of human capital including not only average years of formal schooling and cognitive test scores but also measures of job experience, health, and on-the-job-training. While subsets of all of these measures have been considered in the development accounting literature, to the best of our knowledge ours is the first paper that combines them all. For instance, Klenow and Rodriguez-Clare (1997) find that adding experience to a measure of human capital which includes years of schooling only, increases the explanatory power of the standard model used in development accounting by a mere 4–5% points, in stark contrast to increases by almost 70% in Lagakos et al. (2012). These differences are partly due to the fact that—similar to the current paper—the former assume the same returns to experience across countries, while Lagakos et al. (2012) estimate different returns to years of job experience across countries. Furthermore, there is no question that health is an important determinant of human capital. However, Acemoglu and Johnson (2007) point out that the effect on output per capita is ambiguous, as improvements in health which lead to individual higher productivity also imply larger populations due to increased life expectancy. Nevertheless, Weil (2007) finds that differences in health outcomes—measured by average height, survival rates and age at menarche—across countries contribute to around 10% of the variance of log GDP per worker, while Shastry and Weil (2003) consider the prevalence of anemia and adult survival rates and find that differences in these variables explain 1.3 and 19% respectively. Differences in results hence clearly hinge on how health outcomes are measured. For our sample of upper-middle and high-income countries where we observe very little variation in objective measures such as survival rates or average height, we rely on self-reported health data.Footnote 2 Regarding on-the-job-training, Manuelli and Seshadri (2014) show that human capital investments that individuals undertake after completion of formal schooling constitute an important component of human capital, in particular for richer countries as those in our sample. In our benchmark exercise we follow the standard assumptions in the development accounting literature and consider competitive factor markets and perfect substitutability of workers. However, Jones (2014), Caselli and Coleman (2006) and most recently Malmberg (2017) show that considering imperfect substitutability of workers with different education levels increases the explanatory power of human capital in development accounting. We hence extend our basic framework and consider a modified model with imperfect substitutability of individuals with and without college education. For reasonable degrees of substitutability we can account for up to 45% of the cross-country variance in output per worker, i.e. 8% more compared to our baseline model with perfect substitutability. We then confirm the robustness of our results along other dimensions, including alternative measures of cognitive skills and experience, as well as considering additional moments from cognitive skill distributions. The remainder of this paper is organized as follows: Sect. 2 outlines the theoretical framework behind development accounting, and we explain how our multidimensional measure of human capital fits into this framework. Sect. 3 describes the data used for our exercise, and in Sect. 4 we explain how we estimate the parameters for the human capital composite. Sect. 5 reports and discusses our results, in Sect. 6 we provide robustness checks, and Sect. 7 concludes.",8
8.0,4.0,SERIEs,21 November 2017,https://link.springer.com/article/10.1007/s13209-017-0167-8,Cognitive skills and the LOGSE reform in Spain: evidence from PIAAC,November 2017,José A. Robles-Zurita,,,Male,Unknown,Unknown,Male,"The General Law of the Education System (LOGSE, Spanish acronym) was passed in 1990, replacing the previous General Education Law (LGE, Spanish acronym) passed in 1970. It introduced significant changes by increasing leaving age for compulsory schooling, from 14 to 16, and postponing tracking of students into vocational and academic schools up to age 16. There is a need for evaluation since the LOGSE could be considered the most important reform of the Spanish education system in the last 50 years, particularly for secondary education. The analysis of the impact on cognitive skills is especially relevant given the introduction of new, longer, and more comprehensive curricula that replaced the tracking system at the compulsory level. So far, the analysis of the LOGSE focused on non-cognitive-skills outcomes. For example, Felgueroso et al. (2014) study school dropout using the labor force survey for cohorts potentially affected by the LOGSE. Their econometric model exploits the regional variation in the degree of implementation of the reform in the period 1992–2002. Interestingly, they estimate a positive effect on dropout rates for males, and negative effect for females after controlling for the availability of Lower Vocational studies in the period. Also, they find that the abolition of Lower Vocational studies increased dropout for females, although this result is not very significant. In other report, Lacasa (2006) carries out a descriptive analysis showing that the implementation of the LOGSE is correlated with a decline of some educational outcomes: the school enrollment rate at age 17; school life expectancy; percentage of population with upper secondary education; percentage of population at age 18 taking the university access exam, and; university enrollment rate at age 20. In an early work, De Miguel-Díaz et al. (2002) analyze the educational performance of students from several Spanish universities depending on the type of baccalaureate they studied, LOGSE or LGE. They found no systematic differences between these two groups in average grade in high school, university access exam marks, and performance at university. In this paper, we use data from the Spanish sample of the Programme for the International Assessment of Adult Competencies (PIAAC) to analyze the effect of the LOGSE reform on numeracy and literacy proficiency of the adult population. PIAAC measures cognitive and workplace skills of different Spanish cohorts who studied under LOGSE and previous education laws. Applying the same methodology as in Felgueroso et al. (2014) we can estimate the relationship between the degree of exposure to LOGSE and skills. The analysis has at least two novel aspects. First, PIAAC measures skills using an internationally standardized questionnaire so that all individuals are evaluated using the same standards. This way we avoid the use of educational outcomes that are not comparable across education laws. For example, high school grades are based on criteria specific to LOGSE and LGE. The same applies to university access exam marks. In addition, the use of numeracy and literacy proficiency allows us to explore a new dimension that has implications for the labor market and differs from other outcomes previously analyzed, like school dropout or obtaining a university degree. Secondly, the plausible causal relationship between LOGSE and skills may be contaminated by cohort effects (Desjardins and Warnke 2012). For this reason, we estimate an econometric model using different functional specifications controlling for year of birth to isolate the LOGSE effect from the cohort effect. In the estimation of the cohort effect we consider not only those cohorts potentially affected by the reform but also the adults born before and after the implementation period. Notice that the analysis in Felgueroso et al. (2014) considers only those individuals that attended school during the period of implementation of the LOGSE. Basic structure of the Spanish educational system under the LGE and the LOGSE The results indicate that the LOGSE reform was not successful in increasing numeracy and literacy skills of the Spanish population despite an extension of compulsory years of education and postponement of the age of initial tracking into vocational and academic studies. In fact, the relationship of the degree of exposure to LOGSE and skills tend to be negative for the majority of variants of the econometric analysis, although statistical significance is not robust to variations in the functional form of the cohort effect. In the next section, we discuss the LOGSE reform and evidence on relevant educational reforms is reviewed. Methodological details are found in Sect. 3. In Sect. 4, the results are presented. Finally, we include a discussion and conclusion.",1
8.0,4.0,SERIEs,14 November 2017,https://link.springer.com/article/10.1007/s13209-017-0165-x,Non-cognitive skills and individual earnings: new evidence from PIAAC,November 2017,Brindusa Anghel,Pau Balart,,Unknown,Male,Unknown,Male,"Despite the important role that human capital occupies in economic thinking, its non-observable nature has historically limited its measurement. Since the original works by Becker (1962) and Mincer (1970, 1974), years of schooling has remained as the predominant measure of human capital. It was with the improvement in testing techniques and after the works by Hanushek, Woessmann and their coauthors (see Hanushek and Woessmann 2008, 2011, among others) that the use of international standardized achievement tests started to be seen as a more adequate measure of human capital. As argued by Hanushek and Woessmann (2008) achievement tests provide a better measure of human capital than years of schooling because (1) they better capture skills acquired out of the classroom and (2) because there is substantial cross-country variation in skills acquired at each level of education. Nowadays, there is a growing attention towards the distinction between two different types of skills that are embedded in human capital, namely cognitive and non-cognitive skills. A better understanding of the importance of human capital requires the construction of measures that distinguish between these two types of skills. We understand by cognitive skills individuals’ knowledge on basic competences, such as numeracy and literacy. Non-cognitive skills (also known as soft skills or socio-emotional skills) are individual patterns of behaviors, attitudes and personality that are not directly related to individuals’ knowledge. Recent research has found that this distinction is important when studying the relationship between human capital and economic outputs both at an individual (Duckworth et al. 2011; Kautz et al. 2014; Heckman and Rubinstein 2001; Heckman et al. 2006, 2010) as well as at an aggregate level (Brunello and Schlotter 2011). At the same time, as noted by Cunha et al. (2010), the type of interventions required to foster cognitive and non-cognitive skills are notably different which intensifies the necessity of a better understanding of the role of each type of skills for policy recommendations.Footnote 1
 The main challenge for the study of non-cognitive skills is often the availability of proper measures. This is not the case for cognitive skills. The evaluation of cognitive skills notably improved during the last decades of the twentieth century, thanks to the advance in testing techniques. This fostered the emergence of standardized achievement tests allowing for international comparisons of cognitive skills. In contrast, non-cognitive skills and personality traits are less present in internationally comparable datasets. First, because the measurement of non-cognitive skills has been traditionally based on self-reports which raises several methodological concerns due to misreports, difficulties in interpersonal comparisons or the socially desirable response bias (see King et al. 2004; Paulhus 1984; Soest et al. 2011). Second, because attention to these types of skills is relatively new and started to grow at the beginning of the twenty-first century with the works by Heckman and his coauthors. However, recent research has provided alternative measures of non-cognitive skills. First, Borghans et al. (2011), Duckworth et al. (2011) and Segal (2012) show that scores on low-stake achievement tests are not only driven by high cognitive skills but also by high non-cognitive skills. This observation provides an opportunity to have more detailed measures of human capital that specifically distinguish between cognitive and non-cognitive skills. Second, there is an increasing research towards the possibility of using testing and survey behavior as a measure of non-cognitive skills. Hedengren and Stratmann (2012) and Hitt et al. (2016) proposed and validated survey non-response as a measure of non-cognitive skills, while Cheng et al. (2016) found that conscientiousness and openness to experience, two of the personality traits of the big five taxonomy, predict patterns of survey non-response.Footnote 2
Zamarro et al. (2016a, b, 2017) further develop and review non-response as a measure of non-cognitive skills. The above findings raise several questions regarding the measurement of human capital. The first one is, how should one interpret scores of achievement tests? According to the conclusions of Borghans et al. (2011), Duckworth et al. (2011) and Segal (2012), scores on low-stakes achievement tests should not be interpreted as a measure of cognitive skills but as a broader notion of human capital that also includes non-cognitive skills. The second question concerns the possibility of obtaining separate measures for each type of skill. For instance, Borghans and Schils (2012) proposed a methodology to separate PISA scores into a cognitive and non-cognitive component. Third, and based on the findings of Hedengren and Stratmann (2012), Hitt et al. (2016), Cheng et al. (2016) and Zamarro et al. (2016a, b, 2017) one may ask about the possibility of using some already available and internationally comparable assessments to create and study non-cognitive skills. In the present paper we use the PIAAC database to construct several measures of non-cognitive skills and include them in the Mincerian equation. We construct different types of measures for non-cognitive skills. On the one hand, we use the answers in three items of the PIAAC background questionnaire to obtain self-reported measures of non-cognitive skills. On the other hand, we follow Hedengren and Stratmann (2012) and Hitt et al. (2016) to construct measures based on survey non-response in the background questionnaire. Finally, we use some other measures based on the testing behavior rather than on the background questionnaire. In particular, we take advantage of the computer based nature of the PIAAC test, to construct measures that might be related to non-cognitive skills, such as time spent on each item in the test, skipped questions or the number of actions done with the mouse before answering an item. To the best of our knowledge, only skipped test items has been used before as a proxy for non-cognitive skills by Hernández and Hershaff (2014). We use the proposed measures to analyze the association between non-cognitive skills and earnings in a Mincerian equation. Our results show that measures of non-cognitive skills proposed in previous literature (self-reports and non-response in a questionnaire) are significantly associated to earnings in all our specifications (with the exception of self-reports on cultural engagement). When looking at test-based measures, we find that skipped test items and not-attempted items are negatively and significantly associated to earnings in all specifications. However, these two measures are the ones that exhibit higher correlation with test scores so they might be capturing cognitive skills. Average time per correct answer is also statistically correlated to earnings. Additionally, we perform various robustness checks to test the validity of our measures of non-cognitive skills. We also pay attention to the relationship between cognitive skills and earnings and attempt to provide an estimate for the effects of cognitive skills in the Mincerian equation after correcting for the effect of non-cognitive skills. In a recent article, Hanushek et al. (2015) use data from the PIAAC database to study the relationship between human capital and individual earnings. They show that PIAAC scores have explanatory power on earnings above and beyond years of schooling (the traditional measure of human capital). However, as emphasized in the psychology field (Wechsler 1940) and in the economics literature (Duckworth et al. 2011; Segal 2012) performance on achievement tests depends not only on cognitive but also on non-cognitive skills.Footnote 3 Therefore, it is not clear until what extent the association between PIAAC scores and earnings found by Hanushek et al. (2015) is driven by cognitive or non-cognitive factors. In other words, interpreting the results of Hanushek et al. (2015) in terms of cognitive skills may be problematic. We propose a correction of the PIAAC test scores using our proxies for non-cognitive skills. To do so, first we regress PIAAC test scores on non-cognitive skills and, in a second stage, we use the residuals of the previous regression as a corrected measure of cognitive skills. This correction is based on the finding by Cunha and Heckman (2008) and Cunha et al. (2010) that non-cognitive skills foster the development of cognitive skills, while the opposite is not true. Thus, the proposed measure only takes into account variation in cognitive skills that is orthogonal to our measures of non-cognitive skills. Despite reducing the size of the estimates of cognitive skills, cognitive skills remain highly significant and the main determinant of individual earnings after this (over) correction of PIAAC test scores. As cautioned by Hanushek et al. (2015) an important drawback of studies based on cross-sectional international data such as PIAAC relies on the impossibility of obtaining causal results. Ideally, one would like to have an exogenous source of random variation in skills. However, this possibility is extremely limited in the context of international samples as here. Consequently, the present paper is mainly intended to enrich the study of the relationship between PIAAC test scores and earnings started by Hanushek et al. (2015) by providing measures of human capital that address the distinction between cognitive and non-cognitive skills. The structure of the paper is the following. In Sect. 2 we describe the measures of non-cognitive skills that we use in our paper. Section 3 explains the empirical strategy. Section 4 describes the data. Section 5 presents the results on the association between non-cognitive skills and earnings. Section 6 shows the results after computing corrected measures for non-cognitive skills. Section 7 provides a heterogeneous analysis of the results by education, age and gender. Section 8 shows a number of robustness checks. Finally, Sect. 9 concludes.",9
9.0,1.0,SERIEs,26 December 2017,https://link.springer.com/article/10.1007/s13209-017-0171-z,Hierarchical bank supervision,March 2018,Rafael Repullo,,,Male,Unknown,Unknown,Male,"Bank supervision, unlike bank regulation, has not been until recently the subject of much academic interest. As stated by Eisenbach et al. (2016), regulation involves the establishment of rules under which banks operate, while supervision involves the assessment of safety and soundness of banks through monitoring, and the use of this information to request corrective actions. In contrast to regulation, that is based on verifiable information, supervision is about supervisory actions (partly) based on nonverifiable information. This paper contributes to the theoretical literature on bank supervision by constructing a stylized model of a supervisor that collects nonverifiable information on the solvency of a bank and, on the basis of this information, decides on its early liquidation. The quality of the information on the bank’s solvency depends on the intensity of supervision (the nonverifiable costly effort of the supervisor). I assume that the supervisor is not a social welfare maximizer. In particular, its payoff function incorporates a liquidation cost, which may be associated with either reputational concerns or supervisory capture (e.g. revolving doors). The paper characterizes the effort and the liquidation decisions of the supervisor, and shows that supervision will be more intense the lower the costs of supervisory effort and the lower the supervisory bias against liquidation. The model can be interpreted as a model of decentralized supervision , in which the bank is a local bank and the supervisor is a local supervisor, or as a model of centralized supervision, in which the bank is still a local bank but the supervisor is a central (or supranational) supervisor. It can also be used as a building block for a model of hierarchical supervision, in which the central and the local supervisors jointly supervise the bank in order to observe a nonverifiable signal of the bank’s solvency, and then the central supervisor decides on the liquidation of the bank. Under hierarchical supervision, the central and the local supervisors simultaneously choose their efforts, so they will be playing a game. The Nash equilibrium of this game describes the outcome of the hierarchical supervision model. The main contribution of the paper is to characterize the conditions under which one of the three institutional arrangements, namely decentralized, hierarchical, and centralized supervision, dominates in welfare terms the other two. The analysis is based on two key assumptions: (i) the cost of effort is higher for the central than for the local supervisor, and (ii) the cost of liquidating the bank is lower for the central supervisor than for the local supervisor. The first assumption may be justified by reference to distance between the central supervisor and the local bank. In the words of Torres (2015): “The central supervisor has informational disadvantages relative to the national authorities, due to their better knowledge of banks, banking systems and regulatory frameworks, as well as their geographical and cultural proximity to them.” The second assumption may be justified by reference to the looser connections between the central supervisor and the bank. In the words of Torres (2015): “The existence of a supranational supervisor allows to increase the distance between supervisors and national lobbies and politicians, which in principle should reduce the risk of supervisors implementing excessively lax policies.”Footnote 1
 The results show that hierarchical supervision dominates decentralized supervision when the bias of the local supervisor is high and the costs of getting local information from the center are low. But when these forces exceed certain threshold, it is better to concentrate all responsibilities in the central supervisor. The trade-off underlying these results is clear: Decentralized supervision is better because the local supervisor finds it cheaper to gather information, but it is worse because its objective function is biased against liquidation.Footnote 2 The results also show that hierarchical supervision is more likely to dominate when bank profitability is low (e.g. as a result of high competition) and when bank risk-taking is high (e.g. as a result of soft regulation). Interestingly, the model of hierarchical supervision is isomorphic to a model in which the central supervisor gets a signal of the bank’s solvency, the local supervisor gets another signal which truthfully reports to the central supervisor, who then decides on the liquidation of the bank. The original model corresponds to an institutional arrangement in which the supervisors work in teams, while the alternative model corresponds to an arrangement in which the supervisors work independently, but there is no problem of strategic information transmission à la Crawford and Sobel (1982).Footnote 3
 The results provides a rationale for the design of the Single Supervisory Mechanism (SSM), the new structure of bank supervision in Europe that comprises the European Central Bank (ECB) and the national supervisory authorities of the participating countries. Currently, the ECB is responsible for the supervision of significant (i.e. large) banks that comprise about 80% of the banking assets of the euro area. For these banks, supervision is carried out in cooperation with the national supervisors via the so-called Joint Supervisory Teams. National supervisors are in charge of the less significant (i.e. small) banks. To the extent that (i) the cost advantage of local supervisors is smaller for larger, more complex banks, and (ii) supervisory capture is more relevant for larger banks, the results of the model are consistent with the design of the SSM. The model can also shed light on issues related to the organization of supervision in jurisdictions in which multiple agencies are involved in supervising banks. For example, state-chartered banks in the US are under a dual supervisory framework involving both federal and local supervisors.Footnote 4 Also, the Federal Reserve is responsible for the supervision of bank holding companies, but subsidiaries may be supervised by other federal agencies. Two extensions of the model are discussed. The first one examines what happens when the supervisory bias is linked to being responsible for the liquidation decision. This means that moving from decentralized to hierarchical supervision implies a reduction in the liquidation cost of the local supervisor and an increase in the liquidation cost of the central supervisor. In this case, the results show that there will be an increase in the (cheaper) effort of the local supervisor that will compensate the reduction in the effort of the central supervisor, so hierarchical supervision is more likely to dominate. A second extension shows that limiting the size of the central supervisor in the hierarchical supervision setup is welfare improving. The result is closely related to the well-known result that a Stackelberg leader that optimizes over the reaction function of the other agent does better than by playing its Nash equilibrium strategy. The intuition is that putting the central supervisor in a situation of overload forces the local supervisor to increase its (cheaper) effort in a way that compensates the reduction in the effort of the central supervisor.Footnote 5
 
Literature review Carletti et al. (2016) explore the working of a supervisory structure in which a centralized agency has legal power over decisions regarding banks, but has to rely on biased local supervisors to collect the information necessary to act. They focus on how this institutional design affects supervisors’ incentives to collect information and on how this, in turn, influences bank behavior, showing that when the agency problem between the central and the local supervisor is severe, centralized supervision leads to lower information collection and increased risk-taking.Footnote 6
 
Colliard (2015) considers a model in which local supervisors are more lenient, so that banks also have weaker incentives to hide information from them. These two forces can make a joint supervisory architecture optimal. However, more centralized supervision encourages banks to integrate more cross-border. Due to this complementarity, the economy can be trapped in an inferior equilibrium with both too little central supervision and too little financial integration. 
Calzolari et al. (2016) study the impact of shared liability and deposit insurance arrangements on supervisors’ incentives to acquire information on the activities of multinational banks, showing that centralized supervision can induce these banks to expand abroad through branches rather than subsidiaries. In an earlier contribution, Boyer and Ponce (2012) analyze whether banking supervision responsibilities should be concentrated in the hands of a single supervisor, showing that splitting supervisory powers among different supervisors is a superior arrangement in terms of welfare when the capture of supervisors by bankers is a concern. The model in the paper may be considered as a special case of models in the literature on the theory of organizations.Footnote 7 As noted in the seminal paper by Jensen and Meckling (1992), “The assignment of decision rights influences incentives to acquire information. (...) Determining the optimal level of decentralization requires balancing the costs of bad decisions owing to poor information and those owing to inconsistent objectives.” It is also closely related to the recent literature on strategic information acquisition, which departs from the literature on strategic information transmission by assuming that information is not exogenously given, but is obtained through costly effort; see Argenziano et al. (2016). 
Structure of the paper Section 2 presents the basic model of bank supervision in which a supervisor collects information on the solvency of a bank and, on the basis of this information, decides on its early liquidation. This setup may be interpreted as a model of decentralized supervision in which supervisory responsibilities are allocated to a local supervisor or a model of centralized supervision in which supervisory responsibilities are allocated to a central supervisor. Building on this setup, Sect. 3 presents the model of hierarchical supervision in which a central and a local supervisor jointly collect information and then the central supervisor decides on liquidation. Section 4 compares in terms of welfare three possible institutional arrangements: decentralized, hierarchical, and centralized supervision. Section 5 contains the extensions, and Sect. 6 the concluding remarks. The proofs of the analytical results are in the “Appendix”.",6
9.0,1.0,SERIEs,12 October 2017,https://link.springer.com/article/10.1007/s13209-017-0164-y,Contribution of demography to economic growth,March 2018,Miguel Sánchez-Romero,Gemma Abio,Guadalupe Souto,Male,Female,Female,Mix,,
9.0,1.0,SERIEs,02 March 2018,https://link.springer.com/article/10.1007/s13209-018-0176-2,Growing by learning: firm-level evidence on the size-productivity nexus,March 2018,Enrique Moral-Benito,,,Male,Unknown,Unknown,Male,"In Spain and other Southern European countries there is a large number of small firms in comparison to other developed economies. On the other hand, large firms are more productive than small ones. As a result, it is typically argued that a firm size distribution biased towards small firms hampers productivity growth in Spain. Understanding the obstacles to firm growth in those countries is thus at the center of the policy debate (IMF 2015).Footnote 1
 However, causality between firm size and firm productivity may operate in both directions. On the one hand, there are models of firm growth that characterize industries as groups of heterogeneous-productivity firms under the assumption that they learn about their productivity as they operate in the market [see for instance (Jovanovic 1982; Melitz 2003)]. Firms maximize profits choosing the level of employment (and capital) given their productivity shocks, which are assumed to be exogenous. Low productivity firms are less likely to survive and thrive than their more efficient counterparts. On the other hand, there are also mechanisms rationalizing an effect from firm size to productivity growth: expanding firms may invest in new technologies, learn about more efficient methods of production, use more specialized inputs, and/or better coordinate their resources. Nevertheless, these mechanisms are discussed heuristically in some papers [see for instance (Thompson 2001; Halkos and Tzeremes 2001)], but no formal theory is available in the literature to the best of my knowledge. On the empirical front, little is known about the direction of causality in the size-productivity nexus at the firm level. While there is a wide literature on the determinants of firm growth in terms of employment (e.g. Henrekson and Johansson 2010; Lopez-Garcia and Puente 2012), studies analyzing the relationship between productivity and employment growth are scarce and provide contradicting results. Daunfeldt et al. (2010) find that neither high-growth employment is associated to high-growth productivity nor the reverse. They conclude that a trade-off exists between employment and productivity. On the other hand, Du et al. (2013) find that firms which exhibit higher productivity growth are more likely to become high-growth firms in terms of turnover and that high-growth firms are more likely to experience higher productivity growth in the future. Finally, Guillamon et al. (2017) find that high growth in productivity (size) increases the likelihood of high growth in size (productivity) but the magnitude of the effect from size to productivity is smaller than that of the effect from productivity to size. Using firm-level administrative data for Spain, this paper provides evidence in favor of the hypothesis that firm productivity shocks generate firm growth but not the other way around. In particular, two different treatment variables are considered. First, I consider high-growth in terms of (employment) size as the treatment status and productivity growth as the outcome variable. Second, I consider high-growth productivity as the treatment and employment growth as the outcome. In the baseline specification, high-growth episodes are defined as productivity or employment growth above 10% in a given year. I attempt to control for reverse causality and self-selection by using matching techniques that are able to account for observable characteristics but ignore selection on unobservables. Having these concerns in mind, I estimate the effects up to 5 years after each treatment. The estimated effects indicate that high-productivity growth is followed by statistically significant increases in size/employment growth; in contrast, employment growth is not followed by subsequent gains in productivity. These opposite patterns are reassuring. Selection on unobservables and reverse causality would imply, among other things, the presence of a third omitted factor simultaneously causing subsequent increases in size and TFP. Under this scenario, the positive association between size and TFP growth would show up regardless of which increase (either TFP or size) occurs in the first place. Therefore, I argue that my estimates provide suggestive evidence in favor of the hypothesis that TFP growth causes employment growth but not the other way around. I label this pattern as growing by learning because those firms learning about their higher efficiency levels are those that happen to grow more; under this hypothesis, the process of learning and innovation can be interpreted as the mechanism that makes firms thrive. Small and unproductive firms are often linked to low productivity growth at the aggregate level (see for instance IMF 2015). It is thus tempting to link the lack of TFP growth in Spain since 1994, as documented by Conesa and Kehoe (2017), with the Spanish firm size distribution biased towards small firms. According to my findings, Spanish firms are small because they are unproductive rather than the other way around. Under this hypothesis, the predominance of small firms is the consequence and not the cause of the low productivity of the Spanish economy. Structural factors, including low R&D, low quality of human capital, and elements of the regulatory-legal framework (cost of doing business) that constrain firm productivity growth likely contribute to the dismal evolution of productivity in Spain.Footnote 2
 The rest of the article is organized as follows. Section 2 describes the Spanish firm-level data. Section 3 documents the existence of a positive association between size and productivity at the firm level. Then, Sect. 4 presents the econometric methodology in detail, the main results regarding the growing by learning hypothesis as well as some robustness exercises. Some concluding remarks are provided in Sect. 5.",36
9.0,1.0,SERIEs,23 November 2017,https://link.springer.com/article/10.1007/s13209-017-0168-7,Does trade openness influence the real effective exchange rate? New evidence from panel time-series,March 2018,Ernesto R. Gantman,Marcelo P. Dabós,,Male,Male,Unknown,Male,"The study of the determinants of the real exchange rate is a topic that has received much attention in international economics. The first theoretical approach to its conceptualization dates back to Cassel’s (1918, 1922) thesis, which stated that there is an equilibrium exchange rate for money across different countries and that the exchange rate should converge to this value regardless of temporary fluctuations (i.e., appreciation or depreciation due to different inflation rates). This is the core of the PPP theory, which has been extensively tested empirically with different methodological approaches in order to demonstrate that exchange rate time series are stationary. The evidence, however, is mixed (Froot and Rogoff 1995); and these inconclusive results have been explained as resulting from (1) the short time frame of the observation windows and (2) the particular exchange rate dynamics of the countries under analysis (e.g., in high-inflation economies, PPP theory appears to hold, while the evidence for normal economies tends to reject this thesis). Given the existence of long-term deviations from PPP in some countries, scholars were interested in explaining these deviations; and those who believed that PPP theory does not hold attempted to identify the factors behind appreciations or depreciations of the real exchange rate. So far, the studies on this subject are not conclusive about the particular factors that affect the real effective exchange rate (REER).Footnote 1 This paper contributes to our knowledge about its determinants in a number of ways, mostly related to methodological aspects. Since the empirical studies have shown mixed results depending on the countries that have been studied, we use a methodological approach that does not restrict the slope coefficients of the independent variables to be the same for each country. We also take into account the issues of potential endogeneity between the REER and trade openness and of cross-sectional dependence. This latter problem arises because movements in nominal exchange rates and their concomitant effects on real exchanges rates may affect countries not only individually, but collectively as well—for instance, shocks to the US dollar, either caused by exogenous political factors or endogenous economic determinants, have an immediate impact upon the domestic exchange rates of individual countries. Omitting the consideration of cross-sectional dependence leads to a potentially severe bias in the regression coefficients; and our methodological strategy deals effectively with this problem, which is indeed present in the data as we will later show. We also use a new REER dataset that covers a larger numbers of countries over an extended period of time, allowing to observe changes of the REER subject to different economic conditions. In addition, given the issues that have been addressed pertaining the adequate operationalization of trade openness, our analysis uses three alternative indicators of this concept. Taking all these methodological considerations into account, we find robust support for the hypothesis that trade openness is associated to a depreciation of the REER (Dornbusch 1974). This empirical finding is not only of theoretical interest. It also has relevant implications for economic policy. In some countries, problems of overappreciation of the REER cannot be simply solved by means of a nominal devaluation because of limits to monetary policy (e.g., countries that use the Euro as a monetary unit; Ecuador whose monetary unit is the US dollar, etc.). Our study suggests a viable alternative as there are several economic measures that can be readily taken by the economic policy maker to increase trade openness and thus generate a depreciation of the REER. The rest of the paper is organized as follows. In Sect. 2, we briefly review the literature on the determinants of the real exchange rate. Next, we present the data and methods used in Sect. 3. We report the results obtained in Sect. 4. Finally, the concluding section summarizes the findings of the study.",7
9.0,1.0,SERIEs,25 January 2018,https://link.springer.com/article/10.1007/s13209-018-0172-6,"Price dispersion, chain heterogeneity, and search in online grocery markets",March 2018,Xulia González,Daniel Miles-Touya,,Unknown,Male,Unknown,Male,"Price dispersion arises when stores in the same market set different prices for the same homogenous good (Hopkins 2008). Different sources of heterogeneity among sellers as well as information frictions explain much of the price dispersion observed in many markets (Lach 2002; Baye et al. 2006). First, store differentiation (e.g., discount retailers versus well-established national chains) may lead to some stores persistently selling their products at a lower price level than others in the same market. Second, when some consumers are perfectly informed about market prices while others are not, price dispersion could arise as a mixed-strategy equilibrium outcome (Stigler 1961; Varian 1980).Footnote 1 In this case, stores’ relative positions in the price distribution change over time—which makes it more difficult for consumers to identify the lowest-price store (Chandra and Tappata 2011). These two accounts are not mutually exclusive and may, when taken together, explain observed price dispersion. In fact, price dispersion remains after store differentiation is controlled for (Lach 2002; Baye et al. 2006). Price dispersion is a common feature of the grocery retail sector (Zhao 2006). Supermarket chains typically differ in the services offered (i.e., in terms of product assortment, shipping services, return policies, Web design, etc.), and the strategy of unannounced, short-term reductions in the prices of certain products (i.e., sales) is frequently employed. Thus chain differentiation and search frictions might explain price dispersion in grocery markets. In the case of online shopping, store differentiation appears to be less relevant and consumer search costs look lower than the costs of a “physical” search (Janssen et al. 2007); hence one might expect a reduction of price dispersion in the online context.Footnote 2 Nonetheless, empirical evidence suggests that price dispersion remains even when consumers can easily access price information on homogeneous products from retailers’ Web pages or price comparison websites (Brynjolfsson and Smith 2000; Baye et al. 2004a, b; Ellison and Ellison 2009). The aim of this empirical paper is to shed some light on price dispersion and search behavior in the Spanish online grocery sector. In Spain, online grocery sales represent a low yet growing share of the total grocery market. Most brick-and-mortar chains have already committed to this e-grocery segment, and most supermarket chains are increasing the number of services offered online. At the same time, the last few years have seen the deployment in Spain of several online price comparison applications that have significantly increased the price information available to consumers.Footnote 3 The empirical analysis is based on price information obtained from the comparison site Soysuper.com. The data cover a period of 182 days—from 1 October 2013 through 31 March 2014—and include 836,074 price observations. However, this data source does not reveal either the quantities sold online or the market shares of the respective grocery chains. We therefore follow the literature that estimates the search cost distribution when only prices are observed while accounting for chain heterogeneity. In particular, we follow the approach of Wildenbeest (2011) who, building on Hong and Shum (2006) and Moraga-González and Wildenbeest (2008), allows for the estimation of search costs using only price data while accounting for vertical product differentiation. Competition among supermarket stores is highly localized—even for online shopping. Supermarket websites might differ among locations in terms of product availability and prices.Footnote 4 Hence we consider four distinct markets that are geographically distant from each other: Madrid, Barcelona, Málaga, and Vigo. We follow the literature that deals with search costs in the grocery market and estimate those costs for two baskets of goods: one that includes frequently purchased products (beverages, breakfast and cereals, dairy products, pantry, and personal care and household) and another that includes only alcoholic drinks (beer, wine, and spirits), which we assume is purchased less frequently. Each basket includes identical products (at the bar-code level) in all stores. Yet to the extent that online grocery chains remain heterogeneous in terms of reputation and/or range and level of services, the baskets cannot be viewed as homogeneous products. As a preview of our results, we observe that price dispersion is still present (albeit to a lesser extent) even after we control for chain heterogeneity, and it persists over time. We find that chain heterogeneity explains much of the observed price dispersion and also that supermarkets’ relative positions in the price ranking do not remain constant over time. These observed patterns suggest that price dispersion may be due to chain differentiation and mixed strategies, which justifies estimating search costs based on a model of utility competition (as in Wildenbeest 2011). The estimated extent of search is low: across markets, more than two thirds of consumers search just once despite the low cost of searching. Overall, these results are in line with similar studies of the retail food market in other countries (e.g., France, the United Kingdom). In addition, our findings also indicate that the products purchased more frequently tend to entail lower search costs and are likely also to have lower price–cost margins. The rest of this paper proceeds as follows. In Sect. 2 we discuss several empirical studies that measure price dispersion and search cost. Section 3 describes our data, and Sect. 4 presents empirical evidence of price dispersion in Spanish grocery markets. In Sect. 5 we provide details on our model and estimation strategy; the estimation results follow in Sect. 6. We conclude with a summary of our findings in Sect. 7.",4
9.0,2.0,SERIEs,22 December 2017,https://link.springer.com/article/10.1007/s13209-017-0170-0,Are Euro-Area expectations about recession phases effective to anticipate consequences of economic crises?,June 2018,Marco Rubilar-González,Gabriel Pino,,Male,Male,Unknown,Male,"After the occurrence of the Brexit, there is increasing uncertainty about its consequences for European countries, especially when additional nations are starting to evaluate leaving the Euro Area. An economic recession represents the main fear especially when the effects of the Subprime crisis are fresh on mind of agents. A lesson thought by the Sovereign crisis is that economic crises have heterogeneous impacts across Euro-Area countries. This evidence is interesting given this contradicts the achievement of an optimum currency area. This issue can certainly affect expectations of agents for future disturbances. Empirical and theoretical evidence shows that public policies across Euro Area are homogenous (Kose et al. 2012; Calderón and Fuentes 2014); nevertheless, there are still important differences across these countries (Krolzig and Toro 2005; Staehr 2008; Krugman et al. 2012; Kolasa 2013). For this reason, recession phases can be triggered by different roots and can have different characteristics among Euro-Area countries. For instance, the Sovereign crisis had dissimilar impacts across European nations. While some countries had a high and persistent unemployment rate during several years, e.g. Spain, others had minimums effects on the market labor, e.g. Germany. In such an environment, tools which help to characterize the different features of recession phases across Euro-Area countries, which seem to have marked differences, are useful to anticipate deeps impacts of future crises. The objective of this paper is to provide a tool, which can be useful to characterize recession phases of Euro-Area economies. For this purpose, we use information pre Sovereign-crisis in order to characterize business cycle phases. Then, we are able to discuss whether this characterization coincides with the empirical evidence about the recession phases observed for these economies. To this end, we take advantage of the literature on synchronization of Euro-Area business cycles (Mundell 1961; Alesina and Barro 2002; Fidrmuc and Korhonen 2006; Bencik 2011; Morys and Ivanov 2015). In particular, we use expectation surveys and Markov Switching models to show that data alerted from heterogeneous consequences coming from a future crisis. Expectation surveys have two advantages for the purpose of this paper. This information is currently available for long-time periods, especially for developed countries. Furthermore, there is evidence about the important role that this kind of surveys play to anticipated changes on future economic fluctuations (Nardo 2003; Vermeulen 2014; Girardi 2014; Leduc and Liu 2012; Leduc and Sill 2013). Yet, we use the Economic Sentiment Indicator (ESI henceforth), which has close relation with key economic aggregates (Nardo 2003; Pesaran and Weale 2005; Banerjee et al. 2005; Silgoner 2007; Giannone et al. 2009; Girardi 2014). In fact, there is evidence in favor of its ability to forecast turning points over the business cycle (Tayor and McNabb 2007; Ozyildirim et al. 2010). Moreover, since the introduction of Markov switching models by Hamilton (1989), these have become popular to model series that present different regimes over time, i.e. they are non-linear. This characteristic takes relevance given univariate and multivariate linear specifications are not able to efficiently model series that present structural changes or asymmetries in their evolution over time (Granger and Teräsvirta 1993; Mittnik and Niu 1994; Sensier 1996; Milas et al. 2006 ). Here, we follow Krolzig (1997) who extends the MS models by allowing a change in mean, variance, and parameters over the different states of an economic series which is useful to characterize business cycle phases (Goodwin 1993; Artis et al. 2004; Krolzig and Toro 2005). In order to synthesize our analysis, we focus on four economiesFootnote 1: Germany, Spain, the Euro Area, and the European Union during the monthly period from 1985:01 to 2008:02. While Germany is one of the countries less affected by the Sovereign crisis in 2010, Spain has suffered serious and deep impacts in its economy (e.g. high and persistent unemployment rate). Therefore, if expectations contained in the ESI series are a good proxy of the economic activity, they should provide German recession phases that are short and with low probability of occurrence compared to the Spanish ones. Similarly, we study expectations of economic areas with different levels of integration, i.e. the European Union and the Euro Area. This is useful to analyze whether expectations of European countries are synchronized or not. Our results reveal a stability of expectations about German economic activity compared to the Spain case. German recession phases present a lower persistence and probability of occurrence than Spanish recession phases. Moreover, Germany possesses recession time periods shorter than Spain. These results are consistent with the empirical evidence about recession phases during the Sovereign crisis. For instance, Spain had one of the highest unemployment rates across the Euro Area. In contrast, the German economy even decreased its unemployment rate during the crisis period.Footnote 2 Similar relative stability is observed for the European Union compared to the Euro Area. Recession phases have a higher persistence and longer expected time duration in the Euro Area compared to the case in the European Union. We go further and analyze the synchronization between these economies finding a low degree. This evidence is against the achievement of an optimum currency area. Thus, important differences in the evolution of the economic activity of Spain, Germany, Euro Area and European Union are exposed in this paper. This issue should be taken into account by policy makers, otherwise, centralized policies could have undesirable impacts across Euro Area countries. This paper is organized as follows. Section 2 describes the methodology used in this paper. Section 3 presents a descriptive statistics of the data. Section 4 discusses the main results of this paper. Finally, Sect. 5 concludes this article.",
9.0,2.0,SERIEs,12 February 2018,https://link.springer.com/article/10.1007/s13209-018-0174-4,Voting with your feet: migration flows and happiness,June 2018,Helena Marques,Gabriel Pino,J. D. Tena,Female,Male,Unknown,Mix,,
9.0,2.0,SERIEs,06 March 2018,https://link.springer.com/article/10.1007/s13209-018-0175-3,On the stability of buyer groups under key account management,June 2018,Manel Antelo,Lluís Bru,,Male,Unknown,Unknown,Male,"In many industries, there is a tendency to configure purchasing groups as a means to achieve greater bargaining power vis-a-vis suppliers. This practice is widely used and the examples in specific economic sectors are numerous. In the USA, for instance, cable television operators create alliances to acquire programs from content providers (Chipty and Snyder 1999), purchasing groups bring together small drugstores and hospitals to acquire pharmaceutical products (Ellison and Snyder 2010), and auto parts companies group together, like Aftermarket Auto Parts Alliance, which represents some 50 auto parts distributors. Likewise, a number of examples of buyer groups can be found in the grocery, pharmaceutical retailing, and tobacco processing sectors in most European countries (Caprice and Rey 2015). As result of this practice, buyer groups have become the dominant purchasers of inputs in these and other industries. For instance, a survey by Burns and Lee (2008) reports that in the US healthcare products market, around 80% of hospitals participate in buyer groups and route 50% or more of their pharmaceutical spending through group purchasing organizations. Even more, according to the Healthcare Supply Chain Association (HSCA), “nearly every hospital in the U.S. (approximately 96–98%) chooses to use group purchasing organization contracts for their purchasing functions”.Footnote 1
 The emergence of large buyers in the form of buyer groups usually leads to a segmented market, with such buyer groups receiving better deals from their seller than smaller buyers that purchase on an individual basis. Sometimes, it is the buyers’ initiative that leads the way. This happens, for example, with the Centers for Medicare and Medicaid Services in USA when implementing competitive bidding processes for durable medical equipment and similar items (Newman et al. 2017).Footnote 2 Frequently, however, it is the suppliers who react to particular buyers, dealing with them in a more personalized and bilateral fashion than with small and independent buyers. Indeed, sellers develop key account management (KAM) programs for top sales executives, often creating a separate sales force or even a separate corporate division (Jonhston and Marshall 2003). Such business strategies are traditionally justified in terms of the importance of retaining major customers in the face of competition by offering them personalized and preferential treatment (Capon 2001; Jonhston and Marshall 2003). According to the jargon of the marketing literature, “KAM programs are designed to manage strategic accounts intensively and in a coordinated manner and to increase the value derived from the relationships” (Marcos-Cuevas et al. 2014, p. 1216). Thus, the utilization of a KAM program as a selling method can be understood as a reaction to changes in the purchasing behavior of customers (Ivens and Pardo 2007). The goal of this paper is to examine the interaction between a (monopolistic) seller that exerts market power and a continuum of identical price-taking customers in the following scenario. A subset of buyers can create a coalition (buyer group) and the seller has the possibility of dealing with such a coalition separately from customers that, remaining outside the coalition, purchase the good on an individual basis (independent customers). We particularly investigate the customers’ rationale for creating the buyer group, as well as the rationale that leads the seller to react using a KAM program as a selling strategy to segment a market configured by customers in a buyer group and by the remaining independent customers. In this context, we show that market segmentation explains the formation of the buyer group and vice versa. The creation of a buyer group allows its prospective members to gain leverage against the seller, but if there is no market segmentation, the seller treats all customers either inside or outside the buyer group equally; consequently, independent customers profit more from the formation of the group than its founders, as they have an incentive to freeride. This would have the result of leading the buyer group to unravel. However, it is also true that, whenever there is a buyer group, the seller wishes to segment the market and deal with customers in the group separately from independent customers (Ivens and Pardo 2007). If there is no market segmentation, the buyer group can obtain leverage against the seller by restricting its demand as a whole. Market segmentation therefore has two main advantages for the seller: first, the seller can charge higher prices to customers that purchase on an individual basis; second, a more efficient relationship between the two strategic players (the seller and the buyer group) emerges from the utilization of KAM as a business strategy. Interestingly, the fact that the seller reacts to the formation of a buyer group with a KAM approach encourages the actual creation of such a group. This article adds to the literature by describing a rationale for the emergence of a buyer group and the relationship with the implementation of a KAM program as selling method. We find that both decisions are closely linked, not only in the rather obvious sense of the seller creating a KAM program in response to the emergence of a buyer group, but also in the opposite sense of the group emerging and surviving because its prospective participants anticipate being treated separately from customers that remain outside the group. In this context, we point out that, without market segmentation, buyers face the standard free-rider problem of collective action (Olson 1965; Hardin 1971). In fact, a body of literature initiated by Bloch (1996) has considered the stability problem in the process of cartel formation and has analyzed how this can prevent the formation of a coalition (see Bloch 2005, for a sound review of this topic).Footnote 3 However, we also show that if customers expect the seller to react to the formation of a buyer group with a KAM strategy as the selling method, then the free-rider problem no longer holds. The seller, nonetheless, faces a commitment problem in this context. Ex-post, once a buyer group emerges, the seller’s profits increase when the consumers grouped receive preferential KAM treatment and the market becomes segmented. However, ex-ante, the seller’s profits would be greater if the utilization of a KAM program could be saved, since the buyer group would not arise in such a case. To explore why a subset of customers might want to group and why the seller might react with a KAM strategy, we consider an industry with a monopolistic seller of a single product and a continuum of homogeneous buyers. In this set-up, we assume the seller has minimal marketing tools consisting of a supply-function rule.Footnote 4 We also assume that independent customers behave as price-takers, taking the industry price as given, since they are sufficiently small to have a negligible impact on such price. We then analyze both the incentive to create a group through the strategic impact of its members’ demand on the market price, and the seller’s strategic reaction of offering bilateral contracts—the KAM strategy—to the customers grouped rather than a supply-function rule as occurs with independent buyers.Footnote 5
 The impact of bilateral deals and contracts on market competition has long been addressed by one strand of research within the industrial organization literature. For potential entrants into an industry, for instance, it has been argued that contracts between incumbent firms and buyers can act as an entry barrier (Innes and Sexton 1994; Segal and Whinston 2000). Beyond this, our findings suggest that, even in the absence of potential entrants, the utilization of bilateral contracts in transactions between the seller and its larger customers play a strategic role for both of them. In their comparison of the relative merits of different sales modalities in a range of circumstances, a number of studies are of particular relevance to our research. First, when demand is uncertain, competing sellers are better off announcing supply functions rather than posting prices or quantities (Klemperer and Meyer 1989). Second, with asymmetric information and heterogeneous customers, an auction is more profitable for the seller than a posted price (Wang 1993). Third, under asymmetric information, bilateral bargaining is preferable for the seller to posted-price selling in a dynamic context (Wang 1995). Finally, Ausubel et al. (2014) pointed out that large buyers in multi-unit auctions are incentivized to reduce demand and shade bids differently across units, resulting in inefficiencies in both uniform-price and pay-as-bid auctions—with the latter often outperforming the former in terms of efficiency and expected revenues. The remainder of the paper is laid out as follows. Section 2 describes the industry model used, namely, one consisting of a monopolist selling a homogeneous good to a continuum of buyers of equal size. Section 3 analyzes industry performance when all customers purchase the good on an individual basis and the seller submits a supply schedule. Section 4 examines the market outcome when some customers decide to form a coalition (a buyer group) and thus act strategically by submitting an aggregate demand function, whereas the seller continues to submit a supply-function rule as the only selling method with all customers, either inside or outside the group. In this context, we show that the customers not grouped profit more from the existence of the buyer group than the customers within the group, with the result that the group is destabilized. Section 5 analyzes the market outcome when the seller can offer different deals to customers in the group and outside the group; in this case, the seller’s profit increases with market segmentation, independent customers find they are worse off than grouped customers and the coalition then becomes stable. Section 6 concludes the paper.",
9.0,2.0,SERIEs,10 May 2018,https://link.springer.com/article/10.1007/s13209-018-0177-1,Explaining job polarisation in Spain from a task perspective,June 2018,Raquel Sebastian,,,Female,Unknown,Unknown,Female,"Debate concerning the structural evolution of the division of labour and its impact on job quality has been a central theme in social sciences for the last 200 years. In the late 1990s, the idea was that technology is skill-biased, favouring high-skilled workers and substituting low-skilled workers. While skill-biased technical change is a good explanation for the increase in the upper tail distribution of the labour force composition, it cannot explain a recent phenomenon: the decline in the share of middle occupations relative to high- and low-skilled occupations. This phenomenon has been defined as “job polarisation” (Wright and Dwyer 2003; Goos and Manning 2007). The main drivers behind job polarisation are still subject to some debate; however, the main candidate is the so-called routinisation hypothesis (Autor et al. 2003, hereafter called ALM). Due to continuously cheaper computerisation, technology replaces human labour in routine tasks. This labour-capital substitution decreases the relative demand for workers performing routine occupations, while leading to an increase in the relative demand for workers performing non-routine tasks. Since routine workers are characterised as being in the middle of the employment distribution, then the hollowing effect is explained. The notion that middle-skill jobs have been disproportionately destroyed and that the job distribution has hollowed out in the middle has been identified as a key aspect of contemporaneous rising labour market inequality (Acemoglu and Autor 2011; Goos et al. 2009, 2014). Therefore, understanding how the employment structure evolves can advise policy makers in designing policies to best promote a sustainable economic growth. This is especially salient, given the widespread feeling of technological anxiety (Mokyr et al. 2015). Firstly, there is a need to understand whether the shrinking of middle jobs has consequences for the possibility of moving low-skilled workers up. Secondly, an accurate understanding of occupational employment is needed in order to anticipate future skills needs and job opportunities. Despite the importance of this topic, the results of research that assess the existence and degree of job polarisation in Spain are mixed. For example, Anghel et al. (2014) conclude that the employment structure became more polarised between 1997 and 2012, while Oesch and Rodríguez-Menés (2011) and Eurofound (2015) show a pattern of progressive upgrading for the same period. Moreover, two recent studies covering Spain, based on the European Labour Force Survey, diverge in their results. Goos et al. (2009, 2014) conclude that, on average, the employment structure in Spain became more polarised between 1993 and 2006. Using the same period of analysis, Fernández-Macías (2012) conversely shows an upgrading process (high-wage occupations expanding at the expenses of low-wage jobs) and does not provide evidence of a pervasive polarisation. These five papers have relied on graphical inspection to identify the phenomenon: terciles (Goos et al. 2009, 2014; Fernández-Macías 2012), or quintiles (Eurofound 2015; and Oesch and Rodríguez-Menés 2011). Focusing on the Spanish case, this paper makes several contributions to the understanding of the evolution of the employment and wage structure in four complementary ways. First, I shed some light on the literature on employment polarisation in Spain, providing evidence of job polarisation in our sample; between 1994 and 2014, employment share in Spain increased at the two extremes of the job wage distribution, while it decreased for middle-income earners. My study adds to the literature on job polarisation, offering two new ways of representing the phenomenon and enlarging the period of analysis.Footnote 1 I also contribute to widening the literature on employment remuneration in line with employment trends. In the US, Autor and Dorn (2013) find a clear correspondence between employment and wages. However, the polarisation of wages does not seem to be common in Spain, as there is no evidence that changes in pay followed the same pattern as changes in occupations. This contrasts with standard labour markets models, predicting that a positive demand shock increases both employment and earnings. Second, I made methodological progress with respect to previous studies on job polarisation and task specialisation in European countries by measuring the tasks content of occupations from a national survey data instead of relying on US sources, like in the work of Anghel et al. (2014) and Goos et al. (2014). Therefore, no assumption on task composition and the impact of technology between the two countries is needed. Moreover, the EWCS allows for time dynamics to measure routine tasks.Footnote 2 Using this survey, jobs are classified as abstract, routine, and manual tasks, similar to the ALM model. This allows for examination of the association between employment changes and the task content of occupations. Therefore, I perform a shift-share analysis to evaluate the evolution of the tasks’ content, exploring whether the changes of the task content of occupation are due to changes within occupations (intensive margin) or between occupations (extensive margin). Third, unlike previous studies, I explore the relationship between computer use and routine tasks, which I define on the basis of the frequency of repetitive activities that workers are asked to perform on the job. After creating a pseudo-panel analysis, results show a negative relationship between computers and routine tasks, and a positive association between computer and abstract tasks. However no relationship is found for manual tasks. Therefore ALM predictions are satisfied for routine and abstract tasks, but not for manual tasks. Finally, I analysed the role of job polarisation with the relocation of middle-skilled workers. To investigate this phenomenon, the main data source is integrated with two additional datasets: the European Community Household Panel (ECHP) and the European Survey of Income and Living Conditions (SILC). Taking advantage of these new databases, the analysis builds from questions on previous occupations. There are two main findings: in line with the ALM, middle-skilled workers become more mobile over time and have the highest probability levels of mobility. However, after dividing the data into graduates and non-graduates, results suggest that while non-graduate middle workers move towards bottom occupations, graduate middle employees shift towards top occupations. This fact suggests that supply-side changes are important factors in explaining the job expansion at the lower and upper tail of the employment distribution. The paper is organised as follows. Section 2 clarifies the main concepts and provides a review of the literature. Section 3 describes the data and methods used for analysis. Section 4 presents the evidence on labour market polarisation, on both employment and pay rules. Section 5 investigates the task content of occupations. Section 6 looks at the impact of computer adoption on tasks. Section 7 analyses the occupational mobility of middle-pay workers. Finally, Sect. 8 summarises the main conclusions of the paper and provides a guide for future research stemming from this paper’s findings.",7
9.0,3.0,SERIEs,07 July 2018,https://link.springer.com/article/10.1007/s13209-018-0179-z,Discrimination without taste: how discrimination can spillover and persist,August 2018,Rajesh Ramachandran,Christopher Rauh,,Male,Male,Unknown,Male,"Discrimination against certain social groups over long time periods has been a historical feature of many societies. For instance, in the US discrimination in the form of slavery officially ended in 1865 after more than two centuries, though racial segregation was maintained in the form of Jim Crow laws until 1965.Footnote 1 Starting with the civil rights movements in the early 1960’s, one has seen significant advances in the rights and outcomes of the black population. However, today the black population still lags behind whites in a range of socio-economic characteristics. In India, caste, which is inherited by birth, was a marker for social discrimination for centuries. At independence in 1947, the practice of untouchability was made illegal and affirmative action was enshrined in the constitution for disadvantaged groups. However, the lower castes continue to trail significantly behind other social groups in terms of most socio-economic indicators. What contributes to the gap between groups that faced discrimination over long time periods and those that did not? In what outcomes and why might we observe persistent gaps? In this paper, we posit a channel of discrimination, where even under perfect observability of individual ability, the absence of discriminatory social norms, and when taste for discrimination has already died out, to discriminate can be the optimal response. The theoretical mechanism put forth rests on the existence of beliefs about discrimination by others in society, and on distinguishing between activities characterized by the need for interlinkages versus no need for interlinkages. In our model, activities with interlinkages require coordinated actions. If an individual decides to establish interlinkages, she requires the input of two principals to form a productive unit. The success and return for all, the individual and the two principals, is contingent on the participation of all three in the venture. The coordination failure results from the belief that somebody else might discriminate and refuse to participate in the venture, which imposes losses due to the complementarity of inputs in the production process. The classic example would be the case of entrepreneurs who need to establish multiple interlinkages (productive relations) to be able to start and operate a venture (Basu 2010). In the theoretical model, individuals choose between entering activities which require establishing productive relations and those that do not. Individuals intending to enter activities involving interlinkages are randomly matched with a pair of principals, for instance a lender and a distributor, with whom they need to establish interlinkages to form a productive unit. The individual cannot produce without capital and cannot sell without a distributor. In case one of the principals agrees to participate and the other does not, the investment of the first principal is held up and imposes a fixed cost. We show how in the presence of beliefs about discrimination against a certain group, principals without a taste for discrimination also discriminate against that group in equilibrium. We derive the conditions under which the model predicts lower participation rates and higher cost of establishing interlinkages for the discriminated group relative to the non-discriminated group in equilibrium, leading to an overall welfare loss for society. The model also establishes conditions under which the steady-state equilibrium is characterized by the existence of discrimination due to beliefs about the existence of taste discriminators, although there are no taste discriminators left in society. The persistence of beliefs regarding discrimination in the steady state are interpreted as intergenerational transmission of beliefs in the sense of collective memories, consistent with utility maximizing or cultural trait preserving strategies. We empirically test our theoretical predictions in the market for self-employment, an occupation requiring the establishment of interlinkages. In particular, our focus is on the the market for self-employment of blacks and whites in the United States. We show that the representation and payoff for the discriminated group in self-employment, as well as the probability and cost of establishing interlinkages, are in line with our theoretical predictions. Next, using data from the General Social Survey (GSS) from the years 1972–2012 (Smith et al. 2012), we create proxies of beliefs about and tastes for discrimination against blacks by year and region to determine whether the presented belief-based mechanism finds any support in the data. The time trends of taste for discrimination and beliefs about discrimination from the GSS and the self-employment rates for blacks and whites from the Current Population Survey (CPS) for the time period 1972–2012 are shown in Fig. 1.Footnote 2 Taste for discrimination against blacks linearly declines over the observed period, whereas beliefs about discrimination against blacks as well as the gap between the self-employment rates for blacks and whites remain remarkably constant. Figure 1 captures the mechanism and the role of sticky or unchanging beliefs highlighted by the theoretical model in a snapshot. The unchanging beliefs perfectly correspond to the invariant gap in self-employment rates over the period analyzed, as predicted by the theoretical framework. Using a logit model, we find our proxy for beliefs about presence of discrimination to be a significant and negative correlate of the probability of becoming self-employed for blacks in the US. The results are robust to the inclusion of a race dummy to account for other unobservable characteristics of racial groups, as well as year and region fixed effects. Furthermore, using the National Survey of Small Business Finances of 1998 and 2003, we show that beliefs about discrimination also explain other features predicted by our model, namely that beliefs are a significant and positive correlate of blacks having their loan application rejected and being charged higher interest rates, and blacks reporting that they do not apply for a loan due to fear of rejection. The presented statistical associations are persistent across a variety of specifications and present strong evidence in favor of the theoretical framework, though no causal claims can be made on the basis of the available data. Self-employment rates by race and beliefs and taste regarding discrimination in the US The literature of the economics of discrimination was pioneered by the seminal work of Becker (1957). In the setting envisaged, employers hold a taste for discrimination, such that working with members of a particular group imposes a cost on them, and hence these workers have to compensate the employer by either being more productive or accepting lower wages. Extensions involving mechanisms based on consumer discrimination (Borjas and Bronars 1989; Nardinelli and Simon 1990) are again contingent upon the presence of consumers who dislike purchasing from or interacting with members of another race, or in other words, individuals possessing a taste for discrimination. The class of models of statistical discrimination (Phelps 1972; Arrow 1973; Aigner and Cain 1977; Lundberg and Startz 1983, 1998; Coate and Loury 1993; Rosén 1997) and categorical thinking (Fryer and Jackson 2008) rely on the imperfect observability of worker productivity. In absence of complete information, employers base their decision on easily observable characteristics, such as race or gender, to infer the expected productivity of the worker. Mailath et al. (2000) present a model of endogenous discrimination arising from the search decision of firms. The asymmetric discriminatory equilibrium is supported due to the belief that there are more skilled workers available of a particular type, which is borne out in equilibrium. The third class of models is that of Akerlof (1976, 1985) and Peski and Szentes (2013), where not following the established norm of discrimination against certain groups might result in imposition of social sanctions which cause economic losses, making discrimination a rational response. Lang et al. (2005) present a model of a labor market characterized by wage postings where even weak discriminatory preferences can lead to large wage differences and labor market segregation. They discuss how the discriminatory equilibrium is not contingent on the actual use of discriminating hiring strategies by firms but requires only that the black workers believe that they do. The key distinction from our work is that in their setting, discrimination arises from just one side of the market. Thus, if a black person was to apply by mistake, that is, not accounting for her belief about discrimination, her application would be accepted and this would lead to an unraveling of the self-confirming equilibrium. For the equilibrium to unravel in our setting, we require that both the applying individual and the accepting principals do not to account for their beliefs about discrimination. Thus, the self-confirming equilibrium is much more robust and also extends to a range of other phenomena such as neighborhood segregation and ethnic patronage in society (see Sect. 3.3). Our model contributes a new mechanism as to how discrimination can persist, and provides an alternative channel through which phenomena such as racial segregation or ethnic patronage can arise and persist in societies. In our setting the distribution of ability within the two groups is identical ex-ante and ex-post, there is perfect observability of ability, and there are no social norms to discriminate. Moreover, the nature of the coordination failure highlighted does not allow for a single principal who does not discriminate to reap the unrealized profits, a possibility traditionally assumed by Becker (1957), therefore providing a theoretical rational as to why discrimination can persist. To our knowledge, we are also the first to provide empirical evidence, albeit correlational, concerning the belief-based channel of discrimination.",2
9.0,3.0,SERIEs,23 July 2018,https://link.springer.com/article/10.1007/s13209-018-0181-5,Estimating output gap: a beauty contest approach,August 2018,Carlos Cuerpo,Ángel Cuevas,Enrique M. Quilis,Male,Male,Male,Male,"Policymakers strive to understand the dynamics of the business cycle and pinpoint its specific location as it decisively determines the outcome of policy decisions. The slack or output gap, defined as the amount of unemployed resources (i.e. the distance to potential output) is, however, not observable and surrounded by considerable uncertainty. The literature has developed a myriad of estimation techniques over the last decades, ranging from data-driven univariate filters to structural general equilibrium models.Footnote 1 The horse race in search of an optimal output gap estimation methodology seems far from settled. On the one hand, the uncertainty surrounding the output gap estimates has proven a challenging task, leading to unreliable estimates in real time, which happens to be the policy-relevant time frame. On the other hand, confronting output gap estimates with optimality criteria (both statistical and economic ones) has generally led to inconclusive results, as the former might be ill-defined or even incompatible and thus a selection algorithm becomes necessary. The selection criteria should aim at providing a well-defined metric or comparable benchmark for different estimates. In practice, they can generally be split into three dimensions. First, statistical goodness (SG) referring to elements such as minimizing the end-point problem or providing information on the precision of the estimates. Second, economic soundness (ES) implying ex-ante consistency between selected stylized facts and the method’s underlying assumptions. And third, transparency (TR) requirements as seen from a user-specific perspective, reflecting accountability elements such as likelihood of replication or data needs. These three criteria might be considered as a necessary methodological prerequisite. Figure 1 reflects potential tensions in their fulfillment and represents trade-offs faced by some standard methodologies (DSGE models, univariate filters and the production function approach). The internal optimality area represents methods satisfying the necessary conditions (although in different degrees). They are not, however, sufficient conditions as ultimately the acceptance of a specific output gap estimate must pass the smell test or narrative approach, providing an acceptable country-specific narrative that explains the cyclical evolution. Optimality necessary requirements This paper builds upon existing research on output gap measurement techniques and presents an approach for the selection of an output gap estimate that pivots around a multivariate unobserved components (MUC) Kalman filter estimation. Multivariate filters and the unobserved components multivariate Kalman filter technique represent a good compromise between the necessary criteria, falling within the optimality area in Fig. 1. First, the use of a multivariate framework allows for the consideration of additional economic relationships (Okun's Law, Phillips Curve, etc.) going beyond univariate filters while at the same time imposing lighter economic priors than fully structural models and thus sticking more closely to the data. Second, the statistical properties of multivariate techniques clearly outperform other methods such as the production function approach, allowing for example for an integrated estimation of uncertainty. Third, multivariate approaches are generally not data-intensive and thus easily replicable and largely transparent, being more parsimonious than fully-fledged economic models.Footnote 2
 The focus for the selection of a specific output gap estimate is diverted from the traditional model horse race, which focuses on the comparison between different methodologies along the three necessary criteria (ES, SG and TR). Instead, the final estimate is derived from a beauty contest between candidate variables in a MUC framework. Different specifications of the model are tested by combining GDP with potential candidate variables sharing relevant information about the business cycle. The latter can include domestic (capacity utilization, unemployment), open-economy (current account, exchange rate), financial (credit to non-financial corporations) and price (GDP deflator, CPI, house prices) candidates. The selected approach allows for country-specific cycle definitions, generalizing the work in Borio et al. (2017) and Alberola et al. (2013). The paper is structured as follows; Sect. 2 details the estimation methodology, Sect. 3 specifies the necessary and sufficient criteria and develops the selection algorithm, Sect. 4 present an application for Spain as a case study and Sect. 5 concludes. Finally, two appendices complete this contribution, the first one devoted to the implementation of the Kalman filter and, the second one, giving details on the statistical features of the selected output gap estimate for Spain.",3
9.0,3.0,SERIEs,18 July 2018,https://link.springer.com/article/10.1007/s13209-018-0180-6,A simple dynamic contest with a parameterized strength of competition,August 2018,José A.  García-Martínez,,,Male,Unknown,Unknown,Male,"The most common use of contests is as mechanisms to create incentives to work harder. However, they can also be used as selection mechanisms. For example, a contest can select agents that differ in their expected efficiency levels. This can be relevant if the institution or principal cannot either impose the strategy to be followed by an agent or observe the type of agent. We seek to study the role of the number of prizes (k) and the number of contestants (n) in the outcome of the selection process.Footnote 1 These institutional parameters define a ratio \(\frac{k}{n}\), which can be seen as a measure of the strength of competition. For example, if \(k=1\) and \(n=3\), three agents compete for only one prize in every group. Similarly, if \(k=1\) and \(n=10\) then 10 agents compete for only one prize. Note that in the second case competition is higher (ceteris paribus). Hence, the strength of competition increases as ratio \(\frac{k}{n}\) decreases. We focus on a specific kind of selection, in particular this “strength of competition” in a contest, so we take an evolutionary approach. To that end, we consider a large population with two possible types of behavioral agents: A-agents and B-agents, where environments where type A outperforms B are more likely to occur than environments where B outperforms A. In this sense, A is a better type because it has a higher expected success rate.Footnote 2
 In this population of agents, we consider that each individual interacts with randomly selected individuals.Footnote 3 Thus, groups are formed by a random matching process. However, a random matching process would generate a very complicated stochastic system. In the economic literature, for large populations (either countable or uncountable) the population dynamic is usually approximated by a deterministic process, where the frequency of different matches is identified with their corresponding expectations. This simplifying assumption is analyzed in several papers from different points of view. Examples include Boylan (1992, 1995), Alós-Ferrer (1999), and Duffie and Sun (2012). In Sect. 2, we make some assumptions that guarantee the existence of a matching process, so we can consider the deterministic process presented in this paper as a good approximation of the complex stochastic system. In the literature on evolutionary selection models the matching process is usually made in pairs, which is a particular case of group size, \(n=2\), where one agent is selected, \(k=1\). We must consider a more general matching process in which \(n\ge 2\), and \(k\ge 1\) agents are selected. However, the rationale in this setting is the same as in matching in pairs. We consider that there is a continuum population of agents, and we work with the proportions of different kinds of groups of agents. To sum up, this paper analyzes the effect of strength of competition on the characteristics of the successful agents. To that end, we consider that at \( t=0\) each agent is given one behavioral rule, either A or B. They are not strategic, so they become agents of type A or B. At each t, the population of A and B agents is randomly matched in groups of n agents, with members of each group competing with each other, and each group facing a particular environment. The mechanism selectsFootnote 4 the top \(k<n\) performing agents from each group, with \(k\in \{1,2,3,\ldots \}\) and \( n\in \{2,3,\ldots \}\). We assume that at \(t+1\) nonwinners imitate the action of winners at t, so the population at \(t+1\) reproduces the distribution of the type of winners at t.Footnote 5 Consequently, the proportion of A-agents in the population is equal to that of the winners. Then they are again randomly matched and the process repeats. We seek to learn how this competition process changes the characteristics of the population. There is an alternative imitative behavior, which adds a different but very interesting point of view of the dynamic process. Nevertheless, the resulting dynamic is the same as in the first imitative assumption. Let x be the number of A-agents in a group (and \(\left( n-x\right) \) the number of B-agents). Under this second imitation behavior, regardless of what environment a group is facing, if the number of individuals who successfully match the environment exceeds a threshold k then the entire group adopts the strategy matching the environment. However, if the number of agents is below the threshold k then in groups facing environment A only a fraction \(\frac{x}{k}\) of the members adopt the strategy matching the environment, strategy A, and the remaining \(1-\frac{x}{k}\) adopt B.Footnote 6 Similarly, in groups facing environment B only a fraction \( \frac{n-x}{k}\) copy B and the rest copy A. This basically means that even if one strategy proves more successful with the current environment it will not automatically dominate the group unless it is sufficiently represented. The higher k is, the more easily the members of the group imitate successful behavior. Thus, the ratio \(\frac{k}{n}\) measures the minimum proportion of members of the group matching the environment needed to cause all members to copy the successful action. Therefore, this ratio also measures the level of conformity in this population. The higher \(\frac{k }{n}\) is, the more successful agents are needed to cause the group to change behavior, so changing behavior becomes more difficult. On the other hand, a lower \(\frac{k}{n}\) makes success more important relative to conformism and the environment becomes more competitive. Therefore, in this context, conformism and competition are correlated. In addition, notice that, after both imitation rules, the proportion of A-agents among the nonwinners is equal to the proportion of A-agents in the population of winners. Thus, we can focus on the proportion of A-agents among the agents selected and study how the strength of competition changes the distribution of the population. In this model A-agents perform better than B-agents more often, so we should expect an increase in the strength of competition to punish B-agents and the proportion of B-agents to decrease as competition increases. However, our results show that an increase in competition does not always work this way: In particular, we find that for high enough levels of competition B-agents can persist in the long run, despite being expected to perform worse. More precisely, depending on the strength of competition we find three possible cases: Cases L, M and H. First, case L: If the strength of competition is too low, the selection process is not strong enough to offset the inertia of the initial population. The dynamic thus depends on the initial conditions and the population eventually becomes homogeneous, i.e. with only type A or type B persists in the long run. Second, case M: If the strength of competition increases (intermediate level) the whole population will become A-agents for any initial mixed population. In this case the selection process is strong enough to eventually select the best performers, as expected. Finally, case H: if competition increases far enough, B-agents also survive.Footnote 7 Thus, surprisingly, we show that no matter how low the success rate of a type is, if the strength of competition is high enough agents of that type survive in the long run. In other words, too much competition is always harmful to the best performers. The intuition behind our results is broadly explained in Sect. 3.1. The contribution of this paper is twofold. First, it presents a family of contest selection mechanisms that parameterizes the strength of competition in a simple way. In evolutionary models agents are usually matched in pairs and one of them is selected. This paper generalizes this idea in contests and considers matchings of n agents with \(k<n\) agents selected. Second, we show that this generalization is not innocuous but has a surprising result even in a very simple model. As far as we are aware, there are no similar approaches in the literature on evolutionary models. Our approach is concerned with designing a suitable selection mechanism, which depends on the objective function of the institution.Footnote 8 This approach is related to some extent to classic mechanism design, especially principal-agent models. In such models the information that players have about others players and their individual choices has a major role in the design of the mechanism. However, our approach puts the focus on an institutional characteristic, i.e. the strength of competition, and we try to highlight that it can be an important factor to be considered even in a simple model. This paper is related to Harrington (1998, 1999a, b, 2000, 2003) and Garcia-Martinez (2010) because our mechanism can be seen as a generalization of theirs. Harrington uses a selection process in a hierarchical structure to compare the performance of rigid behavior with that of flexible behavior. Agents are randomly matched in pairs (\(n=2\)) and one of them is selected. Thus, the strength of competition is fixed. Garcia-Martinez (2010) analyzes a promotion system that works in two steps. The first step is like Harrington’s mechanism: Agents are matched in pairs and one of them is selected. In the second step, the agents selected in the first step are pooled together and the top fraction \(\theta \) of best-performing agents is eventually selected; this is referred to as “global selection”. In Vega-Redondo (2000) a hierarchical structure is used to select agents, who play in pairs (\(n=2\)) a \(2 \times 2\) coordination game, where there is only global selection. The present paper is also related to the literature on tournaments produced since the seminal paper by Lazear and Rosen (1981), in particular to those papers that focus on the selection role of contests, e.g., Rosen (1986), Section V, Hvide and Kristiansen (2002), Tsoulouhas et al. (2007), Azmat and Möller (2009), and Groh et al. (2012). The rest of this paper is organized as follows: Sect. 2 describes the model and the dynamic equation; Sect. 3 analyzes the dynamics, discusses the results, and provides some intuitions; Sect. 4 analyzes the convergence time; and Sect. 5 concludes.",
9.0,3.0,SERIEs,21 July 2018,https://link.springer.com/article/10.1007/s13209-018-0182-4,Impossibility theorems with countably many individuals,August 2018,Uuganbaatar Ninjbat,,,Unknown,Unknown,Unknown,Unknown,,
9.0,4.0,SERIEs,08 November 2018,https://link.springer.com/article/10.1007/s13209-018-0185-1,"Income, consumption and wealth inequality in Spain",November 2018,Brindusa Anghel,Henrique Basso,Elena Vozmediano,Unknown,Male,Female,Mix,,
9.0,4.0,SERIEs,05 July 2018,https://link.springer.com/article/10.1007/s13209-018-0178-0,Classifying bounded rationality in limited data sets: a Slutsky matrix approach,November 2018,Victor H. Aguiar,Roberto Serrano,,Male,Male,Unknown,Male,"The Strong Axiom of Revealed Preference (SARP) completely characterizes whether a finite set of prices and demand choices can be rationalized. However, this binary approach is incomplete, in the sense that we do not know by how much behavior departs from rationality. Furthermore, we learn little about the demand data that fails to satisfy SARP. This has motivated an important body of literature that has focused on how to measure the size of rationality violations. The revealed preference approach (henceforth RP) is the preferred method to test the rationality hypothesis in consumer behavior, due to its nonparametric nature. The leading contributions of this approach to testing rationality are Afriat (1973) and Varian (1983), which revolve around Afriat Efficiency Index (henceforth AEI). The prevalence of the RP approach is due to the simplicity of its implementation for the commonly used data sets, and to its flexibility, explained by its nonparametric nature. However, in the words of Varian (1983) “the RP approach fails to summarize the data in a useful way.” In a recent contribution, Aguiar and Serrano (2017) propose an index of violations of rationality based on the Slutsky matrix. Their Slutsky matrix norm (SMN) approach helps to answer the question of how far is a given behavior (demand function) from rationality, using the entire observed Slutsky matrix function distance to its closest rational Slutsky matrix function. Moreover, the methodology provides a useful classification of the violations of the classical axioms of revealed demand. The main limitation of the approach, however, is its reliance on infinite data, making its applicability questionable. In this paper, we demonstrate how the approach in Aguiar and Serrano (2017) can be adapted to finite data sets. In doing so, we attempt to provide a complementary approach to the RP methodology, stemming from the tradition of Antonelli (1951), Slutsky (1915), and Hurwicz and Uzawa (1971) by using the Slutsky matrix to test the empirical implications of consumer behavior in a finite data set environment. In particular, we build and implement a methodology that may help to measure and classify boundedly rational demand data by means of an index of deviations from rationality that has a useful decomposition of SARP-inconsistent behavior. We use classical results from the RP approach to show that nonrational behavior will be accounted for by a Slutsky matrix function that fails to satisfy the Slutsky regularity conditions for any extension of the data set. Conversely, if a data set can be rationalized in the sense of Afriat (1973), there exists an extension thereof that offers a rationalization in the sense of Hurwicz and Uzawa (1971). An implication of this observation is that testing rationality in the RP approach and our modification of the SMN approach with finite data is equivalent. In particular, a data set that violates SARP will have a positive Slutsky norm for any extension of the data set, with an informative decomposition of SARP-inconsistencies. These results are obtained because the length of revealed demand cycles, revealed demand axioms, and properties of the Slutsky matrix are neatly connected. To link finite and infinite environments, we exploit the fact that any sequence of data points can be interpolated by a continuously differentiable function. Thus, our main contribution is to propose a way to classify consumer behavior according to its Slutsky matrix norm in limited data sets. We adapt the decomposition of the Slutsky norm into the different violations of axioms of revealed demand for finite data. Our results seem to hold well in simulations.Footnote 1
 To the best of our knowledge, this is the only measure of departures from rationality that allows such a decomposition analysis. In addition, the Slutsky matrix norm differs from the AEI (Afriat 1973), the most frequently used measure of violations of rationality, in that the former is a positive index of rationality, whereas the latter is normative. Indeed, the Slutsky matrix norm measures the error a modeller would make when predicting the correct elasticity behavior of a consumer, if she makes the rationality assumption. In contrast, the AEI quantifies the minimum wasted income that the consumer has incurred without improving his own welfare. Other well-known indices of departures from rationality that can be called normative are the Money Pump Index (Echenique et al. 2011) and the Minimum Cost Index (Dean and Martin 2015). For other positive indices, see Varian (1983) and any other goodness of fit index, concerning failures in predicting out-of-sample demand quantities (behavioral nearness), under the rationality assumption. Relating our approach to some of these, we also provide theoretical results that connect quantitatively the SMN with the AEI and the Behavioral Nearness problem (i.e., the least distance from any given demand to its best rational demand approximation). This is the paper’s outline. Section 2 presents the primitives of our analysis and the model. Section 3 presents an auxiliary equivalence result between the RP and the SMN approaches that provides a logical bridge between the finite data set environment and the Slutsky matrix function approach. Section 4 contains our main results and develops a new class of interpolators of consumer choice data sets that have a minimal Slutsky norm and have desirable properties. Section 5 implements simulation exercises to study the numerical behavior of the minimal Slutsky norm interpolators and a comparison with Afriat’s index. Section 6 discusses formally the connection of the SMN with other measures of bounded rationality. Finally, we present a brief literature review in Sect. 7 and conclude in Sect. 8. The proofs are collected in an “Appendix”.",3
9.0,4.0,SERIEs,06 August 2018,https://link.springer.com/article/10.1007/s13209-018-0183-3,Television and electoral results in Catalonia,November 2018,Iván M. Durán,,,Male,Unknown,Unknown,Male,"The literature on the political economy of media has grown rapidly and several excellent surveys have already been written on this topic (DellaVigna and Gentzkow 2010; Prat and Strömberg 2013; Sobbrio 2014; Strömberg 2015). Nevertheless, most of the empirical research has thus far focused on highly consolidated democracies (e.g. the USA and Scandinavian countries) and some formerly authoritarian countries (e.g. Russia and East Germany). As Sobbrio (2014) points out, additional academic contributions are needed to provide empirical evidence on the effects of media in less explored institutional settings. Catalonia’s political context, characterized by a long-standing secessionist conflict with Spain, is a newfangled case study to analyze the effect of a sub-national media such as TV3, which has had an alleged pro-independence bias, on political outcomes. This paper therefore contributes to the existing literature on the effect of media on politics by identifying to what extent voting results in Catalonia can be explained by exposure of individuals to TV3. In recent years, TV3 has been the subject of much controversy for its alleged support for the Catalan secessionist movement and partisan bias, which has seemingly favored the political coalition Convergència i Unió (CiU). The CiU was a federation of two political parties representing the Catalan nationalist ideology, created in 1978 and dissolved in 2015, that played a crucial role in the creation of the channel in the early 80s as well as its direction since then.Footnote 1
 Given this increased political tension, in which TV3 seems to play a main role, it raises the question of to what extent voting results in Catalonia can be indeed explained by the exposure of individuals to this channel. Providing empirical support to the effect of TV3 over the last decade of political tension in Catalonia is still a challenge, due to the fact that the whole region has been covered by TV3 since the mid-80s, thereby impeding to sort out the self-selection problem. However, there is the possibility of taking advantage of a natural experiment exploiting the staggered expansion of the channel in the early 80s. Although it is true that the secessionist movement has been more vehement in the last decade, the effect of TV3 on Catalan politics may be traced back to the beginnings of the channel. In fact, it is noteworthy that the emergence of TV3 in 1983—that is, in the middle of the first two Catalan Parliamentary elections (1980 and 1984) after 40 years of dictatorship—precisely coincided with the consolidation of the CiU as the strongest political force in the region. The number of votes for the CiU rose from 754,448 in 1980 to 1,345,513 in 1984, which represents an outstanding increment in the vote share from 28 to 47% (see “Appendix 1”). Admittedly, the increase in the CiU’s vote share could have only been a consequence of the consolidation of the Catalan party system, which had been fragmented up until then. Nonetheless, the CiU is the party which exhibited the greatest vote share increase, which leads us to suspect about a possible effect of this channel on the individuals’ electoral behavior, and thus raises the need for closer scrutiny of its role. Exploiting a natural experiment based on TV3’s staggered expansion across Catalan municipalities during 1983 and 1984, this paper looks into the effect of TV3 on the electoral results of the 1984 Catalan Parliamentary election. In particular, we analyze the effect of TV3’s availability at the municipality level on two political outcomes of interest: voter turnout and the CiU’s vote share. To do so, we implement a standard two-periods Difference-in-Differences (DiD) model. Additionally, we examine the impact of TV3’s availability on other political parties competing in the same election, we analyzed the treatment duration effect of the channel on voter turnout and the CiU’s vote share, and we estimate the persuasion rates. As we will document later, although TV3 intended to cover the whole region of Catalonia in a short timeframe, even before the 1984 Catalan parliamentary election, exogenous technical and logistical constraints presumably caused a delay in the timing of the introduction of TV3 across different municipalities. This implies that the presence of TV3 in only some municipalities is plausibly exogenous to political attitudes. This delay, however, did not necessarily change the ordering; that is, the initial assignation to TV3 is not random, but it responded to the spread of the channel from the center (Barcelona) to the periphery, in accordance to the availability of broadcast centers. Thus, the identifying assumption is that the TV3 staggered expansion is unrelated to other factors that influence political outcomes once we take into account relevant initial conditions of TV3 placement. We find that the introduction of TV3 caused a significant increase in voter turnout in the 1984 Catalan parliamentary election, but we did not find evidence that the channel had indeed influenced voting for CiU. Specifically, municipalities exposed to TV3 experienced a larger change in voter turnout between 1980 and 1984, this effect oscillates from 6.8 to 8.4 percentage points, depending on the specification. Thus, at least in this setting, TV3 apparently played a role by increasing electoral participation, which is a positive expected effect of a media outlet in any democracy. On the other hand, there is no evidence to conclude that the channel had an effect on the vote share of CiU in this election. Thus, even though the CiU’s vote share showed a considerable growth between 1980 and 1984 (see “Appendix 1”), it cannot be said that this increment is attributable to TV3, but it was seemingly a phenomenon that covered all of Catalonia. The rest of the paper is organized as follows. Section 2 presents a brief case contextualization. Section 3 reviews some theoretical and empirical literature on media, voting turnout and political persuasion, and we pose the working hypotheses. Section 4 presents the available data. Section 5 explains the empirical strategy. Section 6 shows the main econometric results. Finally, Sect. 7 presents some final remarks.",
9.0,4.0,SERIEs,11 October 2018,https://link.springer.com/article/10.1007/s13209-018-0184-2,Fiscal centralization: a remedy for corruption?,November 2018,Joan Rosselló Villalonga,,,Female,Unknown,Unknown,Female,"Corruption in the public administration is perceived differently, depending on the social and cultural characteristics of each country. According to a special Eurobarometer report on corruption, half of all Europeans (49%) do not think corruption in their country is more widespread than in other EU Member States.Footnote 1 Opinion is very diverse across the EU with respondents in Greece (80%) and Romania (78%) the most likely, and those in Denmark, Sweden and the Netherlands (all 2%) the least likely to think that corruption is more widespread in their countries. This means that an activity that is seen as corrupt in some countries might be regarded as acceptable in others. Different countries and possibly different regions within the same country will therefore have varying degrees of tolerance for corruption. These different perceptions will also lead to differences in the initiatives that are taken to combat corruption and in the demand for political accountability. Nevertheless, in spite of these discrepancies, most western countries seem to be moving toward a common understanding of what corruption in the public administration constitutes. There is an extensive literature that analyzes the effects of corruption (see for instance, Mauro 1995, 1998; Tanzi and Davoodi 1997; Gupta et al. 1998; Alesina and Weder 1999, among others). Nonetheless, fewer studies analyze the causes of corruption (see Ades and Di Tella 1995, 1997; Leite and Weidmann 1999; Tanzi 1998 among others), particularly in terms of the relationship between decentralization and corruption in the public administration. Very often decentralization is pointed to as one of the main causes of corruption in in literature. It is argued that decentralization broadens the potential for corruption because: (i) the public administrations are multiplied, which means that there are more public resources available for rent-seeking; (ii) local officials live in close proximity with the citizens they serve and so local elites can make a greater impact on the decision process; (iii) there may be a shortage of highly skilled public workers to meet the increase in the demand for public officials and politicians, and efforts by local bureaucrats to combat corruption may be less rigorous as a consequence; and, (iv) it creates barriers to change the status quo. From an empirical perspective, Fan et al. (2009) find strong evidence that the danger of uncoordinated rent-seeking increases as government structures become more complex and Treisman (2000) also shows that states with more tiers of government tend to have higher perceived levels of corruption. These results are corroborated by Nelson (2013). On the other hand, some authors (see Shleifer and Vishny 1993; Huther and Shah 1998) find empirical evidence that increased decentralization reduces corruption (see Shah 2006 for a comprehensive literature review). In addition to these authors, Tanzi 1994 argues that personal links between bureaucrats and the people they serve reduce the probability of corruption, which suggests that centralized states will have more corruption. In fact, the World Bank suggests that decentralization is an appropriate way of fighting corruption in developing countries (see Gatti and Fisman 2000; Ivanya and Shah 2010). Freille et al. (2010) argue that there are different definitions of decentralization and that while fiscal decentralization is associated with lower levels of corruption, some forms of political decentralization worsen the positive effect of constitutional centralization on corruption. Lessmann and Markwardt (2010) find that decentralization counteracts corruption in countries where the press has a high degree of freedom, whereas countries with no effective monitoring by the press suffer under a decentralized system of government. Finally, Fiorino et al. (2015) find that there is a negative relationship between corruption and decentralization, with this relationship taking 3–5 years to develop. The inconclusiveness of current empirical work on the subject is partly due to the lack of comprehensive and comparable indicators for measuring decentralization and corruption across countries. There are also very few theoretical models that can help us to understand such data, the exceptions being, as far as we know, Arikan (2004) and Albornoz and Cabrales (2013). Arikan (2004) developed a model to analyze corruption based on a tax-competition model, finding that fiscal decentralization leads to a lower level of corruption. On the other hand, Albornoz and Cabrales (2013) argue that the relationship between decentralization and corruption depends on the degree of political competition and they find that decentralization is associated with lower (higher) levels of corruption when there is high (low) political competition. In this paper, we develop a theoretical model that analyzes the relationship between political and fiscal decentralization and corruption. In particular, we model central provision of public goods with multiple territorial units in a novel way. In the existing literature it is often assumed that the centralized provision of public goods is decided on by politicians who are not swayed by local taxpayers, and that their aim is to maximize the nation’s welfare. Central governments are also often assumed to recruit public workers with no personal or professional links with local agents, thus supposedly limiting the influence of any local elites. However, this is far from realistic, because in numerous fields of central government authority, this is not the system for the provision of public goods that can actually be found. There is a more realistic intermediate scenario, not traditionally taken into account in theoretical models, midway between centralization and decentralization.Footnote 2 In this alternative scenario, responsibilities are delegated by the central administration to hierarchically dependent units created on some kind of spatial basis, a system we refer to as administrative deconcentration.Footnote 3 This regime is what we find in most centralized countries, and it is the one that characterizes the organization of administrative activity of most federal governments. In this alternative scenario, although the central authorities decide the rules to be implemented nation-wide, they are implemented through its offices in each jurisdiction. This means that the public workers who have to implement these rules and provide these public services—such as judges, prosecutors, local attorneys, the central administration’s local representatives, federal policemen, National Park employees or coastline inspectors—do have strong links with local agents, and very often they are local agents themselves. It might be argued that in countries with centralized systems, these public workers are selected through competitive recruitment exams open to all the nation’s citizens and that where they are posted will depend on their qualifications, preferences and the central administration’s needs in each jurisdiction. However, in countries where citizens are reluctant to move or countries whose jurisdictions have some kind of peculiarities, such as a specific culture, language, civil law, geographical location, etc., public workers in these regions are very likely to have been born and raised in the region where they finally work. In this paper, we deal with different systems for the provision of public goods, based on three spatial forms of political organization, and we analyze their impact on corruption at an aggregate level. First, we take the scenario of a “traditional” centralized system of government; second, we analyze a centralized system of government that provides public services through public agencies created on a hierarchically organized territorial basis, that we refer to as deconcentration; and finally, we study the provision of public goods by self-governing regional governments. At this point, it is very important to stress that the distinction between these three regimes is based on a fiscal federalism approach. We mention that because from a formal point of view, based on the solutions to the theoretical model, the reader might think that deconcentration is a special case of decentralization. However, although the solutions to decentralization and concentration are rather similar, from a fiscal federalism approach, deconcentration is a special case of centralization We analyze the three different systems individually, assuming that they do not coexist. In addition, we incorporate the possibility that central and regional governments might be committed to combating corruption to different degrees, which is corroborated by the European Quality of Governance Index.Footnote 4
 Our results are as follows: First, the level of commitment to fight corruption is an economic decision and under certain conditions, public administrations will tolerate certain degrees of corruption. Second, the centralized provision of public goods might be more efficient than their decentralized provision, but under very restrictive conditions. These conditions are homogenous initial levels of corruption across regions, efficient recruiting procedures of staff by the central government and rent-seeking opportunities being lower in a centralized framework. Third, decentralization is more efficient than centralization when heterogeneous jurisdictions are considered. In this scenario, the decentralized provision of public goods would be more efficient because regional governments would dedicate optimal amounts of funding to fighting corruption, while their centralized provision might lead to the over or under-allocation of resources (which would represent a waste of public funding in the first case and a tolerance of corruption in the second). This result must be qualified when the central administration takes into account regional heterogeneity and the specificities of its regions in the provision of public goods. In the next section, we refer what we understand from corruption. In Sect. 3, a presentation is given of our model and, in Sect. 4, we present the main findings. Finally, our conclusions are outlined in Sect. 5.",4
10.0,1.0,SERIEs,23 January 2018,https://link.springer.com/article/10.1007/s13209-018-0173-5,The output effects of tax changes: narrative evidence from Spain,March 2019,Paula Gil,Francisco Martí,Roberto Ramos,Female,Male,Male,Mix,,
10.0,1.0,SERIEs,12 March 2019,https://link.springer.com/article/10.1007/s13209-019-0187-7,Calendar effects in daily aggregate employment creation and destruction in Spain,March 2019,J. Ignacio Conde-Ruiz,Manu García,Jesús Ruiz,Unknown,Male,,Mix,,
10.0,1.0,SERIEs,24 December 2018,https://link.springer.com/article/10.1007/s13209-018-0186-0,Score-driven currency exchange rate seasonality as applied to the Guatemalan Quetzal/US Dollar,March 2019,Astrid Ayala,Szabolcs Blazsek,,Female,Male,Unknown,Mix,,
10.0,2.0,SERIEs,03 April 2019,https://link.springer.com/article/10.1007/s13209-019-0190-z,Fewer babies and more robots: economic growth in a new era of demographic and technological changes,June 2019,Juan F. Jimeno,,,Male,Unknown,Unknown,Male,"In most developed countries, the weight of the working-age population in total population is bound to decrease significantly in the forthcoming decades because of the retirement of the baby boomers, decreasing fertility during the recent past decades, and further increases in longevity. At the same time, there is a new wave of technological changes, built upon the development of robotics and artificial intelligence (AI) that is generating some anxiety about the displacement of human labour with disruptive effects on employment and wages. Awareness of these trends has led to a revival of the secular stagnation hypothesis (Hansen 1939). Its main insight is characterising a macroeconomic regime under which low labour supply growth, population ageing, poor productivity growth, and high public debt leads to a savings glut, depressed investment, and, hence, a very low natural interest rate and a permanent deficit of aggregate demand that may not be corrected by macropolicies. As for technological changes, the conventional wisdom, focused on factor-augmenting technological progress, concludes that productivity growth is associated with changes in the composition of employment by worker skills but does not affect the long-run level of aggregate employment. This view is being challenged on the presumption that robotisation and AI, rather than being complement to human labour and, hence, increase labour productivity, may lead to a global displacement of workers, regardless of their skills. When considered together, demographic and technological changes give rise to some conceptual questions regarding the determinants of economic growth, namely, (i) Does population ageing impulse automation and, hence, productivity growth (and, if so, how)?, (ii) Do robotisation and AI have different economic implications from factor-augmenting technological progress?, (iii) To what extent a very low natural rate of interest associated to population ageing and either low productivity growth or disruptive technological changes constrain macrostabilisation policies?, and if so, (iv) What are the policy alternatives to combat a persistent deficit of aggregate demand? This paper surveys recent literature on macroeconomics and labour economics and provides empirical evidence that have some bearing on these questions. It highlights the main transmission mechanisms involved in the analysis of the macroeconomic implications of demographic and technological changes. Awareness of these transmission mechanisms is important for designing economic policies (both in the macrostabilisation front and with long-run objectives) that could address the big challenges of the new macroeconomic scenario Source: United Nations, Population Division Population ageing. First row: population 50 and above/population 20–49 years of age. Second row: share of population 20–69 in total population The structure of the paper is as follows. Section 2 lays out the characteristics of the demographic trends, reviews models that formalise the secular stagnation hypothesis and the determinants of the natural rate of interest, and revisits the empirical evidence on the impact of demographics on GDP, employment, and productivity. Section 3 is devoted to models of technological progress that beyond factor-augmenting technological progress consider the possibility of global displacement of workers (and not only skills) by robots and AI. Section 4 highlights the main general equilibrium effects and the aggregate constraints relevant to understand the consequences of demographic and technological changes jointly considered. Finally, Sect. 5 concludes with general remarks, some of them related to policy implications.",8
10.0,2.0,SERIEs,15 March 2019,https://link.springer.com/article/10.1007/s13209-019-0188-6,Efficiency evaluation of hotel chains: a Spanish case study,June 2019,Yaguo Deng,Helena Veiga,Michael P. Wiper,Unknown,Female,Male,Mix,,
10.0,2.0,SERIEs,12 March 2019,https://link.springer.com/article/10.1007/s13209-019-0189-5,Spatial mobility in elite academic institutions in economics: the case of Spain,June 2019,Raquel Carrasco,Javier Ruiz-Castillo,,Female,,Unknown,Mix,,
10.0,2.0,SERIEs,13 May 2019,https://link.springer.com/article/10.1007/s13209-019-0192-x,Correction to: Spatial mobility in elite academic institutions in economics: the case of Spain,June 2019,Raquel Carrasco,Javier Ruiz-Castillo,,Female,,Unknown,Mix,,
10.0,2.0,SERIEs,06 April 2019,https://link.springer.com/article/10.1007/s13209-019-0191-y,The market for scoops: a dynamic approach,June 2019,Ascensión Andina-Díaz,José A. García-Martínez,Antonio Parravano,Unknown,Male,Male,Male,"Casual evidence suggests important differences in the quality of journalism from one country to another. The traditional press is one example to look at. Among other possible differences, a striking one is the coexistence of countries where high-quality newspapers have large readerships with others where high-circulation newspapers have surprisingly low editorial standards. The cases of the USA and Spain, where quantity aspects (newspaper circulation) and quality aspects (accuracy of information and trustworthiness) go hand in hand, are thus in sharp contrast to the case of the UK, where the newspapers that lead the market in terms of circulation, The Sun and The Daily Mail, are far down the ranking in terms of trust and accuracy of news. That ranking is led by The Times and The Guardian.Footnote 1
 Differences in quantity and quality go beyond country-specific issues and can be found when different media sectors from the same region are compared. An example is the proliferation of fake news, a phenomenon that affects many spheres and sectors of the media industry but is especially severe and important on social media and online information platforms. How can these differences be explained? What determines which media outlets are more successful in a society? This paper proposes a model of competition for audience that serves to draw predictions as to the fundamental aspects that determine what type of media outlets lead a market. In our opinion, this question requires a dynamic approach, because the accuracy of a story printed today may well affect an outlet’s future audience. Our objective is to determine what ingredients can play a role in explaining the differences observed in the real world. We acknowledge that the problem is complex and that many variables may be involved. For example, issues such as press penetration in different social classes, ideological considerations, and prices, which are not considered in this paper, may have major effects. In this sense, the present paper should be seen as an attempt to identify some initial general properties of the problem. Our contribution is to pin down the importance of two variables in explaining the dynamics: On the one hand, the consumers’ discomfort with fake news, i.e., how harsh consumers are with the publication of false stories, and on the other hand the nature of competition between media outlets, i.e., whether they are homogeneous in their editorial standards for quality and thus compete for the same stories, or differ and offer different products. In this paper, we use two approaches to characterize the dynamics of the market. On the one hand, we use a mean-field approach to find analytical solutions for the long-term average state of a duopoly media market. On the other hand, we numerically follow the stochastic dynamics of a market with several media outlets.Footnote 2 These approaches are not standard in the literature of media economics. In this respect, our simple model could serve as a guideline for future analyses of other, more complex economic systems. The model is as follows. We consider a media industry with N non-strategic media outlets and a finite but large enough time horizon. At each step in time, referred to as a day, one scoop is released. We assume that each scoop can be published by only one media outlet. Media outlets compete for scoops according to a rule that determines the probability of an outlet receiving a scoop at a given step in time. The rule establishes that this probability is an increasing function of the outlet’s audience on that particular day.Footnote 3 Scoops contain information on a relevant variable and the information contained can be ex-ante more or less accurate. We refer to the ex-ante accuracy of the information as the quality of the scoop. We assume that the quality is i.i.d. across periods and that each scoop is either true or false with a probability proportional to its quality. Media outlets are characterized by their own editorial standards for quality, which establish the minimum quality that each outlet requires from a scoop in order to publish it. We consider that the editorial standards of outlets are exogenously given and invariant throughout the dynamics. We further consider that an outlet’s editorial standard determines the behavior of that outlet in that it prescribes that the outlet will publish the scoop when it is of sufficiently high quality and will reject it otherwise.Footnote 4 A scoop that is published is referred to as a story. After publication, consumers learn whether the story is true or false. If no story is published, consumers learn nothing. The behavior of an outlet at a time step, namely whether it publishes a story or not and whether the story is true or false, affects that outlet’s future audience and hence its probability of receiving the next day’s scoop. We analyze the dynamics of the audiences of the outlets and the frequency of scoop publication (referred to as their share of the news). Our objective is to understand the effect on the dynamics of two issues: The valuation of news by consumers and the market dispersion. To that end, we take two complementary approaches: first we propose a mean-field analysis that substitutes the stochastic dynamics by a deterministic one.Footnote 5 In particular, at each time step we substitute the random assignation of a scoop to an outlet by its expected value. This simplification enables us to obtain analytical results for the case of two media outlets. Second, we perform numerical simulations on the stochastic dynamics and obtain predictions for the general case with more than two outlets.Footnote 6
 Our analytical results for the mean-field approach with two outlets show that there are two stationary states: one in which the two outlets coexist, i.e., they both receive positive audiences and shares of the news, and one in which only the outlet with the lower editorial standard is active in the market, i.e., the high-standard outlet ends up with zero audience and zero share of the news. We also study the stability of these two solutions and find that the stationary state in which the outlets coexist is the only stable one. The results that follow therefore focus on this last case. We then study what market features determine which outlet receives a higher audience and/or a higher share of the news. We show that whether it is the low-standard outlet or the high-standard outlet depends on a combination of the two issues described above. In particular, we obtain that when the punishment for publishing false stories is not very high the low-standard outlet always has more audience. In this case, that outlet also has the higher share of the news. In this sense, our results predict that societies that are especially generous with outlets that break news and do not penalize them when their stories are shown to be false will have media industries dominated by outlets with low editorial standards. However, when the cost of publishing a false story is sufficiently high and the outlets are similar enough in their editorial standards, the high-standard outlet may lead the market (in terms of both audience and share of the news). It requires the media outlets to be sufficiently homogeneous. Otherwise, the high-standard outlet may be the leader in audience but the low-standard one will always have a higher share of the news. Regarding the stochastic dynamics, the results of the numerical simulations show that most of the insights gained in the mean-field approach with two outlets are robust to the consideration of more outlets. We also use numerical simulations to obtain predictions that cannot be inferred from the mean-field approach, such as predictions as to how many outlets will survive in the industry in the long run and their characteristics. We find that the harsher consumers are with the publication of false stories and/or the more similar the editorial standards of the media outlets are, the more outlets will survive in the long run. By contrast, when consumers like consuming news and do not care whether printed stories are true or false and/or when outlets are very different in their vetting processes for stories, the results suggest that only a small number of outlets endure in the long run. This work is related to the literature on the economics of the media, an extensive area of literature that has grown rapidly in the last twenty years. The first papers in this literature were mostly interested in analyzing the effects of the media on economic policies and outcomes. See Besley and Burgess (2001), Djankov et al. (2003), Strömberg (2004b). A second generation of papers studied whether news provision was biased (Groseclose and Milyo 2005; Egorov et al. 2009 and Larcinese et al. 2011) and what the determinants of the biases were (Strömberg 2004a; Mullainathan and Shleifer 2005; Gentzkow and Shapiro 2006; Baron 2006; Petrova 2008; Ellman and Germano 2009). At the same time, a different group of researchers focused their attention on the study of the media as a two-sided market. See Rochet and Tirole (2003), Anderson (2006), Doyle (2013). Currently, research on the economics of the media is extremely varied, but there is a tendency toward analyzing online news markets (Yang and Chyi 2011; Halberstam and Knight 2016; Germano and Sobbrio 2017) and their effects on political outcomes (Allcott and Gentzkow 2017; Boxell et al. 2017; Campante et al. 2018), collective action (Acemoglu et al. 2017; Little 2016; Enikolopov et al. 2016, 2017) and more generally, social outcomes (Gentzkow and Shapiro 2011; Quattrociocchhi et al. 2014; Bakshy et al. 2015). Our contribution to this literature is to present a new approach to the study of competition and audience in the media industry. Previous research shows that competition can reduce the quality of news (Zaller 1999; Cagé 2014), can induce outlets to ideologically differentiate from competitors (Gentzkow et al. 2014) and can lead them to bias the news in an attempt to product-differentiate (Anand et al. 2007). It has also been shown that competition can reduce the possibility of media capture by the government (Besley and Prat 2006), increase media self-censorship on issues sensitive to advertisers (Germano and Meier 2013), and increase giving and volunteering (Adena 2016). In contrast to previous research, our interest lies not in the effects of competition but in its determinants, from a long-run perspective. To the best of our knowledge, this is new in the literature. The rest of the paper is organized as follows. In Sect. 2, we propose a stochastic dynamic model of competition and audience in the media industry consisting of N recursive equations that account for changes in audience in a market with N outlets. In Sect. 3, we substitute the stochastic dynamics by a deterministic dynamics and analyze the resulting mean-field model for the case of two media outlets. This simplification enables us to obtain analytical solutions for the audiences of outlets and study the stability of the stationary states. In Sect. 4, we perform numerical simulations to study the stochastic dynamics with two, five, and ten media outlets. Finally, Sect. 5 concludes. The paper also contains Appendices, which are organized as follows. In Appendix A, we propose and analyze a one-shot model with rational agents that helps us show the rationale behind the behavioral assumptions that we make in Sect. 2. In Appendix B, we present results which are complementary to the stochastic dynamics analyzed in Sect. 4. Finally, Appendix C contains the proofs.",1
10.0,3.0,SERIEs,09 November 2019,https://link.springer.com/article/10.1007/s13209-019-00211-2,Introduction to the special issue on taxes and transfers,November 2019,Juan Carlos Conesa,Javier Pérez,Virginia Sánchez-Marcos,Male,,Female,Mix,,
10.0,3.0,SERIEs,04 July 2019,https://link.springer.com/article/10.1007/s13209-019-0195-7,The effects of the introduction of tax incentives on retirement saving,November 2019,Juan Ayuso,Juan F. Jimeno,Ernesto Villanueva,Male,Male,Male,Male,"Tax incentives of retirement saving are present in many countries as a mean to achieve that individuals accumulate sufficient wealth for retirement, either by promoting total savings or by changing the composition of wealth portfolios giving more weight to “long-run” savings (pension plans), less liquid and, thus, less likely to be used before retirement. The rationale for those incentives is that financial planning for retirement is a complex task, with little room to make adjustments once the individual has decided to retire.Footnote 1 Contributions to pension plans increase with tax incentives, particularly among individuals with age close to retirement and facing high-income tax rates.Footnote 2 However, the extent to which tax incentives rise retirement savings is a controversial issue. In the USA, contributions to IRAs and 401(k) plans are considered as net additions to saving for some authors (see, for instance, Poterba et al. 1995, 1996; Gelber 2011), while others conclude that tax incentives of retirement savings have a strong effect on the allocation of saving and wealth, but little or not effect on the level (see, for instance, Gale and Scholz 1994; Engen et al. 1996, and Attanasio et al. 2004).Footnote 3 For other countries, studies mostly point at a limited impact of tax incentives on either contributions or wealth.Footnote 4 Three problems make it very difficult to identify the effects of tax incentives on savings: (i) the wide heterogeneity in the individual responses to tax incentives, as these responses depend on variables like age, the existence of liquidity constraints or the degree of patience, (ii) the lack of microeconomic data on consumption, saving, and wealth  to observe the wide range of financial and personal characteristics determining marginal tax rates, earnings volatility, pension wealth, discount and interest rates, together with individual-/household-level information on income, wealth and its composition, and (iii) the  differential impact that tax incentives may have at the moment when they are introduced with respect a situation in which they have been operative for a long period, as there may be some gradual adjustment to the desired level of savings and to the optimal wealth composition after the introduction of tax incentives.
 In this paper we provide empirical evidence on the impact of tax incentives on saving by examining the effects of the introduction of tax incentives of retirement in Spain in 1988. By using the introduction of the exemption as an arguably exogenous-to-the individual change in incentives to save, our analysis is less affected by problems (i) and (ii) than previous work. Of course, by focusing on the introduction of the exemption, our analysis is affected by problem (iii): we cannot estimate the impact of the exemption when the program has been operative for a long period. However, we describe how contributions to pension plans are distributed in 2014, thirty years after its introduction, and make an admittedly limited discussion of whether those changes go into the direction of generating more or less new saving. We analyze the impact of the introduction of these tax incentives in two steps. First, we use a panel of tax returns to identifying the population groups who most used these incentives. Second, we use a panel of household consumption to estimate the impact of tax incentives on consumption/saving of different population groups. Our paper contributes to the literature on tax incentives to save in two different ways. First, we are able to use data spanning the periods before and after the introduction of tax-favored retirement plans. Thus, we are able to observe consumption choices in a situation in which tax incentives are not present. In the absence of a controlled experiment, such as in Duflo et al. (2006), examining the evolution of savings around the introduction of the tax exemption mitigates some of the problems in the analysis of IRAs or 401(k)s that typically study the impact using post-introduction trends among different groups in the population. In addition, Chetty et al. (2014) show that a substantial share of Danish taxpayers are passive investors that do not react to tax incentives aimed at increasing retirement saving, and that lack of attention limits the effectiveness of programs that depend on voluntary contributions. The introduction of pension plans is a salient event when tax incentives vary abruptly, thus allowing us to examine if investors revise their consumption decisions when changes are sufficiently large. Second, we focus on the impact of the introduction of the pension plans program on household consumption, rather than on household wealth. While household wealth is a very important outcome, household consumption conveys important information. For example, in the presence of employer contributions, household consumption is more likely to reflect how the flow of active household saving is affected by tax incentives than household wealth (see Chernozhukov and Hansen 2004). Moreover, according to the permanent income model, household wealth is more sensitive to transitory income changes than household consumption (Blundell and Preston 1998). Thus, any analysis that focuses on group-specific changes in household wealth over time faces the problem of disentangling between the impact of tax incentives and the impact of different forms of between-group income or wealth changes. Finally, even studies with comprehensive information on wealth from administrative records like Chetty et al. (2014) have limited information on housing wealth or mortgages. Unfortunately, tax-favored retirement accounts generate incentives to postpone the repayment of mortgage debt and invest in pension plans—see (Amromin et al. 2007). Detailed expenditure data permits a direct assessment of whether or not incentives generate new saving. We extend the insights in Attanasio and De Leire (2002), who infer the impact of tax incentives on new saving by comparing the consumption changes of new contributors to those of old contributors. In a life cycle model, new contributors effectively experience an increase in the return to their saving (i.e., their consumption path is altered by contributing to a pension plan), while previous contributors do not. As they find that old and new contributors have similar consumption growth, they infer that tax incentives do not generate new saving. Anton et al. (2014) apply similar insights to longitudinal Spanish data and find that household expenditure does not fall when households contribute to the program. However, households that choose to start contributing to pension plans are very different from either those that continue to do so. Hence, variation associated with actual contributions may reflect the influence of variables correlated with the incentive to save.Footnote 5 To get around such omitted variables problem, we build a variable that summarizes the incentives to contribute and is arguably less affected by endogeneity biases. Our instrumental variable is the interaction between the income marginal tax rate and the age of the individual at the time of introduction of the exemption. Individuals with higher-income marginal tax rates experiment a higher increase in post-tax returns (Milligan 2002) and age proxies income risk and preference for liquid assets. We check that our variable is indeed a strong predictor of contributions: it was mainly filers in the top quartile of labor earnings who exempted contributions and, within that group, average contributions increased monotonically with age. Using a separate expenditure survey, we then examine whether the consumption growth of broad age groups in the top income quartile, relative to our control group of young households, experienced a drop around the introduction of the exemption. Our results suggest that there is indeed substantial heterogeneity in the contributions to pension plans and in the response of household saving to tax incentives. While the overall amount of new saving we estimate is limited (around 19 cents per euro contributed on average, not very precisely estimated), saving responses differ substantially across age groups—a finding consistent with previous literature using household wealth.Footnote 6 In particular, we document very small fall in consumption among the group of households between 56 and 65 years of age, the group that most actively contributed to the plan. Instead, we find a larger decrease in consumption expenditures of the group of households between 46 and 55 years of age. We use a simple permanent income model to interpret such pattern of responses. The model predicts that households in the verge of retirement find pension plans and other saving forms as strong substitutes, and tend to exhaust tax-exempted contribution limits by reshuffling their wealth portfolios. In that case, tax incentives do not generate a meaningful substitution effect. Conversely, groups further away from retirement, with plausibly less accumulated wealth and for whom there is a trade-off between the post-tax return of retirement saving and the illiquidity of the investment, contribution limits are not binding and tax incentives may generate new saving. Actually, data on tax returns confirm those predictions: within the group of individuals with the highest marginal taxes and who were close to retirement, 30% of tax filers exhausted the contribution limits, while the fraction is three times smaller among younger individuals. The structure of this paper is as follows. Section 2 provides a description of the main regulation of pension plans in Spain when tax incentives were introduced in 1988. Section 3 contains some theoretical discussion of the factors determining the impact of the introduction of tax incentives on retirement saving. Section 4 discusses the characteristics of the datasets we use and lays out our empirical strategy. Section 5 examines the incidence of contributions across age and income groups, while Sect. 6 presents the main empirical results. Section 7 quantifies the impact of contributions on savings. Section 8 compares the distribution of contributions to pension plans in 2014 to that in 1988–1991, while Sect. 9 contains some concluding remarks.",7
10.0,3.0,SERIEs,29 August 2019,https://link.springer.com/article/10.1007/s13209-019-00203-2,Assessing the impact of a minimum income scheme: the Basque Country case,November 2019,Sara De La Rica,Lucía Gorjón,,Female,Female,Unknown,Female,"Most European Union Member States currently provide some form of minimum income scheme so as to ensure a minimum standard of living for households when they lack other sources of financial support. The emergence of these schemes dates back to 1992, when a European Council recommendation assessed the need to develop last resort schemes which recognised the basic right of every individual to ensure a decent minimum standard of living. These programmes were part of comprehensive, consistent plans to combat social exclusion. Since then, implementation of minimum income scheme (MIS) across European Countries has varied in coverage and effectiveness. The most widely used are the so-called simple and comprehensive schemes, which basically cover every person/household in need of support, without confining their effects to particular categories of people.Footnote 1 In 2008, the European Council endorsed the objective of combining adequate income support with labour market activation measures so as to facilitate entry of recipients into employment. Following the recommendations, minimum income schemes are therefore a combination of passive and active policies. Although the implementation of these schemes is progressing in most European countries, albeit heterogeneously, there is no sufficient assessment of their impact on aspects such as poverty and inequality reduction, labour market participation of recipients and/or the impact of activation measures on their recipients in terms of entry. Gorjón (2017) addresses the extent to which the Basque MIS reduces poverty, which is the first and most important aim of minimum income schemes. The author reproduces household income in the absence of MIS in order to compare this counterfactual situation with the real one and hence measure the impact of MIS on reducing several measures of poverty. The author finds that the Basque MIS is very effective at the time of reducing the intensity and the severity of poverty, although the extreme poverty is far away to be eradicated. Our work should be understood as a complementary study to Gorjón (2017), as we pose additional research questions to the Basque MIS system related to the labour market participation of MIS recipients. 
Around the world, some pilot projects and ex-ante or ex-post assessments of similar policies can be found. An example of such studies includes Gouveia and Rodrigues (2002), who assess the impact of particular MIS on poverty reduction in Portugal. According to their results, the Portuguese policy measure has a modest effect on reducing poverty incidence, but a substantial effect on alleviating the intensity and severity of poverty. An ex-ante assessment of a proposal in Québec is provided by Clavet et al. (2013). Their result shows that the proposed scheme would have strong negative impacts on labour market participation rates, mostly among low-income workers. Jones and Marinescu (2018) evaluate the impact of a basic income in Alaska. The authors find no impact on the labour supply. Additionally, Salehi-Isfahani and Mostafavi-Dehzooei (2018) evaluate the impact of a universal basic income in Iran on labour supply finding no evidence on its reduction. A different evaluation by Ayala and Rodríguez (2010) tries to find the factors underlying that MIS recipients leave the programme (off-welfare spell). Using administrative data set for the minimum income programme (Ingreso Madrileño de Integración) of a regional Spanish Government (Madrid), they find that the first off-welfare experience is essential for ex-MIS recipients to lengthen the time spent outside the programme. A more general review of the related literature passes through the debate of the impact of passive policies on exit to employment. A usual criticism is that cash transfers might reduce unemployed individuals’ incentives to work (Ayala and Paniagua 2016). An increase in the reservation wage of the unemployed workers can delay their exit into employment. The literature on passive policies has largely shown that unemployment insurance has negative effects in the transition from unemployment to employment. Rebollo-Sanz and García-Pérez (2015) find that non-insured unemployed workers experience a greater rate of transition to employment than insured workers. Furthermore, Card and Levine (2000) and Meyer (1990) prove that exhaustion of unemployment accelerates transitions towards employment. In the same line, Lalive and Zweimüller (2004), Van Ours and Vodopivec (2006) and Røed and Zhang (2003) conclude that benefits strongly affect the duration of unemployment. Given that the MIS is indefinite by nature, its disincentive impact might be particularly strong. In order to prevent chronification in unemployment for MIS beneficiaries, compulsory activation measures are directed to them. Some activation measures have been proved to be more effective than others. Card et al. (2010) synthesise the main results in the active labour market policies (ALMP) literature. One of the main results they find is that the impact of the programmes varies over time: training programmes, for example, have more positive impacts after 2 years than in the first one, while guidance services are especially helpful in the short run. However, subsidised public sector jobs and for youth programmes are proved to be less successful. Surprisingly, we are not aware of any study that assesses both potential impacts of the MIS, as a combination of passive and active policies. Our paper seeks to fill this gap. On the one hand, the MIS might delay recipients on their entry into work, the extended undesirable indirect effect of cash transfers. On the other hand, the activation programmes directed to the beneficiaries may have an accelerating effect into employment. According to Eichhorst and Konle-Seidl (2016), empirical evidence has influenced the design of labour market reforms themselves, not least in the area of active labour market policies and activation. This is precisely the ambitious goal of this paper. Specifically, our study assesses the impact of a minimum income scheme that operates in a northern region of Spain—the Basque Country, called Renta de Garantía de Ingresos. This region pioneered the introduction of MIS in Spain in 1989. The Basque Country is currently the only Spanish region with a Simple and Comprehensive MIS Scheme.Footnote 2 Given that we use monthly administrative records of the Basque Public Employment Service, we have access to the whole universe of unemployed registered individuals at any month from February 2015 to January 2016. With this rich individual longitudinal information, we first assess whether the Basque MIS delays entry into labour market for its recipients. Then, we test the efficacy of policies aimed at enabling its recipients to enter employment. We do this by using the inverse probability weighting methodology, which enables MIS recipients to be compared with a similarly observed fictitious group created by weighting non-recipients. By doing so, the treatment is dissociated from observed individual characteristics and hence pseudo-randomised. Our results indicate that, on average, the Basque MIS does not, per se, delay entry into work for its recipients. Interestingly, however, the impact differs from one demographic group to another. Furthermore, active labour market policies designed for MIS recipients, in particular training, have a strong positive impact on entry into employment.Footnote 3 The results obtained from this analysis cannot be directly extrapolated to other regions given that regional labour markets in Spain exhibit very different patterns. Hence, the impact of MIS in different regions must be studied separately as it may lead to very different outcomes. The rest of the paper is organised as follows: Sect. 2 reviews institutional aspects of the MIS implemented in the Basque Country. Section 3 gives a description of the data and the main descriptives of MIS recipients. Section 4 presents the methodological and analytical assessment methods and the empirical findings. Finally, Sect. 5 summarises and concludes.",3
10.0,3.0,SERIEs,11 October 2019,https://link.springer.com/article/10.1007/s13209-019-00208-x,The elasticity of taxable income in Spain: 1999–2014,November 2019,Miguel Almunia,David Lopez-Rodriguez,,Male,Male,Unknown,Male,"The impact of personal income taxes on the economic decisions of individuals is a key empirical question with important implications for the optimal design of tax policy. Not surprisingly, the modern public finance literature has devoted significant efforts to study behavioral responses to changes in taxes on reported taxable income, as reviewed by (Saez et al. 2012). Most of this work focuses on the elasticity of taxable income (ETI), which captures a broad set of real and reporting behavioral responses to taxation. Indeed, reported taxable income reflects not only individuals’ decisions on hours worked, but also work effort and career choices as well as the results of investment and entrepreneurship activities. Besides these real responses, the ETI also captures tax evasion and avoidance decisions of individuals to reduce their tax bill. In this paper, we estimate the elasticity of taxable income in Spain, an interesting country to study because during the last two decades it has implemented several major personal income tax reforms, plus a variety of smaller legislative changes, including the ability of regional governments to modify the tax schedule. Due to data availability, we focus on the reforms implemented in the period 1999–2014, which provide useful variation to identify the ETI. We consider all legislative changes in the personal income tax, although identification mainly comes from three major reforms. In particular, the 2003 reform lowered the top marginal tax rate (45–43%) and also reduced the marginal rate for the bottom bracket (18–15%). The 2007 reform was an overhaul of the income tax system, turning the standard personal deduction into a tax credit, which increased the progressivity of the tax. It also modified the definition of tax bases, shifting a substantial part of capital income from the “general” to the “savings” tax base and thereby lowering the marginal rate on capital income. Finally, the 2012 reform, introduced in the middle of a severe recession, increased tax rates across the board, pushing the top marginal rate up to 52% (further increased to 56% in some regions, like Andalusia and Catalonia). The empirical literature on the ETI has stressed two challenges that could prevent obtaining consistent estimates of this parameter: mean reversion and heterogeneous income trends. Mean reversion is due to transitory shocks to income. When taxpayers receive a transitory income shock in a given year, they tend to revert to their permanent income in subsequent years. This makes it difficult to disentangle changes in reported income due to mean reversion from changes induced by tax reforms. Moreover, the potential bias due to mean reversion has opposite signs for tax cuts and tax increases. The impact of mean reversion is particularly acute in the top and bottom tails of the income distribution, affecting taxpayers with low and high permanent income, who are precisely the groups often targeted by tax reforms. Heterogeneous income trends across groups of taxpayers differently affected by tax reforms are problematic for similar reasons. Much of the discussion in the literature has centered around the increase in income inequality in the USA in the 1980s (documented by Piketty and Saez 2003), because many papers in this literature evaluate the reduction in the top marginal rate in the 1986 Tax Reform Act. In that setting, it is hard to disentangle whether increases in taxable income by top earners are caused by the tax reform or by the underlying trend toward higher inequality. In the case of Spain, the existence of secular income trends seems less problematic for the estimation of the ETI, as there has been no comparable increase in income inequality over the period under study. However, the asymmetric impact of the Great Recession across groups of taxpayers could also create challenges for identification. In the empirical analysis, we use an administrative panel dataset of income tax returns compiled by the Instituto de Estudios Fiscales (IEF) with information provided by the Spanish Tax Agency (AEAT). The panel is a random stratified sample with 8.1 million observations over the period 1999–2014 (about 3% of each year’s income tax returns). It contains detailed information on the main components of each income source (labor, capital and self-employment), income-related deductions, the legal tax bases, details on a broad range of tax credits and the overall tax liability for each taxpayer. We use this information to construct a stable definition of taxable income over time. This homogenization is necessary to provide consistent estimates of the ETI for a long period such as 1999–2014 during which significant tax base changes were introduced (e.g., 2007 reform). Since the marginal tax rate (MTR) is not contained in the data, we construct our own tax calculator (in the spirit of the NBER’s TAXSIM for the USA) to calculate the MTR for each taxpayer, and also to build the predicted tax rate instruments described below. We calculate the MTR as a weighted average of the MTR applicable to each income source (labor, financial capital, real estate capital, business income and capital gains). We use various panel-based two-stage least squares (2SLS) diff-in-diff estimators to obtain consistent estimates of the ETI. The baseline instrument is the predicted change in the net-of-tax rate, defined assuming that income stays constant (in real terms) between a pair of years. This instrument has been used extensively in the literature dating back to Gruber and Saez (2002). Identification comes from heterogeneous changes in the personal income tax schedule across groups of taxpayers due to tax reforms. In all regressions, we control for socio-demographic characteristics of taxpayers (e.g., age, gender, geographic location and indicators for joint filing, children and old-age dependents) that proxy for their permanent income. The presence of mean reversion and heterogeneous income trends in the data motivates the inclusion of nonlinear functions of base-year income, aiming to resolve the potential biases introduced in the estimates. As we discuss in more detail below, we also use state-of-the-art estimation methods proposed by Kleven and Schultz (2014) and Weber (2014) to deal with these identification challenges. We begin the empirical analysis by examining the existence of mean reversion and heterogeneous income trends in Spain over the 1999–2014 period. Mean reversion is present at the bottom and top tails of the income distribution, which makes it essential to account for mean reversion in the regression analysis. Regarding changes in the income distribution, we show that top income shares have been relatively stable in Spain. This suggests that the existence of secular income trends across groups of taxpayers differentially affected by tax reforms is not a first-order concern in this context. Our first set of estimates corresponds to an unbalanced panel of taxpayers for the entire period 1999–2014. We obtain estimates of the ETI around 0.35 using the Gruber and Saez (2002)’s estimation method, 0.54 using Kleven and Schultz (2014)’s method and 0.64 using Weber (2014)’s method. This response is robust to the inclusion of nonlinear controls of base-year income in alternative specifications to deal with mean reversion. We undertake several sensitivity tests showing that these estimates are robust to the consideration of alternative base-year income thresholds commonly used in the literature, the examination of permanent taxpayers (balanced panel), the inclusion of pensioners or the exclusion of taxpayers that change their regional fiscal residence during the period of analysis. These baseline ETI estimates are comparable to the “consensus” estimates obtained for the USA (Gruber and Saez 2002; Weber 2014) and other European countries such as Germany (Doerrenberg et al. 2017), but higher than those obtained for Denmark (Kleven and Schultz 2014). In addition to the average estimates of the ETI, we analyze heterogeneous responses across groups of taxpayers and sources of income. The results on the anatomy of the response are in line with the predictions from the literature. As expected, stronger responses are documented for groups of taxpayers with higher ability to respond. In particular, self-employed taxpayers have a higher ETI than wage employees, while real estate capital income and business income respond more strongly than labor income. The elasticity of broad income (EBI) is between 0.10 and 0.24, substantially lower than the ETI, indicating that deductions play a significant role in taxpayers’ responses. Indeed, we find large responses on the tax deductions margin, especially private pension contributions. Nevertheless, the EBI is relevant in quantitative terms, particularly for self-employed taxpayers, suggesting that there may be real labor supply responses to taxation or evasion behavior that go through reported broad income. The paper proceeds as follows. Section 2 briefly reviews the large literature on the ETI, including earlier estimates for Spain. Section 3 describes the Spanish personal income tax and the main tax reforms in the period 1999–2014 and also describes the administrative panel dataset used in the empirical analysis. Section 4 provides a simple conceptual framework and discusses our estimation strategy to obtain consistent estimates of the ETI. Section 5 reports the main empirical results and the sensitivity analysis. Section 6 concludes.",4
10.0,3.0,SERIEs,27 August 2019,https://link.springer.com/article/10.1007/s13209-019-00204-1,Tax reforms and Google searches: the case of Spanish VAT reforms during the great recession,November 2019,Joaquín Artés,Ana Melissa Botello Mainieri,A. Jesús Sánchez-Fuentes,Male,Female,Unknown,Mix,,
10.0,3.0,SERIEs,16 August 2019,https://link.springer.com/article/10.1007/s13209-019-00202-3,Tax structure for consumption and income inequality: an empirical assessment,November 2019,José Alves,António Afonso,,Male,Male,Unknown,Male,"Is there a correlation between tax structures regarding consumption and inequality levels? This question arises from the extreme importance to understand and to contribute with novelties for the effects of taxes on each variable. On the one hand, and as trivially recognized, consumption is one of the most important concepts for economic science. Consumption happens mostly through economic markets. However, consumption can also lead to several economic disruptions, namely the non-inclusion of several externalities—either positive or negative. On the other hand, income inequalities are another subject that have been a recurrent subject over the past few years. In fact, inequalities have been introduced as an explanatory variable for several economic issues, as it is the case of economic growth. In fact, several economists point out inequalities as one of the most challenging dimensions in explaining economic performance among countries. Therefore, and aiming to study both variables, we opt to investigate them dynamics related to fiscal policy. In detail, we investigate whether tax systems arrangements can lead us to conclude for better tax designs to efficiently promote consumption and to reduce income distribution inequalities. Furthermore, several sources of taxation, such as on individual income and property, can lead to a reduction in income inequalities, as mentioned by Piketty (2014). Other some studies, as Mo (2000), Cingano (2014) and Ostry et al. (2014), highlight the positive linkage between income inequality reduction and economic performance. Additionally, and besides taxes being the privileged source to satisfy the financial needs for government to conduct its policies, it also makes use of taxation in order to correct from the economic disruptions regarding consumption and inequalities. Affecting the price system, taxes are a really useful instrument for governments to stabilize economic performance. Summing up, our intention is to empirically assess the effects of tax structure, from the ratio of revenues over the GDP of each tax source over consumption, measured by the households’ consumption (in GDP terms), and inequalities dynamics, making use of Gini index. Moreover, we also try to assess nonlinear relationships between taxes and both variables. This nonlinear assessment can lead us to find possible tax thresholds regarding each variable under research. In fact, while we hope to find thresholds that can minimize or maximize the dynamics of each variable, we also expect with this empirical exercise to provide empirical values for tax structures to fiscal policy in order to promote household consumption and, on the other hand, to decrease income inequalities. This provision of efficient macroeconomic tax systems can provide new insights to fiscal authorities to arrange their tax systems in order to prioritize them in accordance with the desired fiscal policy goal. Our article resorts to panel data techniques, covering all OECD countries, for the period between 1980 and 2015. The article’s analysis is conducted for both short and long run, by assessing tax items effects on both consumption and inequalities in a yearly and a 5-year average, respectively. The results of our study provide some thresholds regarding a subset of tax sources under analysis. In detail, and in what respects consumption dynamics, while we found a maximizing threshold of 5.67% for taxes on firms income, we find a minimizing threshold for payroll taxes of 1.44%, both thresholds for the short-run analysis, while we find a 11.80% of social security contributions over GDP that maximizes consumption, only in long run. Regarding inequalities, several thresholds are found. With the exception of individual income taxes and taxes on goods and services in what are found thresholds that maximize inequalities, the other tax source seems to present minimizing thresholds, which can be useful to reduce inequalities. The paper is structured as follows: Sect. 2 reviews the literature regarding taxation effects on both consumption and income inequality; Sect. 3 presents the methodology employed, as well as the data and its sources; Sect. 4 reports and discusses the results; and lastly, Sect. 5 provides our conclusions, some tax policy recommendations as also to outline some possible lines of future research.",3
10.0,3.0,SERIEs,06 June 2019,https://link.springer.com/article/10.1007/s13209-019-0193-9,Investment expensing and progressivity in flat-tax reforms,November 2019,Javier Díaz-Giménez,Josep Pijoan-Mas,,,Male,Unknown,Mix,,
10.0,3.0,SERIEs,30 October 2019,https://link.springer.com/article/10.1007/s13209-019-00209-w,Tax efficiency in a model of endogenous markups,November 2019,Esra Durceylan,,,Female,Unknown,Unknown,Female,"Governments tax consumption mainly for two reasons. First, they want to raise revenue to finance spending on public goods. Second, they use taxation to limit negative externalities that arise from consumption or production of certain goods. Such taxes are known as corrective taxes. Taxing tobacco products, alcoholic beverages, and fuel are some well-known examples. Corrective taxes are implemented as unit taxes or ad valorem taxes. The UK government, for example, taxes gambling revenue via ad valorem taxes, whereas it taxes alcohol and tobacco with unit taxes.Footnote 1 Similarly, in March 1995, the UK government published a paper on landfill tax and proposed ad valorem taxation rather than a unit tax charged by weight arguing the balance between environmental concern and practical feasibility. However, the government introduced the landfill tax as a unit tax per weight in August 1995 (Seely 2009). Which one is more efficient? The choice of these taxes can in many ways affect their impact on the industry. Both types of taxes distort the pricing behavior of different firms differently. The type of taxes chosen by the policy maker not only affects consumer welfare, it also affects the aggregate productivity in an industry by changing the composition of surviving firms. In this paper, I study the efficiency ranking of output equivalent unit and ad valorem taxes in a model of monopolistically competitive firms that produce differentiated products. A key feature of the model is that firms adjust their markups in response to changes in the economic environment. De Loecker et al. (2019) calculate markups of all publicly traded firms covering all sectors of the US economy over the period 1955–2016. They find that not only markups are not constant within an industry, but also the distribution of markups has become more skewed over time. They conclude that policy makers have to be cautious not to use the average markup as that of a representative firm to draw conclusions about the economy as a whole.Footnote 2 I compare the tax efficiencies in two possible settings. First, with homogenous firms, I show that ad valorem tax is more efficient than an output equivalent unit tax. It generates more tax revenue and consumer surplus. Firms are larger (hence less variety) but they price lower. Hence, a firm’s markup is relatively lower under ad valorem tax regime. Allowing firms to change their markups plays a key role in this finding and overturns the superiority of unit tax under almost identical model setting with constant markup pricing by Dröge and Schröder (2009). Next, I introduce firm heterogeneity to the model. Consumer surplus is higher under ad valorem tax regime. However, with heterogenous firms, unit tax becomes a better instrument to collect higher tax revenues. Since ad valorem tax causes the marginal surviving firm to be more productive, the market is relatively more competitive under this regime. As a result, highly productive firms survive with relatively larger output levels and lower prices. This comes at an expense of lower tax revenues. Firms with relatively higher (lower) levels of productivity choose higher (lower) levels of markups under unit taxes than under ad valorem taxes. Moreover, as the tax rates increase, these gaps expand. Hence, allowing for endogenous markups augments the difference in tax revenue and number of varieties under the two tax regimes in favor of unit tax when the tax requirements are high. As the existing studies state, the choice of taxes poses a trade-off between revenue extraction and variety generation. On one hand, ad valorem tax is a better revenue extractor. On the other hand, a unit tax generates a greater variety due to the larger tax overshift and thus enables firms to survive at lower output levels. In addition to this trade-off, if one adds firm heterogeneity into these models, these taxes have nontrivial effects on the selection of firms and thus on aggregate productivity. Since unit taxation allows firms to survive at lower levels of output, the marginal surviving firm under unit tax regime is less efficient than the marginal surviving firm under ad valorem tax regime. Moreover, resource allocation is more efficient under ad valorem tax regime since relatively more productive firms have higher market shares. In addition to these forces, this paper contributes to this literature by arguing that allowing for firms to adjust their markups plays a key role in the efficiency ranking of these taxes. First, firms are able to pass the tax burden onto consumers better under unit taxes. Second, with endogenous markups, the tax regimes cause different firms to pass the burden to consumers differently. Majority of the existing studies that compare efficiencies of these taxes use the equal yield criterion. Early contributions state that in a perfectly competitive market, the choice of tax policy does not matter; since firms are price takers, only the cost-price increase generated by the tax is relevant. Consequently, for every unit tax rate, there exists an equivalent ad valorem tax level. On the other extreme, when there is a monopoly, this equivalence result breaks down: an ad valorem tax generates more welfare than a unit tax [see Suits and Musgrave (1953) and Skeath and Trendel (1994)]. The result is driven by the fact that a profit-maximizing monopolist increases its output when an ad valorem tax is imposed. The superiority of ad valorem tax is shown to hold for a wide range of market structures.Footnote 3 In a monopolistic competition setting, Schröder (2004) with Dixit–Stiglitz preferences that result in constant markup pricing shows that an ad valorem tax remains welfare superior. This result is robust to add cost asymmetries to this setting (Schröder and Sørensen 2010). This paper also contributes to this literature by numerically showing that efficiency ranking of revenue equivalent ad valorem and unit taxes depend on the level of tax rates if one accounts for markup variation. In line with the intuition above, endogeneity of markups leads to reversal of the efficiency ranking as tax rates increase. The literature on the relative efficiency of corrective taxes uses equal output criterion as most of the time, the size of the externality is correlated with the size of output or consumption. For perfectly competitive markets, the two regimes generate identical outcomes. In a Cournot setting, if the externality is small, ad valorem taxation is optimal. If the externality is large enough, then unit taxation becomes a better instrument (Pirttilä 2002). Dröge and Schröder (2009) show that unit tax regime is optimal if the market is composed of monopolistically competitive homogenous firms. This paper contributes to this literature by showing that, first, accounting for markup variation overturns the superiority of unit taxation shown by Dröge and Schröder (2009). Second, if one further incorporates firm heterogeneity along with endogenous markup, the parameter-independent results are challenged. At higher (lower) tax requirements, unit (ad valorem) tax regime is better at maximizing welfare. The remaining of this paper is organized into four additional sections. Section 2 details consumer preferences, demand for differentiated good and labor market. Section 3 explains supply side of the market populated with homogenous firms under these two tax regimes. Section 4 introduces heterogeneity of firms in cost of production. Finally, Sect. 5 concludes.",
10.0,3.0,SERIEs,16 July 2019,https://link.springer.com/article/10.1007/s13209-019-0199-3,The timing of optimal capital income tax reforms: the role of intangible capital investment,November 2019,Juan Carlos Conesa,Begoña Domínguez,,Male,Female,Unknown,Mix,,
10.0,3.0,SERIEs,11 July 2019,https://link.springer.com/article/10.1007/s13209-019-0197-5,The Spanish personal income tax: facts and parametric estimates,November 2019,Esteban García-Miralles,Nezih Guner,Roberto Ramos,Male,Male,Male,Male,"This paper makes two contributions. First, we use administrative data on tax returns to characterize the distributions of before- and after-tax income, tax liabilities and tax credits in Spain. We also calculate effective average and marginal tax rates that individuals and households face. We use the most recent available data, 2015 for individuals and 2013 for households, but also discuss how the income distribution and taxes have changed since 2002. Second, we provide estimates of effective tax functions. These functions map gross incomes of individuals or households into taxes that they pay, summarizing the complicated structure of taxes in easy-to-interpret and easy-to-use parametric forms. As such, they provide valuable inputs for quantitative studies of fiscal policy in models with heterogeneous agents.Footnote 1\(^{,}\)Footnote 2 Our approach follows Gouveia and Strauss (1994), Heathcote et al. (2017) and Guner et al. (2014), who estimate tax functions of the US personal income tax. Calonge and Conesa (2003) provide estimates of effective tax functions for Spain for the early 1990s. Our data come from an administrative dataset containing a stratified random sample of tax returns, which includes a large set of fiscal and socio-demographic information that taxpayers provide in their returns.Footnote 3 The dataset is representative of the population of Spanish taxpayers and income variables are not censored, which makes it ideal for our purposes. The dataset has both a cross section and a panel component. Repeated cross sections are available from 2002 to 2015, and they have a large sample size. The 2015 sample contains 2.7 million observations, about 14% of the population. It is not possible, however, to match household members, a husband and wife, who file individual tax returns, in this dataset. The panel dataset covers the period 1999–2013 and has a smaller sample size, but allows us to link individual tax filers from the same household, and compute taxes at the household level.Footnote 4 The key takeaways from our analysis of the data are as follows: First, the data exhibit a significant degree of inequality, in both incomes and tax liabilities. The bottom (top) quintile of the income distribution accounts for about 4.6% (47.1%) of gross income, and the share accounted for by the top 1% is about 9.5%. A similar picture emerges for households. The top (bottom) quintiles account for 4.4% (49.9%) of gross income, and the top 1% accounts for 9.7% of gross income. The Gini coefficients for individual and household gross incomes are 0.42 and 0.45, respectively. Second, given the progressive tax system in Spain, tax liabilities are even more unequally distributed. The top quintile, which accounts for 47.1% of gross income, pays about 73.2% of taxes, while the share of the top 1% in total tax liabilities is 21%. As a result, the after-tax income distribution is more equal than the before-tax income distribution. The shares of the top quintile and top 1% of taxpayers in after-tax individual income decline to 42.8% and 7.6%, respectively. The Gini coefficients for after-tax income are 0.38 for individuals and 0.40 and households. Our analysis also shows that the Gini coefficients for both before- and after-tax incomes have been fairly stable since 2002. Other measures of income inequality, such as 90-to-10 and 50-to-10 income ratios, however, did increase since the 2008 crisis. Our estimates of the household income distribution are quite similar to the ones we obtain from the Bank of Spain’s Survey of Household Finances (Encuesta Financiera de las Familias or the EFF).Footnote 5 Third, labor income constitutes the most important source of total income for most households. Even at the top quintile, it represents about 87.1% of total income. The capital and self-employment income, on the other hand, account for a more significant share of total income for the top 1% of taxpayers. About 24.1% and 12.7% of their total income come from capital and self-employment income, respectively. Interestingly, capital and self-employment incomes also account for a large share of income at the lower end of the income distribution. Fourth, we find that higher-income quintiles enjoy larger tax deductions, which lower their taxable income, and larger tax credits, which lower their tax liabilities. The top quintile, for example, accounts for 25.2% of all deductions and 28% of all credits. The same numbers for the bottom quintile are 16.1% and 4.9%, respectively. This reflects both the fact that some benefits, such as deductions due to social security contributions or due to contributions to private pension plans, are enjoyed more by richer households, and the fact that poorer households are more likely to reach quickly zero tax liabilities due to deductions and credits. Finally, there is also a large dispersion in effective average tax rates that individuals face. About 37% of all taxpayers do not pay any taxes. Indeed, effective rates are close to zero in the two lowest quintiles. The top quintile faces an average effective tax rate of 19.0%, while the tax rate for the top 1% is 30.6%. The tax system in Spain taxes the so-called general income, which mainly consists of labor and self-employment income, and savings income, which mainly consists of capital income, at different rates. Taxes on general income are higher and more progressive than taxes on savings income.Footnote 6 Hence, to each of these income categories certain deductions are applied and then the corresponding tax liabilities are calculated. Tax liabilities corresponding to these two categories are then summed, and tax credits are applied to the total tax liabilities to figure out what the taxpayer owes to the state. Given this structure, for the estimation of the effective tax function, we follow two different approaches. First, we estimate one single function for the final tax liabilities as a function of gross income for each year between 2002 and 2015. We focus on two different specifications: one proposed by Benabou (2002) and Heathcote et al. (2017), which we call the HSV specification, and the GS specification, used by Gouveia and Strauss (1994). In our estimation we account for the fact that low incomes are subject to zero effective tax rates, and estimate an income threshold below which tax liabilities are zero. In the second approach, we estimate three different functions: a function that relates general income to general tax rates; a second function that links the savings income to the savings tax rates; and a third function that accounts for the amount of tax credits as a function of total gross income. We show that both approaches result in tax functions that accurately estimate both the level and the distribution of the tax liabilities observed in the data. As an illustration of the use of these tax functions, we apply them to the EFF survey data and calculate after-tax incomes for each household, a variable not available in the original survey. The rest of the paper is organized as follows. Section 2 describes the Spanish Personal Income Tax. Section 3 describes the dataset and lays out the definitions and sample restrictions. Section 4 presents the basic facts of the income and tax distributions. Section 5 presents the parametric estimates of the tax functions. Section 6 presents the basic facts of the after-tax income distributions for administrative and survey data. Section 7 concludes.",21
10.0,3.0,SERIEs,17 July 2019,https://link.springer.com/article/10.1007/s13209-019-0198-4,A politico-economic model of public expenditure and income taxation,November 2019,Laura Mayoral,Joan Esteban,,Female,Female,Unknown,Female,"Political economy models of fiscal policy have typically treated separately the choice of income taxation and the choice of public spending. Models of income taxation have focused on voting over purely redistributive linear tax schedules, where individuals care about labor effort and disposable income only. This approach ignores the complementary role of taxation as a source of revenue to finance the government’s supply of goods and services, the in-kind transfers, such as education or health. The classic result in this literature is that the tax rate that obtains majoritarian support is the one preferred by the voter with the median income and that the marginal tax rate is increasing in the gap between the mean and the median income.Footnote 1 Such neat and intuitive result has not obtained support from the data.Footnote 2 In this paper, we explore the possibility that these poor empirical results could be improved by enriching the model by jointly considering the choice of the income tax schedule and the composition of public expenditure. Our point is that individuals care about (private) consumption but also about in-kind benefits from goods and services supplied by the government. Hence, there is a trade-off in the evaluation of higher taxation: It reduces disposable income, but it increases the amount of the goods supplied by the government. It follows that the choice of these fiscal policies (the income tax schedule and the composition of public expenditure) should be jointly modeled. In this paper, we consider jointly the determination of the income tax schedule and the composition of public expenditure. Our starting point is the fact that public debate typically concentrates on a reduced set of salient issues. Political scientists have thoroughly studied the relative importance of expenditure policy and income taxation in the political debate. As it appears, partisan competition turns out to be strong on expenditure policy and mild on income taxation.Footnote 3 This view is patent, for instance, in the codification of party manifestos in all democratic countries since 1945 by Budge et al. (2001, 2006). Income taxation or income redistribution has such a modest presence in the manifestos that neither deserve an entry as a relevant topic. In contrast, public good expenditure on issues such as environment, culture, social services, social security, health, or education represents nearly 20 percent of total entries. Bruninger (2005) and Tsebelis and Chang (2004) find evidence of a significant effect of the composition of governments on the type of public spending. Inspired by this, we choose the salient issue to be the composition of public spending. The political economy model that we envisage is based on these observations and redefines the political process for the choice of fiscal policy variables as follows: (i) The salient issue over which individuals vote—and political parties base their manifestos—is the composition of public spending, more specifically, the extent to which the kind of goods and services provided benefits the poor or the rich; and (ii) given the voted expenditure policy, the political actors look for a distribution of the tax burden, an income tax schedule, that minimizes controversy and does not open additional fronts of partisan confrontation. Our results show that for any chosen expenditure policy, there exists a unique consensual tax function [to be made precise as follows], with the corresponding net tax revenue and the budget balancing size of government. Consequently, voting over the salient issue and designing the rest so as to be consensual deliver the simultaneous determination of expenditure and taxation policies. With regard to public expenditure, the government uses the net tax revenue to finance the public supply of goods and services, which constitute the provided in-kind benefits. As in Arrow (1971), we assume that the government can target the individual beneficiaries of its expenditure by choosing the weight given to types of expenditure that mostly benefit each specific segment of the income ladder. We consider two different expenditure principles. One is equal treatment, providing a uniform benefit across the population. The other is incentive-motivated and provides benefits in accordance with the individual fiscal effort. We parametrize the expenditure policies by the weight given to the egalitarian principle relative to the incentives motive. The value of this parameter is chosen by majority voting. The impact of the public provision of goods on individual well-being critically depends on whether those goods can be substituted by goods that are privately provided through the market. This will also have a critical impact on attitudes toward taxation. Consider, for instance, the case when security was the monopoly of the public police force, with no close substitute in the private market, rich individuals with a high demand for security had to accept a well-funded government because there was no other way of obtaining it. As soon as private security can be purchased in the market, the taxpayers might find the state more dispensable and hence are less willing to pay taxes. The joint determination of the income tax schedule and the composition of the public expenditure, with individuals caring about market and publicly provided goods, sheds new light on the relationship between the progressiveness of income taxation and income inequality. Our model predicts this relationship to be mediated by the elasticity of substitution between market and publicly provided goods.Footnote 4 More specifically, our model delivers two main implications: (1) There is a positive relationship between the marginal tax rate and inequality when the elasticity of substitution private/public is low, and a negative relationship when this elasticity is high; and (2) the size of government is negatively related to inequality, and the marginal effect of inequality on the size of government is larger (in absolute value) if the value of the elasticity of substitution is lower.
 We test these predictions by considering a panel of (at most) 131 countries over the period 1981–2008, and our empirical results are in line with the model predictions. What is the net value added of our shifting of partisan competition from income taxation to public expenditure? This shift yields three interesting novel insights: (i) It brings into stage the interdependence between taxation and public expenditure in in-kind transfers; (ii) it unveils the impact on fiscal policy from public decisions modifying the substitutability between publicly supplied goods and market goods [via privatizations, for instance], and (iii) it explains why inequality has a non-monotonic relationship with income tax progressivity, through its interaction with the substitutability of public versus privately supplied goods and services. The paper is organized as follows. In Sect. 2, we present the model. In Sect. 3, we obtain the consensus income tax schedule and government size, associated with each chosen expenditure policy and show that the majoritarian expenditure policy will be the one preferred by the individual with the median income. All the proofs are in “Appendix A.” We specifically examine the case with CES preferences. Section 4 empirically tests the two main implications of our model, and Sect. 5 concludes. Additional results related to the empirical analysis and not included in the main text can be found in Online Appendix (“Appendix B”).",1
11.0,1.0,SERIEs,08 July 2019,https://link.springer.com/article/10.1007/s13209-019-0196-6,Maturity and school outcomes in an inflexible system: evidence from Catalonia,March 2020,Caterina Calsamiglia,Annalisa Loviglio,,Female,Female,Unknown,Female,"The fact that a unique school cutoff date determines when a child begins primary education induces large heterogeneity in the age at which children enter school. Older children will be substantially more mature than their younger peers, which may lead them to initially perform better. For instance, in Spain the school entry cutoff date is January 1, and children start primary education in September of the calendar year in which they turn 6 years old. Therefore, children born in January are about \(20\%\) older than their peers born in December. In this paper, we use detailed administrative data for the Spanish region of Catalonia to provide evidence that age at enrollment in primary education affects students’ outcomes throughout their education. Work by Heckman and co-authors shows that early child development is complementary to later learning—see Cunha et al. (2006) for a review. Bedard and Dhuey (2006) use international data to show that these initial maturity differences have long-lasting effects on student performance. Several papers look at the effects within a country: Fredriksson and Öckert (2014) for Sweden, Puhani and Weber (2007) for Germany, Schneeweis and Zweimüller (2014) for Austria, Black et al. (2011) for Norway, Crawford et al. (2010) for England, McEwan and Shapiro (2008) for Chile, Ponzo and Scoppa (2014) for Italy, and Elder and Lubotsky (2009) for the USA. The international evidence agrees that relatively older students perform significantly better during primary education. The magnitude of the effect and the consequences for later outcomes seem to vary across countries. For instance, Black et al. (2011) show that in Norway the school starting age barely affects the results in a cognitive test at age 18 and does not matter for their prime-age earnings. Conversely, studies for countries with early tracking such as Germany (Puhani and Weber 2007) and Italy (Ponzo and Scoppa 2014) show that difference in performance is persistent and affects students’ allocation to academic or vocational education, which in turns will affect their labor market outcomes. This suggests that the longer run effect of age at enrollment depends upon the educational system in place. The case of Spain, more specifically of Catalonia, deems particularly interesting for several reasons. First, children are not allowed to postpone or anticipate entrance to primary school, ensuring a clean identification. Second, grade repetition is common, especially during advanced grades of compulsory education. It is important to explore how age at entry and grade repetition interact. In fact, on the one hand, grade repetition might offset the effect of age at entry if it gives more time to less mature students for mastering the material. On the other hand, the empirical literature provides mixed evidence of its effectiveness in improving student performance (Fruehwirth et al. 2016) and suggests that when retention takes place in advanced grades, the probability of dropout substantially increases (Jacob and Lefgren 2009). Therefore, retention may widen the gap between younger and older students rather than closing it. Third, there is no tracking until the end of lower secondary education (tenth grade). In other countries, the effect of age at entry on performance or on the choice of dropping-out from school may be confounded with the different quality of the education offered by different tracks, especially if students’ maturity affects their selection into tracks. Fourth, after completing lower secondary education, students can choose whether to acquire further education, of either academic or vocational type. Only the former gives direct access to University; therefore, if age at entry affects students’ choice of enrolling in academic upper secondary education, it is very likely that it will affect their future educational and labor market outcomes. While some previous cross-country studies include Spain among the countries under analysis, they can only analyze the effect of maturity on standardized assessments administered to a subsample of students who are in education at a given point in time.Footnote 1 In this paper, we use administrative data of the universe of public schools in Catalonia for children enrolled in grades from 1 to 12 in the academic years 2009/2010–2013/2014. We study the effect of age at enrollment on several outcomes over time, including performance, retention, educational choices of whether to complete lower secondary education and whether to undertake upper secondary education, and diagnosis of learning disabilities.Footnote 2 We find that relatively younger children are retained more often and perform worse both in tests administered at the school level and at the regional level. Particularly stark are the results on retention: while a small fraction of students are retained at the beginning of primary education (about 3% during the first two grades), those born at the end of the year are two times more likely to be retained than the average child. At age 14, they are almost 11 percentage points more likely to be in a lower school grade than their peers born at the beginning of the year. The negative effect of being younger on performance is decreasing as children get older, although significant throughout. In particular, the gap between younger and older children is almost 0.6 standard deviations when they are in second grade and still 0.2 standard deviations in middle school. The effect is significant and sizeable for all performance levels, as confirmed by quantile regressions. Grade repetition does not close the gap between younger and older students. Contrary to what others have found for other countries, younger children in Catalonia drop out from education more often.Footnote 3 A child born at the end of December is 2 p.p. more likely to leave lower secondary education without obtaining a diploma than a similar child born at the beginning of January. Moreover, younger students are less likely to enroll in academic upper secondary education. About 50% of students overall enroll in the academic track, but the probability of choosing it is 5 p.p lower for the youngest. Finally, younger children are more frequently diagnosed with learning disabilities or attention-deficit disorders (e.g., ADD, ADHCD). The probability of being diagnosed for a child born at the end of the year is 50% higher than for an otherwise-identical child born at the beginning of the year. In the literature, there is also evidence that the gap in achievement due to the age at enrollment may be larger for children coming from families with high socioeconomics (SES). In particular, Elder and Lubotsky (2009) show that in the USA, the impact of maturity is larger for high SES than for low SES. They argue that preschool experience changes with SES and hence the impact of spending one more year at home or in preschool is particularly benefiting for children coming from high SES. On the other hand, high SES could also help fill the gap during school years, by providing support and additional training to the relatively younger who are facing greater challenges. We find that in Catalonia the effect of maturity on performance is quite homogeneous by SES. This may be because there pre-primary education is guaranteed and free, and 97% of children attend preschool. This means that in the extra year before primary education older children from all family background spend a large fraction of their time in a relatively similar environment. Hence, there may be less heterogeneity due to the experience acquired in that additional year. The rest of this paper is organized as follows. Section 2 describes the Catalan educational system and enrollment rules. Section 3 describes the data and discusses descriptive statistics. Section 4 outlines the empirical strategy. Section 5 presents the main results. Section 6 discusses robustness checks. Section 7 discusses heterogeneity analyses. Finally, Sect. 8 concludes.",10
11.0,1.0,SERIEs,15 June 2019,https://link.springer.com/article/10.1007/s13209-019-0194-8,Which estimator to measure local governments’ cost efficiency? The case of Spanish municipalities,March 2020,Isabel Narbón-Perpiñá,Maria Teresa Balaguer-Coll,Emili Tortosa-Ausina,Female,Female,Female,Female,"Managing available resources efficiently at all levels of government (central, regional, and municipal) is essential, particularly in times of crisis, such as the one that until recently had serious effects on several European countries. Given that increasing taxes and deficit is politically costly (Doumpos and Cohen 2014), a reasonable way to operate in difficult circumstances (not only during crises) is to improve economic efficiency (De Witte and Geys 2011). Since local regulators must provide the best possible services at the lowest cost, developing a system for evaluating local government performance that allows benchmarks to be set over time could have relevant implications (Ferreira Da Cruz and Cunha Marques 2014). As a consequence, local government efficiency has attracted much scholarly interest in the field of public administration and the literature now is extensive (see, for comprehensive reviews, Narbón-Perpiñá and De Witte 2018a, b). However, despite the high number of empirical contributions, a major challenge is the lack of clear, standard methodology to evaluate municipalities’ efficiency. Although this problem is well known in the efficiency measurement literature, in local government most studies focus on one approach only, few have attempted to use two or more methods for comparative purposes. For instance, De Borger and Kerstens (1996a) analysed local governments in Belgium using five different reference technologies, two nonparametric (data envelopment analysis or DEA, and free disposal hull or FDH) and three parametric frontiers (one deterministic and two stochastic). They found large differences in the efficiency scores for identical samples and, as a consequence, suggested using different methods to control for the robustness of results whenever the problem of choosing the “best” reference technology is unsolved. Other studies drew similar conclusions after comparing the efficiency estimates of DEA and the stochastic frontier approach (SFA),Footnote 1 or DEA and FDH or other nonparametric variants.Footnote 2 Some contributions have also attempted to compare parametric and nonparametric efficiency methods (mainly SFA with DEA or other DEA variants) by using Montecarlo simulations.Footnote 3,Footnote 4 However, none of these studies concluded that there is a superior method (Andor and Hesse 2014). Since there is no obvious way to choose an efficiency estimator, the method selected may affect the efficiency analysis (Geys and Moesen 2009b) and could lead to biased results. Therefore, if local government decision makers set a benchmark based on an “incorrect” efficiency score, the economic impact might not be neutral, mislabelling some inefficient municipalities as efficient and vice versa. Hence, although we note that the measurement technique might be not entirely neutral in the case of local governments’ efficiency, one should ideally report efficiency scores that are more reliable, or closer to the “truth” (Badunenko et al. 2012).Footnote 5 The present investigation addresses these issues by comparing four nonparametric methodologies and uncovering which measures might be more appropriate to assess local government cost efficiency in Spain. In other words, we attempt to ascertain which method leads to the most reliable results when evaluating cost efficiency in our particular dataset. The study contributes to the literature in three specific aspects. First, we seek to compare four nonparametric methodologies that cover traditional and more recently developed nonparametric proposals, namely DEA, FDH, the order-m partial frontier (Cazals et al. 2002) and the bias-corrected DEA estimator proposed by Kneip et al. (2008). These techniques have been widely used in the previous literature, but little is known about their performance in comparison with each other. Second, we attempt to determine which of these methods should be applied to measure cost efficiency in a given situation. In contrast to previous local government efficiency literature, which has regularly compared techniques and made alternative proposals, we carry out an experiment via Monte Carlo simulations, identifying those methods that perform better in different settings. In doing so, we adapt the simulated technology in order to adequately describe the characteristics of the local government sector by employing a cost function setting with multiple outputs. Finally, based on the simulation results, we discuss the relative performance of the efficiency estimators under various scenarios and seek to determine which method should be used in each one. Our final contribution is to identify which methodologies perform better with our particular dataset. From the simulation results, we determine in which scenario our data lies in, and follow the suggestions related to the performance of the estimators for this scenario. Therefore, we use a consistent method to choose an efficiency estimator, which provides a significant contribution to previous literature in local government efficiency. We use a sample of 1846 Spanish local governments of municipalities between 1000 and 50,000 inhabitants for the period 2008–2015. While other studies based on Spanish data (as well as data from other countries) focus on a specific region or year, our study examines a much larger sample of Spanish municipalities comprising various regions for several years. The sample is also relevant in terms of the period analysed. The economic and financial crisis that started in 2007 had a huge impact on most Spanish local government revenues and finances in general. In addition, the budget constraints became stricter with the law on budgetary stability,Footnote 6 which introduced greater control over public debt and public spending. Under these circumstances, issues related to Spanish local government efficiency have gained relevance and momentum. Evaluation techniques give the opportunity to identify policy programmes that work, to analyse aspects of a programme that can be improved, and to identify other public programmes that do not meet the stated objectives. In fact, gaining more insights into the amount of local government inefficiency might help to further support effective policy measures to correct and/or control it. Therefore, it is obvious that obtaining here a reliable efficiency score would have relevant economic and political implications.Footnote 7 Our results suggest that there is no one approach suitable for all efficiency analysis. When using these results for policy decisions, local regulators must be aware of which part of the distribution is of particular interest and if the interest lies in the efficiency scores or the rankings estimates. We find that for our sample of Spanish local governments, all methods showed some room for improvement in terms of possible cost efficiency gains, although they present large differences in inefficiency levels. Both DEA and FDH methodologies showed the most reliable efficiency results, according to the findings of our simulations. Therefore, our results indicate that the average cost efficiency would have been between 0.5417 and 0.7543 during the period 2008–2015, suggesting that Spanish local governments could have achieved the same level of local outputs with about 25% and 46% fewer resources. From a technical point of view, the analytical tools introduced in this study make an interesting contribution that examines the possibility of using a consistent method to choose an efficiency estimator, and the results provide evidence on how efficiency could certainly be assessed to provide some additional guidance for practitioners, scholars and policy makers. The paper is organised as follows: Section 2 gives an overview of the methodologies applied to determine the cost efficiency. Section 4 describes the data used. Section 3 shows the methodological comparison experiment and the results for the different scenarios. Section 5 suggests which methodology performs better with our dataset and presents and comments on the most relevant efficiency results. Finally, Sect. 6 summarises the main conclusions.",14
11.0,1.0,SERIEs,01 August 2019,https://link.springer.com/article/10.1007/s13209-019-0201-0,The effect of education on health: evidence from national compulsory schooling reforms,March 2020,Raquel Fonseca,Pierre-Carl Michaud,Yuhui Zheng,Female,Unknown,Unknown,Female,"There is abundant evidence on the relationship between education and health.Footnote 1 Many studies, whether country-specific (Etile 2014; Kim 2016) or international, have documented that this relationship exists in multiple countries, although magnitudes might differ (Banks et al. 2006; Andreyeva et al. 2007; Mackenbach et al. 2008; Michaud et al. 2011). These results deserve attention: if the link is causal, then the effect of education on health should be taken into account when designing education and health policies. This paper aims to estimate the causal effect of education on various health outcomes using cross-country variation in the timing of education reforms. Many mechanisms have been suggested on how education could improve health: raising efficiency in health production (productive efficiency) (Grossman 1972), changing inputs in health production (allocative efficiency) (Grossman 2005), changing time preference (Becker and Mulligan 1997), changing behavioral patterns, e.g., smoking, obesity, preventive care (Huisman et al. 2005; Mackenbach et al. 2008; Barcellos et al. 2018); and finally, gaining more resources, e.g., higher income, occupational status, better housing, better food, better quality of care, and living environment (i.e., Case and Deaton 2005; Cutler and Lleras-Muney 2008). However, there is a persistent problem with these studies. Observational studies examining correlations between education and health cannot be interpreted causally because education might be endogenous. First, earlier health endowments could affect both education and health later in life. Second, an unobserved variable, like time preference, genetic factors or family background, could affect both education and health. To solve this problem, we follow the line in the literature which has employed institutional changes as instruments for education, for instance, Lleras-Muney (2005), Cutler and Lleras-Muney (2006), Cutler and Lleras-Muney (2008), Clark and Royer (2013), Brunello et al. (2016) and Galama et al. (2018), among others.Footnote 2 This literature has generated different results. Our goal is to add to the literature on understanding the causal effect of education on health. We use three data sets including nationally representative samples of individuals aged 50 and over from 14 OECD countries. These are the Health and Retirement Study (HRS) for the USA, the English Longitudinal Study of Ageing (ELSA), and the Study of Health, Ageing and Retirement in Europe (SHARE). We instrument education using differences in educational reform across these countries, the hypothesis being that different compulsory schooling laws can affect education differently across birth cohorts and across countries in an exogenous way, given that the laws can change by time and/or by country. We consider compulsory schooling laws that would impact individuals born between 1905 and 1955. For eight out of the fourteen countries in our analysis, a nationwide change in compulsory schooling laws was noted for cohorts born between those years. For the other countries, there was either no such change, or the change varied geographically within a country. We find that more years of education lead to a lower probability of self-reporting poor health (SRH) and self-reported difficulties with activities of daily living (ADLs) as well as instrumental ADLs (IADLs), and lower prevalence in chronic illness. More specifically, one additional year of schooling is associated with 6.85 percentage points (pp) reduction in reporting poor health, 4.6 and 3.8 pp reduction in having ADL and IADL limitations. Concerning the chronic illness indicator, we document a 4.4 pp reduction: 2.7 pp reduction for diabetes, 3.3 pp reduction for heart disease, 4.6 pp reduction for hypertension, 7 pp reduction for arthritis and 1.4 pp for lung disease. These effects are larger and different than the probit estimates alone, which do not control for the endogeneity of education. We do not find conclusive evidence that education has a causal effect on cancer, stroke and psychiatric illness. This paper adds to the current literature in the following ways. First, we find a causal relationship between education and health by using cross-country variation in compulsory schooling laws over time as an instrumental variable. Second, we examine a wide range of health outcomes, from SRH to functional status and instrumental functional status, and a set of chronic conditions. This differentiates our work from that of recent literature, for instance, Galama et al. (2018), who focus mainly on mortality and its two most common preventable behavioral causes, smoking and obesity, or Gathmann et al. (2015), who also focuses on mortality, or even Crespo et al. (2014), Brunello et al. (2013) and Mazzonna (2014). This also differentiates our work from that of Brunello et al. (2016), who concentrate on self-reported health and health behaviors (smoking, drinking, exercising and the body mass index) for European countries.",12
11.0,1.0,SERIEs,19 September 2019,https://link.springer.com/article/10.1007/s13209-019-00205-0,Restrictions for different functional forms of the matching function,March 2020,Ausias Ribó,Montserrat Vilalta-Bufí,,Unknown,Female,Unknown,Female,"The use of matching functions in economic models allows for the introduction of market frictions in a tractable fashion. The matching function describes how the number of job searchers and the number of open vacancies relate to the number of new job matches that occur within a period. It is common in this literature to interpret the number of matches per job searcher as the average job-finding rate. This class of models has proven important when studying unemployment and its relationship with other phenomena. The extensive body of literature on matching models shows its many applications (see Petrongolo and Pissarides 2001; Rogerson et al. 2005; Yashiv 2007 for surveys). In this paper, we find the restrictions that different functional forms of the matching function must satisfy to ensure that the number of matches per job searcher can be interpreted as a job-finding probability. We do so in the context of the Pissarides matching model with a free-entry condition for firms. Our results are relevant in a discrete time framework, when the job-finding rate is a probability and, therefore, must lie below 1. They also apply to the empirical estimations of the matching function when variables are measured in a discrete time framework (monthly, quarterly, etc). These restrictions are useful in those models where the matching function is simply an instrument to introduce frictions in the labor market without explicitly modeling their micro-foundations (Blanchard and Galí 2010). The role of these frictions is then to allow for the existence of unemployment in equilibrium. In these cases, it is natural to interpret the number of matches per job searcher as a probability, i.e., the probability to find a job by a worker.
 The paper is organized as follows. In the next section, we specify the basic framework of our analysis. In Sect. 3, we derive the theoretical restrictions that the most commonly assumed matching functions (a Cobb–Douglas and two CES matching functions) must satisfy to get an interior equilibrium. We also provide the bounds on the probability to find a job and to fill in a vacancy originated as a result of these restrictions. We summarize our results in the concluding section.
",
11.0,2.0,SERIEs,18 July 2019,https://link.springer.com/article/10.1007/s13209-019-0200-1,Unemployment in administrative data using survey data as a benchmark,June 2020,Cristina Lafuente,,,Female,Unknown,Unknown,Female,"Administrative datasets are gaining popularity among economists.Footnote 1 They offer some advantages over traditional Labour Force Surveys. Most administrative datasets can identify firm-worker pairs and have detailed and extensive working histories (large N, large T). However, using these datasets for the study of unemployment presents some challenges. Firstly, these data were not designed for research, but rather for administrative bookkeeping: calculating contributions to the social security and benefit entitlement of workers. Secondly, the definition of unemployment used in administrative datasets is not the same as the International Labour Office (ILO) standard. Finally, in some countries the administration only keeps track of the unemployed while they are receiving benefits. These discrepancies are particularly relevant for the case of Spain. Since 2004, the Social Security and the Ministry of Labour of Spain has made one such administrative dataset available to researchers: the Muestra Continua de Vidas Laborales (MCVL). This dataset provides complete employment histories of 4% of the Spanish Labour Force in a given year. It can be linked to anonymised tax records, providing comprehensive information on wages and benefits. The MCVL adopts the administrative definition of unemployment: only workers receiving unemployment benefits are considered to be unemployed. This measure systematically excludes individuals who have not accumulated enough contribution periods to be eligible for unemployment insurance. These are mostly young workers in short temporary contracts. Moreover, after the 2008 financial crisis, the number of workers whose unemployment benefits had expired increased considerably. As a result, the unemployment rate as measured by the Labour Force Survey (Encuesta de Poblacion Activa) diverged substantially from the MCVL. By the end of 2013, there was a gap of 10 percentage points between the two measures of unemployment. This paper aims to reconcile this gap by expanding the administrative definition of unemployment in two ways. Firstly, unemployed workers whose benefits run out will appear in the administrative data as if their spell was over. This has been noted by previous authors (García Pérez 2008). The first expansion, which I call the long-term unemployment (LTU) expansion, adds the missing days between the end of a registered unemployment spell and the next employment spell to correct for these artificially short spells. Secondly, workers that are not entitled to any unemployment benefits will not appear as unemployed. Using the institutional framework, three possible situations emerge: quits, workers with too short tenures to qualify for benefits and self-employed workers out of a job. I identify spells corresponding to these cases and add them to the MCVL. I refer to this expansion as the short-term unemployment (STU) expansion. Including these spells is crucial for young people and women, who have a higher incidence of part-time and temporary contracts. This approach refines the most common approach in the literature, which considers all non-employment spells as unemployment.Footnote 2 Together, both expansions can explain most of the gap between the two data sources, supporting the use of the MCVL for the study of unemployment. The paper then shows how the expanded MCVL can complement the Labour Force Survey for the study of labour market flows. The MCVL can be used to address two of the main challenges faced when trying to quantify labour market transitions: non-responsiveness of the unemployed (attrition) and changes in the survey design. In the first case, up to one in five unemployed individuals fail to respond to two consecutive interviews.Footnote 3 This overstate flows from unemployment to employment—which are mostly to temporary work. The MCVL does not suffer from attrition problems, as it tracks all of the changes in the status of individuals (except flows into non-participation). The 2005 change in survey design creates breaks in the transition rates of temporary and permanent workers in the LFS. The MCVL does not fundamentally change in this period, so using the 2005 wave it is possible to examine these changes. Finally, the MCVL allows for the observation of high-frequency transition rates. Due to its quarterly frequency, the Labour Force Survey is not adequate to capture very short, frequent employment–unemployment spells. The MCVL can identify these spells with precision, making it a valuable tool for the study of frictional and youth unemployment. This last observation has broader implications beyond Spain, as young workers across Europe are becoming more exposed to unstable employment. From mini-jobs in Germany and zero hour contracts in the UK to the gig economy worldwide. The paper is structured as follows: Sect. 2 describes the two datasets, their advantages and disadvantages. Section 3 presents the unemployment gap between the LFS and the MCVL and discusses its likely sources. Section 4 expands the MCVL definition of unemployment, comparing the resulting unemployment figures with the Labour Force Survey estimates. Section 5 provides further robustness checks. Section 6 demonstrates the uses of comparable data sources for the study of labour market flows. Section 7 concludes.",4
11.0,2.0,SERIEs,06 June 2020,https://link.springer.com/article/10.1007/s13209-020-00216-2,Correction to: Unemployment in administrative data using survey data as a benchmark,June 2020,Cristina Lafuente,,,Female,Unknown,Unknown,Female,"This document corrects 6 errors in Unemployment in administrative data using survey data as a benchmark. The errors consist of 5 superficial typographical errors and a missing footnote. These are as follows:  In Section 3, “The unemployment gap”, line 3 of 7th paragraph, “4 years” should read “6 years” instead. In Section 4.2, “STU expansion”, line 2 of 3rd bulleted list, “4 years” should read “6 years” instead. In the first reference of “References” section, the author names should be “Arranz JM and Serrano CG (2011)” In the 14th reference of “References” section, the author names should be “Arranz JM and Serrano CG (2014)” There should be a footnote to Table 5 stating “Please note that the columns do not add to the total, since these numbers refer to the destination of completed unemployment spells, while “Total” refers to all spells, completed and uncompleted.” In Section 5.4, “Alternative LFS measures”, line 3, the old acronym of Instituto Nacional de Empleo, INEM, is incorrectly used as it should refer to the Servicio Público de Empleo Estatal, SEPE. The author would like to thank Carlos García-Serrano for helpful comments.",
11.0,2.0,SERIEs,05 October 2019,https://link.springer.com/article/10.1007/s13209-019-00206-z,Business cycle synchronization: is it affected by inflation targeting credibility?,June 2020,Carlos Delgado,Iván Araya,Gabriel Pino,Male,Male,Male,Male,"Globalization, through its effects on both the real and the financial sectors, has increased the interaction between business cycle phases of different economies. This phenomenon is known as business cycle coupling or synchronization. Due to its importance in coordinating economic policies between countries (commercial, technological, financial, and monetary policies), business cycle synchronization (BCS, henceforth) has received considerable attention during the last decade, with an increasing amount of research but few conclusive results (Pesce 2017). Theoretical foundations in BSC highlight trade intensity, for both inter-industry trade and intra-industry trade,Footnote 1 as a factor that affects the degree of synchronization between two economies (Frankel and Rose 1998). Moreover, the degree of production structure symmetry and levels of financial linkages between countries are additional factors explaining the synchronization of business cycle phases (Imbs 2004; Calderón et al. 2007; Schiavo 2008). Macroeconomic policies can also have an effect on the level of BCS. Economic authorities define policy framework to achieve objectives associated with fiscal, trade, and exchange rates and monetary aspects. Among the latter, central banks primarily attempt to stabilize the evolution of aggregate price levels. Inflation targeting (IT, henceforth) has become a leading strategy for inflation stabilization. Since the 1990s, when the first countries began to use IT, the number of nations implementing this regime has increased drastically, especially in emerging economies (Schmidt-Hebbel and Carrasco 2016). The existing evidence on the impact of IT on BCS has suggested that IT promotes synchronization among economies—although the evidence is neither abundant nor conclusive. Arguments in favor of a positive effect of IT on BCS state that by adopting IT central banks can set interest rates in order to stabilize inflation, resulting in a domestic output that is more sensitive to external shocks (Flood and Rose 2010). In the same vein, Inoue et al. (2012) present evidence that IT encourages BCS for some Asian countries. Nowadays, there are an increasing number of countries that have adopted and maintained IT. Using IT, central banks have been able to make and prove their commitment to price-level stabilization, which has helped the strategy gain more credibility. Credibility is understood as the degree of expectations that agents have about whether inflation converges to target levels within the deadlines announced by policy makers (De Mendonça and E Souza 2009). Therefore, it is possible to conclude that IT’s effectiveness is driven by its degree of credibility. In this paper, we focus on the impact of central bank credibility on BCS. To do this, we measure credibility based on the reputation of central banks. More specifically, we look at whether greater credibility allows for a better anchoring of inflation expectations toward inflation targets. In this way, increasing credibility would promote a coupling effect of business cycles phases as a positive externality of monetary policy in IT countries. We postulate that interest rate is a mechanism by which IT can impact BSC. A positive transitory shock that increases price levels also decreases expectation for future inflation. Therefore, policymakers decrease interest rate in order to maintain inflation expectations around the target level. In such an environment, interest rate reduction intensifies the effect on domestic output caused by a positive external shock. Thus, correlation between domestic and external outputs increases under IT regimes (see a deeper discussion about this issue in Flood and Rose 2010). The objective of this paper is to study the impact of central banks’ degree of credibility on BCS. Our main contribution is focusing on the degree of credibility instead of focusing on IT, per se, as a factor affecting BCS. We also provide evidence on the explicit impact of IT credibility on BCS—evidence which is certainly not abundant. For instance, Flood and Rose (2010) show theoretically that IT promotes BCS by assuming perfect credibility. Empirically, mixed evidence for Asian economies is found by Inoue et al. (2012), whose alternative findings could have been driven by varying degrees of credibility on implemented policies. We used a sample of 15 countries that have adopted IT for a long period of time and have been successful in their convergence to their inflation targets. This sample includes both developed and emerging economies for the quarterly period 1985:Q1 to 2015:Q4 for real, seasonally adjusted GDP data. We focus on the synchronization of these economies with G-7 countries, which represent an appropriate proxy for world economic activity (Colomo 2015).Footnote 2 By computing a reputation-based credibility measure (following De Mendonça and E Souza 2009) and using a panel vector autoregressive model (PVAR, henceforth), we obtain dynamic multipliers to analyze the effect of the degree of credibility on BCS. Our main findings indicate that a greater degree of credibility allows for a greater anchoring of agent’s expectations and, thus, an increased effectiveness of monetary policy in stabilizing prices. This paper is organized as follows: Sect. 2 presents the methodological approach, and Sect. 3 shows data used in this paper. Section 4 discusses our main results, and Sect. 5 concludes the paper.",
11.0,2.0,SERIEs,04 October 2019,https://link.springer.com/article/10.1007/s13209-019-00207-y,On the design of equity-oriented pharmaceutical copayments,June 2020,Paula González,,,Female,Unknown,Unknown,Female,"Pharmaceutical copayments are used in almost all health systems, although there is significant variation across countries in access to prescription drug insurance. In Spain, patients under the age of 65 have to pay 40% of their drug costs, while patients over 65 years of age (pensioners) or patients disabled by industrial accidents do have to pay a lower percentage (or do not pay anything). Belgium, Denmark, France, Greece, Luxembourg, and Portugal also charge patients a percentage of the cost of the drug. In other countries, like Austria and the UK flat-rate copayments per prescription exist. Moreover, in all countries protection mechanisms may apply to particular groups of people—for instance low-income individuals, or to particular types of product—for example, essential drugs or drugs for chronic or life-threatening illnesses.Footnote 1 In a public health system, copayments serve either of two objectives, or the two combined: (a) to control unnecessary demand and (b) to raise funds for the health system. Since the seminal paper by Pauly (1968), the literature has focused on the use of copayments as an efficiency-enhancing tool. Their main objective has been to avoid over-consumption, that is, the use of medical services whose costs are larger than their benefits. However, they also play a role as a cost-containment instrument and some governments introduce copayments with the objective of making the health system financially sustainable (Donaldson and Gerard 2005).Footnote 2 In this paper, we elaborate on this second objective of copayments. Our aim is to offer some guidance on the principles that should govern copayment mechanisms if their main objective is to contribute to the financing of the health system and its budgetary sustainability. While there is a huge literature on the use of copayments to reduce moral hazard, there are hardly any papers that have dealt with this second role of copayments from a theoretical point of view. Chernikovsky (2000) suggests that if the objective is to raise funds copayments should be placed on those treatments where elasticity of demand is low. However, this is problematic since among treatments with low elasticity of demand are those that produce the largest benefit. Increasing copayments for those treatments could, therefore, raise important equity issues. This is precisely the aim of this paper, that is, to suggest how copayments should be established in order to raise funds while preserving, at the same time, some equity principles. As far as we are aware there is only one paper (Smith 2005) that has dealt with the issue of how equity should influence copayments if the objective is to balance the health budget. The motivation of our paper is thus similar to Smith (2005). We focus on a health system with a given benefits package that it is budgetary unfeasible to fund only through taxes. At the same time, policy makers do not consider the option of restricting the package of care provided by the health system. That is, we live in a second-best world as is the case in Smith (2005). The choice of the policy maker is, hence, to provide free access for a limited amount of health services or to provide partial funding to a larger amount of services. The main conceptual difference between this paper and Smith (2005) is in the objective of the health system. Smith (2005) assumes that it is to maximize health. He incorporates equity introducing a societal weight on health based on income, that is, the health gains for poor people receive, ceteris paribus, a higher weight than the health gains of rich people. This changes the problem of the policy maker from health maximization to a problem of maximizing a weighted health-benefits function. His copayments are directly related to the cost/benefit ratio. This result is not purely utilitarian because “the benefit of treatment is scaled in proportion to the social weight attached to the marginal patient deterred” (Smith 2005, p. 1026). Our approach departs, we believe, more radically from the traditional utilitarian (health maximization) approach, and it is more strongly linked with philosophical equity principles such as fair innings or severity (Nord 2005). We introduce equity in the objective of the health system itself. We assume that the objective of the health system is not to maximize health (weighted or not), but to ensure that copayments charged to population preserve some basic equity principles. Given this objective, we find that copayments should be influenced by both the severity of the illness of the patient and her possibilities of recovery. These elements are not present in Smith (2005). Moreover, copayments are no longer directly proportional to the cost/benefit ratio of the medical treatment. In order to avoid confusion between our approach and the more traditional perspective of copayments as an efficiency-enhancing device, we assume that health managers have enough information to determine the optimal consumption of health services for each kind of patient. Medical treatments can be provided only to those patients whose benefits outweigh costs. Under these circumstances, “optimal insurance should cover the cost of treatment in full for those with severities high enough to yield benefits greater than costs, and should not pay at all for treatment of people with lower levels of severity” (Pauly and Blavin 2008). In spite of that, we assume that the government cannot raise enough taxes to fully fund those services. The problem is how to distribute available resources in an equitable way. In this framework, copayments are, so to speak, a residual. Since public resources are not enough to fully fund all medical services, subjects will have to provide the rest from their own pocket. Thus, depending on the amount of the budget that each patient is allocated, she will face a different copayment. Our objective is to establish some equity principles to allocate this budget and, in consequence, to determine how much each patient will have to pay to access medical treatments. The theoretical framework that we will use is the so-called axiomatic bargaining literature.Footnote 3 This literature interprets the budget allocation problem as a bargaining process between agents and advocates for sharing solutions that fulfil a series of a priori desirable properties (axioms). Then, the model presented in this paper views the problem as a bargaining situation in which two patients (or groups of patients) have the opportunity to agree with each other on a division of the budget which satisfies them both. The game does not actually have to be played, rather we could imagine the health planner solving the resource allocation problem by treating the situation as if it were a bargaining game. The use of an axiomatic bargaining framework to solve a resource allocation problem in health care was first suggested by Clark (1995), who compared the healthcare budget allocation between two patients under four alternative rules. Later, Cuadras-Morató et al. (2001) enriched Clark’s original setting by allowing for the possibility that agents have claims about the resources they would like to receive.Footnote 4 A central element in our resource allocation problem is the aggregate need or claim. In our model, claims are measured in terms of the utility to which patients are entitled in an ideal situation with no budget constraints. This “ideal situation” depends on the reference point the health planner uses to evaluate equity. Equity is not a concept so well defined in the health literature as efficiency (see Culyer and Wagstaff 1993 for a discussion). Thus, in order to design equity-based copayments we first need to conceptualize equity, as different concepts of equity give rise to different copayments. In this respect, we use two reference points to evaluate equity that yield two scenarios for claims. In the first scenario—the constrained scenario in what follows—the health planner links the right of the patients to request a higher amount in the final sharing of resources to their health improvement given the treatment available. This interpretation of equity coincides with the definition of need “as capacity to benefit” suggested by Williams (1974). In this scenario, the claim is constrained to be the utility that patients would enjoy if they had access to the drug free of any charge. One may think, however, that if the health planner constrains patients’ rights to what is medically feasible, this limits the equity of the final sharing. Thus, we analyse a second scenario—named unconstrained scenario—where the health planner assumes that all patients have the right to enjoy a certain level of well-being based on an exogenously determined amount of health [the quality-adjusted life expectancy (QALE) at birth, for instance]. This second approach is more in line with the fair-innings theory (Williams 1997), that argues that individuals should be compensated for those circumstances beyond their control, like their type of illness or their possibilities of recovery. In this second scenario, the claim is exogenous and requires that all individuals enjoy the same level of utility irrespective of their personal circumstances or treatment possibilities. Notice that the two claim scenarios we consider may be suitable to deal with different types of diseases. In this sense, the health need approach would be appropriate to analyse situations where patients suffer from mild or common illnesses for which there exist effective treatments, while the “fair-innings” approach is more adequate for dealing with patients suffering from severe or rare illnesses with no effective treatment available. In order to allocate resources, we consider two well-known solutions that emerge from the axiomatic bargaining literature. The first solution advocates for a sharing of resources that is proportional to what the agents would obtain in the absence of scarcity and is, therefore, labelled as the proportional solution.Footnote 5 The second alternative is to equally share the difference between what the agents would obtain if the resources were unlimited and the overall amount of available resources to share. This rule, that equally splits the overall loss among the agents, is named the equal-loss solution.Footnote 6 Our results show that copayments based on equity lead to a relation between copayments and clinical status that diverges from those proposals based on efficiency arguments. In the constrained case, only costs of treatment matter. Moreover, depending on the specific rule used to allocate resources (proportional vs equal-loss rule) copayments vary from a fixed percentage of the cost of the treatment (as copayments in Spain), to a flat rate per prescription (as it is in the UK). In the unconstrained case, we find that copayments should be increasing (rather than decreasing) in the health benefits of the treatment. Equity-based copayments, in this case, try to avoid a “double jeopardy” problem, and thus, they favour the patient who suffers a larger burden of disease through a larger subsidization. The paper is organized as follows: Sect. 2 lays out the model and presents the rules we use in the resource allocation problem. Section 3 computes the copayments in the constrained and unconstrained scenarios. Section 4 studies the incentives that our equity-based copayments provide to patients. Section 5 introduces income considerations in the model. Finally, Sect. 6 concludes. All of the proofs are in “Appendix A”.",
11.0,2.0,SERIEs,12 November 2019,https://link.springer.com/article/10.1007/s13209-019-00210-3,"Car usage, \({\text {CO}}_{2}\) emissions and fuel taxes in Europe",June 2020,Gustavo A. Marrero,Jesús Rodríguez-López,Rosa Marina González,Male,,Female,Mix,,
11.0,3.0,SERIEs,29 May 2020,https://link.springer.com/article/10.1007/s13209-020-00217-1,Job loss at home: children’s school performance during the Great Recession,September 2020,Jenifer Ruiz-Valenzuela,,,Female,Unknown,Unknown,Female,"One of the most distinct features of the past Great Recession was the high incidence of job losses on either side of the Atlantic. In Europe (EU-27), there were almost 7 million fewer people in paid employment in 2012 than in 2008.Footnote 1 In the USA, the Displaced Workers Survey data show a record high rate of job loss in the 2007–2009 period, with the rate of unemployment peaking at 10% in the first quarter of 2010 (Farber 2015). In addition to the high level of job destruction, the Great Recession was also characterised by low reemployment rates. The available evidence indicates that job losers suffer short-run earning losses that persist in the long run, have prevalent feelings of job insecurity, worse physical and mental health, an increased risk of divorce and, upon re-employment, a moderate increase in workplace injuries.Footnote 2 Other than the negative consequences borne by the worker, this dramatic weakening of the labour market has thus the potential to generate serious spillover effects for other members of the household, particularly for children. This paper analyses the intergenerational impact of parental job loss by investigating how job losses that occurred during the Great Recession in Spain affect children’s school performance. Given the high unemployment incidence during the Great Recession, the effect of parental job loss on children’s educational outcomes during this period is potentially large. This could be an important and underemphasised cost of recessions. Most of the negative consequences of job loss have a direct effect on variables that are normally seen as inputs of the production function of cognitive achievement. These include impacts on parental income, shifts in parental time investments and deteriorated mental and physical parent’s health. For instance, reduced household income following parental job loss could alter the financial resources available to children’s education. Parental job loss could also be linked to changes in both the quantity and quality of time devoted to children, as well as a shift in the time allocation devoted to childcare in two-parent households. Moreover, it could be challenging to shield children from the harmful consequences on parent’s mental health that could be felt after parental job loss. Importantly, the context of these job losses might exacerbate the potential negative consequences for children’s cognitive development. As Kalil (2013) noted in her review of the effects of the Great Recession on child development, we know very little about the Great Recession’s impacts on children. The results in this paper will shed light on how a deep economic recession disproportionately affecting some sectors in the economy impacts the educational outcomes of students whose fathers are severely hit by the downturn. As Rege et al. (2011) note, estimating a causal relationship between parental job loss and children’s outcomes is subject to two main challenges: finding a source of exogenous variation for parental job loss and the scarcity of appropriate data. Data sets like the Panel Study of Income Dynamics (PSID) or the Survey of Income and Program Participation (SIPP), do not offer precise and repeated information on student outcomes; i.e. most of the times, the only school-related outcome available is the maximum education level reached at a certain age. Another important contribution of this paper is to exploit a panel data set put together by the author, with detailed information for over 300 students in a school in the province of Barcelona.Footnote 3 Collecting data through parental surveys, I obtain information about the parental labour market status and, for those losing the job during the Great Recession, the date and reason of job loss. I can link this information to repeated information on their children’s school performance for academic years 2008–2012. This is crucial, since it allows for a children fixed effect methodology. As will be made clear in Sect. 2, key contributions to the literature have used plant closures to identify the causal impact of father’s job loss on their children’s outcomes. However, Card et al. (2013) show that there is a non-random selection of workers into closing or struggling firms. One of the ways in which this paper addresses this challenge is by controlling for all those unobserved, time-invariant parental characteristics that might be behind the selection of workers into losing their jobs. Importantly, the results using the fixed effect model are compared to those obtained in a fashion similar to that used in the plant closure literature, and those coming from value-added regressions. The findings show that the estimates coming from the two latter strategies are considerable larger in magnitude than those obtained using fixed effects. This suggests that if the data are not sufficiently rich to control for potential determinants of school performance (which are in turn linked to parental job loss), previously used strategies could render estimates that suffer from potential bias due to selection of whom is laid off. The proportion of job losses occurring in the sample during the period is very similar to the pattern seen in the Spanish labour market. Figure 1 shows the unemployment rates in the EU-27, euro area and Spain. The figure shows how the Spanish unemployment rate reached its lowest point in 2007 and started increasing dramatically thereafter, approaching 25% in 2012. I use this fact to try to get a closer approximation of causal effects. In this sense, I define the treatment group by using an event that could potentially resemble a natural experiment (i.e. students whose parents lost their jobs due to the Great Recession). However, even during a recession, those who lose their jobs—or lose their jobs first—might have different characteristics than those who keep them (or keep them for longer). The use of student fixed effects (and additional checks that suggest that there is indeed exogenous timing of job loss in the sample used in this paper) helps overcome potential selection into job loss. I find that the impact of father’s job loss on the average grade is negative and statistically significant. Paternal job loss entails an average decrease in children’s grades of around 15% of a standard deviation. The average impact of mother’s job loss on school performance is close to zero and non-significant. These results are in line with those reported by Rege et al. (2011) that argue that a disparate effect of job loss across fathers and mothers is consistent with recent empirical studies documenting that the mental distress experienced by displaced workers is generally more severe for men than for women [see, for instance, Kuhn et al. (2009)]. Additionally, the results suggest that the negative impact of father’s job loss on school performance is mainly driven by those fathers that suffer longer unemployment spells. Related to this, the effect of father’s job loss appears to be largely concentrated among children of already disadvantaged families in terms of father’s education. Source: Eurostat Unemployment rates Note: Unemployment rates in percent, for the EU-27, euro area and Spain; from 2003 to 2012 One of the potential mechanisms that could be driving the results is the observed decline in income after father’s job loss. However, the heterogeneous results for different subgroups are not fully explained by different income losses. Moreover, it is important to note that these results are obtained for students that are enrolled in the same school during the period of observation. The observed reduction in income cannot be linked, therefore, to changes in the school attended after job loss. An alternative channel by which reductions in income could partly explain these results is if income declines after father’s job loss entail higher stress or financial anxiety and uncertainty for affected individuals and households; an effect reported in some social psychology and health economics research [see, for instance, Lim and Sng (2006) and Kuhn et al. (2009)].Footnote 4 I explore the robustness of these results in a number of ways. Placebo tests show that the average grade prior to father’s job loss is not affected by future job losses experienced by the father. Additionally, the negative effect of father’s job loss does not seem to be driven by those students whose fathers had shorter tenure at the firm prior to job loss, but rather, by those fathers that had a more stable situation prior to losing their jobs. Also, the average grade does not exhibit a negative trend prior to treatment and the results are robust to the inclusion of group-and-year, as well as group-and-stage of education specific effects. The structure of the paper is as follows. Section 2 reviews the literature most closely related to this paper. Section 3 describes the original data set used in the paper while Sect. 4 presents the empirical strategy. Section 5 shows the main results and robustness checks, and Sect. 6 concludes.",4
11.0,3.0,SERIEs,28 February 2020,https://link.springer.com/article/10.1007/s13209-020-00213-5,Family labor participation and child care decisions: the role of grannies,September 2020,Gema Zamarro,,,Female,Unknown,Unknown,Female,"The increase in the proportion of working women is one of the most significant long-term trends observed during the past half-century in labor markets across OECD countries. However, even though female labor market participation has increased significantly across all OECD countries, not all countries show comparable levels. Ahn and Mira (2002) divided OECD countries into three groups. The first is the high participation group in which the female participation rate is higher than 60%. This group includes the USA, Canada, UK, Sweden, and Norway. The second is the medium participation group with participation rates between 50 and 60% (Germany, France, Austria, and Portugal are in this group, among others) and the third is the low participation group with participation rates below 50% (Italy, Spain, and Greece). As a result of this dramatic rise in young women’s labor force participation, the demand for non-maternal child care has increased significantly. As a consequence, different European countries have developed family-friendly policies to help improve access to affordable and quality child care. However, the nature and coverage of these policies differ significantly across countries. In particular, Southern European countries have a lower level of social protection. That is, they have lower social expenditures for families and children. This along with the characteristics of the local labor markets may have led families to provide the necessary services through informal channels, namely through grandmothers.Footnote 1 Indeed, grandmothers have become one of the primary providers of child care for children in Europe. On the other hand, since the mid-1980s labor force participation rates among middle-aged and older women have also been on the rise and governments across Europe have been increasing eligibility ages for retirement pensions stretching the working lives of their older citizens. Less attention, however, has been paid to the link between child care and labor force participation in older ages in the literature. This paper will help answer the key question of how and to what extent child care is provided by working-age grandmothers and how this task is combined with paid work in ten European countries. I also revisit the question of whether this time transfer provided by grandmothers is encouraging the labor participation of their offspring. To do so, I use data from families drawn from the first wave of the Survey of Health, Aging and Retirement in Europe (SHARE), which includes grandmothers and their offspring with children. I also complement these data with information on different features of the formal child care systems across European countries (i.e., generosity of parental leaves, fees for child care) and study their effect on the provision of care and on mothers’ labor participation. The empirical approach takes into account the simultaneity of labor market decisions and caregiving activities while controlling for grandmother’s unobserved heterogeneity on their willingness to provide care to their grandchildren. Here I exploit the fact that I have information about multiple offspring with children for most grandmothers. In line with recent work that used eligibility ages for retirement as external instruments for grandmothers’ labor participation (see Aparicio Fenoll and Vidal-Fernandez 2015; Aparicio Fenoll 2018; Battistin et al. 2015; Bratti et al. 2018), I use the information on eligibility for early retirement as an exclusion restriction to aid identification of the empirical model. I find a negative and significant effect of participating in the labor market on the probability of working-age grandmothers of taking care of their grandchildren on a regular basis. I also find evidence that, for some countries, the child care provided by grandmothers has a positive effect on the labor participation of their daughters. The rest of the paper is organized as follows. Section 2 reviews the economic literature on child care and mothers’ labor participation. Section 3 gives and overview of child care systems across European countries, and Sect. 4 presents the dataset and the variables that I use in the empirical analysis. Section 5 discusses the econometric methodology. Section 6 presents the empirical results. Finally, Sect. 7 contains the conclusions.",10
11.0,3.0,SERIEs,04 March 2020,https://link.springer.com/article/10.1007/s13209-020-00214-4,"Productivity, competition and bank restructuring process",September 2020,Vanesa Llorens,Alfredo Martín-Oliver,Vicente Salas-Fumas,Female,Male,Male,Mix,,
11.0,3.0,SERIEs,06 February 2020,https://link.springer.com/article/10.1007/s13209-020-00212-6,Characterization of efficient networks in a generalized connections model with endogenous link strength,September 2020,Norma Olaizola,Federico Valenciano,,Female,Male,Unknown,Mix,,
11.0,4.0,SERIEs,08 November 2020,https://link.springer.com/article/10.1007/s13209-020-00224-2,Reforming the individual income tax in Spain,December 2020,Nezih Guner,Javier López-Segovia,Roberto Ramos,Male,,Male,Mix,,
11.0,4.0,SERIEs,25 November 2020,https://link.springer.com/article/10.1007/s13209-020-00226-0,Optimal progressivity of personal income tax: a general equilibrium evaluation for Spain,December 2020,Darío Serrano-Puente,,,Male,Unknown,Unknown,Male,"Many modern governments implement a redistributive fiscal policy, where personal income is taxed at an increasingly higher rate, while transfers tend to target the poorest households. The taxation of personal income is not a minor issue, since most of the OECD economies obtain a large proportion of their tax collection through it.Footnote 1 In Spain, there is an intense debate about how to finance the fiscal stimulus recovery plans to alleviate the economic consequences of the COVID-19 crisis and, more precisely, about how to deal with the unavoidable and needed fiscal consolidation process that will surely follow the enormous government fiscal effort. In particular, this growing political debate is taking up many headlines on the so-called “fiscal justice,” which is putting on the table a tax rate increase for the high-income earners, i.e., an increase in the progressivity of the personal income tax. These policies are initially developed to produce a more egalitarian distribution of income and, consequently, to provide social insurance for both currently living households that suffer from large income fluctuations, and for future generations that face uncertainty about what their initial state will be. Raising taxes on higher incomes may be potentially justified by the increase in income and wealth inequality in recent years, especially after the 2007 crisis. These concerns over rising economic inequalities have resulted in a huge body of literature. One of the clearest examples is the paper by Piketty (2015), which triggered a widespread discussion on the nature and evolution of wealth inequalities worldwide. A recent study by Anghel et al. (2018) provides an overview of the inequality levels in Spain and their evolution. They show that the wave of unemployment caused by the 2007 crisis resulted in an inequality increase in per capita income. As for the wealth inequality in Spain, they show that it exceeds income inequality and it increased after the crisis, which may be due to financial assets outperforming real assets according to their vision. By international standards, Spain’s wealth inequality is moderate, as the ownership of real assets is more widespread than in other countries. Beforehand, one is likely to consider that raising taxes on the income-rich households could reverse the growing concentration at the top. However, this type of policy could be very costly in terms of efficiency in advanced market economies. The optimal design of a redistributive tax system is subject to many constraints. Bakis et al. (2015) emphasize three of them in their study about the transitional dynamics of setting an optimal progressivity level. First, agents may have access to self-insurance through savings or bequests, and then, increasing the redistributiveness of the fiscal policies would alleviate the need for such self-insurance and would crowd out capital accumulation, leading to reduced investment and output. Second, misinformation may prevent government from observing individual productivity and, by raising taxes on certain agents, it could provoke incentive problems that discourage labor and thereby reduce output. Third, large-scale shifts in labor and capital supply (savings) alter the wage rate and the interest rate, which may have unexpected repercussions for income redistribution. This is why having a quantitative theory that accounts accurately for the observed income and wealth inequality is crucial when assessing the aggregate, distributional, and welfare implications of certain policies. For that purpose, a heterogeneous households general equilibrium model is here calibrated to replicate some characteristics of the Spanish economy and used to compare the steady-state consequences of setting an optimal progressivity level in the Spanish personal income tax. For the Spanish case, general equilibrium models with heterogeneous agents have already been used to examine the effects of certain reforms.Footnote 2 However, not many studies are encountered to use general equilibrium models with heterogeneous agents to explore the relationship between fiscal policy variables and the endogenous cross-sectional distribution of income and wealth in Spain, in turn the main topic of analysis of this study. The references that we find with respect to this concern are mentioned in the following lines. Pijoan-Mas and González Torrabadella (2006) quantify the aggregate and distributional implications of an array of revenue neutral flat tax reforms for Spain. Viegas and Ribeiro (2015) attempt to characterize the Spanish debt consolidation process in order to assess its effects on economic inequality and welfare. And finally, Guner et al. (2020) uses a life cycle model to evaluate the impact on fiscal revenues of changes in the progressivity of personal income taxes in Spain. The study herein presented is closely related to that research, but the focus in the present work is on the welfare implications of the reforms. In general, the literature on optimal taxation in a general equilibrium framework is vast, but these pieces of work do not have their focus on the Spanish context. Kindermann and Krueger (2018), Conesa et al. (2009), and Guner et al. (2017) study the effects of taxing higher incomes and particularly assess whether and to what extent capital should be taxed. Moreover, Bakis et al. (2015), Heathcote et al. (2017), and Conesa and Krueger (2006) provide an assessment of the optimal progressivity of personal income tax and how redistributive the government’s fiscal policy should be. Finally, Díaz-Giménez and Pijoan-Mas (2019) evaluate the gains that a progressive consumption tax could have with the same modeling framework that is used in the present analysis. Hence, due to the topic and the underlying methodology, the work contained herein would contribute to this body of literature. Again, although some previous studies have analyzed the Spanish economy with general equilibrium models devoted to study policy implications for wealth and income inequality, none of them combined the main characteristics of the dynastic and of the life cycle abstractions (hybrid model with retirement and bequests). Contrarily, these models are built in either dynastic or life cycle fashions. This is where this study adds value, as it proposes other methodologies previously used in other contexts to be applied to the Spanish scenario. The theoretical framework is built for Spain following Castañeda et al. (2003), who also rely on a hybrid approach to account for the US earnings and wealth inequality.Footnote 3 Heterogeneity is introduced in this setup via distinct labor market opportunities using an uninsurable process on the endowment of efficiency labor units that features nonlinear dynamics. Given the labor market opportunity, the households choose their work effort. In other words, the labor choice is set here to be endogenous, as in Pijoan-Mas and González Torrabadella (2006). Life cycle characteristics are modeled using aging and retirement, and dynastic links are modeled in a way that households are altruistic toward their descendants. Once the model is properly calibrated to match some empirical statistics of the Spanish economy, these features ensure that households save for precautionary motives (life cycle reasons and altruistic reasons), as argued by Díaz-Giménez and Pijoan-Mas (2019). This model economy replicates the distributions of income and wealth in very much detail. Further, it also works well when replicating the very top tails of those distributions.Footnote 4 Further, once the theoretical framework is defined, a bunch of potential progressivity reforms are assessed through the calculation of many different general equilibria (one equilibrium for each degree of progressivity evaluated). Then a Benthamite social planner, who takes into account all households in the economy by putting the same weight on each of them, discerns the optimal progressivity reform. The findings suggest that aggregate social welfare is maximized when the level of progressivity of the Spanish personal income tax is increased to some extent. More precisely, in the optimally reformed scenario (setting the optimal level of progressivity), welfare gains are equivalent to an average increase of 3.08% of consumption. By decomposing the aggregate welfare change, it is shown that most of the welfare gains are obtained by direct improvements in the tax system. It means that most of the aggregate welfare gains come from a majority of households facing a lower tax rate, i.e., the poorest households facing lower effective income tax rates and richest households affronting higher effective income tax rates. On the contrary, the general equilibrium effects of the optimal reformed economy (higher interest rate and lower wage) and the effects resulting from changes in the equilibrium distribution of households across income levels (larger mass of households at lower income levels) show a welfare loss, but these losses are so small that together cannot overpass the welfare gains directly coming from the reformed tax system, jointly resulting in positive aggregate welfare changes. These welfare gains are decomposed by household type, where it is observed that the poorest working and non-working households are the ones who benefit the most from the reform. Contrarily, the most efficient working households and the wealthiest ones (either working or non-working) are those who experience the largest trade-off between (i) positive welfare effects derived from higher income (due to an increased interest rate that pushes up capital returns) and (ii) adverse effects emerging from higher tax payments (due to the increase in progressivity of the income tax that discourages labor and savings). The losses from this trade-off are particularly high in top parts of the income and wealth distributions and clearly offset the potential welfare gains of the households populating such areas. Therefore, knowing that these agents would be the losers of the reform, despite positive aggregate welfare effects, the consequences on aggregate capital, labor, and output would be negative, which means that the economy would experience an efficiency loss. Moreover, looking at the distributional implications, this reform would reduce income and wealth inequality. Finally, the theoretical results are evaluated with Spanish tax microdata. From the point of view of a Benthamite social planner, households between the 20th and the 80th percentiles would experience a decrease in their average tax rates under the optimal progressivity reform. For example, the effective average tax rate encountered by a household situated within the 40th and the 60th percentiles of the income distribution would drop from 0.067 to 0.056, which involves a change of 1.1 p.p.. On the other hand, households above the 80th percentile would experience a drastic increment in their effective average tax rate. For instance, the top 1% households of the gross income distribution would go from confronting an average tax rate of 0.284 in the actual scenario to dealing with an average tax rate of 0.330 in the optimal one. The remainder of the paper is structured as follows. The model is formally introduced in Sect. 2. Section 3 describes how the model has been calibrated to match Spanish aggregate and distributional data. Section 4 presents the optimal reform of the progressivity based on a welfare comparison between steady-states and details aggregate and distributional implications. Finally, Sect. 5 concludes.",3
11.0,4.0,SERIEs,25 July 2020,https://link.springer.com/article/10.1007/s13209-020-00218-0,The schooling response to a sustained increase in low-skill wages: evidence from Spain 1989–2009,December 2020,Aitor Lacuesta,Sergio Puente,Ernesto Villanueva,Male,Male,Male,Male,"The response of human capital to changes in anticipated returns to schooling determines the productivity of future cohorts and the evolution of inequality. Unlike other advanced economies, including the USA, the UK or Germany, Spain experienced between the mid-90s and the Great Financial Recession a drop both in the returns to medium and tertiary education, following the housing boom (see Pijoan-Mas and Sanchez-Marcos 2010; Lacuesta and Izquierdo 2012; Bonhomme and Hospido 2017). This paper estimates how educational attainment responds to increases in low-skill wages relative to high-skill. The basic model of human capital acquisition stresses the role of the expected return to skill acquisition as a key determinant of the decision to enroll in formal education—see Becker (1962). In the absence of credit constraints, the opportunity cost of attending school relative to expected returns of acquiring formal skills determines the supply of human capital in an economy. On the empirical side, there is evidence that young adults’ enrollment probabilities decrease with the wages of unskilled workers—Sanders et al. (2005), Neumark and Wascher (1995)—the unemployment rate—Clark (2011) for the UK, Dellas and Sakellaris (2003) for the USA, Petrongolo and San Segundo (2002) for Spain—or proxies for the demand in industries intensive in unskilled labor (Black et al. 2005; Atkin 2016; Charles et al. 2018).
 While school enrollment is a very important outcome, it provides little information about the type of skills that are finally acquired in formal education. Firstly, early school leaving decisions may be compensated later on by other forms of human capital acquisition, such as GEDs in the USA. Secondly, being enrolled in school for some time does not guarantee the acquisition of the final title pursued. Despite this fact, less is known regarding the relationship between expected returns and educational attainment. We concentrate on that particular margin, similarly to Abramitzky and Lavy (2014) and Aparicio (2016). In particular, Abramitzky and Lavy (2014) examine changes in high school completion in kibbutzim where full income pooling among all members was substituted by a distribution of income that let individuals obtaining their full earnings. More closely related to our object of study, Aparicio (2016) analyzes the relationship of the difference in the completion of non-compulsory courses between men and women by regions in Spain and the difference of their corresponding ratios of skill and unskilled wages. She instrumented the skill premium with the share of the construction activity in each region exploiting the fact that the housing boom shall benefit males in a differential way. To our knowledge, we provide one of the first estimates of the response of the supply of academic or professional skills to changes in (potentially) observable measures of the wage structure. Using a different technique López-Mayán (2010) estimates a structural model of grade progression in Spain for the 1985 cohort, finding that students of vocational training are most sensitive to relative changes in the return to vocational training. Her estimation uses actual wages. This measure of economic opportunities might be potentially contaminated by changes in the composition of the workforce. This point is going to be important in our paper as it will be clearer below. This distinction between the academic and the vocational track is relevant, because the type of skills obtained are very different, even for the same number of years studied. We document below that about 80% of workers with a college degree work in occupations with abstract tasks, such as managers, professionals or technicians. Comparatively, workers with vocational training degree concentrate in occupations that involve routine tasks, like assemblers, machine operators and craftsmen. Those patterns are consistent with the curricula of those studies. Recent evidence for a variety of countries has found a decline in the share of middle wage occupations (Autor et al. 2003; Goos et al. 2009; Anghel et al. 2014) and a drop in their relative wages, typically attributed to a decrease in the demand for occupations performing routine job contents and which can be easily mechanized. On the other hand, it has also been documented that occupations which perform jobs that are more difficult to be substituted by technology such as abstract qualified occupations or unqualified jobs which require interpersonal skills are rising in relative employment terms. As a consequence of this trend, losing university degree holders might decrease the capacity of an economy to cope with a growing demand of high-skill tasks. The tracked nature of the Spanish education system permits an examination of how responsive is the supply of the different set of abilities to increases in unskilled wages. As already stated, and differently to all similar papers above, we treat the simultaneity between the quantities of unskilled workers entering the labor market and the relative changes in unskilled wages, by measuring the wage structure perceived by youths at the age of 17 using skill-specific wages determined in collective agreements at the province–industry level, distinguishing between unskilled, mid-skill and college wages. Agreed wages are better suited than actual wages in this setting because of three reasons. First, there is substantial concentration in the wages of the youth around collectively bargained wages for unskilled workers making them very “visible” for young adults who consider joining the labor. Second, the evolution of average agreed wages at the sector and province level are free of changes in the composition of workers as opposed to actual wages, something important in a context of massive creation of jobs. Finally, we argue that bargained wages are insensitive to changes in the supply of young workers due to the fact that unions primarily represent the interests of existing workers. Hence, the evolution of bargained wages might reflect changes in demand instead of changes in supply of youth workers. In order to test this last hypothesis, we show that the variation in wages set in collective agreements does not reflect the relative bargaining power of unskilled youth in the province—measured by the age-specific one-year lagged unemployment rate. We document the following set of results. A 10% increase in the relative wages of unskilled workers, holding the returns to college constant, leads cohorts that turn 17 to be between 2 and 6 percentage points more likely to complete compulsory education only. The result depends on whether we adjust for unemployment differences across skill levels. The increase in the share of male youths with basic schooling comes at the expense of the share of male youth with academic high school track and is most pronounced among males whose mother have a lower level of schooling. We find weaker schooling response to the relative increase of unskilled wages among females, a finding that could be due to wage increases below those observed in construction in the industries where unskilled females work. Our results imply that the housing boom decreased the population educational attainment in Spain because of the abundance of low skill job opportunities and that it is expected that with its bust, educational attainment will recover. Our findings are in line with those that document how local booms affect enrollment, suggesting that youths react to the number of low-skilled jobs available [Black et al. (2005) for Appalachia, Atkin (2016), for Mexico, Aparicio (2016) for Spain, for the USA]. However, those studies cannot disentangle whether human capital investments respond to the number of jobs or to changes in the wage structure. In contrast, we study the human capital response to variations in observable measures of the wage structure that are orthogonal to the availability of jobs. Our results lend support to the notion that youths respond to wage changes. In other contexts, it has been shown that wealth effects could act as a potential channel by which university enrollment might increase following a housing boom. In particular, Lovenheim (2011) documents an increase in US college enrollment derived from a wealth effect of low resource families—also coming from a housing boom. Easier access to finance may represent a lower bound of the total effect on schooling of the US housing boom if construction affects the educational decisions at other stages—see Charles et al. (2018) for estimates of the impact of housing boom on the employment of non-college workers. In Spain, it is not surprising that low-skilled wage opportunities dominate wealth effects because the cost of college enrollment in the sampling period was certainly small, and almost no one asked for a loan to pay university.Footnote 1 Section 2 provides some background on schooling and wages in Spain. Section 3 discusses the wage measures we use. Section 4 describes the data and the methods. Section 5 shows the results regarding the effects of wages on enrollment and educational attainment. Section 6 provides some discussion about the results and implications. Finally, Sect. 7 concludes.",
11.0,4.0,SERIEs,12 August 2020,https://link.springer.com/article/10.1007/s13209-020-00220-6,An anatomy of the Spanish current account adjustment: the role of permanent and transitory factors,December 2020,Mar Delgado-Téllez,Enrique Moral-Benito,Francesca Viani,Female,Male,Female,Mix,,
11.0,4.0,SERIEs,12 August 2020,https://link.springer.com/article/10.1007/s13209-020-00219-z,Structural breaks in the interaction between bank and sovereign default risk,December 2020,Lidija Lovreta,Joaquín López Pascual,,Female,Male,Unknown,Mix,,
11.0,4.0,SERIEs,20 October 2020,https://link.springer.com/article/10.1007/s13209-020-00222-4,Oil price pass-through into inflation in Spain at national and regional level,December 2020,Ligia Topan,César Castro,Andrés Barge-Gil,Female,Male,Male,Mix,,
12.0,1.0,SERIEs,21 May 2020,https://link.springer.com/article/10.1007/s13209-020-00215-3,Agency theory meets matching theory,March 2021,Inés Macho-Stadler,David Pérez-Castrillo,,Female,Male,Unknown,Mix,,
12.0,1.0,SERIEs,27 August 2020,https://link.springer.com/article/10.1007/s13209-020-00221-5,Sixty-seven years of the Nash program: time for retirement?,March 2021,Roberto Serrano,,,Male,Unknown,Unknown,Male,"The Nash program is an important research agenda initiated in Nash (1953). It is intended to bridge the gap between the noncooperative and cooperative counterparts of game theory. The program is thus turning sixty-seven years old, but I will argue it is not ready for retirement yet. Judging by the number of papers that it has produced recently, it is still full of energy. A rough count of papers in the Nash program, cited here and published or listed as working papers since my previous survey in 2005, is the following: Year Number of papers 2006 3 2007 4 2008 6 2009 4 2010 6 2011 3 2012 7 2013 5 2014 5 2015 7 2016 2 2017 4 2018 3 2019 5 2020 5 More importantly, exciting directions to be explored are waiting for good papers to be written. Many results can be found in the several decades of the program, and the reader is referred to Serrano (2005, (2008, (2014) for complementary surveys and commentaries. This paper completes and updates these previous pieces, and suggests several directions for future research. To avoid repetitions, and given that I see this paper as a new chapter in the saga of previous surveys I have written on the subject, I will spare the reader of the section on preliminaries that introduces mathematical notation. I refer the reader to those papers for it. Nonetheless, I have attempted to make the material contained here sufficiently informative and self-contained so that the reader can gain an appreciation of the recent progress made in the program. The plan of the paper is thus the following. Section 2 is devoted to interpretations and new directions for the Nash program, while Sect. 3 consists of a list of recent contributions to it. Section 4 contains a few suggestions for new research. As is always the case in surveys, the list of papers mentioned here will be incomplete and I apologize in advance to the authors of those worthy contributions that surely I will have missed.",15
12.0,1.0,SERIEs,31 March 2021,https://link.springer.com/article/10.1007/s13209-021-00229-5,Aggregative games,March 2021,Luis C. Corchón,,,Male,Unknown,Unknown,Male,"A game is aggregative when, for any player, payoffs depend on her own action and an aggregate that encapsulates all interactions in the game. Usually, this aggregate is taken to be the sum of the strategies of all players or, when the number of players is variable, the average of this sum. The assumption that all interactions are channeled through a single number is, perhaps, a little bit extreme. In a society or in a market, I interact closely with my friends and relatives or with a small number of firms and somehow on a more anonymous way with the rest of the society.Footnote 1 The theory of aggregative games focus on anonymous interactions. This brings an enormous simplification to the players involved in the game and to the analyst using game theory as a tool: As a player, to take my optimal decision, I just need to forecast the value of the aggregate and how this aggregate changes with my actions (of course, I should also know my preferences). As an analyst, to predict the consequences of a shift in a parameter, I do not need to disentangle the effects on this or that strategy. It suffices to focus on the aggregate. This aggregate may be an official statistic or belief that is used by every player. It turned out that some models that economists have been using for a long time have this aggregative structure, see Table 1. So we have an additional payoff: a result obtained in the realm of aggregative games can be applied to models in different fields of economics. This point will be expanded and make more precise in Sect. 6. The rest of the paper goes as follows. Section 2 spells the model. Section 3 presents the first results obtained in this framework. Section 4 presents the key contributions made in 1993–1994, while Sect. 5 describes the development of this area before we attain the boundary of our present knowledge. Section 6 presents some recent contributions dealing with extensions and applications both old and new. We end with a section suggesting future avenues of research.",1
12.0,1.0,SERIEs,19 March 2021,https://link.springer.com/article/10.1007/s13209-021-00230-y,A review of cooperative rules and their associated algorithms for minimum-cost spanning tree problems,March 2021,Gustavo Bergantiños,Juan Vidal-Puga,,Male,Male,Unknown,Male,"Several problems involving network formation have been studied in operations research and economics. The operations research literature is more focused in efficient algorithm designs and computational complexity. The economic literature focuses on aspects such as cost sharing within networks. In this paper, we focus on the cost sharing aspect. Hence, this review belongs to the well-known literature of cost allocation.
 In this paper, we consider minimum-cost spanning tree problems, briefly mcstp. A group of agents, which are located at different geographical places, want a particular service which can only be provided by a common supplier, called the source. Agents will be served through costly connections. Agents are indifferent between being connected directly or indirectly to the source. There are many situations that can be modeled in this way. For instance, several towns may draw power from a common power plant, and hence have to share the cost of the distribution network (Dutta and Kar 2004). Bergantiños and Lorenzo (2004, 2005, 2008) study a real situation where villagers had to pay the cost of constructing pipes from their respective houses to a water supplier. Other examples include communication networks, such as telephone, Internet, or cable television. The literature on mcstp starts by defining algorithms for constructing minimal (cost spanning) trees (mt for short). We can mention, for instance, the papers of Borůvka (1926), Kruskal (1956), Prim (1957). However, constructing an mt is only part of the problem. Another important issue is how to allocate the cost associated with mt among agents. Claus and Kleitman (1973) were the first to address the cost sharing aspect, and Bird (1976) proposed a rule, now known as the Bird’s rule, through Prim’s algorithm. Bird (1976) also associated a cooperative game with any mcstp . He proved that this rule belongs to the core of the cooperative game. Many papers have followed addressing the cost allocation problem arising from mcstp. Early works and reviews are due to Aarts (1994), Feltkamp (1995), Curiel (1997), Borm et al. (2001). Trudeau (2013) reviews some cost sharing rules focusing on two rules (to be addressed below): the folk rule and the Kar’s rule, comparing several axiomatic characterizations. Trudeau and Vidal-Puga (2019) review the main axiomatic characterizations of the rules based on the Shapley value in mcstp.
 There are two possible ways for defining rules in mcstp. The first way is the direct approach, which defines rules directly from the structure of the problem. In this review, we describe rules that are defined through the algorithms for computing the mt defined above (namely Boruvka, Prim, and Kruskal). The idea of such rules is as follows: the algorithm selects the arc, and the rule decides how to divide its cost between the agents. Each agent pays the sum of the assigned costs over all selected arcs by the algorithm. We also review the rules that are defined through a cone-wise decomposition. Each mcstp can be decomposed as a linear combination of the so called elementary problems, where each cost is either 0 or 1. The rule states how to divide the cost of each elementary problem between the agents. Each agent pays the sum of the assigned costs over all elementary problems. The second way is an indirect approach through cooperative games. First, we associate with each problem a cooperative game. Second, we compute a solution for cooperative games (Shapley value, core, ...) in the associated cooperative game. Third, we define the rule in the original problem as the solution applied to the cooperative game associated with the original problem. This indirect approach is quite standard and has been considered in many economic problems. Some classical examples are the airport problem (Littlechild and Owen 1973), where the cost of a runway has to be divided among different airplanes, and bankruptcy problems (O’Neill 1982; Aumann and Maschler 1985) where an estate should be divided among several claimants. Other recent examples are the museum pass problem (Ginsburgh and Zang 2003; Bergantiños and Moreno-Ternero 2015), where the revenue generated by the sale of museum cards has to be divided among the museums, and the broadcasting problem (Bergantiños and Moreno-Ternero 2020), where the revenues from broadcasting sport league events must be divided among the teams. Other examples regarding the Shapley value are in Algaba et al. (2019). In this second approach, the most studied case in the literature is the case of private nodes. It is assumed that, when computing the value of a coalition, agents in such coalition can use only the nodes of such coalition. However, there are other possible approaches, as, for example, the case of public nodes, where it is assumed that agents in a coalition can use nodes outside the coalition. So far, there have been at least five cooperative games in the literature of mcstp: the private game, the irreducible game, the optimistic game, the public game, and the cycle-complete game.  Bird (1976) associates with each mcstp a cooperative game with transferable utility where the worth of each coalition S is computed assuming that agents in \(N\setminus S\) are not participating. It is a pessimistic approach because agents in \(N\setminus S\) are supposed not to cooperate. The private assumption makes their nodes unavailable. Given an mcstp \(\left( N_{0},C\right) ,\) Bird (1976) defines the irreducible associated problem \(\left( N_{0},C^{*}\right) .\) The idea is to reduce the costs of C as much as possible without reducing the cost of the minimal tree of C. The irreducible game associated with a mcstp \(\left( N_{0},C\right) \) is the private game associated with the irreducible problem \(\left( N_{0},C^{*}\right) .\) Bergantiños and Vidal-Puga (2007b) associate with each mcstp a cooperative game with transferable utility where the cost of each coalition S is computed assuming that agents in \(N\setminus S\) are already connected. This game is called optimistic because agents in \(N\setminus S\) are already connected to the source and agents in S can connect to the source through agents in \(N\setminus S\) for free. Bogomolnaia and Moulin (2010) were the first to formally consider the case where the cost of each coalition S is computed assuming that even though agents in \(N\setminus S\) are not connected to the source, agents in S can connect to the source through agents in \(N\setminus S\) by paying the costs of the arcs they use. Given an mcstp \(\left( N_{0},C\right) ,\) Trudeau (2012) defines the associated cycle-complete problem \(\left( N_{0},C^{**}\right) .\) The idea is to achieve concavity by reducing the costs of C as much as possible without reducing the cost of any minimal cycle. The cycle-complete game associated with a mcstp \(\left( N_{0},C\right) \) is the private game associated with the cycle-complete problem \(\left( N_{0},C^{**}\right) .\) The most studied solutions in the five cooperative games we review are the core and the Shapley value. Nevertheless, some other solutions, for instance, the nucleolus (Schmeidler 1969) and weighted Shapley values (Shapley 1953a; Kalai and Samet 1987), have also been considered. We have tried to be exhaustive and to mention all papers studying deeply some aspect of the five cooperatives games. In this survey, we discuss the relation between both approaches. Actually, all the rules mentioned which are defined through some algorithm or the cone-wise decomposition are related to some of the cooperative rules. We give two examples. First, a rule obtained through Kruskal’s algorithm coincides with the Shapley value of the irreducible game. Second, in the irreducible game, the convex combination of the payoffs given by rules obtained through Prim’s algorithm coincides with the core. There also a third approach, which is not discussed in this paper, which is the non-cooperative approach. Instead of providing a rule for dividing the cost, we provide a bargaining protocol to the agents. Thus, agents bargain among themselves, following such bargaining protocol, how to divide the cost. Papers using this approach in mcstp are Bergantiños and Lorenzo (2004, 2005, 2008), Moulin and Velez (2013), Hernández et al. (2016). Other non-cooperative results are part of a relevant research agenda known as the Nash program for cooperative games. The Nash program arises from Nash (1953) as a tool to bridge the gap between cooperative and non-cooperative games by finding non-cooperative procedures yielding cooperative solutions as their equilibrium payoff allocations (Serrano 2005, 2020). Papers which show how the folk rule arises in equilibrium following different non-cooperative protocols are Bergantiños and Vidal-Puga (2010), Hougaard and Tvede (2012), Hernández et al. (2020). The paper is organized as follows. In Sect. 2, we introduce mcstp. In Sect. 3, we review the rules obtained through algorithms and the cone-wise decomposition. In Sect. 4, we review the rules obtained through cooperative games and discuss the relations with the rules defined in Sect. 3. In Sect. 5, we conclude.",3
12.0,2.0,SERIEs,02 June 2021,https://link.springer.com/article/10.1007/s13209-021-00238-4,The economics of the energy transition,June 2021,Natalia Fabra,Xavier Labandeira,,Female,Male,Unknown,Mix,,
12.0,2.0,SERIEs,25 May 2021,https://link.springer.com/article/10.1007/s13209-021-00234-8,CO2 emissions and energy technologies in Western Europe,June 2021,J. Barrera-Santana,Gustavo A. Marrero,Antonia Díaz,Unknown,Male,Female,Mix,,
12.0,2.0,SERIEs,10 February 2021,https://link.springer.com/article/10.1007/s13209-020-00227-z,Are we moving toward an energy-efficient low-carbon economy? An input–output LMDI decomposition of CO\(_{2}\) emissions for Spain and the EU28,June 2021,Darío Serrano-Puente,,,Male,Unknown,Unknown,Male,"There is huge evidence and consensus that global emissions of greenhouse gases are causing global air temperatures to increase, resulting in climate change.Footnote 1 At a global level, the potential consequences include rising sea levels, increased frequency and intensity of floods and droughts, changes in biota and food productivity, and upstream trends in diseases.Footnote 2 Thus, climate change has posed a severe threat to the sustainable development of the human society, the economy, and the environment. At the particular level of the European Union (EU28, hereafter), conforming to the European Environment Agency (2015), more than 80% of the total greenhouse gas emissions are encountered to be a consequence of energy production and energy consumption by the end-use sectors (agriculture, industry, commercial and public services, households, and transport).Footnote 3 These energy-related greenhouse gas emissions are mainly compounded by carbon dioxide (CO\(_{2}\)) emissions, an essential environmental pollutant that has greatly contributed to global climate change, as shown by Ozturk and Acaravci (2010).Footnote 4 Despite not being the world’s largest emitter of energy-related CO\(_{2}\), the EU28 contributes to the mentioned global emissions by 10%, which indicates that it has a non-insignificant role in the global warming trends.Footnote 5 Hence, while efforts to mitigate the adverse effects of climate change are partly focused on limiting the emissions of all greenhouse gases, particular attention is being also paid to energy production and consumption due to its crucial importance for the evolution of the energy-related CO\(_{2}\) emissions. There is a clear interrelationship between energy consumption, the share of low-carbon energy sources in such consumption, energy efficiency, and greenhouse gas emissions. Therefore, the energy and climate targets set by supranational bodies and national authorities approach all these elements. For instance, at an United Nations conference in August 2007, it was agreed that an emission reduction in the range of 25–40% with respect to 1990 levels is necessary to avoid the most catastrophic forecasts. More recently, “doubling the global rate of improvement in energy efficiency” or “increasing substantially the share of renewable energy in the global energy-mix” were set as key objectives by the United Nations (2015) in their “2030 Agenda for Sustainable Development”. Turning again to the European sphere, together with the well-known targets established by the European Comission (2012-10-25, later modified in 2013) in its Europe 2020 Strategy or Horizon 2020 (H2020, hereafter), the European Union authorities have defined an even more ambitious climate scenario that is amongst their main priorities. For 2030, (1) greenhouse gas emissions must be reduced by 40% with respect to 1990 levels (20% for H2020), (2) primary energy use must experience a 32.5% reduction to be achieved by improving energy efficiency (20% for H2020), and (3) a share of 32% in the final energy-mix in favor of renewable energies must be reached (20% for H2020). Furthermore, the European Comission (2019-10-31) declared in a report to the European Parliament and the Council that the objective is to achieve climate neutrality by 2050, i.e., net-zero greenhouse gas emissions in 2050. This translates into a plan to decarbonize the European economy by 80–95% with respect to the emission levels of 1990, accompanying this with a strong reduction of energy consumption, which points out again the relevance of making progress toward energy efficiency. Within those forming the EU28, Spain is another country that, due to its geographical location and socioeconomic characteristics, is also vulnerable to climate change, as shown by the Ministerio de Medio Ambiente (2005). Conjointly with the rest of the EU28 member states, Spain faces strong commitments derived from the ambitious European climate targets for 2020 and 2030. Each member state can set its own targets as long as they match those defined at European level. In this sense, according to the Ministerio de Turismo, Energía y Agenda Digital (2017) and the Ministerio de para la Transición Ecológica (2017), the targets fixed by the Spanish authorities would entail (1) achieving a 42% share of renewable energies in the final energy use for 2030 (20% for 2020),Footnote 6 (2) improving the country’s energy efficiency by 39.5% for 2030 (20% for 2020), and (3) reducing greenhouse gas emissions by 23% with respect to 1990 levels for 2030 (10% with respect to 2005 levels for 2020).Footnote 7 Aiming to comply with the targets set by the European Union as well as by the national authorities, both Spain and the EU28 as a whole adopted different policies and measures. An overview of these policy trends is recovered from the ODYSSEE database published by ODYSSEE-MURE (2020b). Some of these measures are (1) the promotion of renewable energy (including electricity from renewable sources), (2) the creation of the EU emissions trading scheme (a market for carbon dioxide allowances to ensure that emissions reductions can be made where it is most economically efficient), (3) the development of combined heat and power, (4) the improvement in the energy efficiency performance of buildings, (5) the stimulus to use alternative fuels in transport (in particular biofuels), (6) the reduction of the average CO\(_{2}\) emissions of new passenger cars, and (7) the taxation of certain energy products and electricity.Footnote 8 Following the implementation of these measures, mainly after the 2007 crisis, it can be noted that both the EU28 and Spain were progressively moving toward meeting the H2020 targets in recent years. This is shown in Fig. 1. Further, in Fig. 2 we observe that Spain has done a great effort in reducing emission levels since 2005. However, this positive evolution cannot compensate the huge increase of emissions occurred from 1995 to 2005, which still leaves Spain in 2017 with higher emission levels than those observed in 1995. On the contrary, the EU28 has experienced a long-term downward trend, but at a lower decreasing growth rate than the last years of the Spanish trend. Considering the year 2017, the last year of analysis in this study, we recognize how greenhouse gas emissions (in Panel A of Fig. 1) are the only magnitude that meets its European target H2020 both in Spain and in the EU28. The other two H2020 targets (the share of renewables in the final energy-mix and the use of primary energy) are not met, either in Spain or in the EU28 (in Panels B and C of Fig. 1, respectively). We can only notice how the reduction target for primary energy use was fulfilled in Spain during the years 2013 to 2015, but in the last two years the magnitude is again not complying with the H2020 target. Compliance with H2020 Targets. Note: Levels above 100 indicate target compliance Consequently, although the reduction of greenhouse gas emissions is on a positive trend that leads Spain (in 2017) to accomplish the European target H2020 for such magnitude, both the EU28 and Spain have to continue making efforts to fulfill the rest of the 2020 targets. Furthermore, Spain should be careful with the last developments of CO\(_{2}\) emissions, which experienced a slight increasing trend that could lead to a deviation form the target compliance. In addition, both regions must continue working vigorously in a direction that permits them to later satisfy the 2030 targets, which are even more ambitious than those for 2020, as we have seen above. Besides, according to some analyses published by the World Bank and ClimateWorks Foundation (2014), this line of work to control the emissions can offer opportunities for the economic performance of the country, generate new jobs, benefit agriculture, and boost the development of better technologies for the supply of energy. Energy-related CO\(_{2}\) emissions. Note: This figure is depicted using the estimation approach presented in this document. The energy-related CO\(_{2}\) emissions shown are those associated to final energy consumption. This final energy consumption has been climate-adjusted in order to abstract from potential weather effects, which results in a magnitude that is comparable across regions Obviously, one of the major areas to be addressed in order to effectively control emissions is the efficient use of energy. Improving energy efficiency seems very handy to offer a win-win situation, as it decreases energy costs, energy use, and at the same time, negative impacts related to such energy use, like CO\(_{2}\) emissions. Further, using less energy for a certain task gives better possibilities to use energy sources with a predictable price development, which in practice means domestic energy sources, especially in countries that heavily depend on energy imports, like Spain. These arguments clearly highlight the need to implement measures in this regard. However, not all the increase in energy efficiency is translated into energy savings. Some energy equipment could experience an efficiency increase, but if this equipment is not utilized at its maximum rated capacity, sometimes the efficiency improvement is not translated into energy savings. Moreover, technological or efficiency improvements generate cost savings, but these savings could be devoted to new energy consumption and investment, which also requires more energy services, which could consequently increase energy-related emissions. Both pathways generate more activity and may reduce, and even eliminate, the environmentally positive effects of the improvements. This is the so-called rebound effect. Indeed, this effect may be large enough to exceed the maximum expected energy savings from technological or efficiency improvements. Hence, for a better understanding of the impacts of efficiency improvements on our process of energy-use reduction, rebound effects must be incorporated to our analyses. Therefore, care about these rebound effects needs to be taken by policy-makers when calculating the energy saving potential of different measures oriented to improve energy efficiency. Freire-González and Puig-Ventosa (2015) argue that for energy-efficiency-improving policies to be effective, they must be accompanied by other measures such as an effective communication and awareness of the citizens, regulatory instruments and/or an appropriate taxation. An effective combination of traditional efficiency measures with new policies oriented to tackle the rebound effect would maximize the effectiveness of the policy objective of reducing energy consumption. For Vivanco et al. (2016), it is crucial to establish economic instruments for the energy efficiency measures to be completely effective and deal with rebound effect problems. These authors suggest that economy-wide cap-and-trade systems as well as energy and carbon taxes, when designed appropriately, emerge as the most effective policies in setting a ceiling for emissions and addressing energy use across the economy. In addition, these rebound effects vary across end-use sectors. In this sense, Medina et al. (2016) intends to identify the Spanish economic sectors where investment from energy-efficiency-improving measures should be allocated in order to reach the targeted energy efficiency levels in the overall economic system. Besides, only if these energy-efficiency-improving measures are always pursued alongside the decarbonization of the energy system, the carbon-reducing potential of such measures can be guaranteed, as suggested by Malpede and Verdolini (2016). However, these efforts to develop an adequate energy efficiency policy and to promote the use of a lower-carbon energy-mix should not damage the domestic competitiveness of the economy. The relationship between economic growth, energy consumption and CO\(_{2}\) emissions is an essential issue that we face in the 21st century, and it is of far-reaching concern to scholars worldwide. To investigate this matter, several methodologies have been traditionally applied. Zhang et al. (2018) list some of the main ones: the Kuznets curve theory, the Granger causality analysis and co-integration tests, the vector auto-regressive models used to analyze the long-term dynamics, and the decoupling models. The latter approach is followed by Fernández-González et al. (2014), who show that there is a usually a coupling process between energy consumption and economic growth in advanced economies. Therefore, in these economies is more difficult to reduce energy consumption and alternative efforts should be made in order to achieve the decarbonization of the economy, as suggested by Román-Collado and Colinet (2018). Nevertheless, we must bear in mind that the above-mentioned measures to promote efficiency do not explain or influence by themselves alone the evolution of the energy-related CO\(_{2}\) emissions. There may be many potential factors underlying the progression observed both in Spain and in the EU28 and their convergence to the established targets, irrespective of the impact of the energy efficiency policies and measures, as suggested by Economidou and Román-Collado (2019). Some of these factors could be the economic activity level, the efficiency of the conversion sector, the demography, lifestyle changes, the weather, etc. For example, the 2007 crisis could have a profound impact on the industrial sectors and services which in turn could affect energy consumption and consequently energy-related CO\(_{2}\) emissions. Another example includes weather fluctuations, which could affect the heating and air cooling demand provoking that, in a particular warm year, energy consumption may simply drop due to lower heating demand in the residential and services sectors. Therefore, in order to support the most appropriate energy policy decisions, an integrated analytical method to understand the driving forces behind the observed developments of energy-related CO\(_{2}\) emissions, energy consumption and energy efficiency (the three main energy and climate targets previously presented) is irremediably needed. It is precisely here where our work enhances the available related literature, since we develop a methodological framework to investigate the contributions of various influencing factors to the evolution of the energy-related CO\(_{2}\) emissions between 1995 and 2017 both in Spain and in the EU28. With our proposed method, in addition to many macro and efficiency influencing factors discussed before, we are able to capture the role that the primary energy consumption and the share of renewable sources in the energy-mix play in the developments of the energy-related CO\(_{2}\) emissions. This implies that all magnitudes for which the main energy and climate targets are defined and their interrelationships can be monitored within one comprehensive methodological framework. Our period of analysis, 1995–2017, is determined by the availability of data. We should mention that for the findings about the changes that occurred between 1995 and 2017 to be representative of what certainly happened, we must identify two clearly distinct sub-periods, as shown in Fig. 2. These sub-periods are delimited by the year 2007, since it marks the end of a economic expansion period and the beginning of a deep recession followed by a posterior recovery. In this way, we first analyze the 1995-2007 sub-period, and subsequently the 2007–2017 sub-period, both for the EU28 and for Spain. The results that we present give interesting information related to the drivers and inhibitors of the energy-related CO\(_{2}\) emissions in both the Spanish economy and the European economy as a whole. These results are useful not only for researchers, but also for private utility companies and policy-makers, as they can contribute to construct and implement the optimal saving and efficiency measures to achieve the mentioned climate and energy targets. In fact, this paper speaks directly to Spanish and European authorities in the field of energy and climate. The remainder of the document is organized as follows. Section 2 sheds light on the relevance of our analysis by reviewing the existing literature. Section 3 presents the methodology and the databases utilized in our work. Section 4 reports the results. And finally, Sect. 5 concludes.",4
12.0,2.0,SERIEs,27 November 2020,https://link.springer.com/article/10.1007/s13209-020-00225-1,"Energy-efficient design, consumer awareness, and public policy",June 2021,Carmen Arguedas,Sandra Rousseau,,Female,Female,Unknown,Female,"Citizens and governments are increasingly worried about satisfying societal needs without exceeding our planetary boundaries (Raworth 2017). Resource scarcity, climate change, and environmental concerns have stimulated companies and regulators to promote a move toward a decarbonized and sustainable economy. The energy transition plays a crucial role in this evolution as ‘access to clean, affordable and reliable energy has been a cornerstone of the world’s increasing prosperity and economic growth’ (Chu and Majumdar 2012, p. 294) or as Peter Voser as CEO of Royal Dutch Shell said ‘energy is the oxygen of the economy’ (World Economic Forum 2012, p. 2). Core elements of the energy transition are decarbonization and decreasing energy demand. A wide variety of actions and policies are used, and needed, to facilitate this transition. In this paper, we focus on firms’ decisions regarding energy-efficient product design. Energy-efficient product design is closely related to the concepts of green design or eco-design, which aim at reducing the environmental impact of products, including the energy consumption throughout their entire life cycle while preserving a product’s quality level (Fullerton and Wu 1998). Therefore, energy-efficient product design includes a focus on embodied energy in the good as well as energy consumption during the use phase (Rahimifard et al. 2010). As mentioned by Morini et al. (2019), embodied energy and carbon footprints are interesting indicators for selecting materials with a lower environmental burden during the product life cycle. The focus on energy efficiency during the use phase, on the other hand, has led to widespread concerns regarding rebound effects (Gillingham et al. 2016), and mitigation of these rebound effects is increasingly seen as a crucial aspect of energy policies (Vivanco et al. 2016). The European Commission has acknowledged early on that there is a worldwide demand for more efficient products to reduce energy and resource consumption, which resulted in the implementation of the Eco-Design DirectiveFootnote 1 in 2009. This directive provides consistent EU-wide rules for improving the environmental performance of products, such as household appliances, and information and communication technologies. The EU legislation on eco-design and energy labeling is an effective tool for improving the environmental performance of products by setting mandatory minimum standards for their energy efficiency. This eliminates the least performing products from the market, significantly contributing to the EU’s energy efficiency objective. As reported by the European Commission (2019), by 2020 this framework is estimated to deliver energy savings of around 154 Mtoe per year in primary energy, which translates into €470 savings per EU household per year on energy bills. The framework also contributes to energy security by reducing the import of energy into the EU by the equivalent of 1.1 billion barrels of oil each year, and it contributes to the mitigation of climate change by reducing CO2 emissions by 320 million tons annually. Concerned stakeholders such as businesses, local authorities, and environmental NGOs support the European regulator’s desire to impose minimum standards for energy efficiency for different product categories (e.g., Friends of the Earth 2010; EEB 2019). However, past studies have revealed that mandatory regulation and use of economic incentives can crowd out voluntary action (e.g., Frey and Oberholzer-Gee 1997; Kreps 1997; Nyborg and Rege 2003; Underhill 2016). For example, the implementation of a monetary reward system can actually reduce volunteering, even though—once implemented—the size of rewards increases volunteering (Frey and Götte 1999). Thus, stakeholders’ awareness can influence, or be influenced by, regulation. Political scientists are well aware that shifts in public opinion can lead to policy shifts (Hakhverdian 2012). For example, Anderson et al. (2017) show that as public opinion shifts toward prioritizing the environment, a significant and positive effect on the rate of renewable energy policy outputs by governments in Europe can be found. However, the opinion–policy link can work in both ways as politicians can also influence public opinion (Hakhverdian 2012). Democratic leadership occurs when the public’s policy preferences align with the government’s preferences (Geer 1996), while the counter-movement claims that public opinion may run counter to government policy in certain instances (Wlezien and Soroka 2012). Thus, while opinion and policy may move together in some policy domains, this is not necessarily the case for all domains. Positive as well as negative feedbacks between public opinion and policy may exist. For example, Stadelmann-Steffen and Eder (2020) investigate the link between existing domestic energy policies and individual policy instrument preferences in 21 European countries. While they do not find evidence of a general link between existing policies and future policy preferences, they do find that individuals with strong climate change attitudes and high levels of political trust experience positive feedback effects. As another example of a positive feedback, several studies on renewable energy infrastructure found that the public’s opposition toward wind parks or high voltage lines decreased with real-life experience and exposure (e.g., Firestone et al. 2012; Olson-Hazboun et al. 2016). On the other hand, Stokes (2013) found a negative feedback in the case of Ontario’s feed-in tariff policies which was mainly driven by expected future energy costs. In this paper, we investigate how firms’ incentives for energy-efficient design are affected by these interactions between consumers’ awareness and public policy. Explicitly accounting for such interactions is crucial because they are sensitive to the policy instruments used. Regulators can choose between several policy options in order to stimulate energy-efficient design by firms. Instruments can target firms directly through standards or subsidies; or they can influence firms indirectly by targeting firms’ stakeholders and raising consumer awareness through information campaigns and education. While the first option seems to target firms directly, it can also generate an indirect effect through its impact on consumer awareness. On the one hand, consumers can perceive the regulatory action as a signal of the seriousness of the societal problem, and consequently, they can decide to reward firms’ efforts toward increasing energy efficiency more than before. On the other hand, social norms can be crowded out by the regulatory action, which may lead to lower social pressure by consumers. Specifically, we analyze the pivotal role of consumers on the effectiveness of subsidies, product standards, and education in improving firms’ environmental performance through energy-efficient product design. In particular, we investigate the importance of the interaction effect between the regulation and consumers’ environmental awareness. To this end, we first consider a base model where a monopolist chooses prices and product energy efficiency under four different scenarios: (1) a baseline laissez-faire scenario, (2) an education-based policy, (3) a product standard, and (4) a technology subsidy. While crowding-in effects reinforce the policy effectiveness and may even incentivize over-compliance, the net effect of crowding-out is ambiguous. Next, we investigate the impact of competition on these results by looking at a model of product differentiation where two firms take simultaneous decisions on prices and product energy efficiency. We find that competition decreases the average level of product energy efficiency in the market as well as the prices. The net effects of crowding-out very much depend on the type of policy instrument used, but in general, we can conclude that the counteracting effects of the regulatory standard on crowding-out are less powerful than under monopoly. Finally, for completeness, we change the timing of the duopoly game and allow firms to first select the energy efficiency level of the product and then decide on the prices. This leads to a higher degree of product differentiation and may even stimulate one of the firms to be more energy-efficient than in a monopoly situation. Counteracting effects of the regulatory policy are also present under this alternative formulation. Our paper is close to the literature on environmental policy in product differentiation models, but the main difference in our setting is that we model consumer awareness to depend on the stringency of the regulatory policy, while the related studies consider the two variables as being independent from each other. One of the most related studies is Moraga-Gonzalez and Padron-Fumero (2002), who analyze a duopoly setting in a vertical product differentiation model. Firms simultaneously decide on the production technology (being cleaner or dirtier) in the first stage, and they compete in prices in the second stage. In equilibrium, a cleaner and a dirtier variant coexist, and a maximum unit emission standard reduces unit emissions of both variants and thus boosts firms’ sales with negative impacts on aggregate emissions. Besides the addition of crowding-in and crowding-out effects on this analysis, we study an alternative timing where the two firms take simultaneous decisions on prices and product energy efficiency. In this setting, we find the opposite effect to the one found in Moraga-González and Padrón-Fumero (2002). While the imposition of the standard causes an effect in the same direction on both firms in the model of sequential moves, in the model of simultaneous moves we find opposing effects that diminish differentiation and end up in zero prices. Hence, the timing of decisions becomes crucial regarding the effectiveness of environmental policy stringency. Other related works are Espinola-Arredondo and Zhao (2012), who investigate how a tax/subsidy policy can promote the consumption of green products in the context of horizontal product differentiation; Casino and Granero (2018), who model the market entry decision as well as the product differentiation decision in a spokes model with a large number of potential varieties; or Rodriguez Ibeas (2007), who consider a vertical differentiation model to analyze the effect of (exogenous) changes in consumer awareness on social welfare. However, none of these studies analyze the changes in consumer awareness as a result of changes in the stringency of the environmental policy. We organize the remainder of the paper as follows. We describe the base monopoly model in the following section. In Sect. 3, we present the results under different scenarios. In Sect. 4, we consider two versions of a duopoly model (one with simultaneous decisions and the other with sequential choices), and we compare the results with those obtained in Sect. 3. We conclude in Sect. 5. All the proofs are in a mathematical ‘Appendix.’",2
12.0,2.0,SERIEs,16 October 2020,https://link.springer.com/article/10.1007/s13209-020-00223-3,Emission taxes and feed-in subsidies in the regulation of a polluting monopoly,June 2021,Ángela García-Alaminos,Santiago J. Rubio,,Female,Male,Unknown,Mix,,
12.0,2.0,SERIEs,16 May 2021,https://link.springer.com/article/10.1007/s13209-021-00233-9,Emission trading systems and the optimal technology mix,June 2021,Gregor Zoettl,,,Male,Unknown,Unknown,Male,"Cap and trade mechanisms are one of the centrally important policy tools to achieve ambitious greenhouse gas reduction targets required in the context of recent climate agreements (e.g., the 2015 Paris agreement, UNO 2015). In the present article, we analyze the design of cap and trade mechanisms and their impact on firms’ investment decisions in different technologies and on their final production decisions. We then derive the optimal design for ideal market conditions, but also for non-ideal situations where firms either exercise market power or competition authorities’ decisions are partially constrained by requirements of political or legislative processes. Cap and trade mechanisms designed to internalize social cost of pollution enjoy increasing importance in environmental legislation worldwide. Well-known examples are given by the European Union Emission Trading System (ETS), the California Cap-and-Trade programme or the Korea ETS. “As of April 2020, there were 23 emissions trading systems covering around 9% of global emissions” (see IEA 2020). A very prominent example of a very large planned system is the China national ETS which is likely to start in 2021.Footnote 1 An important aspect when introducing cap and trade mechanisms is the possibility of competition authorities to grant emission permits for free (see, e.g., ICAP 2020). This apparently allows to crucially facilitate the political processes leading to the introduction of cap and trade systems. As Convery (2009) in an early survey on the origins and the development of the EU ETS observes: “The key quid pro quos to secure industry support in Germany and across the EU were agreements that allocation would take place at Member State level [...], and that the allowances would be free.” Very similar observations can also be found in many other contributions to that issue.Footnote 2 A one and for all lump sum allocation of permits which is entirely independent of firms’ actions has a purely distributive impact in case firms take the market price of emission permits as given. This fundamental insight in principle dates back to Coase (1960). The design of free allocations in currently active cap and trade systems typically does not have such lump sum property, but includes explicit or implicit features of updating (see IEA 2020). Updating of free allocation schemes designed to consistently adapt to an industry’s dynamic development has an impact on firms’ behavior, however.Footnote 3 First, it leads to a distortion of the operation of existing production facilities since for existing cap and trade systems current output and emissions do have an impact on allocations granted to those facilities in the future. Second, it also has an impact on firms’ incentives to modify their production facilities through upgrading, retiring and building of new facilities since typically free allocations to some extent are granted contingent on newly installed or retired facilities.Footnote 4 The literature which analyzes the impact of updating on firms’ behavior up to now has focused on the first aspect. That is, those contributions provide very rich insight on the impact of updating based on past output or emissions on firms’ production and emission decisions, and they abstract from changed free allocations to a firm due to changed installed facilities, however. The most prominent contributions in this context include Moledina et al. (2003), Böhringer and Lange (2005), Rosendahl (2008), Mackenzie et al. (2008), Harstad and Eskeland (2010)), Böhringer et al. (2017), Qiu et al. (2017), Meunier et al. (2017) or Meunier et al. (2018). Reviewing current and planned emission trading schemes worldwide reveals, however, that legislations which provide free emission permits also include elements which update based on newly installed or retired production facilities (Narassimhan et al. 2018; IEA 2020).Footnote 5 It is the purpose of the present article to focus on the above-mentioned second aspect and its impact on the technology mix chosen by firms. To the best of our knowledge, this is the first article that formally analyzes the impact of updating on firms’ investment incentives in an analytical framework. Usually, production facilities in most industries allow for production during longer horizons of time. Since demand typically varies over time, it is optimal for firms to invest in a portfolio of different technologies. Technologies with high investment cost which allow for production at low marginal cost are installed to run most of the time, whereas technologies with low investment cost which allow for production at higher marginal cost cover more infrequent demand periods. Prominent examples of capital-intensive and long-lasting investments in production facilities and fluctuating demand due to limited storage possibility are given by, e.g., the production of cement, steel or electricity (with electricity being the most prominent and well-studied example in this context). We thus analyze a framework with fluctuating demand where (strategic) firms first determine their technology mix by investing in different production technologies with different emission intensities. Production facilities allow firms to produce for a continuum of spot markets subject to fluctuating demand. Production at each spot market causes emissions, and total emissions at all spot markets are capped by an emission permit market. We consider the impact of a cap and trade system where different amounts of free allocations are granted for newly built facilities. After having established the market equilibrium both for firms’ investment and production decisions and for the emission permit market, we first determine the first best solution as a benchmark. Analogous to most of the previous literature, if distributional concerns do not matter, in an ideal market with perfectly competitive firms updating is not optimal (i.e., no free allocations should be granted to any investment), and the total emission cap should be set such that the permit price equals to marginal social cost of pollution. In the main part of the paper, we then analyze the optimal design of a cap and trade system if the market is not ideal. First, we consider the case that firms behave imperfectly competitively when making their investment and their production decisions. In this case, investment and production are inefficiently low. As we show, measures undertaken to stimulate investment incentives (such as capacity paymentsFootnote 6) have a very close relationship to free allocations made for newly installed production facilities. In this case, it is optimal to either grant capacity payments or free allocations in order to stimulate inefficiently low investment incentives. As we show, however, in a closed system with endogenous permit market, it is not optimal to implement total investment at first best levels since this would imply an inefficiently high permit price which excessively depresses spot market output. Our results do thus have direct implications for the design of capacity markets in the presence of a cap and trade system. Second, we analyze the case where the design of the cap and trade mechanism is subject to political constraints (compare the second paragraph of this section) and the competition authority has to determine the optimal market design given those constraints.Footnote 7 We first analyze how the optimal target on total emissions should be set if free allocations in all technologies are exogenously fixed. As we find, for moderate levels of free allocations the target on total emissions should be set such that the equilibrium permit price is above marginal social cost of pollution. For high levels of free allocation, for example, in case of full allocation where all permits used by a certain technology during a compliance period are freely allocated, the total cap on emissions should be set such that the equilibrium permit price is below marginal social cost of pollution. We then analyze the case that free allocation only for a specific technology is exogenously fixed and determine the optimal level of free allocation for the remaining technology. In order to avoid excessive distortions of the resulting technology mix, it is typically optimal to grant free allocation for the remaining technology. That is, the insights obtained from the first best benchmark that free allocations are never optimal, are no longer true if allocation to one of the technologies is exogenously fixed. Moreover, if this technology is relatively dirty (as compared to the technology with exogenously fixed allocation), the level of free allocation should remain below the exogenously fixed allocation. If on the contrary the remaining technology is relatively clean, the level of free allocation should even be above the exogenously fixed allocation. Observe that often observed practices of full allocation (see IEA 2020; ICAP 2020) induce a pattern of free allocation which is completely opposed to those findings.
 Let us finally mention further related strands of the literature. First, several articles analyze firms’ incentives to adopt cleaner technologies. Those include, for example, Requate and Unold (2001), Montero (2002) or Requate and Unold (2003), for a survey of this literature, compare Requate (2005). Whereas all those contribution focus on the comparison of emission taxes and cap and trade systems, our contribution is the first one to consider the phenomenon of updating in the context of technology choice. Second, from a modeling perspective the present paper also contributes to the literature which analyzes investment decisions in several technologies. For a survey on this literature, see Crew et al. (1995). Further contributions include Zöttl (2010), or more recently Grimm et al. (2017). Our article is (to the best of our knowledge) the first one to introduce an endogenous emission permit market in such a framework, and to derive the optimal design of a cap and trade mechanism with technology-specific free allocations both for ideal and imperfect market conditions. The remainder of the article is structured as follows: Sect. 2 states the model analyzed throughout this article, and Sect. 3 derives the market equilibrium for a given cap and trade mechanism. We determine the optimal market design for the benchmark case of perfect competition in Sect. 4, for the case of imperfect competition in Sect. 5 and for partially constrained cap and trade mechanisms in Sect. 6. Section 7 concludes.",
12.0,3.0,SERIEs,19 April 2021,https://link.springer.com/article/10.1007/s13209-021-00231-x,Forecasting Spanish unemployment with Google Trends and dimension reduction techniques,September 2021,Rodrigo Mulero,Alfredo García-Hiernaux,,Male,Male,Unknown,Male,"Unemployment is an issue currently faced by the vast majority of economies. It is a red-hot topic in studies carried out by economists and forecasters. Analyses are often based on offering explanations, consequences and possible solutions to the problem, by different models that simplify real complexity. Numerous jobless suffer constrains that generate problems of a macroeconomic nature, such as a decrease in consumption and investment which, eventually, affect GDP. Moreover, unemployment is also related to welfare problems as inequality and social exclusion. At least for these reasons, it is of most importance to correctly predict and evaluate unemployment in order to monitor its evolution, anticipate trend shifts, and design pro-employment policies. Spain is a country with a high unemployment level compared with its peers, peaking, in the 2013 recession, to 5 million registered unemployed workers. For the purpose of this study, we use the official figures provided by the Spanish Public Employment Service (SEPE).Footnote 1 Typically, data unemployment is released with certain delay which means that the use of leading, or coincident, indicators will be useful to anticipate its evolution and improving its forecasts (see, e.g., Stock and Watson 1993, for details on leading indicators). With this in mind, the aim of this work is to propose some simple alternatives to univariate models for predicting the Spanish unemployment. We search for models which include additional, free of charge and available-to-everyone up-to-date information. We look for this information on the Internet search engines. These applications contain a large amount of information, available almost instantaneously, and reveal many aspects of the individuals’ preferences through their search histories. In this paper, without losing generality, we focus on searches in Google. More specifically, we use one of its tools, known as Google Trends (GT). Our hypothesis is that, using updated search indices obtained from GT there is a large margin to improve the predictions of the Spanish unemployment provided by a suitable univariate model. However, any forecaster will soon discover that GT is not the panacea. As we will discuss in the next sections, some not trivial decision must be made when trying to optimize the information gathered from GT. This issue is treated in the paper in an application to the Spanish unemployment forecasting, although the procedures suggested could be applied in other contexts. By means of a recursive forecasting exercise, we find that a SARIMA model with additional GT queries, applied to the Spanish unemployment series and relative to a univariate benchmark model, yields a statistically significant improvement in terms of forecasting accuracy that ranges 10–25%. This gain depends on the way the GT information is treated, with Principal Components Analysis (PCA) or Forward Stepwise Selection (FSS), and is robust to the variables that affect the results of the forecasting exercise. In our application, FSS outperforms PCA. The paper is organized as follows. Section 2 provides a revision of the literature in the use of GT as explanatory variables, focusing on unemployment applications. Section 3 details the data employed in the analysis, paying particular attention to the GT queries and how those are generated and obtained. Section 4 presents the benchmark model, the proposed alternatives and their relation with other common methods in the literature. The latter are based on data reduction methods, which are introduced in Sect. 5. Section 6 compares the forecasting results of the proposed models relative to the benchmark and Sect. 7 analyzes the robustness of the previous results. The last section highlights the main findings of the paper.",7
12.0,3.0,SERIEs,15 June 2021,https://link.springer.com/article/10.1007/s13209-021-00237-5,Economic policy uncertainty and investment in Spain,September 2021,Daniel Dejuan-Bitria,Corinna Ghirelli,,Male,Female,Unknown,Mix,,
12.0,3.0,SERIEs,02 June 2021,https://link.springer.com/article/10.1007/s13209-021-00232-w,The distribution of wealth in Spain and the USA: the role of socioeconomic factors,September 2021,Pedro Salas-Rojo,Juan Gabriel Rodríguez,,Male,Male,Unknown,Male,"Recent studies on wealth inequality have found a wide heterogeneity on the type of assets owned along the wealth distribution. While the bottom third is asset-poor and the middle-class’ wealth is mostly composed by real estates, the upper tail is characterized by possessing a high quantity of financial assets, which are also considered to be the main contributors to the increase in wealth inequality observed during the last decades (Demirgüc-Kunt and Levine 2009; Gennaioli et al. 2014; Badarinza et al. 2016; Lusardi et al 2017; Anghel et al. 2018). The sources of wealth disparities among countries have been, however, more elusive to find out. The comparison between the USA and some European countries has shown that covariates like education, labor status, household structure or income distribution do not explain the large cross-country differences in wealth inequality (Christelis et al. 2013; Doorley and Sierminska 2017; Cowell et al. 2018a). Consequently, aggregate disparities have been attributed to a wide variety of institutions and other latent factors. But, why do socioeconomic factors not seem to account for wealth inequality disparities across countries? In this paper, we suggest that we should change the wealth inequality concept under consideration. In particular, we propose to study the factors that condition the opportunities of individuals to accumulate wealth. According to the inequality of opportunity literature, certain economic outcomes such as wealth, income or health are actually a composite measure of two types of variables (Roemer 1998; Van de Gaer 1993). In the first group, we find individual circumstances, factors beyond individual’s control like the inheritances received, the parental education, race, sex or health endowments. In the second group, we have individual efforts, factors under the responsibility of individuals like the occupational choice or the number of hours worked. As a result, overall inequality is the combination of two types of inequality: inequality of opportunity (IOp), the part of total inequality explained by circumstances and inequality of efforts. In this context, any society concerned with fairness should minimize the IOp component, as the distribution of circumstances is morally arbitrary (Rawls 1971; Sen 1980).Footnote 1 The IOp literature has traditionally focused on income (Bourguignon et al 2007; Rodríguez 2008; Ferreira and Gignoux 2011; Marrero and Rodríguez 20112012), health (Trannoy et al. 2010; Jusot et al. 2013; Tsawe and Susuman 2020), education (Gamboa and Waltenberg 2012; Lasso de la Vega et al 2020) and happiness (Li Donni et al. 2015). However, mainly due to the lack of appropriate data, its implementation to the analysis of wealth inequality has been scarce. Only a few recent works have highlighted the role of two circumstances, the inheritances received and parental education, on the process of wealth accumulation (Adermon et al. 2018; Palomino et al. 2020; Nolan et al. 2020; Salas-Rojo and Rodríguez 2020). Following this literature, we calculate wealth IOp for total, financial and real estate in Spain (Survey of Household Finances, 2014 EFF) and the USA (the Survey of Consumer Finances, 2016 SCF) by using the inheritances received and parental education as our circumstances. Wealth is, by definition, a stock variable accumulated over time, so it is affected by all the decisions that the individual makes during his life (De Nardi 2015). In fact, the variables of wealth surveyed at a certain moment in life by databases like the EFF and SCF summarize the aggregate result of individual past decisions about wealth. For this reason, we control for the life cycle and sex of individuals before the IOp method is applied to measure the relationship between our controlled circumstances and the distribution of wealth. To study whether a set of covariates (income, labor status and education) explains the differences on total, financial and real estate wealth inequality and wealth IOp between Spain and the USA, we apply the DiNardo–Fortin–Lemieux (DiNardo et al. 1996, DFL from now on) decomposition method. This approach has been applied to the decomposition of overall wealth inequality (Cowell et al. 2018a, b) but, to the best of our knowledge, this is the first time that the DFL is used in the IOp framework. This method will allow us to determine whether the above covariates affect the individual opportunities to accumulate (total, financial and real estate) wealth. After estimating overall wealth inequality and wealth IOp, we use the DFL decomposition method to impose the distribution of income, education and labor status of the USA into Spain. In this manner, we create a counterfactual country whose wealth distribution is characterized by the covariates of the former and the institutional framework of the latter. In line with the existing literature (Christelis et al. 2013; Doorley and Sierminska 2017; Cowell et al. 2018a), we find the wealth distributions of this counterfactual to be similar to those of Spain. As a result, cross-country wealth inequality disparities between Spain and the USA are not explained by the aggregate effect of the covariates. Specifically, only around 3.1% of total wealth inequality differences, 1.2% of financial wealth inequality differences and 4.4% of real estate wealth inequality differences are attributed to the set of considered covariates. A Shapley value decomposition is applied to show that while the education and labor status distributions of the USA reduce the wealth inequality of the counterfactual, the income distribution increases it, so the net effect is closed to zero. With these results at hand, we should blame—as the literature does—different institutions and other latent factors for the remaining differences in wealth inequality between both countries. Once this is done, we focus on (total, financial and real estate) wealth IOp. First, we find that wealth IOp in the USA is always higher than in Spain, no matter the inequality index nor the wealth definition considered. After imposing the covariates distribution from the USA into Spain, we find a significant rise in wealth IOp measures in the counterfactual. The covariates now explain 20.4% of total, 76.3% of financial and 6.2% of real estate wealth IOp disparities among Spain and the USA. Opposed to the analysis for overall wealth inequality, a Shapley value decomposition shows that the education and labor status distribution of the USA increases wealth IOp in the counterfactual, while the income distribution decreases it. To better understand these results, we focus on the relationship between circumstances and covariates. First, a higher educational persistence is observed in the counterfactual than in Spain, i.e., those who receive high inheritances and have well-educated parents are more likely to reach high education levels. Second, a similar relationship is found for labor status: having well-educated parents and receiving high inheritances increases the probability of being employed in the counterfactual than in Spain. On the contrary, income is more equally distributed in the counterfactual, particularly across those with high and intermediate parental education, equalizing the individual opportunities to accumulate wealth. Hence, it seems that the type of wealth inequality considered is relevant for the analysis of covariates. Decomposing wealth IOp cross-country disparities with the DFL method highlights the role of socioeconomic factors as mediating variables between individual circumstances and wealth accumulation. While the IOp literature had already found bequests and parental education to explain a remarkable share of wealth disparities (Palomino et al. 2020; Salas-Rojo and Rodríguez 2020), the wealth inequality framework had signaled the strong relation between human capital or enjoying a stable labor status with higher wealth stocks (Lusardi et al 2017; Anghel et al. 2018). Our approach joins both frameworks, opening new avenues to further exploration on the transmission channels of opportunities across generations. The reminder of the article is structured as follows. Section 2 explains the method used to estimate wealth IOp and how we perform the DFL counterfactual decomposition. Section 3 describes the database, defines the variables under consideration and comments their main statistics. In Sect. 4, we present the main findings for overall wealth inequality and wealth IOp, while Sect. 5 concludes.",2
12.0,3.0,SERIEs,08 March 2021,https://link.springer.com/article/10.1007/s13209-021-00228-6,Love is blind: partisan alignment and political corruption in Spain,September 2021,Miguel Ángel Borrella-Mas,Martin Rode,,Male,Male,Unknown,Male,"Political corruption and malfunctioning governments are a widespread phenomenon on a worldwide level.Footnote 1 International organizations regularly highlight that the diversion of public funds for private benefit is probably one of the most important threats to the stability of many developed economies (Kaufmann et al. 2004, 2009), with estimations equating the overall cost to be more than 5% of global GDP (OECD 2013), and increasing by about 10% the average cost of doing business (World Economic Forum 2008). Other studies have looked at the detrimental consequences for economic growth (Mendez and Sepulveda 2006), investment (Mauro 1995; Wei 2000), and international trade (de Jong and Bogmans 2011), to name only a few.Footnote 2 Contrasting with the relatively well-documented economic outcomes, empirical evidence on the political determinants of corruption still presents many unknowns, according to surveys by Pellegrini and Gerlagh (2008) and Dimant and Tosato (2018). In particular, the role played by decentralization is regarded as uncertain, where some find federalism to be an important element in containing corruption (Dell’Anno and Teobaldelli 2015; Villalonga 2018), while others highlight the potential multiplication of corruption possibilities through decentralized administrations (Fisman and Gatti 2002; Gerring and Thacker 2004; Erlingsson et al. 2008; Fan et al. 2009; Albornoz and Cabrales 2013). Related to this, the frequent re-election of corrupt politicians also highlights the complex interplay of different incentives at work in national, regional, and local electoral processes, where punishment of corrupt incumbents is not always the default option (Miller 1999; Reed 1999; Manzetti and Wilson 2007; Ferraz and Finan 2008, 2011; Jiménez et al. 2013). Similar evidence has also been found at the Spanish local level by Costas-Pérez et al. (2012), Jiménez and García (2018), or Fernández-Vázquez et al. (2016), where the effects of news coverage, political ideology, and income seem to play crucial roles for electoral decisions.Footnote 3 Following this literature, uncovering the political determinants of corruption is key to understanding the role that institutions play in its emergence. In this paper, we examine whether ruling a regional and local government at the same time, henceforth partisan alignment, has an impact on political corruption among local governments in Spain. To the best of our knowledge, this possibility has not been considered to date in the relevant literature, and our paper is the first to do so empirically. Theoretically, alignment could impact corruption in several different ways: On the one hand, alignment may induce corruption, and several overlapping mechanisms could be at work. First, a variety of studies have already shown that aligned municipalities benefit from more favorable budgetary decisions taken at a higher government level for countries as diverse as Albania (Case 2001), Brazil (Brollo and Nannicini 2012), France (Fabre 2014), India (Arulampalam et al. 2009; Dey and Sen 2016), Italy (Bracco et al. 2015; Carozzi and Repetto 2016), Portugal (Migueis 2013), Spain (Solé-Ollé and Sorribas-Navarro 2008; Curto-Grau et al. 2018), and the United States (Levitt and Snyder Jr 1995; Larcinese et al. 2006). According to this literature, the possibility of corruption could increase with the number of public programs that are being implemented at the local level, a possibility which is even higher for aligned municipalities. Seen from this point of view, public sector corruption is simply a function of the available transfers that can be diverted (Holcombe 2013). Second, and following a very similar logic, aligned municipalities may be better able to “afford” to be corrupted. That is, any negative effects from corruption for local public finances would be compensated for by the comparatively higher transfers from aligned regional governments. Third, if local politicians benefit from better connections via alignment when obtaining public contracts and projects with firms, private sector-related corruption possibilities could be more abundant for politicians of aligned municipalities (cf. Gutmann and Lucas 2018). Finally, a lack of monitoring from upper-tier governments and a tendency to turn a blind eye toward corruption when the same party controls both levels may create incentives for local politicians to seize corruption opportunities more readily when they are available. Essentially, corruption here constitutes a potential alternative for regional administrations to reward aligned (and loyal) local governments. On the other hand, the effect of partisan alignment on local level corruption could be negative, if regional governments are better able to influence aligned municipalities to behave according to the law by more credibly threatening them, an influence which cannot be exercised in the case of non-aligned municipalities. For example, via the threat of removing party support, forcing corrupt politicians to resign from office or leave the party, etc. Finally, there may be no association at all with regard to monitoring if regional governments behave similarly toward aligned and non-aligned local governments. In the following, we investigate the important question of whether partisan alignment impacts corruption at the local level, constructing a dataset on corrupt practices from press news published over a period of 15 years, matching it with local and regional electoral data over three consecutive electoral cycles between 1999 and 2011. Findings show that aligned municipalities are significantly more associated with corruption than non-aligned ones. The association is stronger where regional governments enjoy an absolute majority, and in municipalities receiving higher capital transfers from regional governments, which is very much in line with previous studies on the allocation of transfers to aligned municipalities in Spain (Solé-Ollé and Sorribas-Navarro 2008; Curto-Grau et al. 2018). By contrast, we find that “throwing the rascals out” may be an effective strategy for curbing the corrupt practices of aligned municipalities when allegedly corrupt mayors or parties are actually removed from power. In the tradition of Olson (1982), this actually suggests that a certain level of political instability is desirable, potentially making it more difficult for corrupt politicians to completely dominate a local political system. Finally, findings are robust to the inclusion of additional controls, several estimation methods, different definitions of alignment, and also to falsification tests. The remainder of the paper is organized as follows: Sect. 2 outlines the relevant institutional setting in which Spanish local and regional elections take place. Section 3 describes our data and the research design in detail, while Sect. 4 presents the results. Finally, Sect. 5 discusses the results and concludes.",1
12.0,3.0,SERIEs,12 June 2021,https://link.springer.com/article/10.1007/s13209-021-00236-6,"Incentives, ability and disutility of effort",September 2021,Silvia Martinez-Gorricho,Miguel Sanchez Villalba,,Female,Male,Unknown,Mix,,
12.0,4.0,SERIEs,09 November 2021,https://link.springer.com/article/10.1007/s13209-021-00255-3,Inequality and psychological well-being in times of COVID-19: evidence from Spain,December 2021,Monica Martinez-Bravo,Carlos Sanz,,Female,Male,Unknown,Mix,,
12.0,4.0,SERIEs,16 August 2021,https://link.springer.com/article/10.1007/s13209-021-00242-8,"Decoupling synthetic control methods to ensure stability, accuracy and meaningfulness",December 2021,Daniel Albalate,Germà Bel,Ferran A. Mazaira-Font,Male,Unknown,Male,Male,"Since the seminal works of Abadie and Gardeazábal (2003) and Abadie et al. (2010), the synthetic control method (SCM) has been increasingly adopted as a technique to evaluate causal effects under quasi-experimental design (see, among others, Montalvo 2011; Billmeier and Nannicini 2013; Cavallo et al. 2013; Kleven et al. 2013; Bohn et al. 2014; Percoco 2015; Acemoglu et al. 2016; Kreif et al. 2016; Albalate and Bel 2020; Sun et al. 2019). The method provides a practical solution to the evaluation of case studies in which either only a single unit or very few aggregate units are treated (countries, regions, cities, etc.) and it is considered one of the most influential recent contributions to empirical policy evaluation (for instance, Athey and Imbens 2017, p. 9). The SCM creates a hypothetical counterfactual (the synthetic unit) by taking the weighted average of pre-intervention outcomes from selected donors (control units). The impact of treatment is quantified by the simple difference between the treated unit and its synthetic cohort after the treatment (post-treatment period).
 As discussed in a series of papers by its pioneering authors (see Abadie and Gardeazábal 2003; Abadie et al. 2010, 2015; Abadie and L’Hour 2019), the SCM has two main advantages over other methods, such as regression-based counterfactuals or nearest-neighbor matching. First, by being constrained to nonnegative weights that need to sum one, it does not impose a fixed number of matches and ensures sparsity, while avoiding negative weights or weights greater than one that would imply an unchecked extrapolation outside the support of the data and complicate the interpretation of the estimate. Second, weights are calculated to minimize the discrepancies between the treated unit and the synthetic control in the outcome and the values of certain matching variables or covariates. Thus, the SCM is intended to ensure that the synthetic unit reproduces the control unit not only in terms of the outcome, but also in terms of the drivers that explain the evolution of the outcome of the treated unit before treatment. In spite of the influential contribution made by the SCM, the method suffers from some weaknesses that, if not properly addressed, may erode the reliability and robustness of its causal estimates and, consequently, of its policy implications. For instance, Ferman et al. (2020) have highlighted that lack of guidance on how to choose covariates gives researchers specification-searching opportunities that directly influence the choice of comparison units and therefore the signification of the results. Abadie (2020) also pointed out that even assuming a proper set of covariates and a counterfactual that matches the treated unit, interpolation biases may arise if this matching is obtained by averaging donors that have large differences in covariates but compensate each other to match the treated unit. As stated by Albalate et al. (2020), the bilevel optimization design of the SCM and its NP-hardFootnote 1 nature helps to explain why quasi-experimental methods for estimating covariate importance under the SCM are unstable and highly dependent on the donor pool, thus affecting weight estimation. The contribution of this paper is twofold. First, we develop a proposal of decoupling synthetic control methods, to overcome the limitations of the bilevel design of the SCM. Our approach is simpler and more operational, since it breaks down the NP-hard problem of the nested optimization into two independent problems of quadratic optimization with linear constraints. The method we propose ensures robustness of the estimation of both covariate importance and the weights. By decoupling the estimation of covariate importance from that of weights, it minimizes interpolation biases and guarantees economic sense. To estimate covariate importance, we use a new methodology for estimating feature importance suggested by Lundberg and Lee (2017; 2019): SHapley Additive exPlanation (SHAP) Values. This method allows us to analyze the marginal effects and average contribution of the different features of a model, even in the case of nonparametric models. Thus, we can obtain sound estimates for each unit of the relation of the different covariates with the outcome and define a distance between the donor pool and the treated unit in terms of how covariates influence the outcome. To estimate weights, the procedure we use minimizes quadratic error in the pre-treatment outcome, restricting the donor pool to the most similar units to the treated unit. Roughly speaking, we obtain a synthetic control that is the benchmark that best reproduces the pre-treatment outcome and whose behavior is explained by the same factors that explain the treated unit. Second, to illustrate the main advantages of our proposal, we apply both methods to an evaluation of the causal economic effects of the 10-month-long government formation impasse in Spain, after the December 2015 elections. In line with the approach taken by Albalate and Bel (2020) for the 18-month government formation deadlock in Belgium, we use the SCM to build an appropriate counterfactual to identify and isolate the gap between Spain’s actual GDP per capita growth rate and the rate at which it would have grown without a government formation deadlock. Our results indicate that the growth rate was not affected by government deadlock, ruling out any damage to the economy attributable to the institutional impasse. Moreover, as a robustness check of the advantages of the decoupled synthetic control method, we use our methodology to reproduce two previous studies: the impact of German reunification (analyzed in Abadie et al. 2015) and the effect of tobacco control programs in California (Abadie et al. 2010). The rest of this paper is organized as follows: First, we describe the standard SCM and we evaluate its stability, consistency and economic meaningfulness. In light of the limitations identified, in Sect. 3 we propose a new decoupled SHAP-distance synthetic control method (DSD-SCM) that overcomes the limitations of the standard SCM. In Sect. 4, we apply both methods to the estimation of the causal economic effects of a long government formation deadlock in Spain between December 2015 and October 2016. We discuss the findings, focusing on the magnitude of the differences between the two methods (SCM vs. DSD-SCM), the advantages of the DSD-SCM, and the economic implications of the impasse. In Sect. 5, we present the replication of two case studies, as a robustness check of the improvements of our methodology with respect to the original synthetic control. Concretely, we replicate the analysis of the impact of German Reunification and of the effect of the tobacco control program in California. In Sect. 6, we offer our main conclusions and considerations about the new method proposed.
",1
12.0,4.0,SERIEs,27 July 2021,https://link.springer.com/article/10.1007/s13209-021-00241-9,The child penalty: evidence from Spain,December 2021,Alicia de Quinto,Laura Hospido,Carlos Sanz,Female,Female,Male,Mix,,
12.0,4.0,SERIEs,08 June 2021,https://link.springer.com/article/10.1007/s13209-021-00235-7,Price competition and nominal illusion: experimental evidence and a behavioural model,December 2021,Antonio J. Morales,Enrique Fatas,,Male,Male,Unknown,Male,"Humans are not immune to psychological biases when taking economic decisions (see Kahneman 2003; Thaler 2000). One such bias, money illusion, was first defined in Fisher (1928) as the “failure to perceive that the dollar, or any other unit of money, expands or shrinks in value”. This tendency of people to think of money in nominal, rather than real terms has been widely documentedFootnote 1; although until recently (Tyran 2007), it has played only a limited role in explaining economically relevant behaviour. Recent empirical papers document how money illusion may persistently mediate inflation and drive real prices in a variety of economic environments, including the housing market (Brunnermeier and Julliard 2008) and the stock market (Cohen et al. 2005; Acker and Duck 2013). The standard view in economics is that nominal illusion is a transitory phenomenon. Economic agents will eventually see through the nominal veil and will start making the right choices. Implicit in this argument is that nominal illusion entails a cost, because nominal and real payoffs are not aligned. It is, however, common to come across situations where nominal and real payoffs are perfectly aligned, as it happens when choices are done using different currencies, and nominal illusion is transient. For example, one of the most studied cases is the changeover to the Euro in the European Economic and Monetary Union in 2002. The overall conclusion is that money illusion happened and it was transitory (see for example Kooreman et al. 2004; Cannon and Cipriani 2006 and more recently Bittschy and Duppel 2015). The transience of nominal illusion seems a well-established fact in the economic literature. In this paper, we show that nominal illusion persists in competitive price settings. The basic intuition is that in a competitive setting, nominal illusion may facilitate collusion and once firms are making extraordinary profits, they have no incentives to modify their behaviour. We present experimental data from a one-shot entry game where players must decide in which of three markets to enter. Once the entry decision is done, players compete in prices for 20 periods in a standard static, full information, Bertrand duopoly game. The interesting twist is that all markets are equivalent, meaning that they are but different nominal representations of the same economy, e.g. all markets are identical in real terms and only differ in the local currency in which prices are nominated. We find that subjects spread evenly among the three local markets, which is consistent with the prediction yielded by all markets being identical in real terms. But when we compare pricing behaviour across different nominal representations, we find a positive correlation between posted prices and nominal exchange rates: coarser currencies are associated with higher prices. For large enough exchange rate, this pattern becomes statistically significant. Even more, this monotone nominal illusion is of a permanent nature as there is no convergence to the Nash prices: prices stay consistently high without declining to the Nash equilibrium prices. Most experimental papers on money illusion consider situations where nominal and real payoffs are not aligned, as in Fehr and Tyran (2001, 2007, 2014), that report transient money illusion. Fehr and Tyran (2008) and Noussair et al. (2012) compare prices before and after a nominal shock, and report a pronounced inertia in the convergence to the unaltered (in real terms) equilibrium, although the rate of convergence depends on whether the shock is positive or negative (Noussair et al. 2012), or whether actions are strategic complements or substitutes (Fehr and Tyran 2001). There is only one instance in which a temporary money illusion phenomenon has a permanent effect. Fehr and Tyran (2007) consider a price competition game with three Pareto ranked equilibria and two treatments: one in which the payoff matrix is presented in nominal terms and other in which it is presented in real terms. By design, the payoff-dominant equilibrium in nominal terms is the payoff-dominated equilibrium in real terms. They find that in the condition in which payoff information is provided in real terms, subjects quickly converge to the Pareto-dominant equilibrium whereas when the payoff information is provided in nominal terms, subjects coordinate on the equilibrium that provides them with the highest nominal payoff (but the lowest real payoff). Money illusion, although temporary in nature, has permanent effects because when players finally see through the nominal veil, it is too late to get away from the “bad” equilibrium. Thus, those players suffering from the illusion effect end up worse off. In our setting, the Nash equilibria are also Pareto ranked, but their ranking is the same regardless of whether information is provided in real or nominal terms. In addition, we show that different nominal representations select different Nash equilibria. Closer to our paper, Eisenhuth (2017) analyses the market survival of money-illusioned economic agents in a dynamic financial market model populated by rational economic agents. Eisenhuth (2017) shows that market forces can wipe out rational agents in the long run, leaving a market full of money-illusioned agents. In our experiment, we find a result similar in spirit: in the long run, all local markets are populated by money-illusioned firms. Our proposed mechanism is though different: if economic agents start focusing on the nominal representations because they are simpler and more salient (as it is the standard view in the literature, see for example Shafir et al. 1997), and they keep using them because it is a very profitable strategy, then competitive forces are simply perpetuating collusive practices. We propose a behavioural model that rationalizes the monotone money illusion based on participants choosing grids as a simplifying procedure. The rest of the paper is as follows. Section 2 presents the experimental design, procedures and hypotheses. Experimental results are discussed in Sect. 3. Section 4 presents the behavioural model and finally, Sect. 5 concludes.",
12.0,4.0,SERIEs,19 November 2021,https://link.springer.com/article/10.1007/s13209-021-00253-5,Redistribution of tax resources: a cooperative game theory approach,December 2021,Emilio Calvo,,,Male,Unknown,Unknown,Male,"This paper addresses the problem of how to distribute public sector spending among the regions of a country. Decisions about how much to spend in each region are increasingly up to local governments. Local institutions want to decide not only on what and how to spend the public budget in their region, but also the total amount to be spent in their region. There are three basic principlesFootnote 1 that appear recurrently in the search for a well-functioning regional financing system: Non-discrimination Distributed funds must provide a uniform level of public services throughout the country. Fairness in redistribution Allocation of funds should vary directly depending on fiscal needs and inversely according to the tax capacity of each jurisdiction. Ordinality The results of the equalisation should be tolerable for donors and recipients alike. They should narrow financing disparities across regions without altering their per capita relative wealth ranking. They should not carry equalisation beyond a generally acceptable level. Unfortunately, how to make all these principles fully compatible with each other is not evident. Large disparities between regions in terms of their per capita wealth could make it difficult to apply the principle of non-discrimination, since it could imply high transfers of income from rich to poor regions, always viewed with suspicion by richer regions. When designing a regional financial system, the tax system should therefore minimise the complaints from a region, or group of regions, regarding the total budget obtained following that funding rule. Obviously, how to measure and compare such grievances becomes a key problem in determining the level of equalisation in per capita wealth that should be considered generally acceptable. This exercise can be transferred, point by point, to the problem of the distribution of the budget between the states of a federal country, or between the states of a confederation of countries, such as the European Union, simply by substituting the regions of a country for the federal states, or by the confederation countries. Until now, fiscal balances (FBs) were used for the analysis of such complaints. Fiscal balances determine the differences between public revenues collected and public expenditures allocated in each region. These net per capita balances have been used to evaluate and compare the budget distribution between regions. We can see in the FB literature two opposite (more or less explicit) fiscal sovereignty assumptions, which we might call full versus shared sovereignty. In the full sovereignty approach, all resources collected in a region are considered to belong exclusively to that region. In this case, fiscal balances are used as benchmarks by which the degree of satisfaction or disagreement with the total budgetary expenditure obtained by the region is measured. It should be noted that there is little room for negotiation on budget distribution with this approach. Significant differences between what is collected and obtained are viewed with suspicion. The objective of a stable budget distribution is therefore that what is spent in each region should be as close as possible to what is collected. This is so because the distribution of the budget is seen as a zero-sum game: What one region gains is at the expense of what another loses. The shared sovereignty approach is opposed to this point of view, in which all the regions that make up a country are considered part of the same economic entity. It is understood that all the resources collected come from the mutual cooperation resulting from the exchange of goods and services between the economic agents of the country. From this perspective, regions are distributing all the economic gains from their cooperation between their citizens, regardless of where they are. In the first approach, any monetary transfer is only seen as a voluntary act of solidarity between regions. From the second perspective, it is nothing more than an obligation in order to obtain the common welfare of the country’s citizens. However, in our opinion, none of these perspectives are as irreconcilable as they appear at first glance. In this paper, we address this problem of budget distribution using tools from the theory of cooperative games. We believe this may help clarify the discussion on how much should be allocated to each region. Roughly speaking, the problem can be solved by determining more precisely how much of the fiscal resources belong exclusively to the region (or group of regions), and how much should be considered common property. We first enrich the FB model by explicitly incorporating interregional economic relationship in its construction. This is accomplished by constructing what we call the tax cooperative game, which determines for each region, or set of regions (coalitions), all tax revenues derived exclusively from the economic interaction within regions of the coalition, as well as any imports or exports from abroad. According to the total sovereignty approach, we can interpret this amount as the minimum that any coalition of regions should receive in any acceptable regional budget allocation. Here, we exclude all tax revenues that coalition members derive from their current interaction with the rest of the country, as according to the shared sovereignty approach they are common property, and consequently, their distribution can be subject to negotiation. We propose measuring the grievances of a region, or coalition of regions, against the budget distribution, by means of the cooperative game theory concept of the excess of a coalition. This is the difference between what the tax cooperative game gives the coalition minus what the budget allocation spends on it. The lower the allocation given, the greater the excess becomes, and therefore the greater the disagreement. The excess is therefore a measure of such disagreement. The goal is thus to find budget allocations that minimise such excesses. The familiar concept of the core of a cooperative game is associated with the excesses obtained by the coalitions, i.e. the set of all the budgetary distributions whose excesses are all negative. In this paper, we will continue with the usual custom in game theory of qualifying the stability of a budgetary allocation according to its situation with respect to the core of the tax game. In this way, we will evaluate the stability of the current budgetary distribution of a country, as well as any other alternative proposals we may consider. The first finding was that for any tax cooperative game there are always stable budget allocations. Thus, it is always possible to propose stable budget allocations. Secondly, we can undertake the theoretical exercise of applying solutions from the cooperative games literature to the tax game at hand. For example, the purpose of the egalitarian spending rule is that no citizen should be discriminated against in terms of public spending, matching the public spending per capita in all regions of a country. Unfortunately, wide disparities in per capita wealth between regions mean that this spending rule easily produces unstable allocations. Two other relevant rules are the nucleolus (Schmeidler 1969), which always belongs to the core when it is non-empty, and the Shapley value (Shapley 1953a, b). In general, both rules produce different allocations, and their computation is an arduous task. Nevertheless, given the matrix structure of the tax game, we demonstrate that the nucleolus and the Shapley value of every tax cooperative game coincide. The value formula also takes a simple expression: it assigns each region its own fiscal resources plus half the common fiscal resources of its trade interaction with all other regions. We shall call this rule the balanced allocation \(\varphi\), and this is placed at the centre of the core. However, \(\varphi\) does not perform well in terms of wealth redistribution. To improve this poor redistributive behaviour, we propose to consider the weighted balanced allocation \(\varphi^{w}\), where the weights \(w\) are the per capita gross domestic product (\(GDP\)) of regions. Now, the welfare contribution that each region makes to each other in the form of tax transfers is balanced inversely proportional to their \(GDP\) per capita. This allocation coincides through its construction with the weighted Shapley value (Shapley 1953a), and we show that it offers a greater degree of solidarity between regions than \(\varphi\). Given the convexity of the tax game, this rule is also stable. We thus believe that \(\varphi^{w}\) is a reasonable trade-off between these two principles: stability on the one hand, and solidarity on the other one. Finally, the qualification of a budget distribution as stable or not, depending on whether it is inside or outside the core, is not entirely satisfactory. It is simply a binary assertion. To compare two alternative budget allocations, we would like to know the degree of stability they enjoy. For that purpose, we modify the tax game by means of a parameter \(\alpha \in \left[ {0,1} \right]\). In the computation of the modified tax game \(v_{t}^{\alpha }\) we add a proportion \(\alpha\) of the initially excluded tax revenues (considered as a common sovereignty). This parameter allows a gradual transition from the concepts of stability of shared fiscal sovereignty to full fiscal sovereignty. As \(\alpha\) increases, therefore, the amount of those common fiscal resources that the region agrees to share with all the others decreases. Accordingly, the set of stable budget allocations reduces to the ideal full sovereignty target (\(\alpha = 1\)), in which the budgetary expenditure obtained by the region is equal to the totality of taxes collected in it. In addition, \(\alpha\) provides a normalised measure of each region’s degree of dissatisfaction with respect to a given budget allocation. It is equal to the value \(\alpha\) for which the region’s excess over the modified tax cooperative game is equal to zero. This normalisation allows a better comparison between the excesses of the regions and, eventually, a comparison between the stability observed in different countries. We illustrate how the theoretical concepts introduced here perform through a practical exercise: we consider the Spanish case by using fiscal data from 2011 to 2014. We start by illustrating how to build the tax game from FB and inter-regional commercial trade data. The current Spanish financial rule has a moderate redistributive effect of wealth between regions; the rule treats regions better the poorer they are. Surprisingly, we show that the current financial system belongs (unexpectedly) to the core of the tax game during those years. However, we should nuance the system stability. There are several autonomous communities (CAs) whose excesses are very close to being positive (some, such as Catalonia, with strong secessionist feelings), coexisting with other CAs whose situation in relative terms can be considered privileged, such as the group of “foral communities” formed by the Basque Country and Navarre. This situation produces a relatively unstable political cocktail. Spain involves disparities in terms of per capita wealth across regions, and therefore, as expected, we show that computing the egalitarian rule yields an unstable allocation. The budget distribution of these four rules, the current system, the egalitarian, the balanced allocation \(\varphi\), and the weighted balanced allocation \(\varphi^{w}\), are compared. We organise the paper as follows. Following this introduction, Section 2 is dedicated to reviewing the literature related to the problem in both public economics and game theory. Section 3 defines the tax cooperative game and summarises the main stability results. In Sect. 4, we introduce the budget allocation rules \(\varphi\) and \(\varphi^{w}\). In Sect. 5 we present the extended tax cooperative game associated with \(\alpha\). This parameter allows us to consider the degree of stability of any particular budget allocation. Section 6 is dedicated to applying these new concepts to the Spanish case. Section 7 ends with some final comments and remarks. We place the proofs of theorems, and some additional tables, in “Appendix”.",1
12.0,4.0,SERIEs,22 November 2021,https://link.springer.com/article/10.1007/s13209-021-00251-7,Seasonal adjustment of the Spanish sales daily data,December 2021,Ángel Cuevas,Ramiro Ledo,Enrique M. Quilis,Male,Male,Male,Male,"Economic data provided by tax sources are gaining popularity among economic analysts and forecasters due to its timely availability, reliability, coverage and direct economic meaning. In this way, tax-based data have a clear function in the design of a model aimed at nowcasting and short-term forecasting. Updated and reliable forecasts play a critical role for budgetary planning and for the anticipation of risky situations due to adverse shocks. In particular, the daily information provided by the new Immediate Supply of Information system (SII, Suministro Inmediato de Información), based on the Value Added Tax (VAT) forms and developed by the Tax Agency, opens new perspectives for the integration, on a real-time basis, of tax-based reliable quantitative information with macro-data observed at lower frequencies. This paper presents a system for seasonal adjustment of very high frequency data derived from tax sources. Specifically, the model treats the daily sales time series provided by the SII. Seasonal adjustment (SA) of daily data is necessary, in the same way as happens to monthly and quarterly data, in order to provide a meaningful signal of its underlying evolution. However, this task is notably more difficult than in the case of monthly or quarterly data due to the complexity of its seasonal component, formed by several subcomponents, some of them linked to fractional periodicities and its noisy nature (Ladiray et al. 2018). The paper is organized as follows. The second section presents the main characteristic of the data. The third section develops the econometric methodology, which is implemented in two stages. In the first one, we apply a preliminary treatment of the deterministic effects and, in the second one, we use an univariate structural model, based on an unobserved component representation, to decompose and forecast the daily sales data. The empirical results are presented in the fourth section. The fifth section is focused in predictive evaluation. The paper ends presenting the main conclusions and future developments.",1
13.0,1.0,SERIEs,10 May 2022,https://link.springer.com/article/10.1007/s13209-022-00262-y,Introduction to the special issue in honor of Juan Jose Dolado,May 2022,Laura Mayoral,Evi Pappa,,Female,Female,Unknown,Female,,
13.0,1.0,SERIEs,26 November 2021,https://link.springer.com/article/10.1007/s13209-021-00244-6,Lost in recessions: youth employment and earnings in Spain,May 2022,Samuel Bentolila,Florentino Felgueroso,Juan F. Jimeno,Male,Male,Male,Male,"Spanish youth face huge challenges due to the succession of two deep economic crises in less than a decade. In the Great Recession (2008–2013), the youth unemployment rates reached record levels and young workers also belong to the most affected groups in the current economic crisis caused by the COVID-19 pandemic. Moreover, the delicate position of Spanish youth is not new. The Spanish labor market is a hostile environment for young workers, and none of the recent reforms implemented has improved the functioning of the youth labor market. In the words of Juan J. Dolado, a great economist, a close friend of ours, and a movie buff, Spain is “no country for young people”. A couple of figures make this evident. Over the period 1983–2019, the average unemployment rate for workers aged 20–24 years old was equal to 32.7%, and for those aged 25–29 years old, it was equal to 22.3%. In contrast, in the (then) European Union of 28 countries (EU-28), the corresponding averages were, respectively, 17.8% and 11.5%, in other words, about half the size of the rates in Spain. Furthermore, these poor numbers are not simply the result of the overall unemployment rate being significantly higher in Spain than elsewhere in Europe. Young Spaniards suffer on every front: they have low employment rates, high rates of temporary employment with correspondingly high levels of job rotation and mismatch, and low wages. The economic analysis of the causes of low employment rates and high unemployment rates among young workers can be divided into two strands of research. The first one studies the relevance of education, labor market institutions, and employment policies on the transition from school to work and its consequences for the determination of structural unemployment. A second strand of research analyzes the cyclical behavior of youth unemployment and the scarring effects of entering the labor market during recessions. Starting with the first type of issue, the problems in the school-to-work transition are found on two fronts. The first one is the weak link between education and the labor market. Spain suffers from a high dropout rate—which was especially high during the expansion that preceded the Great Recession—and at the same time, it has a relatively high share of university graduates (mostly in Law, Humanities, and Social Sciences) and a low share of upper secondary education graduates, mostly due to a shortfall in vocational education and training. This peculiar distribution of educational attainment leads to mismatches on the labor market that hamper the labor market integration of youth. The second front is related to labor market institutions, which strongly worsen the employment opportunities of new entrants. With an entrenched dual configuration, both employment protection legislation and the structure of collective bargaining create a strong insider–outsider divide that place a significant burden on youths (Bentolila et al. 2012, 2020; Felgueroso et al. 2018). Beyond the structural causes of youth unemployment and the long-run trends affecting this segment of the labor market—i.e., demographics, technology, and human capital accumulation—the current cohorts of young workers have been very negatively affected by two deep recessions taking place in short sequence. Indeed, the economic impacts of both the Great Recession and the COVID-19 crisis have been particularly acute in comparison with the rest of Europe. The former was associated with the bursting of a housing bubble and the subsequent downsizing of the construction sector—where many school dropouts were employed—and the latter has strongly reduced employment opportunities in the service sector—in which job entry ports for young people were overrepresented. Thus, we should expect the scars from entering the labor market in a recession to be deeper for the current cohorts of young Spaniards than for their European counterparts. To gauge the magnitude of these adverse effects, we document the impact of previous recessions on the labor market outcomes of youth. Furthermore, for individuals with a college degree, we provide estimates of the career effects of graduating in a recession. There is a growing literature on the scarring effects of recessions. Most studies focus on university graduates and base their empirical strategy on the seminal contribution of Oreopoulos et al. (2012) for the case of Canada. This study exploits the variation across cohorts and provinces in the unemployment rate to study how the labor market conditions at graduation condition entrants’ experience profiles of earnings and employment. In line with Oreopoulos et al. (2012), there are several studies that find substantial scarring effects for college graduates in the US (e.g., Altonji et al. 2016; von Wachter and Schwandt 2019; or Rothstein 2021). According to these studies, deep recessions cause a drop in initial earnings of up to 10%, with estimated semi-elasticities of earnings with respect to the unemployment rate in a range between 2 and 3. The negative effects remain significant up to seven years after entry. The drop in initial earnings is due to the combination of fewer days of work and a reduction in the quality of the entrants’s first employer. Furthermore, the studies for the US and Canada point at substantial variety in the strength of the scarring effects by field of study, college attended, and type of program. In general, the scarring effects are weaker and less persistent for individuals with the highest predicted level of future earnings. Finally, Altonji et al. (2016) and Rothstein (2021) document larger than expected losses for individuals who entered the US labor market during or after the Great Recession. Altonji et al. (2016) attribute this trend break to a rise in the cyclical sensitivity of the employment of college graduates. Apart from Canada and the US, evidence on the scarring effects of recessions is available for a range of countries.Footnote 1 As for Spain, Fernández-Kranz and Rodríguez-Planas (2018) use Social Security records that cover the period 1980–2008 to analyze the scarring effects from recessions for males at all levels of education, using a setup based on Kahn (2010).Footnote 2 In this simplified setup, the initial unemployment rate is interacted with a continuous variable that measures entrants’ potential experience rather than years of potential experience fixed effects as in Oreopoulos et al. (2012). According to their findings, the negative effects from entry in recessions fall with the educational attainment of entrants. In particular, for high school graduates, they obtain initial earnings losses in deep recessions of 25% that drop to 3% after 10 years. By contrast, for university graduates, the initial impact is half as big and the penalty ceases to be significant after five years. Our paper offers various contributions to the analysis of youth employment and earnings of young Spaniards. First, our sample period spans the period 1987–2019. Hence, our estimation includes the cohorts that entered during the Great Recession. Furthermore, while our empirical strategy is closer to the one developed by Oreopoulos et al. (2012), we allow for changes over time in the earnings and employment profiles of entrants. This issue is key because inspection of the data reveals a clear negative trend in the initial labor market outcomes of university graduates in Spain. To be more precise, in recessions, we observe a steep deterioration in the initial outcomes of graduates, but in the subsequent recovery, the initial conditions for later cohorts do not recover their pre-recession levels. In other words, a substantial share of the deterioration in labor market outcomes of graduates in recessions is consolidated, causing a trend decline in the initial conditions of university graduates. This result sheds new light on the scarring effects of recessions and casts some doubts about the validity of the standard estimation procedure of Oreopoulos et al. (2012), as it relies on the implicit assumption of a time-invariant relationship between the initial unemployment rate and the experience profiles of entrants. The rest of this paper is organized as follows. We start by documenting in Sect. 2 the trend deterioration experienced in the labor market for youth in Spain over the last three decades, focusing on aggregate youth unemployment and employment rates, and temporary and part-time employment rates, and we discuss developments in education and in the days worked and earnings of young workers. Then, in Sect. 3, we adopt a cohort approach and show, over the same period, the longitudinal evolution of employment and earnings of young workers from job market entry onwards, finding that losses suffered during recessions are not made up in the subsequent expansion. In Sect. 4, we estimate the size of scarring effects. We trace how the state of the labor market at the graduation dates affects the employment and earnings of young workers during their first decade in the labor market. We find that while there is some evidence of scarring effects, the overriding force appears to be a trend worsening of youth labor market outcomes. In Sect. 5, we summarize the conclusions from our analysis.",3
13.0,1.0,SERIEs,29 March 2022,https://link.springer.com/article/10.1007/s13209-022-00261-z,Internship contracts in Spain: a stepping stone or a hurdle towards job stability?,May 2022,Sara De la Rica,Lucía Gorjón,,Female,Female,Unknown,Female,"Spain faces two structural weaknesses in the labour market that have been hindering its proper functioning for more than three decades: abnormally high unemployment rates, which reach 25% in times of recession and do not fall below 8% even in strongly expansive times; and the so-called contractual duality, which, through the instrumentalisation of high firing costs, protects approximately 70% of the working population from unemployment—those who have a permanent/indefinite contract. In contrast, the rest of the working population is forced to hold temporary contracts, of increasingly shorter duration, with frequent entries and exits from unemployment, which leads to high labour instability. This unstable situation mainly affects young people, women and immigrants, regardless of their level of education (Dolado et al. 2000, 2002).Footnote 1 Young people in Spain are particularly affected by these two labour market problems since their unemployment rate is more than twice that of the general population,Footnote 2 reaching over 50% in times of recession. In addition, temporary employment among workers under 30 years of age also exceeds 50%. These anomalies have consequences not only in terms of job instability/precariousness but also affect other areas of the lives of those who suffer such instability, such as a delay in emancipation and, hence, in the formation of a family, which significantly affects other societal outcomes, such as population ageing. Furthermore, there is empirical evidence that this precariousness at the beginning of their working lives has negative consequences for their long-term working lives, as shown by García Pérez and Vall Castelló (2015). Since the mid-1990s, successive governments have tried to address both problems; unfortunately, the initiatives implemented have generally not been successful. From the public authorities, the measures aimed at limiting the youth unemployment rate and mitigating their labour instability include (1) facilitating their access to employment and (2) promoting or encouraging stable hiring. These measures are part of the so-called active labour market policies (ALMP), and in Spain, both access to employment and incentives for stable hiring have been mainly addressed by means of subsidies, either for hiring in general or for stable hiring.Footnote 3 The internship contract (IC), which is the subject of this study, is an example of an ALMP whose purpose is to promote employment insertion as well as theoretical and practical training for young people who have attained secondary or higher education to reach stable and decent jobs. The internship contract law itself establishes that “the fundamental objective of this instrument is to increase labour stability for youth in the chosen area of study by developing professional internships related to the level and field of study, as well as to provide incentives to companies to retain young people in their companies once the internship period is over”. These contracts are subsidised, as will be detailed in the following section, first to encourage the hiring of young people and second to help companies pay for the monitoring of the internship. The specific question we seek to address in this study is whether the internship contract instrument has been an effective ALMP to achieve more stable employment trajectories for its beneficiaries. Beyond the objective of achieving stable employment trajectories, an interesting potential effect of the internship contract, given that it is subsidised, is to facilitate the hiring of young people who would otherwise be unemployed. The subsidy lowers labour costs and, therefore, in principle, should encourage job creation. This is undoubtedly an interesting question, but it is not addressed in this paper due to a lack of adequate information. In particular, we would need individualised and detailed information on the trajectories of unemployed young people, who would be the comparison group; unfortunately,this information is not available. Consequently, we compare the employment trajectories of young people who begin their employment stage through this type of contract, which is subsidised, with similar young people whose entry is also through a temporary contract but not subsidised. We try to address whether the expenditure incurred in subsidising temporary hiring through an IC has any subsequent impact on the trajectories of those who benefit from it compared to those who do not. Unfortunately, the paper cannot address the analysis of whether the IC promotes the hiring of otherwise unemployed youth, a question that is undoubtedly also relevant.Footnote 4 To our knowledge, no one has attempted to assess the analysis of IC to date. Indeed, the empirical literature is very scarce regarding the study of the impact of an internship contract versus an unsubsidised temporary contract. The studies closest to ours address the impact of a paid versus an unpaid internship contract. In particular, O'Higgins and Pinedo (2018), through a survey on European data, analyse the disparities between a paid versus an unpaid internship contract and find positive differences for the subsequent employment path of the paid internship contract. This result seems to be consistent with previous studies, such as Saniter and Siedler (2014), which attempt to assess the impact of paid versus unpaid internship contracts for Germany through the methodology of causal inference. Kopečná (2016) evaluates an internship programme implemented in the Czech Republic as part of a Youth Guarantee programme and finds a positive effect on the income of those who enjoyed it 1 year after its use. On empirical evidence in Spain, the closest study is Jansen and Troncoso-Ponce (2018), who evaluate the impact of apprenticeship contracts, which, unlike internship contracts, are directed to young people whose educational stage has not finished. Jansen and Troncoso-Ponce (2018) find that apprenticeship contracts do not reduce labour precariousness compared to the experience of young people whose labour entry is through unsubsidised temporary contracts. Our study focuses on the group of new entrants into the labour market without any previous professional experience, as previous experience would undoubtedly affect the subsequent labour market trajectory. It is also for this group of new entrants that this instrument was originally created since its objective was to offer a dignified transition from the educational stage to the labour stage. The evidence shows, as seen later, that many young people start an internship contract after several years of previous work experience. However, we focus on the group of new entrants to the labour market, as it helps to better identify the impact of this instrument on the subsequent labour market trajectory. The analysis is approached from two temporal perspectives. The first and most relevant is the short-term context, i.e., after the end of the first contract (either an internship or a nonsubsidised temporary contract). We compare young entrants into the labour market who held an internship contract as their first labour market experience (treated) with a control group of potential IC signers but whose first employment episode is through a nonsubsidised temporary contract. For this time scope, we measure the impact of the internship contract on the following employment outcomes: (1) the probability of firm retention after the end of the contract and (2) the probability of signing a stable contract after the end of the contract, whether in the same company (stayer) or in another company (mover). Finally, we examine whether there are wage differences between the two groups after the end of the first contract, again differentiating between stayers and movers. Second, we analyse possible differences between both groups 2 and 5 years after the finalisation of the first contract. We denote these two points of time as medium and long run. For this time frame, we contrast possible differences between both groups in (1) the probability of having a job, (2) the probability of having a stable contract, and (3) wage differentials. To create a control group, we use the exact matching methodology, i.e., among the potential signers, we select a specific group of individuals identical to the treated group on main observable characteristics and pair each treated individual to one nontreated young worker with the same characteristics. For the sake of brevity, we will refer to these control units as twins. The comparison between the results with and without the exact matching technique allows us to capture the importance of the applied methodology in reducing the heterogeneity between treated and control individuals. As a result, we get as close as we can to identifying the causal effect of the subsidised contracts, although we are aware of the possibility of further unconfounded differences that might be behind the results. The presentation of the results is separated by the short-, medium- and long-term effects. In the short run, the internship contract leads to greater subsequent job instability of the IC beneficiaries compared with their twins, unless the IC beneficiaries leave for another company. In that case, it does appear that IC beneficiaries are ahead of their counterparts in terms of employment stability, understood as the holding of permanent contracts. With respect to wages, the IC has a negative impact in their second employment episode, either for those who stay in the company (stayers) or for those who move to another company (leavers). However, in the medium and long run, the main disadvantages found in the short-term vanish. In particular, the negative impact found earlier on wages disappears, while the employment stability of the beneficiaries of the internship contract is greater. One interpretation is that, although in the short-run firms use these contracts to lower hiring costs, the beneficiaries might send a positive signal to the market that benefits them in the future. In terms of methodology, the exact matching technique clearly reduces the heterogeneity between the treated and control individuals. While the sign of the impacts is generally the same with and without the matching technique, the magnitude of the effect differs greatly. These differences corroborate the need to use an appropriate methodology to achieve robust results. The paper is structured as follows. Section 2 describes the legislative aspects of the internship contract. Section 3 explains the dataset and develops a descriptive analysis. Section 4 describes the empirical strategy used in the analysis, and the results are presented. The last section provides some final conclusions.",1
13.0,1.0,SERIEs,02 December 2021,https://link.springer.com/article/10.1007/s13209-021-00257-1,Temping fates in Spain: hours and employment in a dual labor market during the Great Recession and COVID-19,May 2022,Cristina Lafuente,Raül Santaeulàlia-Llopis,Ludo Visschers,Female,Unknown,Male,Mix,,
13.0,1.0,SERIEs,23 November 2021,https://link.springer.com/article/10.1007/s13209-021-00249-1,So different yet so alike: micro and macro labour market outcomes in Germany and Spain,May 2022,Maia Güell,Cristina Lafuente,Hélène Turon,Female,Female,Female,Female,"The standard comparison between the German and Spanish labour markets features Spain with a much higher and cyclical aggregate unemployment rate than Germany. In Fig. 1, we show that both countries have had similar aggregate shocks in the last 25 years in terms of real GDP growth, yet the evolution of aggregate unemployment rates is very different, as the left hand panel in Fig. 2 shows. The great recession in Spain was characterized by a sharp rise in the unemployment rate until 2013 followed by a sharp decrease; while in Germany unemployment has been falling since 2005. Different labour market institutions are a key factor for understanding the difference in the propagation of shocks to the aggregate unemployment rate. In particular, since the 1980s, Spain and Germany introduced flexibility in their labour markets using very different forms of atypical employment (i.e. non-regular permanent employment). Spain is the OECD country with the highest share of fixed-term or temporary contracts, that is, contracts of fixed duration with negligible firing costs; while Germany is associated with marginal employment and the so-called mini-jobs, which are low paid jobs with a maximum number of hours. Several papers have documented the relationship between temporary contracts/mini-jobs and the unemployment rate. See, for example, Bentolila et al. (2020) for a recent overview of empirical and theoretical research on dual labour markets. Bentolila et al. (2012) offer an analysis of the differential response of unemployment to the Great Recession in France and Spain and its relation to firing costs and the regulation of temporary contracts. Carrillo-Tudela et al. (2021) is a recent and comprehensive analysis of employment transitions after the Hartz reforms in Germany.Footnote 1 Temporary contracts and mini-jobs can be thought as two alternative ways of introducing flexibility in the labour market. They, however, yield different aggregate implications. In Germany, the Hartz reforms and the expansion of mini-jobs have helped reduce unemployment with mixed effects on participation and a rise in the take-up of mini-jobs by women previously out of the labour force (Carrillo-Tudela et al. 2021; Rothe and Wälde 2017). There is no evidence that temporary contracts have increased employment (according to Kahn (2010) with the ECHP in 1996 to 2001). They may have helped to reduce the incidence of long-term unemployment, but not the duration of long unemployment spells in Spain (Güell 2003). Yet, from a worker’s point of view, these different contracts offer a very similar experience: they are just different forms of non-stable, non-permanent employment and yield levels of uncertainty and income that are only one step better than those experienced in unemployment. Real GDP growth (%). Notes: The figure displays the real GDP growth (%) by country. Source: IMF Source: EUROSTAT. The right panel shows the survival plot for all unemployment spells. Dashed lines represent fitted exponential distributions. Sources: SIAB and MCVL administrative data sets Unemployment rates and durations. Notes: The left panel displays the unemployment rate (%) by country  In this paper, we compare the German and Spanish labour markets by studying individual labour market spells in two categories: stable employment spells versus all other spells, that we label as unstable spells. The latter amalgamates any form of unstable job and unemployment.Footnote 2 Forty years ago, before the introduction of atypical forms of employment in Europe, it made sense to focus on employment and unemployment as these two states comprised the bulk of the labour market. However, now that the incidence of atypical forms of employment has grown to a substantial share of the labour market, it is informative to aggregate all forms of labour market experience that are not stable permanent employment. That is, we merge unemployment and all types of atypical employment with volatile income and status. We implicitly consider that workers value these states much less than stable employment and engage in (stable) job search while in them. Analysing the distribution of the duration spent in this unstable state and presenting a comparison between Spain and Germany is relevant to an understanding of workers’ welfare in the two countries. We use comparable data from each country: administrative data social security records. A more conventional approach would be to simply focus on unemployment spells. The right panel in Fig. 2 displays the survival rate of unemployment spells by duration. As expected, the probability of leaving unemployment is much lower in Spain at any duration, particularly in the first 3 years. The average hazard rate in Germany is 25% versus 10% in Spain. This is a large difference. Our main finding is that, in contrast with this, the distribution of durations of unstable spells is much more similar in the two countries. This is in a context where aggregate shocks faced by the two economies are very similar in the sample period, but labour market reforms have followed different trajectories. In the face of tight employment legislation on permanent contracts, the use of temporary contracts is substantial in both countries, as is the “mini-job” in Germany. We show that, in spite of these differences, the micro-level experience of unstable spells is more similar between the two countries than the standard picture of unemployment durations would suggest. The rest of the paper is organized as follows. Section 2 describes in more detail the labour market institutions in both countries and documents the incidence of atypical employment in each country. Section 3 describes the administrative data used in each country. Section 4 presents our survival analysis. Finally, Sect. 5 concludes.",1
13.0,1.0,SERIEs,07 December 2021,https://link.springer.com/article/10.1007/s13209-021-00252-6,Higher education decisions and macroeconomic conditions at age eighteen,May 2022,Jennifer Graves,Zoë Kuehn,,Female,Female,Unknown,Female,"Business cycles impact individuals in profound ways. Among the most widely studied effects of recessions are the ones on educational and labor market outcomes, because economic downturns alter the opportunity costs between education and labor market participation. However, while there exists a sizable literature that has studied the impact of macroeconomic conditions on higher education decisions for single countries, there is a lack of comparative cross-country studies and in particular regarding multiple higher education decisions which are potentially related. The current paper tests how macroeconomic conditions experienced at age eighteen affect a variety of decisions in post-secondary and tertiary education. To this end, we merge data for 18 countries from the Programme for the International Assessment of Adult Competencies (PIAAC) on individuals aged 25 to 50 with data on youth unemployment for 1980–2005, the years these individuals turned eighteen. Apart from demographic controls and some variables on family background, PIAAC provides measures for cognitive and non-cognitive abilities. Hence, our estimations compare individuals of similar family backgrounds and abilities, and we test how macroeconomic conditions experienced at age eighteen affect the following higher education decisions: (i) enrollment (ii) dropping-out, (iii) type of degree completed, (iv) area of specialization, and (v) time-to-degree. Given a natural path through high school determined largely by age, this “treatment” can be considered exogenous. Conditional on various detailed controls, our estimates hence provide the causal impact of macroeconomic conditions on higher education decisions. When youth unemployment increases, the opportunity cost of continued schooling falls. Everything else equal this should result in increased enrollment. Likewise, if higher education is a way to signal quality to potential employers, we would expect more individuals to enroll when labor markets are tighter. On the other hand, if individuals believe that high unemployment situations will outlast their time in higher education, they may perceive a lower expected return and choose not to enroll. Additionally, economic downturns can also reduce one’s ability to pay for higher education. However, ability to pay could not only affect enrollment but also the probability of college completion if poor macroeconomic conditions persist. In addition, if enrollment increases due to poor labor market prospects, then the marginal student who enrolls is most likely less prepared for college. We would therefore observe lower completion rates as more students drop out. On the other hand, if enrollment falls during recessions, the remaining students may be those best prepared, and dropout rates could be lower and degree completion rates higher. Beyond enrollment and dropout behavior, other higher education decisions could be affected by macroeconomic conditions. For example, college major choice could be altered if students use their specialization to signal employability in a tight labor market. Time-to-degree can also be affected if students are able to remain in college to “wait out the storm.” These decisions may be interconnected should the choice of major also impact time-to-degree. Our results highlight different mechanisms at work across different geographies. For Anglo-Saxon countries, among those experiencing high youth unemployment at age 18, we estimate lower rates of bachelor’s and master’s degree completion, with neutral-to-positive estimates for the completion of any post-secondary degree (including vocational degrees). This is paired with no impacts on enrollment or dropping out of post-secondary programs. Taken together, during economic downturns, as financial constraints tighten, relatively more students seem to opt for professional rather than more expensive bachelor’s degrees. In Southern Europe, on the other hand, where tuition fees for higher education tend to be lower, we find results consistent with reduced opportunity costs of studying during recessions. For individuals experiencing higher youth unemployment at age eighteen, we estimate an increase in enrollment with mixed outcomes. We see a higher probability of completing any post-secondary degree (including vocational, bachelor’s and master’s degrees or higher) as well as an increase in dropout rates. We also find an increase in STEM (Science Technology, Engineering, Mathematics) specializations. Our results for Western Europe are similar to those for Anglo-Saxon countries, and we also find that higher unemployment rates experienced at age 25, i.e., 7 years later, result in longer time-to-degree. This could be indicative of individuals choosing to “wait-out” difficult macroeconomic times by remaining longer in higher education. Finally, for Scandinavian countries, we observe reduced enrollment, lower degree completion rates, and shorter time-to-degree for those experiencing higher youth unemployment at age 18. This is in line with reduced ability to pay or lower perceived future returns to higher education playing a larger role than reduced opportunity costs. Initially, this finding may seem counterintuitive because universities in Scandinavian countries generally charge no tuition fees, and states tend to provide additional financial aid for those pursuing higher education. However, Scandinavian countries are also characterized by individuals leaving the parental home at very young ages, relatively high costs of living as well as some of the lowest contributions from family toward financing higher education (Orr et al. 2008; Schnitzer and Zempel-Gino 2002; Schnitzer et al. 2005). As a result, in Finland, Norway, and Denmark the share of students who claim that they work to cover living costs and that without their paid job they would not be able to afford their education is well above the European average (see Eurostat 2020; Masevičiūtė et al. 2018). We also test for the heterogeneity of our findings along gender and parental background. Results differ across geographies. For instance, in Southern Europe, increased enrollment and dropout rates for individuals experiencing higher youth unemployment at age 18 are driven by women, while men are behind the positive effects on STEM specializations. For Southern European, Western European, and Anglo-Saxon countries, we find that in particular individuals from more advantageous family backgrounds are able to “wait-out” poor labor market prospects in graduate programs. Our different results across the four geographies highlight the importance of analyzing the effects of macroeconomic conditions on higher education decisions for different education systems and labor markets. In addition, by looking at various components of higher education together, we are able to obtain a clearer picture of how during economic downturns potential mechanisms linked to lower opportunity costs of education and reduced ability to pay interact to determine higher education decisions. Finally, the use of PIAAC data allows us to control for individuals’ abilities, which are important determinants of higher education decisions but are often not included in other datasets. The remainder of this paper is organized as follows: The next section provides background information on the existing literature as well as on the institutional context of higher education in our four geographies. Section 3 describes our data, and in Sect. 4 we outline our empirical strategy. Section 5 presents and discusses our results including robustness checks and heterogeneity analyses. Finally, Sect. 6 concludes.",
13.0,1.0,SERIEs,04 October 2021,https://link.springer.com/article/10.1007/s13209-021-00243-7,Work and children in Spain: challenges and opportunities for equality between men and women,May 2022,Claudia Hupkau,Jenifer Ruiz-Valenzuela,,Female,Female,Unknown,Female,"Over the past 25 years, Spain has undergone a striking convergence between women’s and men’s participation in the labour market. In 1990, for every 100 men in the labour force there were only 50 women working, compared to 70 women for every 100 men in Europe overall. By 2010, Spanish women’s labour market participation had overtaken that of women in the European Union, with about 88 active women for every 100 active males, compared to 86 in the EU overall (Fig. 1a). Women have also drastically increased their share of total hours worked, from 29 percent in 1990 to 42 percent in 2019, as can be seen in Fig. 1b. However, as in other developed countries, gender convergence in Spain has stalled since the early 2010s. In this paper, we use data from the Spanish Labour Force Survey (Encuesta de Población Activa, or EPA) and the Living Conditions Survey (Encuesta de Condiciones de Vida, or ECV) to study the evolution of gender gaps in several key labour market indicators over the past 15 years. We then analyse these gaps separately for individuals with and without children, to get a better understanding of the extent to which children might matter for explaining why convergence has stalled in recent times.Footnote 1 In light of the Covid-19-related recession, we separately investigate the impact of the pandemic on gender inequality in labour market outcomes in Spain. Finally, we review the empirical evidence on policies that might help reduce the gender gap associated with the arrival of children. Our results show that despite convergence between men and women in participation rates, women still fare worse on other important labour market indicators. With the exception of a period after the Great Recession, which hit male-dominated sectors disproportionately, unemployment among women remained several points higher than that of men throughout the whole period. Despite the increase in working hours seen in Fig. 1b, the proportion of women working part-time has barely changed in the last 15 years and remains well above 20 percent. Men’s part-time employment share has stayed below 10 percent throughout the period. Women are also more likely to hold job-insecure contracts. Moreover, only about 3 percent of women in Spain work in top-level occupations, such as directors and managers—about half the fraction of men. This is despite the fact that the share of men in such occupations has been decreasing steadily over the last decade. Source: OECD Labour Force Statistics. The left-hand side graph a shows the female to male labour force participation (LFP) rate. A value of one indicates that the female LFP rate is equal to that of men. A value below (above) one indicates that the female LFP rate is below (above) that of men. The LFP rate is defined as the share of the working age population (15–64) that is either employed or unemployed. The right-hand side graph b shows the female share of total annual hours worked among all workers. Total annual hours worked have been computed as the average usual weekly hours worked on the main job multiplied by the total number of employed individuals Trends in female employment and hours worked Labour market inequalities in Spain are further aggravated among people with children, irrespective of the indicator used. We find that convergence in labour force participation rates has stagnated for women with children aged 15 and below over the last seven years, while convergence has continued (albeit at a slower pace than before) among individuals without children. By 2019, women with children under 16 years of age were about 7.5 times more likely than men with children of the same age to work part-time, twice as likely to be unemployed and 20 percent more likely to hold temporary contracts (as opposed to a more job-secure, permanent contract). The gender gaps for people without children in all these indicators are much smaller. We show that it is unlikely that these differences in part-time work uptake are due to women’s preferences alone: by the end of the 2010s, over a third of women with children under five and working part-time stated that they would like to work more hours. This increases to well over half of the women working part-time with children aged five to 15. Additionally, we show that substantial gender gaps in gross annual income exist, but only among people with children. Using the Living Conditions Survey for the period spanning 2008 to 2019, we find that among full-time workers, women with children aged zero to 15 have a gross annual income of between 78 and 87 percent of that of men. This is consistent with results reported in de Quinto et al. (2020), who find that mothers’ earnings in Spain drop by 11 percent one year after childbirth and by 28 percent after ten years, while those of fathers remain unchanged. We also find that the gender gap in the labour market effect of the Covid-19 crisis is larger among people with children. Drops in employment rates were three percentage points higher for mothers than for fathers by the fourth quarter of 2020, and mothers were more likely to have moved into inactivity during the pandemic. Among people without children, we do not find gender differences in the impact of the pandemic on these outcomes. However, in line with recent findings by Bluedorn et al. (2021), we show that the gender gap among people with children had largely been reversed by the first quarter of 2021. It remains to be seen whether the temporary labour market shocks for mothers will have longer-lasting impacts on their careers and on the division of labour in the household. The fact that the arrival of children seems to be one of the main drivers of the remaining gender gap in labour market outcomes suggests that the focus of policy makers should lie in policies that reduce the unequal impact of childbearing on the careers of men and women. Our review of the existing evidence suggests that policies that make it easier to combine work and family for women, such as financial incentives in the form of tax credits for working mothers and subsidized or free childcare for very young children, can positively affect women’s employment probabilities and hours worked (Olivetti and Petrongolo 2017). These latter policies have also been shown to be effective in tackling a related issue: steadily declining fertility rates in developed countries. However, such policies are likely to be more effective if combined with advances in breaking up traditional gender roles. Our paper updates and extends the existing literature on gender gaps in the labour market for the case of Spain. The analysis is most closely related to Guner et al. (2014), who analyse the evolution of gender gaps in Spain from the late 1970s to 2013. We increase the time horizon of analysis up to 2020 and pay special attention to (1) the differences in the gender gaps between individuals with and without children and (2) the effect of the Covid-19 pandemic on existing labour market gender gaps. Additionally, the review of the literature on family policies aims to contribute to the debate on what policies work to reduce gender gaps in the labour market. Source: Own calculations based on EPA microdata. Seasonally adjusted series from Q1/2005 to Q4/2019. Sample of all individuals within the working-age population (16–64 years). Participation rates are computed as the total active population (employed and unemployed) over the total working-age population. Unemployment rates are computed as the total number of unemployed over the total active population. Temporary contracts show the share of individuals with a temporary contract among all those in employment. Part-time employment rates show the share of individuals working part-time among those in employment. All variables are derived using cross-sectional weights. Shaded areas represent 95% confidence intervals Labour market outcomes for men and women (2005–2019) The remainder of this paper is organised as follows. In Sect. 2 we review gender gaps in the Spanish labour market over the past 15 years. Section 3 looks more in detail at how parenthood affects labour market outcomes for Spanish men and women. Section 4 reviews the existing evidence on family policies and their impact on female employment and fertility. Section 5 concludes.",5
13.0,1.0,SERIEs,25 November 2021,https://link.springer.com/article/10.1007/s13209-021-00256-2,Gender distribution across topics in the top five economics journals: a machine learning approach,May 2022,J. Ignacio Conde-Ruiz,Juan-José Ganuza,Luis A. Puch,Unknown,Unknown,Male,Male,"Despite the efforts undertaken for the whole economic profession to fight against discrimination, women are underrepresented in academia. Lundberg and Stearns (2019) make an assessment of the presence of female economists in the profession, and they report a very slow improvement in the last two decades. The picture is as follows. In the beginning of this century, 35% percent of PhD students and 30% of assistant professors were female. Since then, these numbers have not increased.Footnote 1 Additionally, Siniscalchi and Veronesi (2020) summarizing Chevalier (2020) (Report of the Committee on the Status of Women in the Economics Profession) point out that the proportion of women assistant professors in the “top 10” schools has declined to less than 20% by 2019. They document also that female have been less successful in promoting to tenured associate or full professors.
 In economics, the tenure path often requires to publish in the top five (Top 5, or just T5) journals, namely American Economic Review (AER), Econometrica (ECA), Journal of Political Economy (JPE), Quarterly Journal of Economics (QJE) and Review of Economic Studies (REStud). Heckman and Moktan (2020) analyze the tenure decisions of the top 35 Economics departments in the USA, and they conclude that T5 publications are a very powerful explanatory variable of the promotion to tenure. Publishing in a T5 is becoming the main goal of young professors in economics because their professional career may depend on succeeding on this target. In addition, the content published in these journals is also determining the path of research in economics. As a consequence of these facts, the competition to publish in any of these journals has increased in recent years. Card and DellaVigna (2013) analyze the publication records in the Top 5 from 1970 to 2012 showing that the acceptance rate has fallen from 15% (1970) to 6% (2012). They explain this fact as a combination of the increasing number of submissions and a declining number of published papers. Card et al. (2019) further analyze the publication records from two of the T5 journals (the QJE and REStud), together with the Journal of European Economic Association and the Review of Economics and Statistics. They report that the current proportion of accepted papers is 3%. Is the T5 entry barrier harder for women? The answer provided by Card et al. (2019) to this question is ambiguous. On the one hand, these authors do not find any gender biases in the refereeing process, and editors decisions are gender-neutral conditional on the referee advises. On the other hand, they find that conditional on referee process, female authored papers end up accumulating more citations in later years.Footnote 2 A potential explanation for this second result is that journals hold female-authored papers to higher standards. Hengel (2020) uses readability scores and finds that female-authored papers are better written and improve during peer review and as they publish more papers. These results could be related to some “horizontal” features or characteristics of female-authored papers that lead to more citations or better writing standards, but not to higher acceptance rates in the editorial process. As Card et al. (2019) control by research fields (JEL codes), their results may be linked to more subtle horizontal differences. For instance, in the same research field, males may choose a more theoretical approach and females a more applied perspective (which tends to be more cited or subject to less complicated wording), leading to particular career outcomes.Footnote 3 Several papers have pointed out persistent gender differences in the choice of research fields in economics. Dolado et al. (2012) analyze the gender distribution of research fields in the top-50 economics departments in 2005, and show that women are unevenly distributed across fields. Similarly, Chari and Goldsmith-Pinkham (2017) use data from submissions to the National Bureau of Economic Research Summer Institute (2001–2016) and show that the distribution of female researchers is not uniform across fields. From these, we learnt that women are particularly underrepresented in macro, finance and economic theory, and more prevalent in labor or applied microeconomics fields. Beneito et al. (2021) find similar results using data from the annual AEA meetings from 2010–2016, while Lundberg and Stearns (2019) focus on PhD dissertations in Economics from 1991–2017, in almost all major PhD-granting departments in the USA. Using the JEL code for research areas, they find that women are more prone to study topics in Labor and Public Economics than in Macro and Finance. They also show that this pattern has not changed over time.
 We want to contribute to this literature in two directions. First, we focus on exploring the gender horizontal distribution across research topics in the leading economics journals. We do so by using a new methodological approach based on machine learning techniques. This classifies our abstracts’ database into latent topics. We collect all the articles published in T5 journals for the period 2002–2019. We obtain 5311 articles, and we keep track for each article of the authors’ names, year of publication, journal and the abstract. With this information, we can provide a very accurate picture of the performance of men and women publishing record in these leading journals. Our primary objective is to describe what these latent topics are and the gender distribution across them. Notice this is a very particular sample of researchers though. Second, from the universe of algorithms for topic modeling we implement and develop the structural topic model (STM) developed by Roberts et al. (2019). This choice is because the algorithm allows to incorporate document-level meta-data into a probabilistic text model. Precisely, we keep track of journal names and publication years as covariates to improve the estimation of the prevalence of topics in our data. Our abstracts come from different sources and different periods of time, so it is natural to allow this meta-data to affect the frequency with which a topic appears. The output of the algorithm is a stochastic model that generates latent topics and allocate the documents to them in a probabilistic way. The main advantage of this unsupervised machine learning approach is that latent topics are mixtures over words where each word has a probability to belong to the different topics. Therefore, these topics can capture, conditional on covariates and without human intervention, research fields, information regarding the style of writing, methodology, conversational patterns or even different ways of thinking. We start by identifying the number of latent topics for which the stochastic model fits best our data. The result is that female authors are unevenly distributed across latent topics. It turns out that female prevalence dispersion is higher across these topics compared to other approaches. Moreover, we show that although the proportion of females is slightly increasing among the population of T5 authors over the years, the identified horizontal differences persist. We compute the empirical distribution of latent topics by gender and we show some striking differences between male and female expected proportions. We want to emphasize the importance of these results, not only because latent topics may capture subtle horizontal differences, but also because the gender differences we estimate are “automatically” generated given the documents, without any arbitrary allocations to particular categories (as JEL codes, or declared areas). Thus, they are possibly more robust. Notwithstanding, the choice of the number of latent topics, even if optimal as we discuss, is subject to clustering issues. To address these issues, we also choose to reduce the number of topics the algorithm has to generate, and in order to capture the mixtures of words that more closely resemble to research areas. There is a trade-off when choosing ex ante the number of latent topics. On the one hand, a relatively high number of topics usually fits better the data. On the other hand, a lower number of latent topics facilitates the broad semantic interpretation of them. In our setting, a lower number of topics turns out to make them closer to traditional research fields. Consistently with our main findings, we corroborate the uneven distribution of topic/research fields by gender, but now, much more in line with the existing literature cited above. Thus, we can also discuss the link between the existing literature and our class of probabilistic results. Our approach provides complementary evidence from previous literature over horizontal research differences between males and females. The idea is that the larger set of research topics may allow to identify more precisely the gender gaps, and what is more important, may help to understand the driving forces behind these gaps. There are several channels for which the gender differences in the choice of research topic that we identify can have an impact on the probability of publishing in top journals, earning tenure and in general on career success. Conde-Ruiz et al. (2017, 2021) and Siniscalchi and Veronesi (2020) provide two dynamic mechanisms that may explain how “horizontal” gender differences, together with an initially uneven distribution of gender researchers, may generate an unintentional discrimination trap linked with the functioning of academic organizations (journals, departments, etc.). In particular, Conde-Ruiz et al. (2017, 2021) analyze a promotion setting in which workers’ skills are assessed by committees whose members have different abilities to evaluate workers’ signals (they are better at evaluating workers from the same group). This “homo-accuracy” assumption naturally translates to the present academic setting, where promotions and editorial processes are done by “committees” and where evaluators making research in the same research field are able to assess better the underlying quality of the candidate. Under this “homo-accuracy bias,” the group that is most represented in the evaluation committee generates more accurate signals, and, consequently, has a greater incentive to invest in human capital. This gives rise to a discrimination trap. If, for some exogenous reason, one group is initially poorly evaluated (less represented into evaluation committees), this translates into lower investment in human capital of individuals of such group, which leads to lower representation in the evaluation committee in the future, generating a persistent discrimination process. Siniscalchi and Veronesi (2020) focus specifically on the academic labor market and point out a similar unintentional discrimination trap linked to the so-called self-image bias. Research evaluation is biased toward young researchers with similar characteristics to them. The authors build up an overlapping-generations model with two groups of researchers with equally desirable (but a little bit different) research characteristics and identical ex ante productivity distributions. If one group is slightly over-represented into the evaluation group, this group (and its specific research characteristics) may dominate forever. These theoretical results go in line with the empirical findings of Dolado et al. (2012) that show that the probability for a female researcher to work on a given field is positively related to the share of women already working on that field (path-dependence). The proportions these authors find based on JEL codes are very similar to what we find automatically at the same level of aggregation, but we can set forth a lot more field idiosyncrasy under an extended optimal topic choice. At the end of the paper, we discuss various issues for further research in related applications. The paper is organized as follows: the next section presents the raw data and the descriptive analysis of the patterns of publication in T5 journals. Section 3 presents the structural topic model. Section 4 studies the gender differences in the latent estimated topics. Section 5 extends the model to analyze topics as research fields. Last section concludes, and in Appendix we explore several extensions and provide details about the functioning of the structural topic model (STM) algorithm.",2
13.0,1.0,SERIEs,21 October 2021,https://link.springer.com/article/10.1007/s13209-021-00250-8,From He-Cession to She-Stimulus? The labor market impact of fiscal policy across gender,May 2022,Alica Ida Bonk,Laure Simon,,Female,Female,Unknown,Female,"Despite substantial progress in the labor market fortunes of women over recent decades, gaps in wages and employment rates between male and female workers remain significant. In addition, gender differences in industry composition can generate cyclical fluctuations in labor market gaps, as men tend to be employed in sectors more exposed to business cycles.Footnote 1 Notably, young, less-educated and blue-collar men are particularly strongly affected.Footnote 2 The role of fiscal policies in reducing inequalities has recently received increasing interest in the literature, with less attention paid to the gender dimension. Evaluating the ability of government spending to address both policy goals, i.e., to reduce inequalities not only within gender (to assist crisis-hit male groups) but also between genders (to close gender gaps), is important to shed light on potential trade-offs involved.Footnote 3 We find that these trade-offs depend crucially on the type of public expenditure considered. Using micro-level data for the U.S. from the Current Population Survey (CPS), our study provides policy-making insights on the importance of the composition of government expenditure for understanding the impact of fiscal shocks on labor market outcomes across gender. We also examine the impact on demographic subgroups to assess whether fiscal expansions that close gaps can simultaneously offset inequitable business cycle impacts that particularly affect some categories of male workers. Our main findings can be summarized as follows. First, the composition of fiscal shocks matters. Spending on the government wage bill narrows gender gaps in wages and employment rates, while government purchases from the private sector and investment expenditure tend to stimulate men’s wages relatively more than women’s. These results are likely driven by spending components that target specific occupations and sectors which differ in their gender composition. Second, promoting gender equality through fiscal expansions is not fully compatible with offsetting other types of inequalities. The spending component that best closes gender gaps has adverse effects on labor market outcomes of cyclically vulnerable male subgroups: young, less-educated and blue-collar workers. Similarly, investment spending, which fosters employment of these crisis-hit men, is not able to reduce gender inequalities but rather contributes to widening them. Government spending can impact labor market outcomes unequally across gender for four main reasons. First, because men and women sort into different occupations, their labor demand will shift to different extents following fiscal shocks. Such shifts will depend on which type of government spending is boosted. This motivates us to distinguish between different fiscal components in our analysis. Second, since women are more mobile across industries and occupations,Footnote 4 they may be the main beneficiaries of higher wages and expanded employment opportunities after a fiscal expansion. Third, there is solid empirical evidence that female labor supply is relatively more elastic than male labor supply.Footnote 5 Consequently, female employment may respond more strongly than male employment to fiscal shocks. Fourth, women taking up jobs may hire (usually female) caregivers for children and elderly dependents, inducing second-round employment and wage effects. These insights can be valuable for macroeconomists and policy makers. First, our results help to gauge to what extent government expenditure is able to “assist those most impacted by the recession,” which was the explicit purpose of the American Recovery and Reinvestment Act of 2009.Footnote 6 Second, our analysis is insightful for policy makers whose goal is to promote female employment and gender equality, independently of the cycle.Footnote 7 Conversely, this paper highlights the potential damaging effect that cutting government expenditure, especially the wage bill, may have by widening existing gender gaps. Hence, we underline the gender non-neutrality of budgetary decisions and substantiate the importance of implementing “gender budgeting” as suggested by the International Monetary Fund (2017) and the European Parliament (2015). Third, our analysis hints at the importance of encouraging women’s labor force participation as this may increase the effectiveness of fiscal policy as an aggregate stabilization tool. To measure the effects of fiscal policy shocks on gender gaps in the labor market, we estimate several vector autoregressive models using Bayesian estimation techniques. Following Mountford and Uhlig (2009), we identify the fiscal shocks using an agnostic sign restriction approach. The main advantage of this identification strategy is that it allows us to eliminate the confounding influence of other macroeconomic shocks: namely, business cycle, monetary policy and tax revenue shocks. We examine the impulse response functions (IRFs) of gender gaps in wages and employment rates to different types of government spending shocks. Our study encompasses the analysis of two dimensions of heterogeneity. First, we investigate whether the effects of fiscal policy shocks on gender gaps differ depending on the type of public expenditure. Second, we explore how the effects vary across male and female workers with different characteristics, such as age, education and occupation. This paper relates to a strand of the literature that reports heterogeneous effects of fiscal policy across households with different characteristics (such as Giavazzi and McMahon 2012; Misra and Surico 2014 and Anderson et al. 2016), and across industries (notably, Nekarda and Ramey 2011 and Bredemeier et al. 2020). Several studies have emphasized the crucial role of industry composition in shaping gender differences in labor market outcomes, including Hoynes et al. (2012), Olivetti and Petrongolo (2014) and Bredemeier et al. (2017b). However, despite a growing interest in the evolution and the determinants of gender gaps in the labor market,Footnote 8 the literature on the impact of fiscal policy on gender equality is scarce. A few recent studies document that fiscal expansions stimulate primarily female employment, in particular Bredemeier et al. (2017b) and Akitoby et al. (2019). These papers focus on the effects of total government spending. Our main contribution to the existing literature is to explore the effects of various components of public expenditure. We argue that who benefits from fiscal stimuli depends on the type of expenditure under consideration. We also analyze labor market outcomes for male subgroups that are hurt most during recessions to better understand the trade-offs involved when attempting to close gender gaps. Furthermore, our identification strategy is able to better isolate the variations in fiscal policy variables from automatic responses to other macroeconomic shocks. The remainder of this paper is structured as follows. Sections 2 and 3 describe the data and the econometric approach. Results are presented in Sect. 4 and robustness checks and extensions are described in Sect. 5. Section 6 concludes by offering directions for future research. The appendices contain some stylized facts about the components of government expenditure and gender compositions across occupations and sectors. Furthermore, we provide a description of the data and of the algorithm used for estimating the impulse response functions.",
13.0,1.0,SERIEs,30 July 2021,https://link.springer.com/article/10.1007/s13209-021-00240-w,Automation and sectoral reallocation,May 2022,Dennis C. Hutschenreiter,Tommaso Santini,Eugenia Vella,Male,Male,Female,Mix,,
13.0,1.0,SERIEs,02 December 2021,https://link.springer.com/article/10.1007/s13209-021-00246-4,Primary elections and electoral outcomes: evidence from the Spanish Socialist Party,May 2022,Riccardo Ciacci,Ana Garcia-Hernandez,Antonio Núñez-Partido,Male,Female,Male,Mix,,
13.0,1.0,SERIEs,23 November 2021,https://link.springer.com/article/10.1007/s13209-021-00248-2,Dynamic factor models: Does the specification matter?,May 2022,Karen Miranda,Pilar Poncela,Esther Ruiz,Female,Female,Female,Female,"In recent decades, dynamic factor models (DFMs) have been widely used to represent comovements within large systems of macroeconomic and financial variables, in which the cross-sectional dimension is often relatively large compared with the time dimension; see Stock and Watson (2017) for the importance of DFMs in time series econometrics. DFMs generally assume the existence of a small number of unobserved factors capturing the comovements in the system.Footnote 1 Two main types of procedures for factor extraction are popular in the related literature. First, in many applications, factors are extracted using nonparametric procedures based on principal components (PC), which are attractive because they are computational simple and have well-known theoretical properties. In particular, PC is consistent under mild conditions and, as far as the factors are pervasive and the idiosyncratic dependence is weak, it is robust to the underlying dependence of common factors and idiosyncratic components. As a consequence, PC procedures are very popular for factor estimation and several excellent surveys are available in the literature; see, among others, Bai and Ng (2008a) for a technical survey on the econometric theory for PC. However, when the common factors and/or idiosyncratic components are serially dependent, PC procedures do not use this information and, consequently, they are not efficient. Alternatively, after casting the DFM as a state-space model (SSM), factors can be extracted using Kalman filter and smoothing (KFS) procedures. One important feature of these procedures is that they open the door to Maximum Likelihood (ML) estimation of the model parameters. Furthermore, KFS is also very flexible, allowing to handle in a straightforward way data characteristics often observed in practice as, for example, missing data, mixed frequencies, seasonal dependencies, nonstationarity or regime-switching nonlinearity. Moreover, KFS procedures are also of interest in empirical applications because they allow incorporating restrictions on the factor loadings, as in multi-level DFMs, or on the idiosyncratic components, and to perform counterfactual exercises; see, for example, Banbura et al. (2011) for multi-level models and Luciani (2015) for counter-factual analysis. However, the main drawback of KFS is that it requires full specification of the dependence of the common and idiosyncratic components, opening the door to potential misspecification; see Poncela et al. (2021) for a very recent survey on KFS for factor extraction in DFMs. There are few papers looking at the effects of the misspecification of the factors on factor extraction and forecasting and all of them focus on factors extracted using PC. Boivin and Ng (2006) conclude that overestimating the number of factors affects the precision with which they are estimated and the forecasting results, while Barigozzi and Cho (2020a) also conclude that overestimating the number of factors could yield non-negligible estimation errors. In an empirical application forecasting GDP growth for Germany and France, Barhoumi et al. (2013) show that not necessarily more factors imply better forecasting. Gonçalves et al. (2017) analyzing the same data set considered in this paper, conclude that the forecasting ability depends on the specific combination of eight PC factors used in factor-augmented predictive regression. Finally, Breitung and Eickmeier (2011a) conclude that, if the cross-sectional dimension is large, the dynamic properties of the factors are not important for factor extraction and forecasting. This paper contributes to the literature by analyzing the empirical consequences on factor estimation and in-sample predictability and out-of-sample forecasting of extracting factors using not only PC but also KFS under various sources of potential misspecification. In particular, we consider factor extraction and forecasting when assuming different number of factors and different factor dynamics. The analysis is carried out extracting factors from the ubiquitous data base of US macroeconomic variables described by McCracken and Ng (2016) and forecasting some key US macroeconomic magnitudes. Factor extraction procedures have been previously being compared using this data set; see, for example, Poncela and Ruiz (2015) and the references therein. However, as far as we know, the empirical properties of KFS extraction under potential sources of misspecification have not been analyzed before when extracting factors from the same data set; see, Aruoba et al. (2009) for the importance of comparing factor extraction procedures in the context of the same data set. In the particular US macroeconomic data set analyzed in this paper, we show that specifications with more factors and more lags are favored in-sample when looking both at log-likelihood ratio tests and at measures of fit of factor-augmented predictive regressions. However, increasing the number of factors and/or their lag structure does not always lead to an increase in the out-of-sample forecast precision with the out-of-sample mean square forecast errors (MSFEs) being generally minimized when forecasts are based on simple models with one factor extracted using KFS and modelled as an AR(1) process. It is important to note that these results might not be applicable beyond the macroeconomic system considered in this paper. Whether they can be generally applicable is an interesting issue that is beyond our objectives. Careful Monte Carlo experiments could be designed to analyze their general applicability. The rest of the paper is organized as follows. Section 2 briefly describes the representation of DFMs as SSMs and how factor extraction can be performed by PC and KFS. In Sect. 3, the factors are extracted from a system of US macroeconomic variables under the assumption of serially uncorrelated idiosyncratic components. We analyze the differences in terms of point and interval estimation of factors, in sample prediction and out-of-sample forecasting, when factors are extracted using PC and the KFS under different assumptions on the number of factors and their dynamic dependence. Section 4 concludes the paper.",4
13.0,1.0,SERIEs,11 December 2021,https://link.springer.com/article/10.1007/s13209-021-00247-3,Moment tests of independent components,May 2022,Dante Amengual,Gabriele Fiorentini,Enrique Sentana,Male,Female,Male,Mix,,
13.0,1.0,SERIEs,23 November 2021,https://link.springer.com/article/10.1007/s13209-021-00254-4,A tale of three cities: climate heterogeneity,May 2022,María Dolores Gadea Rivas,Jesús Gonzalo,,,,Unknown,Mix,,
14.0,1.0,SERIEs,07 January 2023,https://link.springer.com/article/10.1007/s13209-022-00271-x,The aggregate effects of government income transfers shocks: EU evidence,March 2023,Susana Párraga Rodríguez,,,Female,Unknown,Unknown,Female,"Governments around the world responded swiftly to the economic crisis caused by the covid-19 pandemic. Among the budgetary measures adopted to mitigate the adverse effects on households highlighted cash payments and the reinforcement of government income transfers (see, for example, Cuadro-Sáez et al. 2020; Liu et al. 2021; Kubota et al. 2021). Sound economic policy calls for a quantitative assessment of the adopted measures and benchmarks such as fiscal multipliers or the marginal propensity to consume (MPC). However, the question of what are the aggregate effects of government income transfers shocks has received comparatively little attention in the literature; see, for example, Ramey (2019) literature review of the renaissance in fiscal research in the last decade. This paper contributes to the existing literature estimating the aggregate impact of government income transfers shocks using a panel dataset of 22 EU Member States over the sample period 2007–2015. Specifically, I estimate the multiplier effect and the responses of aggregate expenditure components and labour market indicators to changes in old age pensions. Empirical evidence on the subject is scarce and has focused on the effects that changes in income have on private consumption expenditures. In the framework of the permanent income hypothesis, Poterba (1988) estimates that a $1 increase in transitory income due to the US tax rebates of 1975 raised spending of non-durables and services by about 12–24 cents. Wilcox (1989) finds that a predictable 10% increase in US social security benefits raises durable goods purchases by 3% in the same month. Romer and Romer (2016) construct a series of legislated increases in social security benefits in the USA from 1951 to 1991 and study the effect of innovations to their narrative variable on private consumption. They find that permanent benefit increases have a significant impact on consumption upon impact. This paper complements previous work in Párraga-Rodríguez (2018, 2022) along three dimensions. First, while my previous research focused on a single-country analysis for the USA or Spain, this paper uses a sample of EU countries. Second, this paper estimates the aggregate effects of transfers shocks on an extended set of outcome variables which includes output, aggregate private consumption, investment, and several labour market indicators. Finally, like Gechert et al. (2016), the principal contribution of this research is an estimate for the transfers output multiplier. Like Gil et al. (2019), I use a narrative approach to identify the effects of fiscal policy. Like Oh and Reis (2012) I look at a recent sample period before the pandemic. However, while they focus on the expansionary side of fiscal policy actions in the USA between 2007 and 2009, my economic unit of reference are European countries and the sample period includes both stimulus plans and fiscal consolidations. Evidence at the household level is much more prolific and indicates a positive response of individual spending to increases in government income transfers. Jappelli and Pistaferri (2010) offer a good literature review on the subject. Relevant studies include, for example, a pioneering quasi-experimental approach by Bodkin (1959). He looks at the consumption response of WW-II veterans after the receipt of unexpected transfer payments in 1950, and finds a marginal propensity to consume non-durables as high as 0.72. Hausman (2016) also looks at the consumption response of US veterans, but of WW-I, in a natural experiment setting. He finds that within six months of receiving a large bonus in June 1936, veterans spent between 0.65 and 0.75 cents out of every dollar received, and that they spent a large fraction of their bonus on cars, i.e. durable goods. Parker et al. (2013) exploit the randomisation in the assignation of Social Security numbers in the USA to estimate the effect of the tax rebates of 2008 on households spending. They find that on average households spent about 50–90% of their stimulus payments on durable goods (also mainly cars), and about 12–30% on non-durables consumption goods and services in the quarter of the tax rebate. The estimated spending responses are the largest for low-income, old age and borrowing constrained households.Footnote 1
Stephens (2003) investigates the response of household consumption expenditures to the regular monthly arrival of social security checks in the USA. He finds an increase in the amount and probability of consuming strictly non-durables the immediate days after receiving the checks. The results are even more significant for those households for which social security transfers constitute their main source of income. Finally, in Párraga-Rodríguez (2022), I find that Spanish pensioners have a high marginal propensity to consume (MPC) due to unexpected permanent income increases, but less than the one-for-one responses predicted by the canonical permanent income hypothesis. Moreover, high spending responses by high-income and high-wealth pensioners, particularly on durables, discard liquidity constraints as a key source of MPC heterogeneity for pensioners. Government income transfer shocks are constructed from a new and confidential dataset by public finance experts from the European System of Central Banks (ESCB). The dataset contains detailed information on public revenue and expenditure policies for several EU Member States. Within government income transfers, the data reports policy actions for old age pensions, unemployment benefits, and a residual category for other transfers. This paper though restricts the attention to old age pensions. This restriction is primarily due to a lack of observations of discretionary changes in unemployment benefits and, the difficult economic interpretation of estimates for other transfers due to the variety of benefits included in this category.Footnote 2 The policy actions are reported with annual frequency following standardised questionnaires in the context of regular projection exercises; the data are harmonised across countries. The dataset defines a policy action as any change to legislation which determines benefit entitlements. Furthermore, fiscal actions are measured as the difference relative to a benchmark of neutral fiscal policy. The ESCB dataset compiles discretionary changes in fiscal policy. The challenge for any study of the aggregate effects of fiscal shocks is the potential endogenous policy actions. Policymakers take policies for a variety of reasons. For example, during periods of high levels of inflation, governments may increase income transfer payments to guarantee the purchasing power of their beneficiaries. Another example is that in the event of a recession, extraordinary measures may be needed to help a growing number of unemployed. Then, on many occasions fiscal policy measures are responding to the current state of the economy. The key identifying assumption to produce unbiased estimates of the aggregate effect of transfers shocks is that discretionary changes in government income transfers are exogenous. The ESCB dataset records discretionary changes in transfers. A contribution of this paper is to reclassify these discretionary changes as either exogenous or not exogenous based on their motivation. To do so I use information contained in the descriptions accompanying all measures in the ESCB dataset. I complement this information with several other sources, including country-specific legislation and government reports, country reports by different international organisations, and the occasional newspaper. I find a multiplier effect between 0 and 1. The estimated old age pensions output multiplier is 0.5 upon impact, with a maximum cumulative response close to the unity. Consistent with the existing literature (and household-level evidence) I also find a larger effect on durables consumption than non-durables or services. The response of investment is comparable to that of durables consumption. Moreover, increases in transfers have a positive though modest impact on employment. To gain insights into these results, estimates are also broken down by main motivation behind the policy actions and for three geographic regions, i.e. North, South and East Europe. Estimates by the motivation of the policies indicate similar positive aggregate effects. Regarding regional estimates, I find that the point estimates are only statistically significant for South Europe. An estimate of the transfers multiplier effect is crucial for assessing the effectiveness of fiscal policy actions. A multiplier effect between 0 and 1 indicates limited effectiveness of fiscal actions involving government income transfers. However, this limited effectiveness has different implications for stimulus and austerity programmes. The results indicate that increases in old age pensions might be costly stimulus measures given their modest positive aggregate impact. On the other hand, desirable austerity programmes should include measures that effectively reduce the government deficit while having a contained negative effect on the economy. The remainder of the paper is organised as follows. Section 2 describes the ESCB dataset and the construction of the new measure of transfers shocks. Section 3 gives details about the specification used for estimation. Section 4 explains the main results in terms of the multiplier effect and investigates the transmission mechanism of transfers shocks. Section 5 breaks down the estimates by motivation and economic region. Section 6 offers concluding remarks.",1
14.0,1.0,SERIEs,27 February 2023,https://link.springer.com/article/10.1007/s13209-023-00274-2,Correction: The aggregate effects of government income transfers shocks: EU evidence,March 2023,Susana Párraga Rodríguez,,,Female,Unknown,Unknown,Female,,
14.0,1.0,SERIEs,23 June 2022,https://link.springer.com/article/10.1007/s13209-022-00266-8,A connections model with decreasing returns link-formation technology,March 2023,Norma Olaizola,Federico Valenciano,,Female,Male,Unknown,Mix,,
14.0,1.0,SERIEs,30 August 2022,https://link.springer.com/article/10.1007/s13209-022-00267-7,Minimum age requirements and the role of the school choice set,March 2023,Julio Cáceres-Delpiano,Eugenio Giolito,,Male,Male,Unknown,Male,"Using individual administrative records from Chile, we study the impact of the size of the school choice set, measured by the number of available slots per student in the municipality at the time of enrollment in first grade of primary education, on individual educational outcomes. We do this by relating the literature on school choice with the one on School Starting Age (SSA). The recent literature does not take a clear position in favor or against school choice (Epple et al. 2017). The ambiguity resides in the heterogeneity of the programs implemented and the individuals affected by these interventions. While small-scale programs provide credible experimental or quasi-experimental variation in school choice, (see, for example Angrist et al. (2002; 2006)) the expected benefits are positively related to the size of the intervention.Footnote 1 However, in large-scale interventions, the non-random sorting of students across schools makes it difficult to find a credible control group and speaks for the offsetting effects for some individuals in the population.Footnote 2 In this paper, we study the impact of more/fewer school choices in a context of a country (Chile) with a national voucher system for more than 30 years which in theory allows families making use of school choices to search, use and profit from better school quality.Footnote 3 Even though most of the evidence for the Chilean voucher system points to the effect on “cream-skimming” and mixed results on test scores (see, for example, Hsieh and Urquiola 2006), here we study if a change in the set of schools available to students has an impact on the educational outcomes of these students.Footnote 4 To study the impact of the size of the school choice set, we connect with the literature on School Starting Age (SSA).Footnote 5 Specifically, we rely on a “quasi-experimental” variation coming from the institutional setting that until recently governed minimum age requirements in Chile.Footnote 6 Families/children in Chile faced i) multiple birthday cutoffs defining the minimum age when a student can start the first grade of elementary school and ii) a different composition of schools according to these cutoffs across municipalities. These two features lead students born around several cutoffs, who “endogenously” decided to start early but whose birthdays differ by a few days, to face different school choice sets. Moreover, these changes in the school choice set around cutoffs vary also across municipalities. While minimum age requirements have been extensively used to estimate the impact of SSA, here we use these particular features to estimate the contribution of the size of the school choice set.Footnote 7 Different from a setting with a unique cutoff, by comparing students on either side of the cutoffs across different cutoffs and municipalities, we can address the endogeneity of early start and also learn the contribution of the school choice set.Footnote 8 Specifically, at any cutoff-municipality combination, families face a potentially different subset of the school choice set, whose size changes differently at each cutoff across municipalities. That is, this source of variation not only allows us to control for municipality fixed factors (such as systematic differences in educational markets) but also to account for systematic differences in schools using different deadlines. In other words, our quasi-experimental variation is given by the fact that families with children born just after one of these cutoffs (except for the last one) are not forced to wait for the next academic year to send their children to school, but they face a subset of schools in the case in which they want them to start in the current year. That is, the treatment (slots per student) is different if the student’s birthday is at each side of a given cutoff (seven in total), with its intensity varying also by municipality. Moreover, our population under study are all Chilean children born around the first day of the month, from January to July. Therefore, we are studying the impact of the school choice set on a wider population of students, rather than on a more homogeneous one in terms of family income or previous educational outcomes. We are also able to capture the potential heterogeneity across parents’ education. In addition, our quasi-experimental variation allows us to study families’ behavior and its consequent effects independently of schools’ strategic behavior.Footnote 9 Our results reveal, first, that a larger set of available schools causes modest increases in standardized test scores. However, we find a sizable reduction in the likelihood of dropping out (8% in terms of sample mean) and in the probability that a student switches schools over her/his school life (10% in terms of sample mean). Second, for a subsample of students who have completed high school, we observe a positive effect on the probability of taking the national college admission test the year of high school graduation. Specifically, an increase in a third of slots per student is associated with an increase of approximately 2 percentage points in the probability of taking the college admission test (or 3% in terms of sample mean). Moreover, for families with less-educated parents, we also observe a positive effect on the probability of enrolling in college. In fact, for most of these outcomes, we show that the impact is greater among students whose parents have lower levels of education. This paper provides evidence on the interaction of SSA with school choice. Therefore, we shed light on one institutional feature setting limits in the capability of families to profit from school choice in voucher schemes like the one in Chile. By providing evidence about the potential heterogeneity in the impact of SSA according to the school choice set, we learn about school choice contribution and also about the families who profit the most from it. Furthermore, this evidence about the positive contribution of school choice in a country with a national voucher scheme enables us to learn about the role of school choice in other dimensions other than the non-random sorting of students across schools, but on long-term educational achievements. Finally, this paper stresses the importance of school choice early in students’ academic life. We find evidence about the impact of school choice at the time of first enrollment in elementary school rather than at high school or college.Footnote 10 That is, we contribute to the recent literature studying the impact of early interventions on children’s outcomes.Footnote 11 The paper is organized as follows. Section 2 briefly sketches Chile’s educational system, presents the data set and defines the sample used in the analysis. In Sect. 3 we describe our empirical strategy and the selected outcomes in the analysis. In Sect. 4 we discuss the validity of our source of variation. In Sect. 5 we present our results, and Sect. 6 concludes.",
14.0,1.0,SERIEs,02 December 2022,https://link.springer.com/article/10.1007/s13209-022-00270-y,Born this way: the effect of an unexpected child benefit at birth on longer-term educational outcomes,March 2023,Sergi Sánchez-Coll,,,Male,Unknown,Unknown,Male,"Child benefits remain a popular tool to promote fertility in developed countries, as governments typically spend between 1 and 4% of GDP on family policies (Sobotka et al. 2019). Their role in fostering economic growth and development is well known, as the return on investment is highest at young ages (Heckman 2008). Those policies come in a variety of forms. Universal direct cash transfers are straightforward to implement and, despite their cost and rather modest impact compared to other interventions, they help decrease the cost of having children and thus temporarily increase fertility (Milligan 2005; Sinclair et al. 2012; González 2013; Chuard and Chuard-Keller 2021). The effects of some of those income-increasing policies, especially when targeting disadvantaged families, have been shown to go beyond pure fertility boosts, implying improvements in maternal and children health at birth and early childhood (Cooper and Stewart 2013, 2020). Longer-term effects have also been observed, leading to better educational and human capital outcomes, some even persisting into adulthood. In this paper, I study the longer-term effects on educational outcomes of a universal subsidy implemented in Spain in 2007, consisting of a single €2500 payment given to mothers of a recently born baby or who adopted one. Using microdata containing the universe of sixth-grade students participating in the basic competency tests at the end of primary school in the Catalonia region, I take advantage of the natural experiment that the timing of the measure created, generating a discontinuity at the 1 July 2007 threshold when households became eligible to receive the payment, as opposed to parents of children born immediately before that cut-off, who did not receive it. First, I propose a regression discontinuity design (RD) where I compare children born on both sides of the cut-off. Potential seasonality in children characteristics affecting both sides of the cut-off differently, such as early developmental differences or parental education levels, might bias the results. To overcome those issues and gain precision, I then combine the previous RD of the treatment year with a RD of the previous non-treatment years in a difference-in-discontinuities specification (diff-in-disc). I perform a non-parametrical estimation of the models, using local linear regression with triangular kernel smoothing and data-driven bandwidth selection procedures for the pooled sample of observations. Additionally, I explore the heterogeneity of results in a number of subsamples, considering student gender and nationality, school complexity, ownership and size. The benefit did not have significant positive effects overall on children achievement test scores at age 12, as we can rule out grade improvements greater than 0.1 standard deviation units with 95% confidence. In subsample analyses, RD results show slightly negative effects that are not significantly different from zero except for a decrease of 11% of a standard deviation in Catalan language grades in non-disadvantaged schools. Boys in disadvantaged schools seem to have benefited the most from the subsidy, with increases of up to 43% of a standard deviation in the English language test score. Diff-in-disc estimates lose almost all statistical significance while remaining roughly similar in magnitude, which is especially relevant for those attending disadvantaged public schools, with estimated improvements of 4–11% of the average test score for boys and 1–4.1% for girls depending on the assessed subject, also lacking statistical significance. However, robustness tests suggest that the reported changes in test scores are due to chance rather than caused by the subsidy, as natural variability in the data is significantly higher than the discontinuities reported in the treatment period. Insufficient transfers, its universal nature, the lack of earmarking, the timing and the fading out of effects over time can potentially explain the findings. This paper contributes to the rich literature on the effects of income transfers to families on longer-term human capital outcomes. It specifically complements existing evidence evaluating the 2007 child subsidy in Spain and first estimates its effects on the acquisition of language and mathematical skills when children reach preadolescence and in a low-stakes standardised examination setting. The remainder of the paper proceeds as follows: Sect. 2 gives an overview of the state of the literature and the institutional background of the subsidy and the competency tests. Section 3 describes the identification strategy as well as the empirical specifications. Section 4 presents the data and their main characteristics. Section 5 reports the results, the robustness checks and a discussion, while Sect. 6 concludes the paper.",
14.0,2.0,SERIEs,13 February 2023,https://link.springer.com/article/10.1007/s13209-023-00272-4,Education and internal migration: evidence from a child labor reform in Spain,June 2023,Jorge González Chapela,Sergi Jiménez-Martín,Judit Vall Castello,Male,Male,Female,Mix,,
14.0,2.0,SERIEs,05 April 2023,https://link.springer.com/article/10.1007/s13209-023-00275-1,Housing prices in Spain: convergence or decoupling?,June 2023,Corinna Ghirelli,Danilo Leiva-León,Alberto Urtasun,Female,Male,Male,Mix,,
14.0,2.0,SERIEs,27 April 2023,https://link.springer.com/article/10.1007/s13209-023-00278-y,The price effects of reducing payment card interchange fees,June 2023,Bita Shabgard,Javier Asensio,,Female,,Unknown,Mix,,
14.0,2.0,SERIEs,06 May 2023,https://link.springer.com/article/10.1007/s13209-023-00281-3,Attitudes towards single parents’ children in private and state-dependent private schools: experimental evidence,June 2023,Luis Diaz-Serrano,Sabine Flamand,,Male,Female,Unknown,Mix,,

Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1011514226782,The Special Issue on Traffic Flow Theory,March 2001,H. Michael Zhang,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1011539112438,New Perspectives on Continuum Traffic Flow Models,March 2001,H.M. Zhang,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1011577010852,Complexity of Synchronized Flow and Related Problems for Basic Assumptions of Traffic Flow Theories,March 2001,Boris S. Kerner,,,Male,Unknown,Unknown,Male,,120
1.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1011529127691,A Simple Traffic Analysis Procedure,March 2001,Carlos F. Daganzo,,,Male,Unknown,Unknown,Male,,10
1.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1011581111761,Probabilistic Description of Traffic Flow,March 2001,R. Mahnke,J. Kaupuzżs,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1011533228599,Platoon-Based Multiclass Modeling of Multilane Traffic Flow,March 2001,Serge P. Hoogendoorn,Piet H.L. Bovy,,Male,,Unknown,Mix,,
1.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1011585212670,The Mathematical Theory of an Enhanced Nonequilibrium Traffic Flow Model,March 2001,T. Li,H. M. Zhang,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1011537329508,The Importance of Traffic Flow Modeling for Motorway Traffic Control,March 2001,A. Kotsialos,M. Papageorgiou,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1011589313578,Implementation and Field Testing of Characteristics-Based Intersection Queue Estimation Model,March 2001,Ping Yi,Clara Xin,Qiang Zhao,,Female,,Mix,,
2.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1014559729042,Introduction,March 2002,Jean-Claude Thill,,,Male,Unknown,Unknown,Male,,
2.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1014563113112,Ontologies of Wayfinding: a Traveler's Perspective,March 2002,Sabine Timpf,,,Female,Unknown,Unknown,Female,,58
2.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1014515329951,A GIS Approach to the Spatial Assessment of Telecommunications Infrastructure,March 2002,Guoray Cai,,,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1014567314021,Analyzing Geographic Representation Error in Capacitated Location-Allocation Modeling,March 2002,Robert G. Cromley,Richard D. Mrozinski Jr.,,Male,Male,Unknown,Male,,1
2.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1014519430859,GIS-Based Analysis of Marginal Price Variation with an Application in the Identification of Candidate Ethanol Conversion Plant Locations,March 2002,Charles E. Noon,F. Benjamin Zhan,Robin L. Graham,Male,Unknown,,Mix,,
2.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1015301926168,Guest Editorial:Special Issue on Spatial Economic Activity Analysis: New Methodologies,June 2002,,,,Unknown,Unknown,Unknown,Unknown,,
2.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1015357027077,Estimation of Alonso's Theory of Movements by Means of Instrumental Variables,June 2002,Jacob J. De Vries,Peter Nijkamp,Piet Rietveld,Male,Male,,Mix,,
2.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1015361111147,Simulating the Interplay Between Regional Demographic and Economic Change in Two Scenarios,June 2002,Leo Van Wissen,Corina Huisman,,Male,Female,Unknown,Mix,,
2.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1015313212056,"Proximity, Polarization, and Local Labor Market Performances",June 2002,Guido Pellegrini,,,Male,Unknown,Unknown,Male,,22
2.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1015317329803,A Kohonen Self-Organizing Map Approach to Modeling Growth Pole Dynamics,June 2002,Rajendra Kulkarni,Laurie A. Schintler,Kenneth Button,Male,Female,Male,Mix,,
2.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1015373430711,Testing for Non-Linear Dependence in Univariate Time Series An Empirical Investigation of the Austrian Unemployment Rate,June 2002,Wolfgang Koller,Manfred M. Fischer,,Male,Male,Unknown,Male,,5
2.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1015377515690,Resilience: An Evolutionary Approach to Spatial Economic Systems,June 2002,Aura Reggiani,Thomas De Graaff,Peter Nijkamp,Female,Male,Male,Mix,,
2.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1019911310914,The Impact of Competition in the Italian Mobile Telecommunications Market,September 2002,Livio Cricelli,Massimo Gastaldi,Nathan Levialdi,Male,Male,Male,Male,,
2.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1019923727752,Evaluating Neural Spatial Interaction Modelling by Bootstrapping,September 2002,Manfred M. Fischer,Martin Reismann,,Male,Male,Unknown,Male,,
2.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1019975711823,A Hybrid Deployable Dynamic Traffic Assignment Framework for Robust Online Route Guidance,September 2002,Srinivas Peeta,Chao Zhou,,Male,,Unknown,Mix,,
2.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1019927828661,Two-Way Interconnection with Partial Consumer Participation,September 2002,Aaron Schiff,,,Male,Unknown,Unknown,Male,,
2.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1020862526520,"Introduction: Regional Transportation Models: Microsimulation and Computation, Part I",December 2002,Terry L. Friesz,,,,Unknown,Unknown,Mix,,
2.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1020843410590,Editorial,December 2002,Kai Nagel,Peter Wagner,,Male,Male,Unknown,Male,,1
2.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1020895427428,A Stochastic Route Choice Model for Car Travellers in the Copenhagen Region,December 2002,Otto Anker Nielsen,Andrew Daly,Rasmus Dyhr Frederiksen,Male,Male,Male,Male,,37
2.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1020847511499,Real Cases Applications of the Fully Dynamic METROPOLIS Tool-Box: An Advocacy for Large-Scale Mesoscopic Transportation Systems,December 2002,André de Palma,Fabrice Marchal,,Male,Male,Unknown,Male,,68
2.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1020899528337,A Microscopic Simulator for Freeway Traffic,December 2002,Joachim Wahle,Roland Chrobok,Michael Schreckenberg,Male,Male,Male,Male,,8
2.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1020851612407,Parallel DYNEMO: Meso-Scopic Traffic Flow Simulation on Large Networks,December 2002,Klaus Nökel,Matthias Schmidt,,Male,Male,Unknown,Male,,14
3.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1022078131827,Introduction,January 2003,Terry L. Friesz,,,,Unknown,Unknown,Mix,,
3.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1022092815898,Editorial: Regional Transportation Simulations,January 2003,Kai Nagel,Peter Wagner,,Male,Male,Unknown,Male,,
3.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1022044932736,Cellular Automata Simulations of Traffic: A Model for the City of Geneva,January 2003,Alexandre Dupuis,Bastien Chopard,,Male,Male,Unknown,Male,,16
3.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1022096916806,An Agent-Based Microsimulation Model of Swiss Travel: First Results,January 2003,Bryan Raney,Nurhan Cetin,Kai Nagel,Male,Female,Male,Mix,,
3.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1022049000877,Microsimulation of Urban Development and Location Choices: Design and Implementation of UrbanSim,January 2003,P. Waddell,A. Borning,G. Ulfarsson,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1022001117715,Growing Urban Roads,January 2003,Daniel Yamins,Steen Rasmussen,David Fogel,Male,Male,Male,Male,,42
3.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1022039011626,Book Reviews,January 2003,Terry L. Friesz,,,,Unknown,Unknown,Mix,,
3.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1023931117451,Foreword: Special Issue on Energy,June 2003,Steven A. Gabriel,,,Male,Unknown,Unknown,Male,,
3.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1023955701522,"Computational Experience with a Large-Scale, Multi-Period, Spatial Equilibrium Model of the North American Natural Gas System",June 2003,Steven A. Gabriel,Julio Manik,Shree Vikas,Male,Male,Unknown,Male,,27
3.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1023907818360,Nash-Cournot Equilibria in Power Markets on a Linearized DC Network with Arbitrage: Formulations and Properties,June 2003,Carolyn Metzler,Benjamin F. Hobbs,Jong-Shi Pang,Female,Male,Unknown,Mix,,
3.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1023959902430,Market Incompleteness in Regional Electricity Transmission. Part I: The Forward Market,June 2003,Yves Smeers,,,Male,Unknown,Unknown,Male,,14
3.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1023916120177,Market Incompleteness in Regional Electricity Transmission. Part II: The Forward and Real Time Markets,June 2003,Yves Smeers,,,Male,Unknown,Unknown,Male,,17
3.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1023912019269,A Nested Benders Decomposition Approach to Locating Distributed Generation in a Multiarea Power System,June 2003,Susan McCusker,Benjamin F. Hobbs,,Female,Male,Unknown,Mix,,
3.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1023964003339,Managing Electricity Reliability Risk Through the Forward Markets,June 2003,Afzal S. Siddiqui,,,Male,Unknown,Unknown,Male,,5
3.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1025345717581,Intra-Period (Within-Day) Dynamic Models for Continuous Services,September 2003,Ennio Cascetta,Pierluigi Coppola,,Male,Male,Unknown,Male,,2
3.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1025394201651,The CONTRAM Dynamic Traffic Assignment Model,September 2003,Nicholas B. Taylor,,,Male,Unknown,Unknown,Male,,58
3.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1025346318490,Combined Activity/Travel Choice Models: Time-Dependent and Dynamic Versions,September 2003,William H. K. Lam,Hai-Jun Huang,,Male,,Unknown,Mix,,
3.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1025398302560,The Dynamics and Equilibria of Day-to-Day Assignment Models,September 2003,David Watling,Martin L. Hazelton,,Male,Male,Unknown,Male,,121
3.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1025350419398,Stationary Dynamic Solutions in Congested Transportation Networks: Summary and Perspectives,September 2003,Yurii Nesterov,André de Palma,,Male,Male,Unknown,Male,,43
3.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000004046.97957.41,Erratum,September 2003,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1027311507031,Erratum,December 2003,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1027353520175,Local Search Heuristics for Capacitated p-Median Problems,December 2003,Luiz Antonio Nogueira Lorena,Edson Luiz França Senne,,Unknown,Male,Unknown,Male,,38
3.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1027357904245,Competitive Delivered Spatial Pricing,December 2003,Phillip J. Lederer,,,Male,Unknown,Unknown,Male,,7
3.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1027310021084,A Decomposition Scheme for System Optimal Dynamic Traffic Assignment Models,December 2003,Yue Li,S. Travis Waller,Thanasis Ziliaskopoulos,,Unknown,Male,Mix,,
3.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1027362005154,Total Travel Cost in Stochastic Assignment Models,December 2003,Martin L. Hazelton,,,Male,Unknown,Unknown,Male,,6
3.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/A:1027314121992,Simulation and Optimization of the Implementation Costs for the Last Mile of Fiber Optic Networks,December 2003,P. Bachhiesl,M. Prossegger,H. Stögner,Unknown,Unknown,Unknown,Unknown,,
4.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000015680.58021.cd,Preface,March 2004,Eitan Altman,Laura Wynter,,Male,Female,Unknown,Mix,,
4.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000015653.52983.61,"Equilibrium, Games, and Pricing in Transportation and Telecommunication Networks",March 2004,Eitan Altman,Laura Wynter,,Male,Female,Unknown,Mix,,
4.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000015654.56554.31,Algorithms for Computing Traffic Equilibria,March 2004,Michael Patriksson,,,Male,Unknown,Unknown,Male,,23
4.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000015655.31348.1b,Incentive Compatible Pricing Strategies for QoS Routing,March 2004,Yannis A. Korilis,Ariel Orda,,Male,Male,Unknown,Male,,9
4.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000015656.38255.b0,Deterministic Flow Routing and Oligopolistic Competition in Dynamic Data Networks: Modeling and Numerical Solution Using Multigrid Optimization Techniques,March 2004,Terry L. Friesz,David Bernstein,Niko Kydes,,Male,Male,Mix,,
4.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000015657.07692.be,"A Mathematical Framework for Designing a Low-Loss, Low-Delay Internet",March 2004,Steven H. Low,R. Srikant,,Male,Unknown,Unknown,Male,,54
4.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000015658.75205.ed,Traffic Matrix Inference in IP Networks,March 2004,N. Benameur,J.W. Roberts,,Unknown,Unknown,Unknown,Unknown,,
4.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000015659.39216.83,Multiclass Combined Models for Urban Travel Forecasting,March 2004,David Boyce,Hillel Bar-Gera,,Male,Male,Unknown,Male,,49
4.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000027841.32415.a9,Guest Editorial: Road Pricing Problems: Recent Methodological Advances,June 2004,Hai Yang,Erik T. Verhoef,,,Male,Unknown,Mix,,
4.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000027770.27906.82,Congestion Pricing with Heterogeneous Travelers: A General-Equilibrium Welfare Analysis,June 2004,André de Palma,Robin Lindsey,,Male,,Unknown,Mix,,
4.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000027771.13826.3a,A Genetic Algorithm Based Approach to Optimal Toll Level and Location Problems,June 2004,Simon Shepherd,Agachai Sumalee,,Male,Unknown,Unknown,Male,,80
4.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000027772.43771.94,Dynamic Congestion Pricing in Disequilibrium,June 2004,Terry L. Friesz,David Bernstein,Niko Kydes,,Male,Male,Mix,,
4.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000027773.75570.5f,Strategic Interactions of Bilateral Monopoly on a Private Highway,June 2004,Judith Y.T. Wang,Hai Yang,Erik T. Verhoef,Female,,Male,Mix,,
4.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000039781.10517.3a,Drivers' Mental Representation of Travel Time and Departure Time Choice in Uncertain Traffic Network Conditions,September 2004,Satoshi Fujii,Ryuichi Kitamura,,Male,Male,Unknown,Male,,49
4.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000039782.48154.ef,Link Travel Times I: Desirable Properties,September 2004,Malachy Carey,,,Male,Unknown,Unknown,Male,,22
4.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000039783.57975.f0,Efficient Discretisation for Link Travel Time Models,September 2004,Malachy Carey,Y.E. Ge,,Male,Unknown,Unknown,Male,,11
4.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000039784.00352.66,A Strategic Model for Dynamic Traffic Assignment,September 2004,Younes Hamdouch,Patrice Marcotte,Sang Nguyen,Male,Male,,Mix,,
4.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000047111.94897.d4,A Mixed Integer Stochastic Optimization Model for Settlement Risk in Retail Electric Power Markets,December 2004,Steven A. Gabriel,Supat Kiet,Swaminathan Balakrishnan,Male,Unknown,Unknown,Male,,17
4.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000047112.68868.48,Do European Carriers Charge Hub Premiums?,December 2004,Mark G. Lijesen,Piet Rietveld,Peter Nijkamp,Male,,Male,Mix,,
4.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000047113.37578.91,Infrastructure and Economic Efficiency in Italian Regions,December 2004,Marco Percoco,,,Male,Unknown,Unknown,Male,,21
4.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000047114.31259.3d,Link Travel Times II: Properties Derived from Traffic-Flow Models,December 2004,Malachy Carey,,,Male,Unknown,Unknown,Male,,16
4.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1023/B:NETS.0000047185.73211.78,Book Review,December 2004,K.T. Talluri,G.J. Van Ryzin,,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-6659-2,On the Genesis of Hexagonal Shapes,March 2005,Tönu Puu,,,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-6660-9,A Hybrid Model for Driver Route Choice Incorporating En-Route Attributes and Real-Time Information Effects,March 2005,Srinivas Peeta,Jeong Whon Yu,,Male,,Unknown,Mix,,
5.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-6661-8,Continuous and Discrete Trajectory Models for Dynamic Traffic Assignment,March 2005,Hillel Bar-Gera,,,Male,Unknown,Unknown,Male,,9
5.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-6662-7,Some Consistency Conditions for Dynamic Traffic Assignment Problems,March 2005,H. M. Zhang,Xiaojian Nie,,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-6663-6,A Comparative Study of Some Macroscopic Link Models Used in Dynamic Traffic Assignment,March 2005,Xiaojian Nie,H. M. Zhang,,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-6664-5,Errata,March 2005,Editorial Board,,,Unknown,Unknown,Unknown,Unknown,,
5.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-2625-2,Editorial,June 2005,Kai Nagel,Duncan Cavens,,Male,Male,Unknown,Male,,
5.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-2626-1,Dynamic Submodel Integration Using an Offer-Accept Discrete Event Simulation,June 2005,John E. Abraham,J. D. Hunt,,Male,Unknown,Unknown,Male,,2
5.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-2627-0,Dynamic Game Theoretic Model of Multi-Layer Infrastructure Networks,June 2005,PENGCHENG ZHANG,SRINIVAS PEETA,TERRY FRIESZ,Unknown,Unknown,Unknown,Unknown,,
5.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-2628-z,A Trip Generation Method for Time-Dependent Large-Scale Simulations of Transport and Land-Use,June 2005,F. Marchal,,,Unknown,Unknown,Unknown,Unknown,,
5.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-2629-y,Pedestrian Travel Behavior Modeling,June 2005,Serge P. Hoogendoorn,Piet H. L. Bovy,,Male,,Unknown,Mix,,
5.0,2.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-2630-5,ILUTE: An Operational Prototype of a Comprehensive Microsimulation Model of Urban Systems,June 2005,Paul Salvini,Eric J. Miller,,Male,Male,Unknown,Male,,184
5.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-3034-2,Feedback Control Solutions to Network Level User-Equilibrium Real-Time Dynamic Traffic Assignment Problems,September 2005,Pushkin Kachroo,Kaan Özbay,,Unknown,Male,Unknown,Male,,13
5.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-3035-1,Wholesale Competition in the International Telecommunications System,September 2005,Livio Cricelli,Francesca Di Pillo,Nathan Levialdi,Male,Female,Male,Mix,,
5.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-3036-0,Relations Among Prices at Adjacent Nodes in an Electric Transmission Network,September 2005,J. David Fuller,,,Unknown,Unknown,Unknown,Unknown,,
5.0,3.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-3037-z,An Investigation on the Aggregate Behavior of Firm Relocations to New Jersey (1990–1999) and the Underlying Market Elasticities,September 2005,Jose Holguin-Veras,Ning Xu,Dilruba Ozmen-Ertekin,Male,,Female,Mix,,
5.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-6207-0,Auctions in a Two-Sided Network: The Market for Meal Voucher Services,December 2005,Roberto Roson,,,Male,Unknown,Unknown,Male,,11
5.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-6208-z,Complex Network Phenomena in Telecommunication Systems,December 2005,Laurie A. Schintler,Sean P. Gorman,Jonathan Rutherford,Female,Male,Male,Mix,,
5.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-6209-y,Multi-Period Near-Equilibrium in a Pool-Based Electricity Market Including On/Off Decisions,December 2005,Raquel García-Bertrand,Antonio J. Conejo,Steven A. Gabriel,Female,Male,Male,Mix,,
5.0,4.0,Networks and Spatial Economics,,https://link.springer.com/article/10.1007/s11067-005-6210-5,Compared Analysis of Metro Networks Supported by Graph Theory,December 2005,Domenico Gattuso,Ernesto Miriello,,Male,Male,Unknown,Male,,53
6.0,1.0,Networks and Spatial Economics,19 May 2006,https://link.springer.com/article/10.1007/s11067-006-7681-8,Preface,March 2006,,,,Unknown,Unknown,Unknown,Unknown,,
6.0,1.0,Networks and Spatial Economics,19 May 2006,https://link.springer.com/article/10.1007/s11067-006-7682-7,"Income, Time Effects and Direct Preferences in a Multimodal Choice Context: Application of Mixed RP/SP Models with Non-Linear Utilities",March 2006,Elisabetta Cherchi,Juan de Dios Ortúzar,,Female,Male,Unknown,Mix,,
6.0,1.0,Networks and Spatial Economics,19 May 2006,https://link.springer.com/article/10.1007/s11067-006-7683-6,Preliminary Insights into Optimal Pricing and Space Allocation at Intermodal Terminals with Elastic Arrivals and Capacity Constraint,March 2006,José Holguín-Veras,Sergio Jara-Díaz,,Male,Male,Unknown,Male,,24
6.0,1.0,Networks and Spatial Economics,19 May 2006,https://link.springer.com/article/10.1007/s11067-006-7684-5,A Simultaneous Inventory Control and Facility Location Model with Stochastic Capacity Constraints,March 2006,Pablo A. Miranda,Rodrigo A. Garrido,,Male,Male,Unknown,Male,,62
6.0,1.0,Networks and Spatial Economics,19 May 2006,https://link.springer.com/article/10.1007/s11067-006-7685-4,Dynamic Model for the Simulation of Equilibrium Status in the Land Use Market,March 2006,Francisco Martínez,Ricardo Hurtubia,,Male,Male,Unknown,Male,,14
6.0,2.0,Networks and Spatial Economics,22 May 2006,https://link.springer.com/article/10.1007/s11067-006-7693-4,Preface,June 2006,,,,Unknown,Unknown,Unknown,Unknown,,
6.0,2.0,Networks and Spatial Economics,22 May 2006,https://link.springer.com/article/10.1007/s11067-006-7694-3,Confidence Interval for Willingness to Pay Measures in Mode Choice Models,June 2006,Raquel Espino,Juan de Dios Ortúzar,Concepción Román,Female,Male,,Mix,,
6.0,2.0,Networks and Spatial Economics,22 May 2006,https://link.springer.com/article/10.1007/s11067-006-7695-2,A Comparative Study of Alternative Model Structures and Criteria for Ranking Locations for Safety Improvements,June 2006,Luis F. Miranda-Moreno,Liping Fu,,Male,Unknown,Unknown,Male,,17
6.0,2.0,Networks and Spatial Economics,22 May 2006,https://link.springer.com/article/10.1007/s11067-006-7696-1,A Benchmarking Analysis of Spanish Commercial Airports. A Comparison Between SMOP and DEA Ranking Methods,June 2006,Juan Carlos Martín,Concepción Román,,Male,,Unknown,Mix,,
6.0,2.0,Networks and Spatial Economics,22 May 2006,https://link.springer.com/article/10.1007/s11067-006-7697-0,Comparisons of Urban Travel Forecasts Prepared with the Sequential Procedure and a Combined Model,June 2006,Justin D. Siegel,Joaquín De Cea,David Boyce,Male,Male,Male,Male,,22
6.0,2.0,Networks and Spatial Economics,22 May 2006,https://link.springer.com/article/10.1007/s11067-006-7698-z,Distinguishing Multiproduct Economies of Scale from Economies of Density on a Fixed-Size Transport Network,June 2006,Leonardo J. Basso,Sergio R. Jara-Díaz,,Male,Male,Unknown,Male,,18
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-006-9278-7,Guest Editorial: Reliability and Emergency Issues in Transportation Network Analysis,September 2006,Agachai Sumalee,Fumitaka Kurauchi,,Unknown,Unknown,Unknown,Unknown,,
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-006-9279-6,A Reliability-Based Stochastic Traffic Assignment Model for Network with Multiple User Classes under Uncertainty in Demand,September 2006,Hu Shao,William H. K. Lam,Mei Lam Tam,,Male,,Mix,,
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-006-9280-0,Network Capacity Reliability Analysis Considering Traffic Regulation after a Major Disaster,September 2006,Agachai Sumalee,Fumitaka Kurauchi,,Unknown,Unknown,Unknown,Unknown,,
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-006-9281-z,Optimal Fleet Allocation of Freeway Service Patrols,September 2006,Yafeng Yin,,,Unknown,Unknown,Unknown,Unknown,,
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-006-9282-y,Improving Airline Network Robustness and Operational Reliability by Sequential Optimisation Algorithms,September 2006,Cheng-Lung Wu,,,Unknown,Unknown,Unknown,Unknown,,
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-006-9283-x,Mixed Route Strategies for the Risk-Averse Shipment of Hazardous Materials,September 2006,Michael G. H. Bell,,,Male,Unknown,Unknown,Male,,49
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-006-9284-9,Application of Accessibility Based Methods for Vulnerability Analysis of Strategic Road Networks,September 2006,Michael A. P. Taylor,Somenahalli V. C. Sekhar,Glen M. D'Este,Male,Unknown,Male,Male,,264
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-006-9285-8,Travel Time Reliability in Vehicle Routing and Scheduling with Time Windows,September 2006,Naoki Ando,Eiichi Taniguchi,,Male,Male,Unknown,Male,,122
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-006-9286-7,Risk-Averse Traffic Assignment with Elastic Demands: NCP Formulation and Solution Method for Assessing Performance Reliability,September 2006,W. Y. Szeto,L. O'Brien,M. O'Mahony,Unknown,Unknown,Unknown,Unknown,,
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-006-9287-6,Optimal Routing for Maximizing the Travel Time Reliability,September 2006,Yueyue Fan,Yu Nie,,Unknown,,Unknown,Mix,,
6.0,3.0,Networks and Spatial Economics,25 August 2006,https://link.springer.com/article/10.1007/s11067-9288-5,Dynamic Revenue Management of a Toll Road Project under Transportation Demand Uncertainty,September 2006,Takeshi Nagae,Takashi Akamatsu,,Male,Male,Unknown,Male,,18
7.0,1.0,Networks and Spatial Economics,17 January 2007,https://link.springer.com/article/10.1007/s11067-006-9008-1,Competitive Facility Location,March 2007,Terry L. Friesz,,,,Unknown,Unknown,Mix,,
7.0,1.0,Networks and Spatial Economics,21 December 2006,https://link.springer.com/article/10.1007/s11067-006-9004-5,Aggregation without Loss of Optimality in Competitive Location Models,March 2007,Frank Plastria,Lieselot Vanhaverbeke,,Male,Unknown,Unknown,Male,,16
7.0,1.0,Networks and Spatial Economics,19 December 2006,https://link.springer.com/article/10.1007/s11067-006-9005-4,Planar Location and Design of a New Facility with Inner and Outer Competition: An Interval Lexicographical-like Solution Procedure,March 2007,José Fernández,Blas Pelegrín,Boglárka Tóth,Male,Male,Unknown,Male,,30
7.0,1.0,Networks and Spatial Economics,06 December 2006,https://link.springer.com/article/10.1007/s11067-006-9007-2,The Leader–Follower Location Model,March 2007,D. R. Santos-Peñate,R. Suárez-Vega,P. Dorta-González,Unknown,Unknown,Unknown,Unknown,,
7.0,1.0,Networks and Spatial Economics,19 January 2007,https://link.springer.com/article/10.1007/s11067-006-9006-3,Incorporating Waiting Time in Competitive Location Models,March 2007,Francisco Silva,Daniel Serra,,Male,Male,Unknown,Male,,8
7.0,1.0,Networks and Spatial Economics,21 December 2006,https://link.springer.com/article/10.1007/s11067-006-9013-4,Reaction Function Based Dynamic Location Modeling in Stackelberg–Nash–Cournot Competition,March 2007,Tan C. Miller,Terry L. Friesz,Changhyun Kwon,,,Unknown,Mix,,
7.0,2.0,Networks and Spatial Economics,17 January 2007,https://link.springer.com/article/10.1007/s11067-006-9009-0,Forecasting Travel on Congested Urban Transportation Networks: Review and Prospects for Network Equilibrium Models,June 2007,David Boyce,,,Male,Unknown,Unknown,Male,,45
7.0,2.0,Networks and Spatial Economics,19 December 2006,https://link.springer.com/article/10.1007/s11067-006-9003-6,Location of Alternative-Fuel Stations Using the Flow-Refueling Location Model and Dispersion of Candidate Sites on Arcs,June 2007,Michael Kuby,Seow Lim,,Male,Unknown,Unknown,Male,,177
7.0,2.0,Networks and Spatial Economics,20 December 2006,https://link.springer.com/article/10.1007/s11067-006-9000-9,An Automated Network Generation Procedure for Routing of Unmanned Aerial Vehicles (UAVs) in a GIS Environment,June 2007,Irene Casas,Amit Malik,Rajan Batta,Female,Male,Male,Mix,,
7.0,2.0,Networks and Spatial Economics,19 December 2006,https://link.springer.com/article/10.1007/s11067-006-9001-8,Optimal Measurement-based Pricing for an M/M/1 Queue,June 2007,Yezekael Hayel,Mohamed Ouarraou,Bruno Tuffin,Unknown,Male,Male,Male,,9
7.0,3.0,Networks and Spatial Economics,19 December 2006,https://link.springer.com/article/10.1007/s11067-006-9014-3,A Multiclass Simultaneous Transportation Equilibrium Model,September 2007,Mohamad K. Hasan,Hussain M. Dashti,,Male,Male,Unknown,Male,,19
7.0,3.0,Networks and Spatial Economics,19 December 2006,https://link.springer.com/article/10.1007/s11067-006-9010-7,A Study on Network Design Problems for Multi-modal Networks by Probit-based Stochastic User Equilibrium,September 2007,Kenetsu Uchida,Agachai Sumalee,Richard Connors,Unknown,Unknown,Male,Male,,40
7.0,3.0,Networks and Spatial Economics,20 December 2006,https://link.springer.com/article/10.1007/s11067-006-9012-5,Network-based Accessibility Measures for Vulnerability Analysis of Degradable Transportation Networks,September 2007,Anthony Chen,Chao Yang,Ming Lee,Male,,,Mix,,
7.0,3.0,Networks and Spatial Economics,27 February 2007,https://link.springer.com/article/10.1007/s11067-006-9016-1,Extended Price Cap Mechanism for Efficient Transmission Expansion under Nodal Pricing,September 2007,Makoto Tanaka,,,,Unknown,Unknown,Mix,,
7.0,3.0,Networks and Spatial Economics,29 December 2006,https://link.springer.com/article/10.1007/s11067-006-9002-7,An Investigation on the Effectiveness of Joint Receiver–Carrier Policies to Increase Truck Traffic in the Off-peak Hours,September 2007,José Holguín-Veras,Michael Silas,Brenda Cruz,Male,Male,Female,Mix,,
7.0,4.0,Networks and Spatial Economics,29 August 2007,https://link.springer.com/article/10.1007/s11067-007-9033-8,Transport Networks and Metropolitan Development: New Analytical Departures,December 2007,Aura Reggiani,Peter Nijkamp,,Female,Male,Unknown,Mix,,
7.0,4.0,Networks and Spatial Economics,20 September 2007,https://link.springer.com/article/10.1007/s11067-007-9029-4,Using Raster-Based GIS and Graph Theory to Analyze Complex Networks,December 2007,Laurie A. Schintler,Rajendra Kulkarni,Roger Stough,Female,Male,Male,Mix,,
7.0,4.0,Networks and Spatial Economics,13 August 2007,https://link.springer.com/article/10.1007/s11067-007-9027-6,Network Analysis of Commuting Flows: A Comparative Static Approach to German Data,December 2007,Roberto Patuelli,Aura Reggiani,Franz-Josef Bade,Male,Female,Unknown,Mix,,
7.0,4.0,Networks and Spatial Economics,05 September 2007,https://link.springer.com/article/10.1007/s11067-007-9032-9,Traffic Grammar and Algorithmic Complexity in Urban Freeway Flow Patterns,December 2007,Kingsley E. Haynes,Rajendra Kulkarni,Roger Stough,Male,Male,Male,Male,,2
7.0,4.0,Networks and Spatial Economics,07 August 2007,https://link.springer.com/article/10.1007/s11067-007-9030-y,"Modeling Urban Land Use Change and Urban Sprawl: Calgary, Alberta, Canada",December 2007,Heng Sun,Wayne Forsythe,Nigel Waters,,Male,Male,Mix,,
7.0,4.0,Networks and Spatial Economics,25 September 2007,https://link.springer.com/article/10.1007/s11067-007-9028-5,Meta-Analysis and the Value of Travel Time Savings: A Transatlantic Perspective in Passenger Transport,December 2007,Luca Zamparini,Aura Reggiani,,Male,Female,Unknown,Mix,,
7.0,4.0,Networks and Spatial Economics,14 September 2007,https://link.springer.com/article/10.1007/s11067-007-9031-x,Exploring the Role of Transportation in Fostering Social Exclusion: The Use of GIS to Support Qualitative Data,December 2007,Talia McCray,Nicole Brais,,Female,Female,Unknown,Female,,72
8.0,1.0,Networks and Spatial Economics,27 November 2007,https://link.springer.com/article/10.1007/s11067-007-9039-2,Introduction to the Special Issue on Telecommunications,March 2008,Eli Olinick,S. Raghavan,,Female,Unknown,Unknown,Female,,
8.0,1.0,Networks and Spatial Economics,15 December 2007,https://link.springer.com/article/10.1007/s11067-007-9038-3,Hop-Constrained Node Survivable Network Design: An Application to MPLS over WDM,March 2008,Luís Gouveia,Pedro Patrício,Amaro de Sousa,Male,Male,Male,Male,,30
8.0,1.0,Networks and Spatial Economics,28 December 2007,https://link.springer.com/article/10.1007/s11067-007-9041-8,An Equitable Bandwidth Allocation Model for Video-on-Demand Networks,March 2008,Hanan Luss,,,Female,Unknown,Unknown,Female,,8
8.0,1.0,Networks and Spatial Economics,16 January 2008,https://link.springer.com/article/10.1007/s11067-007-9040-9,The Market for Video on Demand,March 2008,Joakim Kalvenes,Neil Keon,,Male,Male,Unknown,Male,,4
8.0,1.0,Networks and Spatial Economics,28 December 2007,https://link.springer.com/article/10.1007/s11067-007-9042-7,Distributed Algorithms for Rate-Adaptive Media Streams,March 2008,Steven Weber,Vilas Veeraraghavan,,Male,Unknown,Unknown,Male,,5
8.0,2.0,Networks and Spatial Economics,20 December 2007,https://link.springer.com/article/10.1007/s11067-007-9044-5,Preface,September 2008,Concepción Román García,Raquel Espino Espino,José Holguín-Veras,,Female,Male,Mix,,
8.0,2.0,Networks and Spatial Economics,27 December 2007,https://link.springer.com/article/10.1007/s11067-007-9043-6,On Confounding Preference Heterogeneity and Income Effect in Discrete Choice Models,September 2008,Francisco Javier Amador,Rosa Marina González,Juan de Dios Ortúzar,Male,Female,Male,Mix,,
8.0,2.0,Networks and Spatial Economics,29 December 2007,https://link.springer.com/article/10.1007/s11067-007-9045-4,Empirical Identification in the Mixed Logit Model: Analysing the Effect of Data Richness,September 2008,Elisabetta Cherchi,Juan de Dios Ortúzar,,Female,Male,Unknown,Mix,,
8.0,2.0,Networks and Spatial Economics,29 December 2007,https://link.springer.com/article/10.1007/s11067-007-9047-2,"Integrating Travel Delays, Road Safety, Care, Vehicle Insurance and Cost-Benefit Analysis of Road Capacity Expansion in a Unified Framework",September 2008,Luis Ignacio Rizzi,,,Male,Unknown,Unknown,Male,,5
8.0,2.0,Networks and Spatial Economics,13 February 2008,https://link.springer.com/article/10.1007/s11067-007-9048-1,Analyzing Mobility in Peripheral Regions of the European Union: The Case of Canarias-Madeira-Azores,September 2008,Concepción Román,Raquel Espino,Gustavo Nombela,,Female,Male,Mix,,
8.0,2.0,Networks and Spatial Economics,28 December 2007,https://link.springer.com/article/10.1007/s11067-007-9051-6,Theoretical Evidence of Existing Pitfalls in Measuring Hubbing Practices in Airline Networks,September 2008,Juan Carlos Martín,Augusto Voltes-Dorta,,Male,Male,Unknown,Male,,20
8.0,2.0,Networks and Spatial Economics,11 January 2008,https://link.springer.com/article/10.1007/s11067-007-9050-7,The Integrated Dynamic Land Use and Transport Model MARS,September 2008,Paul Pfaffenbichler,Günter Emberger,Simon Shepherd,Male,Male,Male,Male,,48
8.0,2.0,Networks and Spatial Economics,08 January 2008,https://link.springer.com/article/10.1007/s11067-007-9052-5,An Integrated Behavioral Model of Land Use and Transport System: A Hyper-network Equilibrium Approach,September 2008,Luis Briceño,Roberto Cominetti,Francisco Martínez,Male,Male,Male,Male,,24
8.0,2.0,Networks and Spatial Economics,27 December 2007,https://link.springer.com/article/10.1007/s11067-007-9046-3,A Stochastic Process Approach for Frequency-based Transit Assignment with Strict Capacity Constraints,September 2008,Fitsum Teklu,,,Unknown,Unknown,Unknown,Unknown,,
8.0,2.0,Networks and Spatial Economics,29 December 2007,https://link.springer.com/article/10.1007/s11067-007-9054-3,Public Transit Corridor Assignment Assuming Congestion Due to Passenger Boarding and Alighting,September 2008,Homero Larrain,Juan Carlos Muñoz,,Male,Male,Unknown,Male,,22
8.0,2.0,Networks and Spatial Economics,13 February 2008,https://link.springer.com/article/10.1007/s11067-007-9049-0,Estimating the Functional Form of Road Traffic Maturity,September 2008,Antonio Núñez,,,Male,Unknown,Unknown,Male,,2
8.0,2.0,Networks and Spatial Economics,04 January 2008,https://link.springer.com/article/10.1007/s11067-007-9057-0,Road Pricing for Hazardous Materials Transportation in Urban Networks,September 2008,Rodrigo A. Garrido,,,Male,Unknown,Unknown,Male,,7
8.0,2.0,Networks and Spatial Economics,28 December 2007,https://link.springer.com/article/10.1007/s11067-007-9056-1,Productivity in Cargo Handling in Spanish Ports During a Period of Regulatory Reforms,September 2008,Juan José Díaz-Hernández,Eduardo Martínez-Budría,Sergio Jara-Díaz,Male,Male,Male,Male,,29
8.0,2.0,Networks and Spatial Economics,04 January 2008,https://link.springer.com/article/10.1007/s11067-007-9055-2,Model for Facilities or Vendors Location in a Global Scale Considering Several Echelons in the Chain,September 2008,Ricardo Hamad,Nicolau D. Fares Gualda,,Male,Male,Unknown,Male,,11
8.0,2.0,Networks and Spatial Economics,04 January 2008,https://link.springer.com/article/10.1007/s11067-007-9053-4,A Multicommodity Integrated Freight Origin–destination Synthesis Model,September 2008,José Holguín-Veras,Gopal R. Patil,,Male,Male,Unknown,Male,,52
9.0,1.0,Networks and Spatial Economics,03 December 2008,https://link.springer.com/article/10.1007/s11067-008-9092-5,Introduction,March 2009,Ingo Arne Hansen,,,Male,Unknown,Unknown,Male,,4
9.0,1.0,Networks and Spatial Economics,28 November 2008,https://link.springer.com/article/10.1007/s11067-008-9090-7,Evaluation of Railway Networks with Single Track Operation Using the UIC 406 Capacity Method,March 2009,Alex Landex,,,Male,Unknown,Unknown,Male,"Many railway networks have single track sections. These single track sections are more challenging than line sections with more tracks as there are more interdependencies between infrastructure and timetables. Therefore, single track operation requires a diligent coordination of infrastructure, timetables and traffic control. The complexity of single track operation has been a source of inspiration for many timetable models (Ghoseiri et al. 2004; Zhou and Zhong 2007) and optimisation models (Cordeau et al. 1998). These timetable and optimisation models try to maximise throughput, while insufficiently considering the impact of constraints, due to track layout and signalling system, on the capacity consumption of the railway lines, which may lead to underestimation of consecutive delays. The UIC 406 leaflet published in (2004) contains a simple, fast and effective method for evaluating the capacity consumption of railway lines. In the past years, this method has been applied in a number of studies (Höllmüller and Klahn 2005; Landex et al. 2006a, b). However, it is possible to expound the UIC 406 method in different ways, which may lead to different results. In spite of this fact, hardly any analyses of the differences have been carried out. In this article the UIC 406 capacity method is expounded especially for single track railway lines by means of a systems engineering approach. This is done by using fictitious typical examples instead of less illustrative examples from real world infrastructures and timetables. Altogether, the article is a methodological contribution to extending the applicability of the UIC capacity method to single track railway lines (and networks). The article is to some extent based on (Landex et al. 2007). First, in Section 2 a paradox of the UIC 406 method, i.e. operating more trains on a single track line may result in less capacity consumption, is explained and how this is dealt with in Denmark. In Section 3 parallel movements at crossing stations, railway junctions, and idle capacity to operate more trains are analysed. The need for crossovers on double track railway lines in case of contingency operation is described in Section 4, while determination of capacity bottlenecks and analyses of network effects are discussed in Section 4.",33
9.0,1.0,Networks and Spatial Economics,23 November 2008,https://link.springer.com/article/10.1007/s11067-008-9091-6,Design of a Railway Scheduling Model for Dense Services,March 2009,Gabrio Caimi,Dan Burkolter,Marco Laumanns,Unknown,Male,Male,Male,"Railway traffic in Switzerland, as well as in many other countries, has increased considerably for both passenger and freight transportation during the last few years, and this trend is expected to continue. Construction of new tracks, though, is very expensive and hardly possible in many city centers. The capacity of the existing network must therefore be better utilised to meet the customer demand for an enlarged offer. When increasing the density of the timetable, scheduling trains becomes more and more difficult as the chosen schedule not only has to meet safety restrictions, but should also minimise propagation of delays. An automatic generation of conflict-free timetables in reasonable time can be very helpful in order to evaluate several alternative timetables. Therefore, the interest in automatically generating railway timetables has increased over the past years. In particular, the Swiss Federal Railways (SBB), major operator of railway infrastructure in Switzerland, is currently investing efforts into the development of efficient methods for generating and operating railway schedules (Laube et al. 2007; Lüthi et al. 2007). Usually, the strategic timetable generation is done in two steps:
 In the first step, with the help of origin-destination matrices or other evaluations of the passenger demand, an offer of train services with lines and frequencies is developed to meet the customer needs. We call this offer train service intention, since at this point it is not known whether this offer is realisable. A train service intention consists of train lines and frequencies, specifying the customer-relevant information such as stop stations, interconnection possibilities, and rolling stock. For detailed information we refer to Laube and Mahadevan (2008). In a second step, the feasibility of a service intention is checked by trying to generate a feasible schedule. If this is possible, a schedule is provided as proof of feasibility, otherwise both steps have to be reiterated until a feasible service intention is found. Our research focuses on the second step, the construction of a timetable for a given train service intention. We concentrate on the creation of detailed train schedules in which both, an itinerary through the railway topology and passing times, have to be determined for each train. In this way we can guarantee that the provided timetable runs conflict-free, i.e., assuming no delays, all trains can run exactly as planned without creating safety conflicts, and no rescheduling due to resource conflicts becomes necessary. The creation of detailed train schedules is relevant for both strategic and tactical timetable generation. It guarantees feasibility of the corresponding service intention in the long-term case, whereas in the short term it enables the operation of a conflict-free timetable. The problem of finding detailed train schedules for each train is accentuated in major stations with many incoming an outgoing lines, where connections with short transfer times must be provided. As a consequence, trains tend to arrive and leave during a short peak interval and the solution space of feasible routing assignments is more constrained. In contrast, there are less parallel tracks and much less switches in rural regions, resulting in a considerably smaller number of potential itineraries. Due to the lower traffic density in rural regions, time reserves can be introduced. We therefore propose a decomposition of the whole network into condensation zones and compensation zones, which can be treated with different models and algorithms according to their distinct properties. The decomposition and the approach to coordinate the different zones are introduced in Section 2. We then focus on the problem of scheduling trains in condensation zones, which are identified as the critical zones in the network. For a given train service intention and an arbitrary subset of boundary conditions, we present a generic model and an algorithm to create conflict-free train schedules in Section 3. Section 4 presents results for the test cases of Berne and Lucerne in Switzerland, and Section 5 concludes with a summary and outlook for further research. Related work (see Huisman et al. 2005 for a survey) reports two principal approaches to the problem of finding a schedule for a whole railway network: one abstracting from the detailed track topology and the other considering partial detailed topologies of the network. Of particular interest in the first case is the Periodic Event Scheduling Problem (PESP), in which a set of cyclic events is modelled via cyclic time window constraints (Serafini and Ukovich 1989). The PESP allows to model large railway networks in an aggregated way to produce draft timetables. Often these draft timetables only include arrival and departure times at major stations on the scale of minutes. PESP enables to schedule trains in a relatively large railway network (such as the Netherlands, see Odijk 1996), but the exact train routing on an aggregated level has to be known a priori and the safety system is only roughly modeled using headway times. However, PESP solutions do not guarantee timetable feasibility on a detailed level. In particular, PESP, as well as other approaches (e.g., Carey 1994), assumes infinite capacity in station regions. In the second approach, the detailed topology of a local region, typically a main station area, is taken into consideration for producing conflict-free timetables (Bourachot 1986; Carey and Carville 2003) or checking the feasibility of a given macroscopic timetable (Caimi et al. 2005; Zwaneveld et al. 1996). Effort to integrate these approaches for a conflict-free scheduling of a whole railway network have remained rare. To our knowledge, the only project that addressed this question is the Dutch project DONS in collaboration with the Dutch Railways. Kroon and Zwaneveld (1995) and Schrijver and Steenbeck (1994) presented a two-level approach for a decision support system to create conflict-free timetables for the Dutch Railways. In the upper level, the train service intention is known, as well as an aggregated railway topology. The module CADANS supports the generation of cyclic hourly draft timetables using the PESP model. In the lower level, the timetable generated by CADANS is checked only by considering the running times over the relevant track sections and the release time, but the blocking times are not checked to be conflict-free. This module is called STATIONS and its model and algorithms are presented in Zwaneveld et al. (1996, 2001). The approach seems to be very interesting for our aims, but it does not take into consideration the different properties of condensation and compensation zones, whose distinct characteristics might be exploited more effectively with disparate scheduling policies. Moreover, additional optimisation potential exists in the interface between the zones, in particular for utilising the compensation zones for buffering against delays.",32
9.0,1.0,Networks and Spatial Economics,22 November 2008,https://link.springer.com/article/10.1007/s11067-008-9087-2,Non-Discriminatory Automatic Registration of Knock-On Train Delays,March 2009,Winnie Daamen,Rob M. P. Goverde,Ingo A. Hansen,,Male,Male,Mix,,
9.0,1.0,Networks and Spatial Economics,15 November 2008,https://link.springer.com/article/10.1007/s11067-008-9088-1,An Advanced Real-Time Train Dispatching System for Minimizing the Propagation of Delays in a Dispatching Area Under Severe Disturbances,March 2009,Andrea D’Ariano,Marco Pranzo,,Female,Male,Unknown,Mix,,
9.0,1.0,Networks and Spatial Economics,21 November 2008,https://link.springer.com/article/10.1007/s11067-008-9089-0,The Influence of Anticipating Train Driving on the Dispatching Process in Railway Conflict Situations,March 2009,Thomas Albrecht,,,Male,Unknown,Unknown,Male,"Conflicts can not be avoided in practical railway operation, especially in complex networks with heterogeneous traffic. A severe disturbance of a single train may cause massive disturbances in the whole railway network. In order to avoid this, train dispatchers have used re-ordering strategies for many years: By modifying the order of the trains in practical operation, delays in the whole system shall be minimized. Dispatching algorithms and systems have been developed which provide the human dispatcher with information on future conflicts and propose solutions for those conflicts by modifying the routes of the trains or by re-ordering trains at switches or crossings (see Goodman and Takagi (2004) and Oh et al. (2006) for an overview and (Adenso-Diaz et al. 1999; D’Ariano and Pranzo 2007; D’Ariano et al. 2007; Ho et al. 1997; Jacobs 2004; Luethi et al. 2007; Rodriguez 2007; Sahin 1999; Wegele et al. 2007) for recent examples of dispatching systems). The proposed plans may be implemented on existing signalling and interlocking infrastructure. With the broad availability of communication and positioning technology at low cost, efficient control of train speed in conflict situations has become feasible. The goal of this kind of anticipating train control is an increase in track capacity, a reduction of energy costs and the avoidance of unscheduled stops. All three effects are obtained by anticipatingly slowing down the train before the critical conflict and then passing the critical infrastructure element with the shortest possible delay (see Fig. 1 and Albrecht 2005b, 2008; D’Ariano and Albrecht 2006; Gauyacq and Tariel 2003; Huerlimann 2001). 
 Fundamental principle of anticipating train control Anticipating driving is practised in daily operation in Switzerland (Achermann et al. 2007), other approaches are still in a development stage but have—on a small scale—been tested in real operation (Mazzarello and Ottaviani 2007; Oetting 2006; Pacciarelli and Pranzo 2006). All systems have in common, that the advisory speed is computed in an off-board centralised computing system and transmitted to the train via radio. In the future this functionality should be integrated into Driver Assistance or Advice Systems (DAS) or Automatic Train Operation (ATO) systems where available, cf. Albrecht (2005c). In all algorithms and systems described in the literature so far, train speed regulation is being regarded as a consecutive step to train dispatching, i.e. that the dispatching system assumes reactive driving to compute the objective function although anticipating driving is applied in practice. This paper discusses, under which circumstances this approach leads to non-optimal solutions and under which circumstances this approach is justified. First, the objective function used for dispatching is described, before the effects of different train driving strategies on this objective function are examined. These effects are analysed in the context of a dispatching system before the findings are finally applied in a case study.",38
9.0,1.0,Networks and Spatial Economics,14 November 2008,https://link.springer.com/article/10.1007/s11067-008-9085-4,Structure and Simulation Evaluation of an Integrated Real-Time Rescheduling System for Railway Networks,March 2009,Marco Luethi,Giorgio Medeossi,Andrew Nash,Male,Male,Male,Male,"Railways must become more efficient if they are to be successful in today’s highly competitive transport market. One way of becoming more efficient is to increase service frequency, but many railways are already operating at or near capacity and so adding trains would increase unreliability—thus making the railway less efficient. This paper describes an approach for rescheduling trains in real-time that will increase capacity without reducing reliability. In the mid-1980s Swiss cantons rejected plans for a new high-speed route on the main east–west axis across the country. After defeat of this plan, the Swiss Federal Railways (SBB) adopted a new rail strategy, called Bahn 2000, based on connecting the entire country with an integrated clock-face timetable. The Bahn 2000 infrastructure plan was based on providing the minimal level of investment that would allow the timetable to be operated. The Bahn 2000 plan was gradually implemented and in December 2004 a major part was put into service. Many routes are now operated on a 30-minute frequency pattern throughout the day. The integrated clock-face timetable provides an optimal timed transfer system for almost the entire country and results in high accessibility and generally shorter travel times for passengers. The service has been extremely successful at attracting more passengers to rail service and demand is expected to increase as additional elements of the Bahn 2000 are completed. The main problem with Bahn 2000 is that the integrated clock-face timetable means that many trains arrive at and depart from main stations in a short time interval. This means that capacity in these critical locations is at a premium. The increased service has also created capacity constraints at other locations on the Swiss railway network. These capacity problems are compounded by the need for trains to arrive at stations in time for passengers to transfer to connecting trains. The research project was designed to evaluate the ability of a new method of real-time rescheduling to reduce the impacts of delays on system-wide operations. The SBB is especially interested in this research, given the degree to which it relies on close connections between trains, but increasing reliability is important for all railways, not least because it enables railways to increase service while maintaining reliability. There are three main ways how railways can increase capacity. They are:
 Infrastructure—build new infrastructure (tracks, junctions, flyovers, etc.); Signalling—reduce train headways by reducing block length or introducing more advanced signal systems (e.g. higher levels of European Train Control System, ETCS) (Eichenberger 2007); and Operations—reduce train headways by reducing buffer times introduced in the schedule to maintain reliability or by harmonising travelling speed of trains. The infrastructure and signalling options are expensive and complicated; therefore the SBB is looking for ways to increase service by reducing buffer times. There are three types of buffer times added to train operating schedules: headway buffers between two consecutive trains, dwell time buffers and running time supplements. Headway buffer times are used to stabilise the system after an interruption, and give the dispatchers time to react. This enables the dispatchers to develop and implement strategies to reduce knock-on delays. Running time supplements are used to reduce the impact of running time variations caused by changing weather conditions or varying train dynamics; it also helps reduce knock-on delays. Reducing buffer times and running time supplement results in a denser level of rail traffic. The disadvantage of reducing buffer times to add additional trains is that it increases the number of interdependencies between train routes and ultimately increases the number of potential conflicts. Consequently, a single small disturbance can have large impacts on the whole network. Furthermore, the increased number of trains makes it more difficult for human dispatchers to identify optimal strategies for reducing knock-on delays quickly, thus making it difficult to prevent delays from propagating throughout the network. The problem is even worse for integrated clock-face timetables, since train connections at stations are broken to stabilise the system, resulting in passengers missing their connections. Given their reliance on the integrated clock-face timetable, the SBB is especially interested in examining new methods, ideas and technology for improving both capacity and service quality together. One possible solution is an integrated real-time rescheduling system combining new railway operational strategies with technology. This paper describes initial research on this approach. The key element of this integrated real-time rescheduling system is that the new timetables and their execution (i.e. driving the trains) must be more accurate than they are today (Stalder et al. 2003). Under this system, when a delay occurs, the rescheduling system generates an extremely accurate timetable that is designed to minimise knock-on delays throughout the system, and that the train can be operated within those parameters. This paper describes such a system and its benefits. The research was designed to answer the following three questions:
 What are the potential benefits for capacity and stability of using the integrated real-time rescheduling system (for a specific area)? What factors have a significant impact on the rescheduling process’s overall performance? What level of timetable and driving accuracy is needed to most effectively use the new approach? The next section of this paper describes real-time rescheduling and the proposed rescheduling system’s structure; this is followed by a case study of system application (simulation of Lucerne station), and finally conclusions including a discussion of results regarding the research questions.",12
9.0,1.0,Networks and Spatial Economics,03 December 2008,https://link.springer.com/article/10.1007/s11067-008-9086-3,USE of Railway Analysis Tools from an Australian Perspective,March 2009,Alex W. Wardrop,,,Male,Unknown,Unknown,Male,"Australia is a very large, sparsely populated, highly urbanised country. It has some significant suburban passenger railway systems associated with the major mainland capital cities. It has an extensive non-metropolitan railway network which is largely used by freight trains. Australia is a major trading nation and its private and public railway networks handle significant volumes of bulk, eg coal and iron ore, export commodities. Its railways also handle a significant non-bulk, eg container, transportation task, but its quantum is an order less than either coal or iron ore. There are imperatives to improve the management of Australia’s railway infrastructure and to operate its trains better in order to improve the railway system’s competitive position vis-à-vis road freight transport. Australia certainly has issues about its railways which catch the attention of its Federal and State Governments. Currently they hinge on the need to efficiently carry export freight flows. Furthermore, most of the suburban passenger railway networks are still growing and some of this growth may conflict with the management of rail freight flows. These issues may be summarised from a research and investigation perspective thus:
 how should Australian railway infrastructure be efficiently configured? how should Australian freight and passenger trains be efficiently composed and operated? how are these issues being examined? are these questions specific to Australia or might they apply to railways in Europe, North American and elsewhere? This paper thus identifies at least four areas of analysis associated with the planning and development of railway infrastructure and operations. It then identifies a range of analytical tools which could be applied to different components of these analytical areas and critiques them from an Australian perspective of their appropriateness. Having completed this assessment, the paper uses a contemporary Australian case study to show how analytical tools could be used and what lessons might be learnt from the process.",1
9.0,2.0,Networks and Spatial Economics,17 December 2008,https://link.springer.com/article/10.1007/s11067-008-9082-7,Time Dependent Origin-destination Estimation from Traffic Count without Prior Information,June 2009,Hsun-Jung Cho,Yow-Jen Jou,Chien-Lun Lan,Unknown,Unknown,Unknown,Unknown,,
9.0,2.0,Networks and Spatial Economics,07 May 2009,https://link.springer.com/article/10.1007/s11067-009-9103-1,Evacuation Transportation Planning Under Uncertainty: A Robust Optimization Approach,June 2009,Tao Yao,Supreet Reddy Mandala,Byung Do Chung,,Unknown,,Mix,,
9.0,2.0,Networks and Spatial Economics,17 January 2008,https://link.springer.com/article/10.1007/s11067-007-9058-z,The Optimal Transit Fare Structure under Different Market Regimes with Uncertainty in the Network,June 2009,Zhi-Chun Li,William H. K. Lam,S. C. Wong,,Male,Unknown,Mix,,
9.0,2.0,Networks and Spatial Economics,24 October 2008,https://link.springer.com/article/10.1007/s11067-008-9077-4,Paradigms to Deploy a Behavior-Consistent Approach for Information-Based Real-Time Traffic Routing,June 2009,Alexander Paz,Srinivas Peeta,,Male,Male,Unknown,Male,"The benefits of real-time traffic network control through information provision using Advanced Traveler Information Systems (ATIS) hinge on the controller’s ability to identify effective routing strategies that entail high levels of acceptability by drivers. Current efforts to deploy information provision strategies are primarily concentrated under the umbrella of Dynamic Traffic Assignment (DTA). However, the behavioral foundations of most DTA models are idealistic and insufficient to address real-world driver behavior (Peeta and Ziliaskopoulos 2001). This is primarily because existing DTA models are not behavior-consistent; they do not realistically factor the drivers’ likely response towards information while generating these strategies. They mostly pre-specify driver behavior. Some assume artificial compliance rates to predict traffic conditions or generate control strategies. Others use the DTA solution route assignment proportions “as is” for route guidance, and use a feedback loop or a consistency-checking procedure to correct for prediction errors. Thereby, most approaches do not have interactive linkages between route recommendations and driver response. Peeta and Yu (2006) propose a consistency-seeking procedure that updates behavior model parameters in an operational context based on unfolding field conditions. However, it is also reactive and does not entail a behavior-consistent paradigm. In summary, DTA models do not simultaneously consider network flow interactions and behavior realism to develop meaningful information-based network control strategies. To address the behavior realism gap of traditional DTA models vis-à-vis determining the time-dependent traffic flow patterns, Paz and Peeta (2007, 2008a) propose a behavior-consistent approach to determine and deploy real-time information-based network control strategies. It determines the information strategies by explicitly accounting for the drivers’ likely response to these strategies while determining them. That is, “behavior-consistent” implies that the information provided to the drivers is determined in such a way that the drivers are likely to follow the route recommendations because the information is based on an explicit estimation of the drivers’ likely route choices under the provided information. This implies solving a fixed-point problem that arises because the information strategies depend on driver behavior and vice versa. The proposed approach enhances system performance while being consistent with driver behavior. It also has reduced sensitivity to data needs as it is based on aggregate if-then rules that preclude the need for information at the individual driver level. These rules relate the route choice decisions to the routes characteristics, the driver attributes in terms of information availability, and level of responsiveness to the information strategies. As drivers are likely to use simple rules and/or a few factors (Nakayama and Kitamura 2000; Peeta and Yu 2005) to make on-line routing decisions due to the associated time constraints, the aggregate if-then rules consist of simple and straightforward one-dimensional left- and right-hand side components (Paz and Peeta 2008b). The behavior-consistent approach proposed by Paz and Peeta (2007, 2008a) implements a control mechanism that continuously directs the traffic system towards a desired system state through information provision. That is, the controller directs the system towards a particular objective such as the time-dependent system optimal (SO) state. Thereby, the controller may need to recommend routes for an origin–destination (O–D) pair to more or less drivers than suggested by the SO DTA solution so as to achieve close to SO route proportions. This is done using a controller-estimated if-then rules based driver behavior model. Further, the approach uses the concept of controllable routes to enhance behavior consistency whereby the route recommended to a driver belongs to the controller’s SO (desired) route set and the preferred route set for that driver. This increases the likelihood of the recommended route being accepted by the driver. It also circumvents a key practical concern that potentially arises for ATIS-based information provision. That is, while some researchers have advocated that drivers could be persuaded to use SO routes, others (such as Hall 1996) stress the value of “honest” information, and that in the long run drivers will resist SO routes that are not user optimal. While the notion of controllable routes enhances behavior consistency, it may entail practical limitations. For example, it is possible that an O–D pair may not have a controllable route as no controller-determined SO route coincides with a driver-preferred route. This motivates the consideration of alternative definitions for controllable routes to enable the deployment effectiveness of the behavior-consistent approach. In this paper, alternative controllable route paradigms are proposed that entail significant overlap of the controller-determined SO routes with the driver-preferred routes, but do not require perfect match. This enables the controller to recommend driver-preferred routes that are not necessarily SO routes, as well as target drivers who do not consider SO routes. At a more basic level, such a study can shed light on the interplay between route quality relative to controller objectives and driver real-time route choice decisions. By definition, the SO solution entails some long routes which may imply fewer common routes with the driver-preferred set. While the alternative overlap paradigms represent one mechanism to increase the controllable route set, another strategy is to use the user equilibrium (UE) solution as the controller’s objective. This is because UE routes have a more defensible behavioral rationale, possibly having a greater degree of commonality with the driver-preferred route set. In this study, we compare the performance of the behavior-consistent approach under the UE and SO objectives. It should be noted here that the commonly cited advantages of the UE paradigm over the SO benchmark for standard DTA models do not necessarily apply for the behavior-consistent approach. Since the behavior-consistent approach provides a trajectory to approach the desired system state in a manner consistent with individual driver routing decisions, the limitations arising from the behavioral underpinnings of the standard SO strategy relative to the UE strategy are obviated. That is, the compliance rates under the behavior-consistent approach are perceptibly higher than under the standard DTA paradigms (Paz and Peeta 2007). Further, the relative gap in compliance rates between UE and SO under the behavior-consistent approach tends to be smaller than under the standard DTA approach. It suggests that focusing on the SO paradigm can represent a legitimate deployment alternative with better behavior-consistent performance, rather than the UE centric focus of the current literature based on the behavior rationale. This aspect is analyzed in depth in this paper. A long-term phenomenon vis-à-vis driver behavior under information-based traffic routing is the influence of learning effects on driver response. Peeta and Yu (2005) show that several information-related phenomena can manifest over time based on past driving experience and the experience with the provided information. These include familiarity, trust in information, inertia, delusion, freezing, etc. Vaughn et al. (1993) and Bonsall and Joint (1991) present evidence that drivers may not comply with information perceived to be inaccurate. Over time, these effects and experiences can lead to changes in the set of routes preferred by a driver. Nakayama and Kitamura (2000) show that drivers may ignore routes associated with poor travel experience and remove them from their preferred route sets. By contrast, it is also possible that a driver may add new alternatives to his/her preferred route set based on positive experiences with a controller-recommended or a newly-explored route. Hence, the driver-preferred route set can potentially change over time. The number of routes in the driver-preferred route set is significantly influenced by the driver’s network familiarity. Familiar drivers are likely to have larger preferred route sets compared to unfamiliar drivers. While this paper does not consider a day-to-day learning framework, we explore the effect of increasing the driver-preferred route set with alternative route type paradigms, and compare the performance of these paradigms from a deployment perspective. The remainder of this paper is organized as follows. Section 2 summarizes the solution framework for the behavior-consistent approach and defines relevant terms. Section 3 describes the alternative controllable route paradigms proposed in this study. Section 4 discusses experiments and analyzes their results. Section 5 presents some concluding comments.",12
9.0,2.0,Networks and Spatial Economics,11 February 2009,https://link.springer.com/article/10.1007/s11067-009-9100-4,A Link-Node Discrete-Time Dynamic Second Best Toll Pricing Model with a Relaxation Solution Algorithm,June 2009,Xuegang (Jeff) Ban,Henry X. Liu,,Unknown,Male,Unknown,Male,"The advent of congestion pricing and emerging technologies in implementing tolling are among the most promising options to address traffic congestion that has become not only an increasingly critical problem to our quality of life but also has serious consequences in terms of economic development (Federal Highway Administration 2007). Current research on congestion pricing has been mainly concentrating on static pricing, aiming to find an optimal (and fixed) toll on all links (i.e., the first-best toll pricing) or a subset of links (i.e., the second-best toll pricing) in a traffic network. Research in this regard is rich and still growing. One may refer to Lawphongpanich and Hearn (2004), Hearn and Ramana (1998), Yang and Huang (2005), and Sumalee (2007) for more details. Static congestion pricing, however, ignores traffic dynamics and the generated tolls are not suitable if time-varying traffic flows are considered. Rather, time varying tolls generated via dynamic congestion pricing can be more effective to solve ever-increasing congestion problems, especially in heavily-congested urban areas (Friesz et al. 2002). Day-to-day and within-day toll pricing have been developed in the literature to account for traffic dynamics. The former captures inter-day (long-term) traffic dynamics, while within-day traffic is usually considered as static, which aims to adjust tolls in a day-to-day basis so that certain system objective or equilibrium can be achieved (Friesz et al. 2002). Within-day toll pricing focuses on short-term (i.e. within-day) traffic dynamics, aiming to adjust tolls in a real-time basis. Dual schemes for combining both day-to-day and within-day toll pricing have also been proposed by several researchers (e.g. Friesz et al. 2007). In this paper, we concentrate on the within-day toll pricing problem, which we refer to as “dynamic congestion pricing” hereafter in this paper. Similar to static congestion pricing, the dynamic congestion pricing may be categorized as first-best toll pricing and second-best toll pricing. The first-best toll pricing assumes that every link in the network can be tolled and marginal cost is the main mechanism for studying first-best toll pricing problems. For example, Carey and Srinivasan (1993) studied marginal costs and tolls in networks with time-varying flows. Wie and Tobin (1998) developed both day-to-day and with-day models for dynamic first-best toll pricing. In particular, the with-day model in Wie and Tobin (1998) assumed fixed demand and was based on instantaneous dynamic user equilibria (DUE). In reality, however, not every link in a network can be tolled due to technical or policy constraints. Therefore, the dynamic second-best toll pricing (DSBTP), which imposes tolls only on a subset of links in a network, receives more attention from both practitioners and researchers. Most previous research efforts (Chu 1995; Yang and Huang 1997; Arnott et al. 1998; Liu and McDonald 1999; Kuwahara 2007) have been focusing on studying dynamic congestion pricing for either a single bottleneck or simple networks (e.g. a network with one OD pair and two parallel routes). For these simplified cases, the underlying route choice model can be much simplified so that analytical results can be obtained with relative ease. Therefore, although some insightful results were discovered from these studies, their models may not be applied directly to DSBTP on general networks. In a general network, it may be more appropriate to model the route choice behavior as DUE as shown in Wie and Tobin (1998). Recently, Friesz et al. (2007) developed a DSBTP model that combines both day-to-day and with-in day traffic dynamics. The model was developed in the continuous-time domain, which makes them infinite-dimensional mathematical programming problems. Currently, theories and algorithms for solving large-scale infinite-dimensional mathematical programming problems are not yet widely available in the literature. In this paper, we take a different stand and formulate DSBTP in the discrete-time domain, resulting in a finite-dimensional mathematical programming problem for which a rich set of theories and algorithms have been developed. We particularly model the underlying route choice as a DUE problem to account for user route choices in general networks. The resulting model is a bi-level programming problem or MPEC (mathematical programming with equilibrium constraints, see Luo et al. 1996), similar to that in Friesz et al. (2007). As pointed out in Wie et al. (2002), finite-dimensional and infinite-dimensional mathematical programming problems are essentially different problems, and should be treated separately. Therefore, although our model is similar to certain extent in its form to those in Friesz et al. (2007), the solution existence conditions and solution algorithms are quite different. The work in this paper is also based on the link-node based discrete-time DUE formulation we proposed recently (Ban et al. 2008). We first show that the bilevel DSBTP model has at least one solution under certain assumptions, independent of the actual formulation of the lower level DUE. This finding may provide some insights on DSBTP models that employ rather complicated traffic flow models in the underlying DUE formulation. We then observe that a recently developed DUE model (Ban et al. 2008) satisfies the solution existence conditions established in this paper for DSBTP. By applying this DUE model, the bilevel formulation can be readily converted to a single level NLP (nonlinear programming problem). To solve the single level model, a relaxation algorithm is adopted as has been used by the authors for solving the static continuous network design problem (Ban et al. 2006). Numerical examples are provided in this paper to illustrate the proposed model and algorithm in this paper. In particular, we show that by varying travel time weights on different links, DSBTP can help traffic management agencies better achieve certain system objectives. Examples are also given on how changes of the weights impact the optimal tolls and associated objective function values.",22
9.0,2.0,Networks and Spatial Economics,04 March 2009,https://link.springer.com/article/10.1007/s11067-009-9102-2,Coincident Cost Improvement vs. Degradation by Adding Connections to Noncooperative Networks and Distributed Systems,June 2009,Hisao Kameda,,,Male,Unknown,Unknown,Male,"There exist numerous systems wherein a number of independent users share and compete for resources on a network. Consider, for instance, a distributed computing system, such as a grid, composed of a network of servers on a local or wide-area network (Foster and Kesselman 1998), or a packet-switched computer network like the Internet (see, e.g., Keshav 1997; Stevens 1994; Chen et al. 1999). The network consists of nodes, i.e., computers (hosts) and routers, which are connected by links, namely, the communication lines. Each job, or packet, sent by a node, is associated with a unique pair of hosts: its origin and destination nodes. Jobs originating at a host flow through a path that consists of a series of interconnected routers to their destination host. Each router may keep routing information in some form, e.g., of a routing table. Such information tells each arriving job to which adjacent router the packet is to be forwarded. Alternatively, so-called ‘source routing’ arises when the path of each job is specified at the origin. Then, each job carries this information about its path while passing through the network. In addition, there exist protocols that provide routing information at each router, so that every job, or packet, may be guided through a path of the shortest cost among the paths that connect the same pair of origin and destination nodes, given the cost of each link. Thus, such a routing protocol assigns to each link the cost that reflects the estimated communication delay through the link. The communication delay and availability of each link may vary from time to time, and so routers need to exchange packets to update routing information. So-called ‘dynamic routing protocols’ work in this way. Such exchange of packets, however, cannot be done too frequently. Otherwise, links would be flooded and performance would degrade. Thus, the information at each router is updated at some regular (but not too short) intervals, and the cost optimization process is not truly ‘dynamic’ but rather ‘quasi-static.’ Job and/or packet generation is regarded as a stochastic process. It is probable that such processes are stable, i.e., in a stochastic equilibrium, during a time period that contains a large number of update intervals. In quasi-static control, a shortest path routing that reflects communication delays as link costs may cause oscillations in the amount of jobs, or packets, that flow through each link, and thus, oscillations in the communication delay of each link or path. Such oscillations could be avoided by means of suitably forecasting the expected delay of each link for the next update interval, and/or by employing, if needed, an adequate mixing strategy of using more than one possible path, each used at a certain ratio or frequency. Thus, if suitably controlled, shortest path routing may bring about situations wherein each job, or packet, flows through one of the paths of the shortest cost (communication delay) among the paths that connect its origin and destination nodes. The resulting network flow must be very close to a Wardrop equilibrium (of nonatomic users). On the other hand, source routing may provide situations quite close to a Nash equilibrium, as in the following example. Consider a situation where the information on the estimated delay of each link, and thus each path, is available at each origin. An autonomous system, e.g., a local-area computer network in a large enterprise, or a telephone network run by an Internet-service provider, may be connected to the network at an origin. Then, the manager or the administrator of the autonomous system would like to minimize the overall cost or mean delay of the jobs that are sent from the origin into the computer communication network. Again, such oscillations mentioned above could be avoided by means of suitably forecasting the expected delay of each link in the next update interval, and/or by employing, if needed, an adequate mixing strategy of using more than one possible path, each used at a certain ratio or frequency. Thus, if suitably controlled, source routing may bring about situations wherein the mean delay of the packets of an autonomous system is at a minimum, given the routing decisions on the packets of the other autonomous systems that are connected to the network. In such cases, the situation must be very close to a Nash equilibrium. Nash and Wardrop equilibria are two related paradigms that describe a stable network flow as a function of its characteristics. While there are similarities between the two notions of network equilibrium, as we shall see in this paper, there are important differences as well. Wardrop equilibria have been discussed extensively in transportation science which continues to develop (Patriksson 2004; Shao et al. 2006). The famous (original) Braess paradox shows that adding a connection (a link) to a network may sometimes degrade the cost for all users in a Wardrop equilibrium. If performance degrades coincidently for all users when a new connection is added to a system, it is called a  paradox. The Braess paradox has attracted the attention of many researchers. A list of references on the Braess paradox is kept in Braess’ home page at http://homepage.ruhr-uni-bochum.de/Dietrich.Braess/#paradox. Only a few studies have provided an estimation of how harmful the paradox can be, i.e., the worst-case degree of coincident cost degradation for all users by adding connections to a noncooperative system (Kameda 2002; Roughgarden 2006a; Roughgarden and Tardos 2004). It has been shown that there exists a system in a Nash equilibrium for any size of the degree of the paradox. In contrast, there has not been found any system in Wardrop equilibrium for which the degree of coincident cost degradation can increase without bound if the number of nodes in the network is bounded. Moreover, we have not seen any estimation of how beneficial the addition of connections to a noncooperative network can be, i.e., the best-case degree of coincident cost improvement by adding connections to a noncooperative system. This paper answers the following questions: 
 For Wardrop and Nash networks (networks in a Wardrop or Nash equilibrium), is there a parameterization that leads to every level of coincident cost degradation up to a bound, and every value of coincident cost improvement, when adding new connections to a network? What is the effect of adding connections to distributed computer systems? Note, in passing, that each user of a network may have decisions on flow control, in addition to routing. In wireless networks, users may have power control. The concept of a Nash equilibrium is also discussed with a paradoxical behavior discovered (Inoie et al. 2006), both of which are not addressed in this paper. As to the literature on the game theory and networks, see also Altman and Wynter (2004). The next section introduces Nash and Wardrop networks. Section 3 presents the measure of coincident degradation (paradox) and improvement for all users by adding connections to systems. Section 4 answers question (i) above, while Section 5 discusses distributed computing systems and responds to question (ii). Section 6 concludes and summarizes the contributions of the paper.",7
9.0,3.0,Networks and Spatial Economics,29 May 2009,https://link.springer.com/article/10.1007/s11067-009-9105-z,Introduction to the Special Issue on the Evolution of Transportation Network Infrastructure,September 2009,David Levinson,,,Male,Unknown,Unknown,Male,,7
9.0,3.0,Networks and Spatial Economics,18 October 2007,https://link.springer.com/article/10.1007/s11067-007-9037-4,Modeling the Growth of Transportation Networks: A Comprehensive Review,September 2009,Feng Xie,David Levinson,,,Male,Unknown,Mix,,
9.0,3.0,Networks and Spatial Economics,08 March 2008,https://link.springer.com/article/10.1007/s11067-008-9060-0,"Inter-Modal Network Externalities and Transport Development: Evidence from Roads, Canals, and Ports During the English Industrial Revolution",September 2009,Dan Bogart,,,Male,Unknown,Unknown,Male,"The pre-railway transport network in England underwent a substantial transformation between 1760 and 1830. This section briefly reviews the trends in road, port, and canal improvements, their legal and organization aspects, and reviews the existing hypotheses about which factors influenced transport development. England had an extensive road network in 1760, but it became larger and better maintained by 1830.Footnote 2 Since the middle ages there was a network of roads connecting London with all major provincial centers, like Bristol, Shrewsbury, York, and Norwich. There were also ‘cross-roads’ connecting provincial centers. After 1760 many primary and secondary roads were resurfaced with gravel and were widened to allow for greater traffic. There was also greater construction of new roads, especially near industrial areas and large cities. New road construction accelerated in the 1790s and early 1800s and continued up to 1830. As an island nation, Britain had ports since time immemorial.Footnote 3 Over time ports were restricted to a set of locations where customs taxes were collected. In 1760, there were 74 ports in England or Wales and most were located in the southeast near London. The expansion of foreign trade during the mid-18th century placed greater demands on existing port facilities. New docks and harbors were needed to handle larger ships and the increased volume. Wet docks were constructed in the major ports of Liverpool, Bristol, London, and Hull as well as in emerging ports like Lancaster, Goole, and Grimsby. There were also improvements to harbors in smaller ports throughout the country, many of which served specific types of trade. All of these improvements contributed to a substantial increase in port capacity and efficiency by 1830. The canal era began in earnest with the opening of the Duke of Bridgewater’s canal in 1761.Footnote 4 It linked the town of Worsley with Manchester. It was enormously successful because it allowed the Duke to sell coal from his estates to a large number of urban consumers and factories. Bridgewater’s canal was quickly emulated. Birmingham had its first canal in 1772 and several river systems, like the Trent, Mersey, Severn, and Humber, were linked by canals in 1777. The initial burst of investment slowed between 1775 and 1789, but was later reignited in the 1790s. The years 1793 and 1794 are famously known as the ‘canal mania,’ in which 28 canals were initiated, mostly in the midlands and the north. The most famous was the 60-mile Grand Junction canal connecting London with Birmingham. The Grand Junction specialized in long-distance transit, but it was unique in that most canals were designed to transport coal or agricultural goods to nearby urban areas. There was another flurry of canal promotion in the early 1810s but it was less substantial than the canal mania. By 1830 the era of new canal construction was largely over, but not before nearly 2000 miles of navigable canals had been completed. Road, port, and canal improvements shared a common feature in that they all required an act of Parliament for their implementation. Common law and statutory law dating from the 1500s dictated that local inhabitants were required to provide for the maintenance of nearby roads, waterways, and ports, but they were not required to improve them. Even if local inhabitants wanted to build a new road, canal, or dock it would have been difficult without first obtaining rights-of-way through an act of Parliament. In the 18th century, acts were very specific in that they gave a group of individuals, a city, or a company the right to improve a particular road, canal, or port. Improvements might include new construction, the diversion or widening of an exiting route, or simply better maintenance. Acts were also crucial because they gave individuals, cities, and companies’ rights to levy tolls and issue bonds secured by the income from the tolls. The tolls and bonds provided the financial sources which paid for most projects. The tolls were particularly important for roads because they allowed local communities to share the fiscal burden with through-travelers (see Levinson 2002). Throughout the 1700s and early 1800s Parliament passed thousands of acts dealing with local infrastructure (Innes 1998). Interestingly very few of these acts were initiated by the Ministry or Parliament. Instead local individuals and officials initiated acts with petitions submitted to Parliament. For example, in 1742, the Justices of Peace, gentlemen, and other persons living near the road between Harlow and Great Chesterford submitted the following petition stating the need for an act to improve their road: The aforesaid road about 25 miles in length, is by reason of the deepness of the soil and many heavy carriages passing through it, become very ruinous and in the winter many parts are almost impassable for wagons and carriages, and also for horses laden; and other parts are dangerous to travelers, and the roads cannot by the ordinary course appointed by laws and statutes be effectually amended without the aid of an act of parliament.Footnote 5
 The preceding petition resulted in the passage of an act for repairing and widening the road between Harlow and Great Chesterford in the same year of 1742.Footnote 6 Such acts are sometimes referred to as ‘turnpike acts,’ because they authorized the erection of a gate for collecting tolls. Similar petitions were also introduced by local property-owners, businessmen, and officials stating the need to build wet docks or canals. In this sense, the development of roads, ports, and canals was highly decentralized. Parliament approved projects, but it did not initiate them. Parliament did influence transport development through its regulations. One of the most important concerned the organizational form of transport authorities. Road acts named several individuals living near the road as trustees and gave them authority to make decisions about improvements, but it forbade trustees from earning direct profits from the tolls. Instead it was expected that trustees would be willing to implement and oversee road improvements for the betterment of their nearby property or business. Many early port authorities were also restricted from earning profits through the tolls, but starting in the early 1800s it was increasingly common for port acts to create joint-stock companies with legal rights to pay dividends to shareholders. Many dock improvements in the major ports, like Bristol, Hull, and London, were undertaken by joint stock companies as opposed to dock trusts (Jackson 1983, p. 203). For canals, joint stock companies were always the dominant organizational form. Their financial success varied, but dividends were generally high, averaging 5.75% (see Duckham 1983, p. 123). The indirect returns from canals were also large, particularly for mining interests and industrialists. The English transport network developed between 1760 and 1830 due to several factors. At a macro-level, annual fluctuations in investment were related to changes in interest rates, trade, or industrial production. T.S. Ashton (1968) argued that infrastructure projects in the 18th century were sensitive to long-term interest rates because they entailed large up-front investments in materials and structures. Ashton’s argument has been criticized in many works, but it remains a key hypothesis in the literature.Footnote 7 Another argument is that higher economic growth in the 1760s, 1790s, and 1810s stimulated transport investment, while the economic slowdown in the 1770s and 1800s deterred investment.Footnote 8 Economic growth was important because it influenced the expected revenues earned by road, port, and canal projects. It also influenced the indirect returns earned by property-owners and businesses. There is some evidence in the literature that network externalities also affected English transport development. For example, it appears that some communities delayed road improvements until other communities improved contiguous roads, particularly those that provided connections to London (Bogart 2007). There is also some evidence that communities learned about the benefits of road improvements from neighboring communities (Albert 1972). Of particular importance in this paper is the possibility that externalities existed between modes, like roads, canals, and ports. Baron Duckham (1983, p. 132) argues that ports, like Liverpool, were spurred by the development of canals in their hinterland and even further away. John Armstrong and Philip Bagwell (1983, p. 161) argue that in many ports, coastal ships “fed the river barges and canal boats with traffic and in return were fed by them.” Eric Pawson (1977) argues that developments in road and waterway transport were inter-dependent; noting that many turnpike roads were connected with river ports (p. 164). Lastly, Charles Hadfield provides evidence that the Duke of Bridgewater changed the route of his famous canal so that it would end in Manchester in part because road access was better (1968, p. 29). The existing literature has noted the inter-connection between road, canal, and port development, but it has not formulated testable hypotheses regarding how road, canal, and port improvements were inter-related. The following section describes data on road, canal, and port improvement acts. The subsequent section uses investment theory to develop testable hypotheses on the inter-relationship between modes.",19
9.0,3.0,Networks and Spatial Economics,20 May 2008,https://link.springer.com/article/10.1007/s11067-008-9064-9,The Efficiency of the Victorian British Railway Network: A Counterfactual Analysis,September 2009,Mark Casson,,,Male,Unknown,Unknown,Male,"A key issue in the history of the UK railway system—and the major focus of this study—is the question of whether the railway system was an efficient response to the traffic requirements of the economy (Simmons and Biddle 1997). While the efficiency question applies to both the construction and operation of railways, the efficiency of operation is heavily constrained by the structure of the system, and so the construction of the system is the key topic in this paper. The study encompasses England, Wales and Scotland, and excludes Ireland, partly because it is geographically separate, and partly because the political and economic factors that impinged on railway development there were very different from the rest of the UK. The Isle of Wight is included and the Isle of Man excluded because the former is included in the official railway statistics for England and Wales and the latter is not. Underground railway systems, such as the London tube lines, are not included. The main period of railway construction was 1825–1914, and so this was the period selected for study. Prior to the opening of the Stockton and Darlington Railway (S and DR) in 1825, railways were normally powered by horses rather than steam locomotives, conveyed traffic consigned by their owners rather than by the public, and carried mainly minerals—especially coal. The S and DR was the world’s first steam-powered public passenger carrying railway, and was quickly followed by the world’s first inter-urban trunk railway—the Liverpool and Manchester (L and MR), opened in 1830, which operated scheduled express services on which businessmen could make a return journey within a day. By 1914 three major railway-building booms had come and gone, and the construction of trunk lines had ended (it did not re-start until the Channel Tunnel Rail Link of the 1990s). The railway’s monopoly of long-distance inland transport, based on its superiority to the canal, was being eroded by road competition—first from the tram, and then from the coach, lorry and motor car. The Railway Grouping of 1923 terminated much of the inter-company competition that characterised the 1825–1914 period, as the Big Four companies formed at the grouping turned their attention to fighting road competition instead. The UK railway system was constructed entirely by private enterprise, with minimal state subsidies. The efficiency of the system is therefore, indirectly, a judgement on the performance of private enterprise. Although Simmons (1994), Turnock (1985) and other transport historians have pointed to apparent inefficiencies in the structure of the system—in particular, the duplication of main lines—no-one has so far spelled out in detail what an efficient railway system would have looked like. If the efficient system looked just like the existing system but without the duplication of main lines then the degree of inefficiency could be deemed quite modest. If the efficient alternative was completely different, however, then the inefficiencies could be very large. This study suggests that the inefficiencies were not only large, but larger than anyone has ever suggested before. In 1914 the railway system comprised approximately 20,000 route miles of track. This study suggests that equivalent social benefits could have been obtained with only 13,000 miles of track. The method of calculation is explained in detail below. The explanation for the result lies partly in the fact that it was not only main lines that were duplicated, but lines to and from mining areas, ports, and industrial centres. There was an excessive density of lines in lightly populated rural areas. Many railway hubs were located at relatively isolated locations, such as Crewe, Swindon and Ashford, rather than at major centres of population nearby, such as Stoke-on-Trent, Trowbridge and Dover. This resulted in duplication of hub facilities, and required additional lines to serve the hubs. Great cities such as London, Manchester and Leeds did not fulfil their potential to be major hubs because passenger routes did not join up there: each railway company, or group of companies, had its own terminus in the city. Although private enterprise was wasteful, it is always possible to argue that state planning would have been no better, or even worse. Evidence suggests, however, that state planning would in fact have resulted in a much better outcome, though not a perfectly efficient one. The Railway Committee of the Board of Trade published detailed recommendations regarding the future structure of the network in 1845, based on over 20 detailed regional plans. Railways that did not conform to these guidelines would not have been authorised by Parliament. Parliament chose to ignore the Committee’s recommendations, for reasons explained below. Had they been accepted, the number of railways constructed would almost certainly have been much lower, and many of the routes would have followed better alignments, and served larger populations en route.",9
9.0,3.0,Networks and Spatial Economics,26 September 2008,https://link.springer.com/article/10.1007/s11067-008-9074-7,Graph-Theoretical Analysis of the Swiss Road and Railway Networks Over Time,September 2009,Alexander Erath,Michael Löchl,Kay W. Axhausen,Male,Male,Male,Male,"The interest in the spatial structure of transport networks has been driven by the inherent impact of the network structure on its performance and its affects on land use. Early studies begun as early as 1960 but were limited by the data availability and the limited computational power. The focus of research was mainly on simple topological and geometric properties (Garrison 1960; Garrison and Marble 1962; Kanskey 1969; Hargett and Chorley 1969). Later, with the availability of travel demand models researchers tried to explore how various network structures might influence traffic flow and travel pattern (Newell 1980; Vaughan 1987). More recently empirical studies analysed both quantitatively and qualitatively patterns of roads especially in urban areas (Marshall 2005). However, further research emerged from fields which are not directly linked to transport. The modelling of complex systems as networks of linked elements has become subject of intense study in the last years. A focus of research was the topology of modern infrastructure and communication networks such as the World-wide Web (Albert et al. 1999), the Internet (Faloutsos et al. 1999) or the Italian power grid (Crucitti et al. 2004). Additionally, also networks like collaborating movie actors (Watts and Strogatz 1998) or the academic co-authorship (Barabási et al. 2002) were investigated. Moreover, biological networks were analysed at different scales: Jeong et al. (2000) studied the metabolism of 43 organisms at the cellular level. Neuronal networks were evaluated by Watts and Strogatz (1998) and on a more aggregate level Camacho et al. (2002) documented seven food webs. Although transport infrastructure are the networks of daily life, only little analytical research can be found for transport networks. A basic difference to other, often social networks is that transport networks are embedded in real space where nodes and edges occupy precise positions in the three dimensional Euclidian space and edges are real physical connections. Therefore they are strongly constrained which has consequences for the degree distribution (the number of edges every node is connected to) which is often used to classify complex networks. Furthermore, the number of long range connections is limited as well, as in planar networks most crossing of two edges leads to a new node. Additionally, it is important to reflect that the addition of links is costly which limits these networks to be not scale-free (Barabási and Bonabeau 2003). Hence, the aim of this paper is to gather existing and propose new approaches of network analysis which consider the peculiarities of transport infrastructure networks. Thereby, a section is devoted to measures which are able to monitor the growth of such networks. Therefore, this work compares networks not only horizontally but longitudinally by comparing the network characteristics from 1950 to 2000 in 10 year steps and discusses the relevance of these measures to transport policy issues. The remainder of the paper is organised as follows: Section 2 discusses recent developments in the analysis of transport infrastructure networks and their application to the infrastructure development in Switzerland. Section 3 provides an overview of the measures while Section 4 presents the data used. Section 5 describes the development of the network using the measures selected and Section 5.3 describes qualitatively the local robustness of today’s network using new approaches. We conclude with Section 6 which provides an assessment of the measures and indicates further research needs and possible applications of transport network analysis.",112
9.0,3.0,Networks and Spatial Economics,17 July 2008,https://link.springer.com/article/10.1007/s11067-008-9068-5,Co-evolution of Density and Topology in a Simple Model of City Formation,September 2009,Marc Barthélemy,Alessandro Flammini,,Male,Male,Unknown,Male,"It has been recently estimated that more than 50% of the world population lives in cities and this figure is bound to increase (UN Population Division 2008). The migration towards urban areas has dictated a fast and short-term planned urban growth which needs to be understood and modelled in terms of socio-geographical contingencies, and of the general forces that drive the development of cities. Previous studies (Christaller 1966; Levinson and Yerra 2006; Fujita et al. 1999) about urban morphology have mostly focused on various geographical, historical, and social-economical mechanisms that have shaped distinct urban areas in different ways. A recent example of these studies can be found in Levinson and Yerra (2006), where the authors study the process of self-organization of transportation networks with a model that takes into account revenues, costs and investments. The goal of the present study is to model the coupling between the evolution of the transportation network and the population density. More precisely, the question we aim to answer is the following. Given the pattern of growth of the entire population of a given city, how is the local density of population changing within the boundaries of the city itself, and how the road network’s topology is modified in order to accommodate these changes? There are in principle a huge number of potentially relevant factors that may influence the growth and shape of urban settlements, first and foremost the social, economical and geographical conditions that causes the population of a given city to increase in a particular moment of its history. We neglect in the present study this class of factors and consider the overall growth in the number of inhabitants as an exogenous variable. In order to achieve conclusions that have a good degree of generality, and, at the same time, to maintain the number of assumptions as limited as possible, we focus on two main features only: the local density of population and the structure of the road network. Population density and the topology of the network constitute two different facets of the spatial organization of a city, and from a purely qualitative point of view it is not hard to believe that their evolution is strongly correlated. Indeed, Levinson, in a recent case study (Levinson 2008) about the city of London in the 19th and 20th centuries has demonstrated how the changes in population density and transportation networks deployment are strictly and positively correlated. Obviously, the road network tends to evolve to better serve the changing density of population. In turn, the road network influences the accessibility and governs the attractiveness of different zones and thus, their growth. However, attractiveness leads to an increase in the demand for these zones, which in turn will lead to an increase of prices. High prices will eventually limit the growth of the most desirable areas. It is the mutual interaction between these processes that we aim to model in the present work. Although there are many other economical mechanisms (type of land use, income variations, etc.) which govern the individual choice of a location for a new ‘activity’ (home, business, etc.), we limit ourselves to the two antagonist mechanisms of accessibility and housing price. These loosely defined notions can be taken into account when translated in term of transportation and rent costs. We note that in the context of the structure of land use surrounding cities, von Thünen (1966) already identified the distance to the center (a simple measure of accessibility) and rent prices as being the two main relevant factors. At first we will discuss separately the two mechanisms of road formation and location choice. In particular, we explicitly consider the shape of the network and model its evolution as the result of a local cost-optimization principle (Barthélemy and Flammini 2008). In classical models used in urban economics, transportation costs are usually described in a very simplified fashion in order to avoid the description of a separate transportation industry (Fujita et al. 1999). Also, when space is explicitly taken into consideration, the shape of the transportation networks is rarely considered and transportation costs are computed according to the distance to a city center (as it is the case in the classical von Thünen’s (1966) or Dixit-Stiglitz’s (1977) models). In these approaches transportation networks are absent, and displacements of goods and individuals are assumed to take place in continuous space. On one side this allows for a more detailed description of the economical processes at play during the shaping of a city. On the other side, these approaches often rely on the hypothesis that the processes shaping a city are slow enough to allow the balancing of the different forces that contribute to these processes, allowing as a consequence the achievement of the global minimum of some opportune cost function. The point of view inspiring our work, instead, is that the evolution of a city is inherently an ‘out-of-equilibrium’ process where the city evolves in time to adapt to continuously changing circumstances. If some sort of optimization or ‘planning’ is driving the growth, it has to be continuously redefined in order to take into account the ever-changing economic and social conditions that are ultimately responsible for the evolution of urban areas. We do not, therefore, assume the optimization of a global cost (or utility) function. Finally, we would like to mention that our goal is not to be as realistic as possible but to consistently reproduce a set of coarse-grained and very general features of real cities under a minimal set of plausible assumptions. Alternative explanations might also be possible and it would be interesting to compare our results with those produced in the same spirit. We hope that this simplified model could serve as a first step in the direction of designing more elaborated models. This paper is organized in three main parts. In the first part, we briefly establish the framework to describe the model and discuss the empirical evidences that motivated it. In the second part, we address the issue of how the growth of the local density affects the growth of the road network. In the third part we will study how the road network affects the potential for density growth in different areas. We finally integrate all these elements in the fourth section, where we study the full model and discuss our results.",53
9.0,3.0,Networks and Spatial Economics,24 July 2008,https://link.springer.com/article/10.1007/s11067-008-9067-6,The Topology of Transportation Networks: A Comparison Between Different Economies,September 2009,Efrat Blumenfeld-Lieberthal,,,Female,Unknown,Unknown,Female,"Cities are complex systems by their nature (Batty and Longley 1996; Portugali 2000; Batty 2005). Similarly to the organization of other complex systems (e.g. the WWW or the molecules in a cell) urban networks have no central force that affects their spatial structure. In other words, there are many factors such as economic considerations, political decisions, etc. that might affect these networks’ structure. Thus, it is reasonable to assume that the spatial behavior of the components of urban systems would fit the characteristics of other complex networks (Barabasi and Albert 1999). And indeed, as several studies have shown, some urban features, such as urban transportation routes, were found to obey the common laws of networks (De Cea et al. 2005; Jiang and Claramunt 2004). Transportation facilities and infrastructure have strong correlations with various economic activities such as productivity, production costs, property values, employment and commuting, etc. (Goddin 2002; Roberts 2002; Njoh 2000; Bhatta 2003). It has been shown that transportation systems have influence on the productivity of the economy (Banister and Berechman 2000; Economic Development Research Group.a. C. S. 1999; Economic Development Research Group.a. K. A. 2004, 2005). Based on the above, we assume that cities with strong economic relationship are characterized by a high volume of connectivity. The connectivity between cities can be measured by transportation infrastructure (e.g. roads, railroads, airports) and facilities (e.g. frequency of train and air routs). The goal of this work is to study the topology of different transportation networks and their characteristics in order to classify different economic systems of cities. For that, we study the topology of transportation networks of urban systems in different countries. We consider cities as the nodes, while train and air routes represent the links (see further discussion in “Section 2.2”). We calculated the degree and the clustering coefficient for the individual nodes and for the entire networks and compared them to the economic productivity of the countries (per capita gross domestic product (GDP)). The importance of examining both of these parameters is the differences between them. The degree represents the number of neighbors each node has, i.e. to how many other nodes it is connected to, and the clustering coefficient measures how clustered the network is, i.e. the likelihood of two nodes that are connected to a third node to be connected to one another. Additionally, we present a new analysis of the networks, based on a new representation of the relations between the degree and the clustering coefficient. This analysis allows us to examine the connectivity of each city within the network, as well as the connectivity of the entire system. Hence, it can indicate qualitative differences in the connectivity (which represents economic relations) between cities and countries (see further discussion in “Section 3.2”). Our results suggest that the connectivity levels of transportation networks could be related to the development level of the systems they represent. As these connectivity levels can be either the cause or the consequence of economic development, they are interesting indicators of economic activity. The rest of the paper includes several sections. The next section provides the definition to the examined networks. “Section 3” includes the networks’ statistics in terms of degree (number of links) and clustering coefficient. “Section 4” presents the relation between the degree and the clustering coefficient of the examined networks. To examine the correlation between the networks and the economic characteristics of the countries they represent, we provide in “Section 5” a comparison between the networks’ properties and the per capita GDP. In “Section 6” we present an example to the change of one transportation network over time with relation to economic and technological changes. A discussion on the results and some conclusions are presented in the last section.",19
9.0,3.0,Networks and Spatial Economics,18 October 2007,https://link.springer.com/article/10.1007/s11067-007-9036-5,Jurisdictional Control and Network Growth,September 2009,Feng Xie,David Levinson,,,Male,Unknown,Mix,,
9.0,4.0,Networks and Spatial Economics,09 June 2007,https://link.springer.com/article/10.1007/s11067-007-9023-x,Method of Successive Weighted Averages (MSWA) and Self-Regulated Averaging Schemes for Solving Stochastic User Equilibrium Problem,December 2009,Henry X. Liu,Xiaozheng He,Bingsheng He,Male,Unknown,Unknown,Male,,152
9.0,4.0,Networks and Spatial Economics,11 November 2008,https://link.springer.com/article/10.1007/s11067-008-9083-6,Decomposition Approaches for Constrained Spatial Auction Market Problems,December 2009,Igor V. Konnov,,,Male,Unknown,Unknown,Male,"Spatial equilibrium problems are usually utilized for describing complex systems when the distributed spatial location of their elements must be taken into account. In particular, these problems appeared to be very suitable for modeling various complex competitive economic systems; see e.g. Harker (1985), Nagurney (1999) and references therein. Rather recently, the necessity to make essential transformations in energy sectors discovered many new challenges; see e.g. Ilic et al. (1998), Zaccour (1998), Alanne and Saari (2006). For this reason, new kinds of spatial models devised to overcome difficulties related to deregulating and restructuring energy sectors have received considerable attention; see e.g. Wei and Smeers (1999), Metzler et al. (2003), Nagurney and Matsypura (2006) and references therein. Since most parts of the energy sector are attributed to natural monopolies, perfect competition seems a strong assumption there and most models are based upon imperfect competition assumptions, hence, represent non-cooperative game problems. As a result, these models are formulated as Nash–Cournot non-cooperative games, variational inequalities, or optimization problems with equilibrium constraints. They appeared suitable for the situation where any explicit regulation of the energy market can be neglected. At the same time, the situation where a state permits mass privatization in the energy sector, but keeps certain tools for influence in this field, deserves a separate consideration. Usually, the auction market mechanisms are utilized in this case; see e.g. Anderson and Philpott (2002), Beraldi et al. (2004) and references therein. Note that the general auction models are mainly formulated as non-cooperative game problems; see e.g. Weber (1985). Hence, rather complex behavior of separate markets (participants) and the presence of network capacity and balance constraints may again lead to very complicated mathematical problems such as global optimization problems with equilibrium constraints or mixed integer programming problems, which have usually a great number of variables. In this paper, we develop the approach to modeling auction markets, which was proposed in Konnov (2006, 2007a, b), where variational inequality models of separate auction markets with general price functions were described, i.e. these models allow for rather complex behavior of traders and buyers. The first goal of the paper is to suggest rather simple variational inequality problems for modeling a system of spatially distributed auction markets joined by transmission lines in a network subject to joint capacity and balance constraints, with taking the account behavior of participants within each auction market. Clearly, this task is essentially more complicated in comparison with that of a separate auction market. The second goal of the paper is to develop efficient methods for investigation and solution of spatial auction market problems via the presented variational inequality models. In this paper, we propose several equivalent formulations of the above problem, some of them involving algorithmically defined mappings. Being based on these properties, we propose iterative solution methods, which reflect decomposition schemes adjusted essentially to peculiarities of the structure of the problem and to possible large dimensionality. We illustrate work of the methods by computational results.",14
9.0,4.0,Networks and Spatial Economics,10 January 2009,https://link.springer.com/article/10.1007/s11067-008-9094-3,Stochastic and Dynamic Shipper Carrier Network Design Problem,December 2009,Avinash Unnikrishnan,Varunraj Valsaraj,Steven Travis Waller,Male,Unknown,Male,Male,"Assuming deterministic values is a common feature of all freight network models developed in the literature. Current freight network models assume that the various input parameters such as supply or demand can be estimated accurately or can be modeled exactly using deterministic demand supply relationships. However, the supply and demand of various commodities in a region is affected by numerous factors including population growth, demographics, seasonal variation in production of commodities due to factors like weather and other key socio-economic variables. Therefore, while making long term strategic decisions such as location of warehouses and the capacity of each warehouse it is important to characterize and account for this variation or uncertainty in parameters. Numerous studies in other domains like stochastic hydro-electric power scheduling and stochastic transportation network design have shown that decision making made based on a single forecasted value of future demand can lead to sub-optimal strategies and significant under-estimation of future system performance (Waller et al. 2001; Waller and Ziliaskopoulos 2001; Jeon et al. 2003; Karoonsoontawong and Waller 2005, 2007). This observation is particularly applicable for the freight transport network as an efficient commodity flow system is vital for a thriving economy. Therefore, developing freight network models which are more resilient to the various sources of uncertainty can lead to significant cost savings for all the stakeholders involved. Deregulation of the freight industry in the early 1980s sparked an interest in mathematical models to study the flow of commodities and to model the interests and the competition between the various agents in the freight transportation industry. In their seminal work, Harker and Friesz (1986a, b) provide an overview of the integrated methodological framework combining both demand and supply models developed to model the interaction between the various agents involved in the freight transportation industry viz. - producers, consumers, shippers, carriers and the government. The demand side attempts to model the interaction between producers, consumers and shippers and the resulting commodity flows using a spatial price equilibrium model. The behavior of producers and consumers are described by elastic but deterministic supply and demand functions. Transportation costs are assumed to be a function of the flow in the network. A detailed explanation of the various features of the spatial price equilibrium model for predicting inter-city freight flows is provided by Harker (1983). The supply side models the actual transportation process and the interaction between shippers and carriers using freight network equilibrium models. A Cournot-Nash model is used to describe the profit maximizing behavior of all the carriers. Friesz et al. (1985) provide a variational inequality formulation and a solution algorithm for the combined simultaneous shipper-carrier freight network equilibrium model. The mathematical relationships characterizing the equilibria and the various properties of the equilibria such as existence and uniqueness are discussed. In order to develop computationally efficient models applicable to large real world data sets, Friesz et al. (1986) relax the simultaneous assumption and develop a sequential shipper-carrier model. Despite lacking the theoretical rigor of the simultaneous model, the sequential shipper-carrier model is one of the very few models which have been validated in real world data sets. In other type of models, Fisk (1986) study the single shipper-multiple carrier problem where carriers choose service levels and tariff rates in an oligopolistic market to reach Nash equilibrium. Hurley and Petersen (1994) developed an integrated system optimizing model where the objective was to maximize the joint profit of the shippers and the carriers where the carriers set non-linear two-part tariffs either individually or in collaboration. More recently, Brotcorne et al. (2000) studied the freight tariff setting problem as a bilevel formulation where the leader carrier sets tariffs on the set of arcs under his control and the shipper routes goods from origin to destination to minimize the cost. Numerous decomposition heuristics are proposed to solve the bilevel problem and their performances are compared. One of the common features of the above models is the static steady state travel time assumption in the routing process. Friesz et al. (2006) use differential variational inequalities to model dynamic shipper competition in an oligopolistic market. However, the interaction between shipper and carriers and dynamics in the routing process is not considered in this work. Zhang et al. (2005) model the interaction between urban freight flows and automobile and data flows using a dynamic game theoretical model and provide an agent based simulation approach to solve the problem. Friesz et al. (2008) propose applying a stochastic differential variational inequality model to study the impact of additive demand uncertainty on pricing decisions in an urban freight environment where there three main players: sellers, transporters and receivers. The authors model each players individual objectives using an optimal control problem and use a differential variational inequality and a nonlinear complementarity formulation to represent the combined decision making process. A detailed analysis of the deterministic case is conducted by solving the nonlinear complementarity formulation. Holguin-Veras et al. (2007, 2008), Holguin-Veras (2007) study the interaction and behavior of carriers and receivers in an urban freight environment and conduct an analysis of the efficacy of various policy initiatives aimed at reducing the truck traffic in peak hours in urban areas. Chen (2008) study the carrier operations planning problem on a hub and spoke network in terms of determining fleet size, routes and schedules and provide a heuristic based on tabu search to solve the problem. Almost all the above mentioned works assume that all the parameters are deterministic and can be estimated or forecasted with reasonable accuracy or do not consider impact of stochasticity in network design decisions. This work addresses the above mentioned gaps in literature by proposing a stochastic and dynamic shipper-carrier model to determine the optimal capacity of storage to be installed on transhipment nodes by the shipper. The deterministic assumption in previous works is relaxed by assuming the demand and supply to follow a pre-specified probability distribution. The current work also accounts for the dynamics in the routing process by using a time-space expanded network. The shipper ships commodities from the sources to the sinks at minimum cost. Carriers are subdivided into the leader carrier and the secondary follower carriers. Every link is assumed to have a unit cost which is used to represent the combined transportation and tariff costs. The carriers operate on a capacitated time expanded network. A similar assumption is made by Brotcorne et al. (2000), Castelli et al. (2004) and can be used to model transport of commodities like coal for the gas industry. The contributions of this paper are a network design formulation in the shipper carrier context under uncertainty, solving the problem using decomposition algorithms which exploit the problems network structure and comparing their computational efficiency, heuristic to improve the performance of the algorithms, and analytical bounds on the optimal solution. The dynamic and stochastic shipper carrier model is formulated as a two stage linear program with recourse. In the first stage, the shipper decides the optimal capacity to be installed for storage on transhipment nodes. In the second stage, based on the realized demand, the shipper chooses a routing strategy to minimize the cost. The work applies the following solution techniques: Stochastic L Shaped Method, Regularized Decomposition and L Shaped Method with preliminary cuts to solve the problem and compares their computational performances. A new capacity shifting heuristic is presented which significantly improves the performance of Regularized Decomposition and provides the best performance. Various methods to arrive at upper and lower bounds of the objective function are discussed. The capacity shifting heuristic can be used to generate a tight upper bound on the objective function. An overview of the problem description along with the mathematical programming formulation is provided in the next section.",11
9.0,4.0,Networks and Spatial Economics,09 April 2009,https://link.springer.com/article/10.1007/s11067-009-9104-0,A Dynamic Transportation Model for the Stockholm Area: Implementation Issues Regarding Departure Time Choice and OD-pair Reduction,December 2009,Ida Kristoffersson,Leonid Engelson,,Female,Male,Unknown,Mix,,
9.0,4.0,Networks and Spatial Economics,25 September 2009,https://link.springer.com/article/10.1007/s11067-009-9114-y,User Equilibrium with Recourse,December 2009,Avinash Unnikrishnan,Steven Travis Waller,,Male,Male,Unknown,Male,"Transportation network models have advanced greatly in terms of their ability to account for dynamic flows, stochastic conditions, and traveler behavior. However, there persists an insufficient degree of fundamental traffic equilibrium modeling work to capture the impact anticipatory user recourse with respect to route choice in response to uncertain network conditions. This online route choice behavior occurs as travelers can learn about the uncertain system states as they traverse the network and make appropriate route choice decisions. The learning could occur either through technology or through own observation of the system conditions. Such models would be highly beneficial in their addition to our basic understanding of network flow behavior under information strategies and the countless applications that relate. This paper develops a static equilibrium model accounting for one-step local information and user recourse. While dynamic models or simulation-based approaches are more commonly employed for the modeling of information in transportation networks, a static approach is adopted so that a tractable mathematical program and solution methodology can be developed for certain variations which allow for more direct analysis and potential extensions to other applications and problem variants. Static approaches have been used to represent the average impact of temporal phenomena. For example, static equilibrium models have been developed to capture the impact of signalized intersections, where the highly dynamic nature of evolving traffic is approximated. Information represents a new dynamic phenomenon which can be roughly approximated with a static approach. The focus of this paper is to capture the long-term expected values of traffic flow or average flow over many days when information is present. The model presented does not attempt to capture any specific hour or scenarios but rather the average over many different days each with different conditions and information sets (in the same way that static assignment does not capture any given queue length at a signalized intersection but still models the average impact of the capacity reduction which results from the signal being present). If information is present, users will anticipate it on a constant basis and adapt their behavior to the availability of such data. Several works on transit assignment employing adaptive paths (Nguyen and Pallottino 1989; Marcotte and Nguyen 1998) use a static formulation for an inherently dynamic information-based problem. This work is a critical first step in developing a fully dynamic equilibrium with recourse model. The benefit of the static formulation presented in this work is that it helps capture the impact of information in a concise and rigorous form with an analytical formulation and solution methodology. Moreover, long-term equilibrium does indeed have a value since it represents the average values of traffic flow given that each day users behave differently because they learn different things. This paper looks at two variations which approximate the problem in different ways. The first variation assumes that all users arriving at a node see the same realization of the stochastic parameter. In the second model, different users arriving at the same node see different realizations of the stochastic parameter. The second model can be viewed as a rough proxy for non-constant conditions. However, there is no systematic dependency on time (i.e., conditions can change between units of flow but are not correlated with specific time values). Relatively little work has been presented on the type of transportation equilibrium with information and recourse problems of interest here (especially those yielding tractable solution methods). One effort, by Nguyen and Pallottino (1989), presented a model and algorithm for the transit equilibrium problem employing shortest hyperpaths (the precise definition of a hyperpath varies somewhat by the specific implementation, but it is essentially a collection of possible paths between a given origin-destination pair). Another promising attempt was contributed in a book chapter by Marcotte and Nguyen (1998) who developed an approach for traffic equilibrium employing hyperpaths again for transit networks. While this work focused on transit and capacitated networks where an arc may or may not exist upon arrival at a node (as opposed to general information learned en route), valuable insights were given for network hyperpaths and their application. Marcotte et al. (2004) extended the hyperpath assignment concept to model traffic assignment in networks with rigid finite capacities. In this model users are assigned to strategies which provide at every node,a set of sub-paths and the order of preference. An explicit travel cost function is not used and the rigid finite capacity assumption is assumed to be a proxy for travel delay. The various theoretical properties of the network were analyzed under two types of loading mechanism: single queue processing which enforces first-in-first-out (FIFO) and parallel queue processing where pre-emption is allowed. A variational inequality formulation was provided and the performance of five solution algorithms based on Frank-Wolfe and projection methods were studied. Hamdouch et al. (2004) extend the above strategic model to a dynamic setting where users are loaded onto strategies or dynamic hyperpaths based on user preference and following the first-in-first-out discipline. Gao (2005) provide a policy based approach for dynamic traffic assignment where at every node a user is specified different links depending on the state of the network. The current work presented here, though similar in concept, differs from earlier attempts by providing convex mathematical programming formulations and exact solution algorithms for the static version of the problem. Friesz et al. (1994) used the tatonnement process to model the day-to-day route adjustment of drivers from one disequilibrium state to the another depending on the quality of information delivered provided by traveler information dissemination devices. Friesz et al. (1996) further developed this work and provided a unified theory to predict time varying urban network flows which rigorously captures the trajectories of the dynamic disequilibrium process. Even though the focus was on day-to-day route adjustments, Friesz et al. (1996) do identify the need for capturing en-route adjustments and conduct a brief analysis on capturing this phenomena. However, the en-route adjustment is in response to the evolving dynamics and not due to stochastic conditions as assumed in this work. Another highly pertinent area of work for User Equilibrium with Recourse (UER) is the problem of stochastic shortest path routing with recourse, or Online Shortest Path (OSP). This problem is vital for analyzing UER as it forms the sub-problem in the same manner that the traditional shortest path forms the sub-problem to the static user equilibrium (UE) problem. Croucher (1978) introduced shortest path with recourse with a heuristic algorithm on a restricted model. Further work includes efforts by Andreatta and Romeo (1988) and Polychronopoulos and Tsitsiklis (1996) who provided novel algorithms but with worst-case running times exponentially related to network size or the number of realizations. An algorithm for the problem of recourse within time-varying networks has been introduced by Miller-Hooks (2001). While this algorithm determines optimal least expected cost hyperpaths with respect to the arrival time at a node, it assumes independence between both arc costs and time periods. A multi-modal stochastic variation of a deterministic algorithm presented in Ziliaskopoulos and Wardell (2000) was presented in Opasanon and Miller-Hooks (2001), again with the assumption of independence over space and time for all probability distributions. Recently, Waller and Ziliaskopoulos (2002) have introduced efficient algorithms for the time-invariant online stochastic shortest path problem when limited forms of spatial and temporal arc cost dependencies are accounted for. The set of algorithms developed by Waller and Ziliaskopoulos (2002) and Polychronopoulos and Tsitsiklis (1996) will be directly employed in this work as they will be shown to follow the required behavior for different variations of the UER sub-problem. Ferris and Ruszczyński (2000) studied the robust adaptive routing in a network where the arcs exist in uncertain states corresponding to uncertain travel times and capacities. The transition between states is described using a continuous time Markov chain. However, the authors do not consider the congestion effect of flow on travel times. In this paper, the UER problem variation studied in this paper and the notations and definitions will be presented first in Section 2. Two problem variations will then be studied in detail. A mathematical programming formulation, solution algorithms and proof that the solution to the convex programming formulation satisfies UER conditions will be provided in Sections 3 and 4. Numerical experiments conducted to show the importance of accounting for uncertainty in arc states will be presented in Section 5. Conclusions and directions for future research will then be provided.",47
9.0,4.0,Networks and Spatial Economics,06 November 2009,https://link.springer.com/article/10.1007/s11067-009-9115-x,A Dynamic Travel Time Model for Spillback,December 2009,Soulaymane Kachani,Georgia Perakis,,Unknown,Female,Unknown,Female,"Congestion in transportation networks has been growing tremendously in the recent years and has become an acute problem. In particular, Barnhart et al. (1998) have estimated that congestion costs around $100 billion each year to Americans alone in the form of lost productivity. Therefore, it is critical to alleviate it by understanding its nature and devising efficient congestion control strategies. Congestion phenomena in urban and highway transportation systems are inherently dynamic. The fast-growing field of Intelligent Vehicle Highway Systems (IVHS) is a case in point. An important component that links with congestion phenomena is travel time that is, the time it takes drivers to reach their destinations in the network taking into consideration what time they started their trip during the day. Our goal in this paper is to continue on the work by Perakis (1997) and subsequently by Kachani (2002), and Kachani and Perakis (2001), in order to provide insight on the nature of travel times (that is, how delays and traffic patterns are formed and dynamically change) and on the role of information in transportation networks. In particular, in this paper we extend our previous work on modeling travel times by also directly incorporating spillback and bottleneck phenomena in our models. Traffic patterns and phenomena such as flow circulation, queues formation and dissipation, spillback and shock waves, are striking evidence that traffic flows resemble gas and water flows. It is therefore natural to apply physical laws of fluid dynamics for compressible flow to model traffic flow patterns. Lighthill and Whitham (1955), and Richards (1956) introduced the first continuum approximation of traffic flows using kinematic wave theory (see Haberman 1977 for a detailed analysis). The dynamic nature of these models gave them instant credibility. Indeed, with the increase of urban and highway congestion, the time variations of flow are too important to be neglected. Dynamic traffic flow modeling has captured the focus of many researchers in the transportation area. A variety of dynamic traffic flow models (microscopic and macroscopic) have since been proposed in the literature. The cell transmission model proposed by Daganzo (1994, 1995a, b) stands out. This model is based on a discrete approximation of the hydrodynamic theory of traffic flow, and is able to capture traffic behavior such as the formation, propagation, and dissipation of queues. The purpose of this paper is to devise models that will allow us to determine (in closed form) travel times in a transportation network operating under a dynamic setting. In particular, we wish to model link travel times for drivers entering a link as well as for drivers already in the link but whose travel times are affected by a significant change in the traffic conditions (e.g. spillback or bottleneck phenomena). Practitioners in the transportation area have been using several families of travel time functions. Akcelik (1988) proposed a polynomial-type travel time function for links at signalized intersections. The BPR function (Bureau of Public Roads 1964), that is used to estimate travel times at priority intersections, is also a polynomial function. Finally, Meneguzzer et al. (1990) proposed an exponential travel time function for all-way-stop intersections. Our goal is to lay the theoretical foundations for using these polynomial and exponential families of travel time functions in practice. While most analytical models in traffic modeling assume an a priori knowledge of drivers’ travel time functions, in this paper, travel time is part of the model and comes as an output. To determine travel times, in this paper we propose an extension of the fluid dynamics travel time models proposed by Perakis (1997) and subsequently by Kachani (2002), and Kachani and Perakis (2001). We extend these models in order to: (i) account for spillback and bottleneck phenomena but also (ii) determine travel times of drivers entering a link as well as of drivers already in the link. This allows us to update our estimates of travel times during the trip in each link as they may be affected by changes in traffic conditions. To achieve our goals:
 We propose two models to estimate travel times as functions of exit flow rates where we account for spillback and bottleneck phenomena: the Spillback Polynomial Travel Time (SPTT) Model and the Spillback Exponential Travel Time (SETT) Model. These models incorporate inflow, outflow and storage capacity constraints. We propose an extension of the general framework in Kachani (2002) for the analysis of the SPTT Model. Based on piecewise linear and piecewise quadratic approximations of the flow rates, we propose several classes of travel time functions for the separable SPTT and SETT models. We further establish a connection between these travel time functions. We extend the analysis of the SPTT Model to non-separable velocity functions in the case of acyclic networks. The paper is organized as follows. In Section 2, we provide some useful notation. In Section 3, we propose an analytical dynamic travel time model. We show how this model accounts for spillback and bottleneck phenomena. To make the problem more tractable, we derive two approximations of this model: the Spillback Polynomial Travel Time Model (SPTT Model) and the Spillback Exponential Travel Time Model (SETT Model). In Section 4, we examine the case of separable velocity functions. Based on an approximation of exit flows by piecewise linear and piecewise quadratic functions, we propose several classes of travel time functions for the problem that rely on a variety of assumptions. We analyze the relationship between these travel time functions and show that the assumptions we impose are indeed reasonable. In Section 5, we extend the analysis of the SPTT Model to the case of non-separable velocity functions in acyclic networks. Finally, in Section 6 we discuss future steps for the study of this model.",6
10.0,1.0,Networks and Spatial Economics,04 April 2007,https://link.springer.com/article/10.1007/s11067-007-9020-0,Mode Selection for Automotive Distribution with Quantity Discounts,March 2010,Mingzhou Jin,Sandra D. Eksioglu,Haiyuan Wang,Unknown,Female,Unknown,Female,,10
10.0,1.0,Networks and Spatial Economics,16 May 2007,https://link.springer.com/article/10.1007/s11067-007-9021-z,Analysis of Shortest Paths and Subscriber Line Lengths in Telecommunication Access Networks,March 2010,C. Gloaguen,F. Fleischer,V. Schmidt,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Networks and Spatial Economics,01 June 2007,https://link.springer.com/article/10.1007/s11067-007-9022-y,Solving the Dynamic User Optimal Assignment Problem Considering Queue Spillback,March 2010,Yu (Marco) Nie,H. M. Zhang,,,Unknown,Unknown,Mix,,
10.0,1.0,Networks and Spatial Economics,20 July 2007,https://link.springer.com/article/10.1007/s11067-007-9024-9,Estimating Freight Flows for Metropolitan Area Highway Networks Using Secondary Data Sources,March 2010,Genevieve Giuliano,Peter Gordon,LanLan Wang,Female,Male,,Mix,,
10.0,1.0,Networks and Spatial Economics,25 July 2007,https://link.springer.com/article/10.1007/s11067-007-9026-7,Simultaneous Departure Time/Route Choices in Queuing Networks and a Novel Paradox,March 2010,Xiaoning Zhang,H. M. Zhang,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Networks and Spatial Economics,19 September 2007,https://link.springer.com/article/10.1007/s11067-007-9034-7,R&D for Quality Improvement and Network Externalities,March 2010,Luca Lambertini,Raimondello Orsini,,Male,Unknown,Unknown,Male,,5
10.0,1.0,Networks and Spatial Economics,17 October 2007,https://link.springer.com/article/10.1007/s11067-007-9035-6,Determining Optimal Police Patrol Areas with Maximal Covering and Backup Covering Location Models,March 2010,Kevin M. Curtin,Karen Hayslett-McCall,Fang Qiu,Male,Female,,Mix,,
10.0,1.0,Networks and Spatial Economics,11 January 2008,https://link.springer.com/article/10.1007/s11067-007-9059-y,A Relaxation Approach for Estimating Origin–Destination Trip Tables,March 2010,Yu Marco Nie,H. M. Zhang,,,Unknown,Unknown,Mix,,
10.0,2.0,Networks and Spatial Economics,22 April 2008,https://link.springer.com/article/10.1007/s11067-008-9061-z,Routing Traffic at Hub Facilities,June 2010,Morton E. O’Kelly,,,Male,Unknown,Unknown,Male,"Despite a great deal of attention paid to hubs in spatial networks, it appears that there are unexplored issues in the analysis of flows within the hub itself. This paper outlines a simple typology, notation, and analytical framework for routing traffic at transfer nodes. The fundamental idea is that flows between network nodes interconnect at various types of hubs. Transfer aspects of hub and spoke systems are widely recognized as a hindrance to efficient completion of transit trips. For example, time-consuming delays at transfer points for bus passengers are a major reason for poor levels of service between some nodes when the origin and destination are on different network spokes. In discussing models for this situation, this paper adapts ideas from diverse types of transportation situations, such as subway systems, internet backbone peering and connectivity, logistics facilities, and border crossings. The ideas here are flexible enough to suggest a framework for further research on a variety of interesting optimization problems that might arise in connection with a hub: the minimization of transfer delay, the design of strong surveillance schemes, or the decision to link certain paths directly. By focusing attention on intra-hub connectivity, some new aspects of hub network efficiency may be realized. There is a precedent for this scale of analysis in Koopmans and Beckmann (1957), who looked at interdependent flow and location decisions in plant layout and showed that they lead to quadratic programs. Their work inspired further exploration of internal economies of scale (Reiter and Sherman 1962). By analyzing internal linkages at hubs, this paper opens a series of questions about network efficiency, in the same way that internal economies of scale are studied in industrial economics. Three approaches to this issue are apparent: a descriptive (and data intensive) study could be carried out to recover historical data from the operational records of the throughput conditions at a hub. A second approach would be to derive generalized models of the flows from spatial interaction theory (Fotheringham and O’Kelly 1989; O’Kelly 1986). A third approach, and the main one adopted in the present paper, is to combine the expected flows with a network design problem to potentially optimize certain aspects of the operational management of the transport system (see also Campbell 1994; Crainic and Kim 2007). This paper deals with a relatively simple case where lines come into the hub at distinct points, and there the flows (passengers, packages, etc.) are either passed through on that same line or are switched to another line.",26
10.0,2.0,Networks and Spatial Economics,14 March 2008,https://link.springer.com/article/10.1007/s11067-008-9062-y,Solving Stochastic Transportation Network Protection Problems Using the Progressive Hedging-based Method,June 2010,Yueyue Fan,Changzheng Liu,,Unknown,Unknown,Unknown,Unknown,,
10.0,2.0,Networks and Spatial Economics,24 May 2008,https://link.springer.com/article/10.1007/s11067-008-9063-x,The Relative Mobility of Vehicles Improves the Performance of Information Flow in Vehicle Ad Hoc Networks,June 2010,Lili Du,Satish Ukkusuri,,Female,,Unknown,Mix,,
10.0,2.0,Networks and Spatial Economics,01 July 2008,https://link.springer.com/article/10.1007/s11067-008-9066-7,Commercial Vehicle Empty Trip Models With Variable Zero Order Empty Trip Probabilities,June 2010,José Holguín-Veras,Ellen Thorson,Juan C. Zorrilla,Male,Female,Male,Mix,,
10.0,2.0,Networks and Spatial Economics,19 July 2008,https://link.springer.com/article/10.1007/s11067-008-9069-4,"A Simple, Analytically Solvable, Dual-Space Economic Agglomerations Model",June 2010,Zheng Shi,Zheng Wen,Jin Xia,,,Female,Mix,,
10.0,2.0,Networks and Spatial Economics,22 October 2008,https://link.springer.com/article/10.1007/s11067-008-9078-3,Dynamic User Equilibrium Model for Combined Activity-Travel Choices Using Activity-Travel Supernetwork Representation,June 2010,Gitakrishnan Ramadurai,Satish Ukkusuri,,Unknown,,Unknown,Mix,,
10.0,3.0,Networks and Spatial Economics,29 May 2010,https://link.springer.com/article/10.1007/s11067-010-9136-5,Introduction to the Special Issue on Location Modeling,September 2010,Michael J. Kuby,Richard Church,,Male,Male,Unknown,Male,,3
10.0,3.0,Networks and Spatial Economics,23 January 2010,https://link.springer.com/article/10.1007/s11067-009-9127-6,A GIS-Based Optimization Framework for Competitive Multi-Facility Location-Routing Problem,September 2010,Burcin Bozkaya,Seda Yanik,Selim Balcisoy,Unknown,Female,Male,Mix,,
10.0,3.0,Networks and Spatial Economics,04 December 2009,https://link.springer.com/article/10.1007/s11067-009-9120-0,"Planning for Agricultural Forage Harvesters and Trucks: Model, Heuristics, and Case Study",September 2010,Victor Blanco,Luisa Carpente,Justo Puerto,Male,Female,Male,Mix,,
10.0,3.0,Networks and Spatial Economics,30 December 2009,https://link.springer.com/article/10.1007/s11067-009-9123-x,Strategic Network Restoration,September 2010,Timothy C. Matisziw,Alan T. Murray,Tony H. Grubesic,Male,Male,Male,Male,"Networked infrastructures are continually at risk of service disruption due to environmental, technological or intentional damage to system components. Natural disasters, such as hurricanes, floods and earthquakes, can have an extensive geographic range of impact, rendering considerable portions of transportation and utility networks inoperable. The effects of hurricane Katrina on the U.S. Gulf Coast, as an example, illustrate that widespread infrastructure-based disruption can arise from such events. In less dramatic cases, such as the December 2006 earthquake off the coast of Taiwan, key components of a network can be impacted, severely limiting the performance of infrastructure systems (CNN 2006; Kitamura et al. 2007). Unexpected events that are smaller in geographic scope can also cause significant damage because their impacts can radiate through a system, resulting in cascading failure of facilities and loss of service. For instance, the initial loss of a single electrical generation plant in Ohio led to widespread facility failure in the 2003 blackout of the Northeastern portion of the U.S. power grid (USCA 2004; Grubesic and Murray 2006). More recently, both the failure of a disconnect switch and the subsequent fire in an electrical substation west of Miami resulted in nearly three million customers losing power throughout Florida (CNN 2008c). Similarly, structural failures, accidents, as well as seemingly innocuous events, such as construction or the dropping of a ship’s anchor, can disrupt service and require time-consuming repair efforts (CGA 2005; CNN 2008a, b). Although accidents and natural disasters are always a concern, other more sinister threats oriented at maximizing infrastructure damage, such as terrorism and acts of war, add additional potential for system disruption (USCOTA 1990). For instance, a major threat to networks heavily reliant on electrical components is a High Altitude Electromagnetic Pulse (HEMP) attack. A HEMP insult consists of discharging a nuclear weapon high above the earth’s surface in an effort to generate an intense electromagnetic pulse. The resulting electromagnetic field has the potential to debilitate electrical, computer and telecommunications equipment over an extremely large area (Foster et al. 2004). For example, in the early 1960s, the United States conducted a high altitude nuclear test over the Pacific Ocean and the resulting electromagnetic pulse disrupted electronic equipment and radio communications 800 miles away in Hawaii (Wilson 2004). In this context, it is believed that a single detonation over Kansas has the potential to impact electronic equipment throughout the continental U.S (FAS 2009). While fortifying or hardening networked infrastructure against such unexpected events is an appealing, proactive planning goal, such options are costly and may not always be available or effective in preventing infrastructure losses. Thus, planning efforts are necessary for ensuring that response plans are in place to guide the restoration of disrupted services. Although the geographic scale of infrastructure damage is highly variable, most extreme events yield massive damage. As a result, planning agencies are faced with the task of efficiently coordinating facility repair in order to restore service to those in need in a timely and efficient manner. Unfortunately, complete recovery of a network is often delayed due to workforce and budget limitations, so the allocation of resources must be prioritized. In an effort to address this complex planning problem, this paper introduces a spatial optimization model that provides a methodological framework for developing and evaluating network restoration contingencies. We begin with a review of previous efforts to model the recovery of disrupted systems. As is discussed, there are many considerations that may be of interest in recovery planning. One theme that emerges in particular is the importance of restoring origin-destination (O-D) connectivity/flow in an efficient manner (Meshkovskiy and Rokotyan 1992). To further explore the complicated nature of infrastructure recovery, a multi-objective recovery model is proposed for identifying tradeoffs between flow restoration and system cost over time. Analysis of a telecommunications network illustrates the tradeoffs between restoration objectives and the value of the proposed model.",88
10.0,3.0,Networks and Spatial Economics,09 June 2010,https://link.springer.com/article/10.1007/s11067-010-9138-3,What Foreclosed Homes Should a Municipality Purchase to Stabilize Vulnerable Neighborhoods?,September 2010,Michael P. Johnson,David A. Turcotte,Felicia M. Sullivan,Male,Male,Female,Mix,,
10.0,3.0,Networks and Spatial Economics,15 June 2010,https://link.springer.com/article/10.1007/s11067-010-9137-4,Stochastic Location-assignment on an Interval with Sequential Arrivals,September 2010,Kannan Viswanath,James Ward,,Unknown,Male,Unknown,Male,"FedEx has as backup, six empty aircrafts to handle unanticipated demand surges or failures of other units in the fleet (Leonhardt 2005). As events unfold, the aircraft(s) are assigned to handle demand requests at one or more locations. So the location decisions for the empty aircrafts—as well as the assignment rule adopted—determines the capability of the shipper’s resilience in the face of uncertainty. Similarly, in the context of emergency response, planners are faced with the task of efficient use of strategic emergency resources to service spatially-distributed demand requests over time. An underlying feature of the above examples, as well as many others, is that a set of response units are located over a geographical region in anticipation of demands that arise at random locations over the region. A unit is assigned to each demand request upon its realization so as to optimize some measure of overall cost. This paper examines an idealized version of such location-assignment problems. Our model is composed of n (>1) servers that are pre-positioned on the unit interval to serve demands with random locations, arriving sequentially. The total number of demands may not be known apriori, but will not exceed the number of servers. After locating the n servers, demands arrive sequentially, i.e., their locations are revealed, and a server is assigned to each demand upon arrival. The assignment cost for a server-demand pair is the distance between them. Once a server is assigned, it is not available for assignment to any subsequent demand. The objective of both server-to-demand assignment and server location is to minimize the expected value of the sum of all assignment cost. Our focus is to examine the mathematical structure of the simplest form of the problem and hopefully gain insight into the spatial characteristics of optimal server locations. Specifically, how does the number of (expected) demands affect the positioning of the servers? Placing the problem context on an interval and assuming uniform demands provides the opportunity to begin to answer such questions. The model belongs to the general class of problems referred to as location-allocation problems. The location-allocation problem (Cooper 1963) is concerned with determining the optimal number of sources, their locations and capacities, given the location of destinations, their requirements and the associated shipping costs. It usually involves two stages; first-stage location decisions, and second-stage allocation decisions. Different restrictions and variations of the problem, deterministic and stochastic, have been addressed that have spawned further studies within themselves, which include the well-studied, transportation problem, capacitated location problems and location-transportation problems. Love (1976) was one of the first papers to address the location-allocation problem in a deterministic setting on a line segment, for which he proposed a dynamic programming algorithm. Wesolowsky (1977) extended it to locating a single facility on a line segment with demand locations having probabilistic weights, and he derives the probability that the facility is optimally located. These problems belong to the class of p-median problems, in which the objective is to find the location of p facilities that minimizes the total weighted distance for the demand points. The optimal allocation policy in the p-median problem is that each demand location is assigned to its closest server. A location-assignment problem is similar to a constrained p-median problem in which each server/facility can serve at most one demand. In a network setting, Zeng and Ward (2005) considered the problem of optimally locating servers, equal to the number of demands, prior to the realization of the demands that arrive simultaneously, and assigning them to the demands upon their realization. The objective was to minimize the expected total assignment cost. The problem was shown to be NP-hard, but with a polynomial-time solution procedure on a line and tree networks. The continuous setting of the location-assignment problem in one dimension has received greater attention due to its structure. Rosenhead and Powell (1973) consider locating two facilities on a line segment so as to minimize the mean travel distance for customers distributed along the line segment to their nearest facility. They derive a necessary condition for optimality of facility locations, and show that it is sufficient, and has a unique solution. Anderson and Fontenot (1992) addressed the problem of optimally positioning n servers on a line segment apriori, to serve a single demand that occurs at a random location on the segment. The objective was to minimize the expected travel distance to the demand from the closest server. With the assumption of a continuous probability density function with its support defined over the segment, and possibly vanishing only at finite number of points, they derive a necessary condition for optimality of a set of server-locations. The special case of a demand with a uniform distribution over the unit interval was considered and was shown to have a simple and unique optimal solution. Levine (1986) showed that it is optimal for a server to be parked at a location vs. patrolling to serve a demand with any general distribution. Carrizosa et al. (1998) extended this result to the multiple server case. Smith (1997) studied the effect of unequal patrol lengths over an interval on mean response distance to a random incident. Carrizosa et al. (2002) addressed the problem of locating a server, with a constraint for mobility, to serve a random demand over an interval. They solve it by reducing it to a problem of locating a stationary server. Puerto and Rodriguez-Chia (2003) address the problem of locating mobile server(s) on a unit interval to serve a demand with unknown probability distribution but whose mean is known. The objective was to minimize the maximum expected distance to a random demand location. This objective is robust with respect to a family of probability measures. Several papers have explored sequential location-allocation problem in which the location decisions are done sequentially. Scott (1970) considers a multi-period planar model in which one uncapacitated new facility is added to a set of existing facilities in each period. Cavalier and Sherali (1985) consider similar problems on a line segment and a tree graph, but allow for probabilistic demand locations. Capacitated versions of the line and tree problems are addressed in Sherali (1991). Huang et al. (2009) formulate and solve a 20-period optimization problem to sequentially locate and size hydrogen production facilities in the face of uncertain demand increases. In all of these problems the locations of demands change from one period to the next, but the allocations made in one period do not constrain the allocations in subsequent periods. To the best of our knowledge, this study is the first to consider multiple random demands in a location-assignment model when the demands arrive sequentially. This study contributes by: (i) extending the literature in stochastic location-assignment by considering multiple demands arriving sequentially; (ii) deriving a necessary condition for optimality of server-locations, and a characterization of the set of all optimal server-locations; and (iii) showing the convexity of the objective function, and the uniqueness of optimal server-locations, for demands that are independent and uniformly distributed over the interval. The remainder of this paper is organized as follows. Section 2 presents the model formulation. Section 3 derives properties for optimal server-to-demand assignment. Section 4 derives properties for optimal server-locations; a necessary condition for optimality of server-locations, characterizes the set of all optimal server-locations, and a condition, when satisfied, that implies the convexity of the objective function. Section 5 shows that this condition is satisfied when the demands are independent and uniformly distributed, and derives some insights, based on numerical results, on the spatial distribution of optimal server-locations. Section 6 summarizes the paper.",
10.0,3.0,Networks and Spatial Economics,05 December 2009,https://link.springer.com/article/10.1007/s11067-009-9121-z,Optimizing the Location of a Production Firm,September 2010,Zvi Drezner,Carlton H. Scott,,Male,Male,Unknown,Male,"The Weber problem (Weber 1909; Wesolowsky 1993; Drezner et al. 2002) is one of the most useful and researched problem in the location literature. It is based on a set of demand points generating demand for a facility and the best location of the facility that minimizes the total transportation cost to the facility is sought. Many papers investigate the problem in the plane with Euclidean distances. For a review see Drezner et al. (2002). Weiszfeld (1937) suggested an iterative procedure to solve the problem using Euclidean distances. When using rectilinear distances the problem is separable into two one dimensional problems and the solution in each dimension is the median point (Love et al. 1988; Drezner et al. 2002). This result is the source of the name 1-median problem that was generalized to the p-median problem when the location of p facilities is sought (Current et al. 2002). Hakimi (1964, 1965) proved that the solution in a network environment to the Weber (1-median) problem is on a node of the network which suggests a very simple algorithm for the solution to the Weber problem in a network environment. Therefore, researchers are interested only in developing solution approaches to the p-median problems which are not that simple (Daskin 1995; Current et al. 2002; Alp et al. 2003). p-median problems on the plane are sometimes called location-allocation problems because demand points are allocated among the facilities and the location of each facility is at the Weber solution based on the subset of demand points allocated to it (Cooper 1963; Beaumont 1980; Chen 1983; Love 1976; Love and Morris 1975; Sherali and Shetty 1977). Moses (1958) introduced specific considerations related to the location of a production firm in his seminal paper about location and the theory of production. He claimed that “there is no need for much of the esoteric paraphernalia sometimes employed by location specialists”, a statement that generated many follow-up papers generalizing his approach (Sakashita 1967; Bradfield 1971; Emerson 1973; Khalili et al. 1974; Osleeb and Cromley 1977; Miller and Jensen 1978). More recent papers on various aspects of the topic are by Martinich and Hurter (1982) and Tobin and Friesz (1986). The book by Hurter and Martinich (1989) provides a comprehensive analysis of the location of a production firm. They consider models that incorporate cost (such as electricity cost that may change by region) and transportation cost (linear in the distance). There are no customers in their models (both the facility and the demand points are part of the firm), and their objective is to minimize the cost to the firm. Another stream of research considers the cost charged to customers as a variable in location models. The planner determines the cost to be charged to customers in addition to the location decision. Drezner and Wesolowsky (1996) investigated the location-allocation problem making the cost charged to users by a facility a function of the total number of users patronizing the facility. Users select the facility to patronize based on the facility charges and transportation cost. In a competitive environment such a situation may or may not lead to an equilibrium solution (Cournot-Nash or Stackelberg). Such models are discussed in Miller et al. (1992, 1993, 1996, 2007); Tobin et al. (1995); Tobin and Friesz (1986); Friesz et al. (1988a, b, 1989). Some models assume that demand is declining when the distance to the facility increases. Drezner and Scott (2006) investigated the Weber problem including a queuing component assuming that demand is declining by the distance and therefore some of the demand is lost. Berman et al. (2003) investigated the problem that the reliability of the service, thus demand served, is declining with the distance to the facility. In the competitive facility literature there are many papers that assume that the demand is declining with the distance to the facility (for example, Huff 1964, 1966; Drezner 1994, 1995). In this paper we investigate a problem where demand depends on the mill price and transportation rate charged by the facility which depend on the distance to the facility. In addition to finding the best location for the facility, there are two parameters to be determined by the planner: the mill price and the transportation rate per unit distance charged by the facility. Demand by customers is elastic and thus depends on the total price charged for the good which depends on the distance between the demand point and the facility. The paper is organized as follows. In Section 2 the problem is formulated and in Section 3 solution algorithms are presented. In Section 4 computational experiments are reported and we conclude in Section 5.",5
10.0,4.0,Networks and Spatial Economics,16 September 2008,https://link.springer.com/article/10.1007/s11067-008-9075-6,Optimal Two-Part Pricing and Capacity Allocation with Multiple User Classes and Elastic Arrivals at Constrained Transportation Facilities,December 2010,José Holguín-Veras,Sergio Jara-Díaz,,Male,Male,Unknown,Male,"Service differentiation is one of the most important trends that have taken place in transportation systems in the last decades. Examples include the rise of priority parcel service, expedited handling of high priority containers at ports, and high occupancy toll lanes in highways, among others. In this context, service and price differentiation are clearly warranted because, among other things, they enable the transportation operators to adjust level of service and price schedules to the different marginal costs and heterogeneous users’ needs and expectations, which are ultimately captured by the users’ willingness to pay. Therefore, higher profits or welfare can be attained depending on the objective than in the single price scheme. In other words, differences in either the demand characteristics or in the underlying cost structures potentially justify the use of service and/or price differentiation. This paper focuses on the simultaneous analytical determination of optimal prices and optimal capacity allocation at transportation facilities with elastic arrivals and multiple user classes. The various fees are collapsed into two main components: an entrance fee and a dwell charge per unit of time spent at the facility, which represents a more general case of the traditional two-part tariff (Oi 1971). The arrival rates at the facility are assumed to be a function of both the entrance fee and the dwell charges, while the dwell time is a function of the dwell charge only. The paper also discusses a number of cases in which this formulation applies. The analytical framework and the corresponding assumptions on demand and costs are presented in Section 2. Sections 3, 4 and 5 discuss optimal prices for the cases of profit maximization, welfare maximization and second best pricing, respectively. Section 6 discusses numerical results for a hypothetical case. Section 7 summarizes and discusses the main findings of this research.",6
10.0,4.0,Networks and Spatial Economics,04 December 2009,https://link.springer.com/article/10.1007/s11067-009-9119-6,Multistage System Planning for Hydrogen Production and Distribution,December 2010,Yongxi Huang,Yueyue Fan,Nils Johnson,Unknown,Unknown,Male,Male,"Seeking environmentally friendly and sustainable alternative fuels for transportation is important for the U.S. economy from both environmental and energy security perspectives. Hydrogen, as one of the alternative fuels, has received considerable attention in recent years for two reasons: (1) hydrogen is a clean energy carrier which can significantly reduce greenhouse gas emissions, and (2) it can be manufactured from a variety of primary energy resources, such as natural gas, coal, wind, nuclear, etc (National Research Council and National Academy of Engineering 2004). Although the advantages of hydrogen have been well recognized, the success of a hydrogen-based economy relies on its cost competitiveness relative to other fuels. During the transition to a hydrogen economy, an entirely new infrastructure system would be required for producing and distributing gaseous fuels. The cost of developing such an infrastructure system is a potential barrier to the deployment of hydrogen. Several studies have attempted to quantify the steady-state costs of various infrastructure components in the entire energy supply chain system (DOE 2006; Freppaz et al. 2004; National Research Council and National Academy of Engineering 2004; Parker 2007). As components of the entire supply chain, hydrogen production and distribution facilities have been specifically analyzed in (Ogden and Yang 2006) and (Yang and Ogden 2007), where several ways of reducing the cost of production and distribution were proposed using engineering-economic models. However, previous studies considered only steady state conditions and assumed deterministic and time-invariant demands, resource supplies, and other model parameters. This treatment may not be appropriate for the design of a hydrogen system where infrastructure is deployed over a long time period (planning horizon). In particular, there is considerable uncertainty regarding the growth in hydrogen demand over time. Hence, a model with an additional dimension for handling uncertainties and dynamics is required. The present paper addresses this void. The problem of gradually building a hydrogen production and distribution system falls within the general category of a dynamic location problem, which is one of the major research interests in location and logistics science. In a recent review article by Melo et al. (2006), it was pointed out that few realistic models consider stochasticities and dynamics and thus more research is needed in this area. Some existing studies based on multistage deterministic or stochastic programming (Chardaire et al. 1996; Dias et al. 2007; Kelly and Marucheck 1984; Sheppard 1974; Wesolowsky and Truscott 1975) have shed light onto our work, even though we approach the problem mainly from a dynamic programming viewpoint. In the later part of this article, we will discuss how the choice among different modeling techniques may affect the computational complexity of the problem. In this paper, a multistage stochastic dynamic programming model (Bellman and Kalaba 1965; Bertsekas and Tsitsiklis 1996; Dreyfus and Law 1977) is established to optimize the process of building and operating hydrogen production facilities during the transition to a hydrogen-based transportation system. Future demand for hydrogen is treated as the major source of uncertainty, and is assumed to increase over time. The location and sequence of production facilities represent the basic spatial and temporal dimensions of the problem. These are strategic planning decisions that are usually made over a long planning period and cannot be easily modified once implemented. In addition, there are operational decisions, such as the production quantities and the deliveries between plants and demand centers, which are examined more frequently and can be adjusted according to newly acquired information. This special feature of the problem leads to our choice of a stochastic dynamic programming model with a master- and sub- problem structure. The master-problem model focuses on the total expected system cost over the entire planning horizon while the sub-problem model focuses on the single-stage operational cost. The master and sub-problem models pass information between each other and are solved together iteratively. The details of this model structure will be provided in the next section. A case study based on the geographic setting of Northern California is included, in which the hydrogen is produced via coal gasification and transported from plant to city gates (demand sites) by cryogenic liquid hydrogen trucks. The demand for hydrogen is assumed to increase as the hydrogen fuel cell vehicle (HFCV) market penetration rate increases from 1% to 25% over a 20-year period (Miller et al. 2005). Sensitivity analyses were conducted to identify important model parameters and to analyze their impacts on the design and cost-effectiveness of hydrogen infrastructure systems.",6
10.0,4.0,Networks and Spatial Economics,23 June 2009,https://link.springer.com/article/10.1007/s11067-009-9106-y,Finding the ϵ-user Equilibrium Solution Using an Augmented Frank-Wolfe Algorithm,December 2010,Hsun-Jung Cho,Yu-Kuang Chen,,Unknown,Unknown,Unknown,Unknown,,
10.0,4.0,Networks and Spatial Economics,26 June 2008,https://link.springer.com/article/10.1007/s11067-008-9065-8,Optimization–Simulation Model for Planning Supply Transport to Large Infrastructure Public Works Located in Congested Urban Areas,December 2010,Jose Luis Moura,Angel Ibeas,Luigi dell’Olio,Male,Male,Male,Male,"Within the overall problem of goods transport in urban areas there is a particular case which has not been sufficiently considered in the literature: transport of supplies to large public infrastructure works being carried out in urban areas. This type of transport has an enormous environmental impact because of the special characteristics of the vehicles being used combined with existing problems of traffic flow in urban areas. This results in greater congestion for private vehicle traffic, increased atmospheric and noise pollution as well as higher costs for private transportation users due to longer daily journey times, etc. The transport of materials to a large public work can be examined as a typical supply chain problem, the building site being the point where materials need to be taken in predetermined quantities at specific times as precised in the job schedule. Our bibliographical analysis shows that other supply chains have been modeled and/or simulated in order to predict and improve their design for many types of industry. One example is Reiner and Traka (2004), who modeled and designed the supply chain structure for a food company. With the same aim in mind other types of tools and techniques have been used to study urban goods movement in supply chains: simulation techniques to study production, accounting and distribution policies, as in the work of Siprelle et al. (2003); microscopic methodology has been used by Boerkamps et al. (1999); models for mode choice and vehicle routing also belong to the microscopic modeling level, as in the work of Shinghal and Fowkes (2002), who use adaptive stated preferences for designing a freight mode choice model and Lin (2001) with a freight routing model of time-definitive delivery. As stated previously, none of the references above mention the particular case of transporting supplies to large public infrastructure works in urban areas, a subject which has hardly been studied. However, some work does exist, such as Shangyao et al. (2008), where an integrated model is designed combining a schedule for concrete production with its dispatch on trucks. The objective of this work was to minimize only the operator costs and consequently social and environmental impacts were not taken into account. Most of the studies that examine social and environmental impacts have mainly concentrated on the development of rules, regulations, measurement and legislation in order to minimize the impact of goods transport in urban areas. The work of Dablanc (2007) stands out in this field which discusses the current status of measures and the effects they have produced in European cities. Three elements are identified: the movement of goods does not take into account the structure of the city; policies aimed at goods movement are inefficient; and logistic services are being provided at a slower rate than that of demand. From the point of view of social implications, the work of Jula et al. (2005) proposes a model for the movement of containers using trucks with time constraints for origin and destination and constraints which guarantee that the drivers did not work more than a certain number of hours per shift. Therefore, this article presents a model to optimize the transport of supplies to large infrastructure public works in urban areas, based on minimizing the overall costs of the system. Apart from quantifying the costs associated with transport planning, the proposed model considers the emission of pollutants and noise pollution throughout the study area and evaluates the increased journey times suffered by private vehicle users due to the supplying of materials to the worksite. The result is a sustainable level of activity from a social, economic and environmental point of view. Section 1 covers the approach taken and the basic hypotheses for the transport system. Section 2 presents a brief introduction to the bi-level optimization problem. In Section 3 the optimization–simulation model is formulated. Section 4 provides specific details of the case study and the main conclusions are presented in Section 5.",10
10.0,4.0,Networks and Spatial Economics,12 July 2008,https://link.springer.com/article/10.1007/s11067-008-9070-y,A Heuristic Algorithm for Hierarchical Hub-and-spoke Network of Time-definite Common Carrier Operation Planning Problem,December 2010,Sheu Hua Chen,,,Unknown,Unknown,Unknown,Unknown,,
10.0,4.0,Networks and Spatial Economics,06 September 2008,https://link.springer.com/article/10.1007/s11067-008-9071-x,Integrated Network Capacity Expansion and Traffic Signal Optimization Problem: Robust Bi-level Dynamic Formulation,December 2010,Ampol Karoonsoontawong,Steven Travis Waller,,Unknown,Male,Unknown,Male,"Due to the complexity of transportation planning problems, it is common to decompose such problems into a sequence of sub-problems. The four-step planning process is a well-known example. However, this is clearly not ideal, and there are attempts to integrate these four steps as the solution from the four-step process is suboptimal compared to an integrated model. Continuous network capacity expansion problem (NCE), traffic signal setting design (SSD) and dynamic traffic assignment (DTA) are classical problems, and are indeed highly interdependent as well. The route choice decisions in the dynamic traffic assignment, which implies the state of the network, are made by travelers based on the link capacities and traffic signal settings. Transportation planners design the network capacity expansion policy and the signal settings based on the state of the network. If one solves these problems in sequence, one would not have a mutually consistent solution where flow is at dynamic user equilibrium, signal settings are optimal, and network capacity expansion decisions are most favorable. In this paper, we formulate the integrated NCE, SSD and DTA problem while explicitly capturing three realisms. Firstly, this integrated problem is naturally bi-level, and can be viewed as a static version of the non-cooperative, two-person game (Von Stackelberg 1952) in the context of unbalanced economic markets. With the assumption of perfect information, the static game means that each player has only one move. The leader (transport planner) goes first, attempts to minimize the total costs, and anticipates all possible responses of his opponent, the follower (road users). The follower observes the leader’s decision and reacts such that the follower achieves his/her optimal benefit (individual’s travel time) regardless of external effects (system-wide costs). Secondly, we capture traffic dynamics by employing the existing dynamic traffic assignment-based models (Ziliaskopoulos 2000; Ukkusuri 2002) that propagate traffic according to the cell transmission model (CTM) (Daganzo 1994). Thirdly, we explicitly treat the long-term time-dependent O-D demands as random variables with known probability distribution function. It is noted that the robust optimization formulation developed in this paper is based on the work by Mulvey et al. (1995). The formulation is considered a parametric model as the planners have to specify the degree of robustness or the weight on the risk term in the objective; i.e. it optimizes the weighted summation of expected cost and expected risk. Subsequently, we develop the two-stage stochastic and deterministic bi-level models, which can be seen as special cases of the proposed robust optimization formulation, and develop three new related single-level models. Since this paper focuses on developing new formulations, and comparing them to the existing formulations, it inevitably involves using several abbreviations in order to refer to various formulations. For convenience, we put together all frequently used abbreviations in Table 1.
 In our view, the contributions of this paper include:
 Develop a new bi-level robust optimization model of the combined network capacity expansion, signal setting design and user-optimal dynamic traffic assignment problem (RO-BLPNCE-SSD) together with an appropriate robustness measure. Develop additional new deterministic and two-stage stochastic linear bi-level formulations (BLPNCE-SSD and SLP2-BLPNCE-SSD) and three new linear single-level models (UONCE-SSD, SLP2-SONCE-SSD and SLP2-UONCE-SSD). Propose an exact solution method for RO-BLPNCE-SSD including a mathematical proof of the reduction of a mixed-zero-one continuous quadratic-linear bi-level program to an equivalent parametric quadratic-linear bi-level program. We consider a small test network with seven cells and nine cell connectors. The main reason for choosing such a system is that the network is simple enough to allow us to gain insight into the problem and understand the effects of alternative optimal signal settings and network capacity expansion policies recommended by a variety of proposed and existing integrated and sequential formulations. From our computational experiences, we gain the following interesting insights:
 UONCE-SSD and BLPNCE-SSD yield underestimated solutions to SLP2-UONCE-SSD and SLP2-BLPNCE-SSD, respectively, although these are not guaranteed. The sequential robust optimization approach (that sequentially solves robust NCE and robust SSD) performs worse than certain sequential and integrated approaches in terms of robustness. As expected, the integrated robust model outperforms the other existing and proposed integrated and sequential models in terms of robustness. The three measures (expected risk, expected total system travel time and expected robustness) of deterministic solutions (BLPNCE-SSD, SONCE-SSD and UONCE-SSD) fluctuates over the range of available budget. Although the objective of the integrated robust model does not directly optimize the variance of total system travel time, we find that the robust solutions also yield the least variance solutions. All models employ different cycle lengths as opposed to a common cycle length, and utilize traffic signal coordination by choosing different time offsets. It is noted that the proposed robust optimization model can be applied to real-size problems by incorporating certain methods in Ukkusuri and Waller (2007) and Karoonsoontawong and Waller (2005) as discussed in the last section. This paper is organized as follows. Section 1 provides the introduction. Section 2 reviews the relevant literature, and the concepts of two-stage stochastic programming and robust optimization. Section 3 shows the development of the proposed models. Section 4 presents exact solution methods for the proposed models. Our computational experiences are discussed in Section 5. Finally, the summary, conclusions and future directions are given in Section 6.",46
10.0,4.0,Networks and Spatial Economics,26 August 2008,https://link.springer.com/article/10.1007/s11067-008-9072-9,Combined Model Calibration and Spatial Aggregation,December 2010,Louis de Grange,Enrique Fernández,Magdalena Irrazábal,Male,Male,Female,Mix,,
10.0,4.0,Networks and Spatial Economics,13 September 2008,https://link.springer.com/article/10.1007/s11067-008-9073-8,A Generalized Model for Locating Facilities on a Network with Flow-Based Demand,December 2010,Weiping Zeng,Ignacio Castillo,M. John Hodgson,Unknown,Male,Unknown,Male,"Facility location is a central problem in real-world, strategic decision-making processes. In traditional facility location theory, demand for service is assumed to occur at fixed locations on a traffic network. This process is generally termed point-based demand. The main purpose of travel for point-based demand is to obtain or provide service. That is, consumers residing at nodes on the network travel to facility locations to obtain service (e.g., weekly grocery shopping and going to school or the workplace); alternatively, service providers located at network nodes travel to consumer locations to provide service as requested (e.g., ambulance, police, and repair services). In this article, we focus on demand for service that is expressed by flows travelling on origin-destination (O–D) pairs of a traffic network: flow-based demand. In contrast to traditional point-based demand, the main purpose of travel for flow-based demand is not necessarily to obtain service but to select from the facilities that are available. If a facility is located on the predetermined journey, consumers may choose to obtain service. Fast-food outlets, automated-teller machines, and gasoline stations are motivating examples of services that experience flow-based demand. Flow-interception location problems can be characterized as identifying good or optimal facility locations on a network with flow-based demand. The most basic location model that incorporates flow-based demand is the flow-interception location model (FILM) developed by Hodgson (1990) and Berman et al. (1992). Since these seminal publications appeared in the early 1990s, over 30 different flow-interception location models have appeared in order to formally characterize a wide spectrum of consumer desires and needs. Their applications have covered the strategic location of automated-teller machines and convenience stores (Berman et al. 1995; Hodgson et al. 1996; Berman 1997; Wang et al. 2002; Turner 2006, Zeng 2007a), advertising billboards (Averbakh and Berman 1996; Hodgson and Berman 1997), vehicle inspection stations (Hodgson et al. 1996; Gendreau et al. 2000), park-and-ride facilities (Horner and Groves 2007), gasoline stations and refuelling facilities (Kuby and Lim 2005; 2007; Kuby 2006; Kuby et al. 2007; Upchurch et al. 2007), pickup and fast-food outlets (Zeng et al. 2007), and cellular base stations (Erdemir et al. 2006). In addition, Berman et al. (1995) have developed several models to address generalizations of FILM in which flows are allowed to deviate from predetermined paths. Zeng (2007b) developed an integrated geographic information system, optimization, and heuristic system for efficiently aggregating flow-based demand data. The application of this integrated system to the FILM model with 2001 Edmonton afternoon peak traffic data is very efficient, inducing zero aggregation errors. The reader is referred to Zeng (2007a, b), Berman et al. (1995) and Hodgson (1998) for more detailed reviews of these models. Location researchers tend to introduce changes in the objective functions and/or assumptions by developing new models. These changes have resulted in numerous disparate models, each viewed as requiring its own solution method, and these models have challenged the development of standardized software that would encourage widespread use of location models in real-world strategic decision-making processes. This article proposes a generalized flow-interception location–allocation model (GFIM) into which most current deterministic flow-interception models can be transformed. Several critical considerations in flow-interception models—such as deviation from a predetermined journey, locational and proximity preferences, and capacity issues—are handled within GFIM’s single modelling framework. Interestingly, real-world examples, using 1989 morning (Hodgson et al. 1996) and 2001 afternoon (Zeng et al. 2007) peak traffic data for the city of Edmonton, Canada, show that a standard optimization engine such as ILOG-CPLEX optimally solves GFIM much more efficiently than the classic flow-interception location model in at least two instances of large real-world data. We believe that GFIM provides a new way of looking at location problems relative to flow-based demand and a new way of identifying similarities and differences among flow-interception location problems. GFIM also provides a fruitful garden for future research, thus making a substantial contribution to the flow-interception problem literature. The remainder of the article is organized as follows. Section 2 considers FILM, the classic flow-interception location model. Section 3 introduces GFIM, the proposed generalized model. Section 4 demonstrates a variety of current and future models to be special cases of GFIM. Section 5 examines the performance of GFIM and FILM using 1989 morning and 2001 afternoon peak traffic data for the city of Edmonton, Canada. The final section offers our conclusions.",49
11.0,1.0,Networks and Spatial Economics,07 October 2008,https://link.springer.com/article/10.1007/s11067-008-9076-5,Multi-objective Optimization Models for Distributing Biosolids to Reuse Fields: A Case Study for the Blue Plains Wastewater Treatment Plant,March 2011,Prawat Sahakij,Steven A. Gabriel,Chris Peot,Unknown,Male,,Mix,,
11.0,1.0,Networks and Spatial Economics,29 October 2008,https://link.springer.com/article/10.1007/s11067-008-9079-2,Nationwide Freight Generation Models: A Spatial Regression Approach,March 2011,David C. Novak,Christopher Hodgdon,Lisa Aultman-Hall,Male,Male,Female,Mix,,
11.0,1.0,Networks and Spatial Economics,05 November 2008,https://link.springer.com/article/10.1007/s11067-008-9080-9,Urban Traffic Jam Simulation Based on the Cell Transmission Model,March 2011,Jiancheng Long,Ziyou Gao,Penina Orenstein,Unknown,Unknown,Female,Female,"There are a number of traffic congestion studies in the literature but the majority of these are concerned with congestion formation on a single bottleneck (Daganzo and Laval 2005; Newell 1993; Ni and Leonard 2005) or the effect of congestion in a traffic network at the macroscopic system level (Gao and Song 2002; Gentile et al. 2007; Lam and Yin 2001). However, there are very few studies of congestion formation and dissipation at the microscopic level. It is important to study traffic jam development and dispersal at the microscopic level because this may yield qualitative insights about how to control traffic jams in general as well as form the basis for incident management techniques. A traffic jam can start in one of three ways (Wright and Roberg 1998): (1) a temporary obstruction, (2) a permanent capacity constraint in the network itself, and (3) a stochastic fluctuation in the demand within a particular sector of the network. In general, incident-based traffic jams can be categorized under the first type. During the past decade, colleagues at Middlesex University have used a combination of theoretical analysis and computer simulation to reveal some unexpected features of jam propagation for idealized grid networks (Roberg 1994, 1995; Roberg and Abbess 1998; Roberg-Orenstein et al. 2007; Wright and Roberg 1998; Wright and Roberg-Orenstein 1999). Roberg (1994, 1995) proposed simulation models which concentrated on a holistic view of traffic jam formation due to incidents, and described a number of strategies which could be exploited to achieve a controlled dispersion of traffic jams. Wright and Roberg (1998) proposed a simple analytical model for incident-based jam growth and discussed the effect of the length of the channelized part of roads and stopline width assignment on jam formation. Roberg and Abbess (1998) applied a simulation model to investigate the diagnosis and treatment of traffic jams. Wright and Roberg-Orenstein (1999) developed simple models for traffic jams and strategies for congestion control on idealized rectangular grid networks. Roberg-Orenstein et al. (2007) developed several alternative strategies for protecting networks from gridlock and dissipating traffic jams once they had formed. The treatment focused on the installation of bans at specific network locations. All the above mentioned works related to traffic jam formation and control have enabled a deeper insight into understanding some of the issues associated with area-wide traffic congestion. However, the studies are deficient because they sacrifice a great deal of realism, and the results are not immediately applicable to real networks. The major networks they used are idealized, uniform, one-way rectangular grid systems, which are not generally found in urban traffic networks. On the contrary, two-way roads are more commonly found in urban traffic networks, and their findings need to be evaluated in this context. Furthermore, the traffic flow model utilized in their models is oversimplified and does not adequately capture real traffic. Consequently, in this paper, we propose to extend the cell transmission model (CTM) (Daganzo 1994, 1995) to investigate the propagation of traffic jams in an idealized two-way rectangular grid network. The underlying reason for choosing the CTM is its plausibility in representing the spillback and shock wave phenomenon found in congested networks. Like the hydromechanical simulation model, the CTM is a discrete approximate model of the LWR model (Lighthill and Whitham 1955; Richards 1956), which is a macroscopical kinetic model. It can capture the realistic traffic dynamics, including important features such as shock waves, queue formation and dissipation, and dynamic traffic interactions across multiple links such as queue spillback. Because of the advantages in modelling network traffic, CTM has been used widely in dynamic traffic simulations (Long et al. 2008; Shang et al. 2007), signal control (Lo 1999, 2001), and dynamic traffic assignment (Ziliaskopoulos 2000; Lo and Szeto 2002; Lian et al. 2007). In this paper, we are concerned only with jams arising from a single incident in a two-way grid network system. With the fundamental work of Wright and Roberg (1998), the CTM is extended to simulate traffic jam formation and dissipation. In particular, both left turning and right turning movements are considered at the downstream channelized area of each link. We examine the process of jam formation and of jam dissipation in this context. In order to compare the properties of traffic jams produced under different conditions, we introduce two measures which include jam size and congestion delay (Lo 2001). The effect of stopline width assignment and the length of the channelized area on jam formation and dissipation are analyzed by computational simulation. ‘Gridlock’ phenomena are also highlighted in our investigation. Furthermore, we examine the effect of incident position and link length on jam propagation. We confirm some of the results described in Wright and Roberg’s work, and we also highlight some interesting new results. This paper has been organized as follows. In the next section, a network traffic simulation model based on CTM is introduced. Simulations and results are given in Section 3. Finally, our summary and conclusions are provided in Section 4.",65
11.0,1.0,Networks and Spatial Economics,05 November 2008,https://link.springer.com/article/10.1007/s11067-008-9081-8,Understanding Port Choice Behavior—A Network Perspective,March 2011,Loon Ching Tang,Joyce M. W. Low,Shao Wei Lam,Unknown,Female,,Mix,,
11.0,1.0,Networks and Spatial Economics,25 November 2008,https://link.springer.com/article/10.1007/s11067-008-9084-5,New Genetic Algorithms Based Approaches to Continuous p-Median Problem,March 2011,M. N. Neema,K. M. Maniruzzaman,A. Ohgai,Unknown,Unknown,Unknown,Unknown,,
11.0,1.0,Networks and Spatial Economics,10 December 2008,https://link.springer.com/article/10.1007/s11067-008-9093-4,A Dantzig-Wolfe Decomposition Based Heuristic Scheme for Bi-level Dynamic Network Design Problem,March 2011,Dung-Ying Lin,Ampol Karoonsoontawong,S. Travis Waller,Unknown,Unknown,Unknown,Unknown,,
11.0,1.0,Networks and Spatial Economics,09 January 2009,https://link.springer.com/article/10.1007/s11067-008-9095-2,Solving Over-production and Supply-guarantee Problems in Economic Equilibria,March 2011,Bing-sheng He,Wei Xu,Xiao-Ming Yuan,Unknown,,,Mix,,
11.0,1.0,Networks and Spatial Economics,20 January 2009,https://link.springer.com/article/10.1007/s11067-008-9096-1,Efficiency Analysis and Target Setting of Spanish Airports,March 2011,Sebastián Lozano,Ester Gutiérrez,,Male,Female,Unknown,Mix,,
11.0,1.0,Networks and Spatial Economics,27 November 2010,https://link.springer.com/article/10.1007/s11067-010-9150-7,Interval Uncertainty-Based Robust Optimization for Convex and Non-Convex Quadratic Programs with Applications in Network Infrastructure Planning,March 2011,Mian Li,Steven A. Gabriel,Shapour Azarm,,Male,Unknown,Mix,,
11.0,2.0,Networks and Spatial Economics,27 January 2009,https://link.springer.com/article/10.1007/s11067-009-9098-7,Optimizing Train Network Routing Using Deterministic Search,June 2011,KePing Li,ZiYou Gao,ChengXuan Cao,,,Unknown,Mix,,
11.0,2.0,Networks and Spatial Economics,28 February 2009,https://link.springer.com/article/10.1007/s11067-009-9101-3,Detecting Braess Paradox Based on Stable Dynamics in General Congested Transportation Networks,June 2011,Koohyun Park,,,Unknown,Unknown,Unknown,Unknown,,
11.0,2.0,Networks and Spatial Economics,10 September 2009,https://link.springer.com/article/10.1007/s11067-009-9108-9,Optimal Information Location for Adaptive Routing,June 2011,Stephen D. Boyles,S. Travis Waller,,Male,Unknown,Unknown,Male,"Transportation systems are inherently uncertain. Events such as incidents, poor weather, variations in travel demand, and the chaotic nature of congested vehicle flow, make it impossible for drivers to predict the travel time on any given route with certainty. A significant amount of travel delay, if not the majority, can be attributed to nonrecurring causes (Lindley 1987), and both researchers and practitioners have long recognized the importance that this uncertainty plays in describing traveler behavior. One common strategy for mitigating uncertainty is information provision through advanced traveler information systems (ATIS), such as variable message signs (VMSs), highway advisory radio (HAR), or many other technologies. These devices often provide information to drivers en route, so while drivers may anticipate receiving information at certain locations, they cannot anticipate the specific message they will receive. Thus, adaptive routing algorithms are needed to describe how drivers respond to this type of information. Within this context, public agencies must make decisions about where to locate devices such as VMSs or HARs. Installing these devices is costly, and a limited budget is available—for instance, an agency may only have sufficient funds for placing three VMS signs in a certain city, and must decide how to locate them to maximize the benefit to drivers. Alternately, the information location problems can also be used to provide adaptive driving directions for individuals. Many services are available which provide a route connecting a given origin and a given destination; however, in congested regions, the expected travel time can be reduced by providing several alternatives which can be used depending on observed traffic conditions. Current online shortest path algorithms can provide some insight on this problem, but their practical application is limited to real-time devices (such as in-vehicle navigation systems) because these typically assume a re-routing decision can be made at every node, and there is no easy way to convey this to drivers through printable directions or other format given a priori. On the other hand, by restricting re-routing decisions to a small number of nodes, one can simply report several complete paths to drivers, which is far more easily understood—the problem becomes one of deciding where to allow this re-routing, which is identical to the VMS location problem faced by a public agency. In this case, it may not be necessary to assume an external information provision device, but base online decisions on qualitative observations made by the driver: “If the freeway is congested, exit onto this arterial.” As mentioned above, considerable research has been performed on adaptive routing algorithms. Andreatta and Romeo (1988) considered a routing problem where arcs may fail, in which case a traveler may need to follow a predetermined “recourse path” to the destination. For acyclic networks, Psaraftis and Tsitsiklis (1993) describe an algorithm which can determine an optimal routing policy. When cycles are permitted, Miller-Hooks (2001) presents a polynomial-time algorithm for independent arc costs, while Waller and Ziliaskopoulos (2002) and Provan (2003) present pseudopolynomial algorithms for several arc dependence scenarios. The case of general dependency is more difficult, and is NP-complete, unless one takes the “reset assumption,” in which arc costs can vary on successive visits; Polychronopoulos and Tsitsiklis (1996) and Provan (2003) develop algorithms for this case. Pretolani (2000) and Miller-Hooks (2001) also address adaptive routing when costs are time-dependent as well as stochastic. Gao (2005) presents an algorithm when arc costs are time-dependent, stochastic, and have general dependency. These algorithms typically assume users wish to minimize their expected travel time; other behaviors have also been considered, such as minimizing schedule delay (Gao and Chabini 2006) or maximizing the probability of on-time arrival (Nie and Fan 2006). Any of these algorithms can be used to help determine the best locations to provide information, depending on the assumptions one makes about the underlying network. Several researchers have also conducted studies regarding optimal locations for providing information. Abbas and McCoy (1999) applied a genetic algorithm to place VMSs at locations that maximize the number of vehicles which observe these signs, but did not consider adaptive behavior in response to this information. Chiu et al. (2001) and Chiu and Huynh (2007) combine a mesoscopic dynamic traffic assignment simulation with a tabu search heuristic to optimally locate VMSs. Incidents were randomly generated using a Monte Carlo scheme, and some drivers would switch routes if their path encounters an incident and a VMS sign; based on the resulting flow patterns, a set of VMS locations was determined to optimize some measure of effectiveness. Huynh et al. (2003) uses a similar analysis framework to find the optimal locations of portable VMSs in a real-time framework, using the G-D heuristic. Although the simulation approach allows a rich set of traffic and behavioral impacts to be modeled, the computational burden associated with many simulation runs on a large network can be troublesome. This limitation was realized by Henderson (2004), who adopted a static equilibrium framework for VMS location, together with a discrete choice model to determine the proportion of drivers who switch routes in response to learning of an incident. Several heuristic techniques are developed and compared, including a genetic algorithm and a greedy approach based on sequential location. While computationally faster, this approach implicitly assumes that drivers do not anticipate receiving information; that is, their initial route choice is not affected by the VMS locations, so links with a VMS do not “attract” drivers who anticipate benefitting from that information, for instance. Although this distinction may seem subtle, this anticipation effect can lead to radically different route choices for rational drivers, even from the origin (Boyles 2006). The research presented here complements these works by providing analytical network algorithms for locating information, where users both anticipate receiving information and adjust their routes adaptively. The remainder of this paper is organized as follows. Section 2 describes the problem context formally, along with rigorous definitions of three information location problems addressed in the paper. Section 3 describes a network contraction procedure which allows candidate solutions to be evaluated extremely rapidly. Section 4 describes exact algorithms and heuristics for solving these three problems, which are then demonstrated in Section 5. Finally, Section 6 concludes the paper by summarizing the key contributions and pointing to future research directions.",16
11.0,2.0,Networks and Spatial Economics,04 December 2009,https://link.springer.com/article/10.1007/s11067-009-9118-7,Inefficiency of Logit-Based Stochastic User Equilibrium in a Traffic Network Under ATIS,June 2011,Hai-Jun Huang,Tian-Liang Liu,Hai Yang,,,,Mix,,
11.0,2.0,Networks and Spatial Economics,07 January 2010,https://link.springer.com/article/10.1007/s11067-009-9124-9,A Dual Variable Approximation Based Heuristic for Dynamic Congestion Pricing,June 2011,Dung-Ying Lin,Avinash Unnikrishnan,S. Travis Waller,Unknown,Male,Unknown,Male,"Congestion pricing has increasingly been seen as a powerful tool for both managing congestion and generating revenue for infrastructure maintenance and development. By carefully levying tolls on roadways, a more efficient and optimal network flow pattern can be generated. In addition, congestion pricing acts as an effective travel demand management strategy which reduces peak period vehicle trips by encouraging people to shift to more efficient modes such as transit. Recently, with the increase in the number of highway Build-Operate-Transfer (B-O-T) projects, tolling is viewed as an effective way of generating revenue to offset the construction and maintenance cost of infrastructure. Despite its obvious advantages and increasing public interest, a careful analysis has to be conducted before determining tolls as sub-optimal tolls can significantly worsen the system performance. In this regard, numerous studies have been conducted suggesting innovative techniques to account for various factors including elasticity of demand, value of time and types of tolling etc. This paper contributes to the growing body of literature in congestion pricing by providing an efficient dual variable approximation based heuristic combined with Method of Successive Averages (MSA) to determine time-varying tolls in networks. The major advantages of the proposed solution scheme are: (1) It can efficiently and effectively solve the first best dynamic congestion pricing problem; (2) The solution scheme can readily handle the second best dynamic congestion pricing; (3) Road users’ departure time decisions are implicitly considered.",15
11.0,2.0,Networks and Spatial Economics,09 February 2010,https://link.springer.com/article/10.1007/s11067-009-9126-7,"The p-hub Model with Hub-catchment Areas, Existing Hubs, and Simulation: A Case Study of Serbian Intermodal Terminals",June 2011,Milorad Vidović,Slobodan Zečević,Snežana Tadić,Male,Male,Female,Mix,,
11.0,2.0,Networks and Spatial Economics,17 June 2010,https://link.springer.com/article/10.1007/s11067-010-9135-6,Competitive Delivered Pricing by Mail-Order and Internet Retailers,June 2011,Phillip J. Lederer,,,Male,Unknown,Unknown,Male,"Mail-order and internet sales have become an important component of retail sales. Traditionally, mail-order sales offered shop-at-home convenience for geographically isolated or busy consumers. In the past 15 years, this trend has continued with the advent of internet shopping. In addition to time savings, both mail-order and internet shopping provide competitive alternatives to local retailers’ prices and product assortment. This paper studies price competition between mail-order/internet sellers and local retailers (often called “bricks and mortar” stores) for the sale of physical goods which need to be transported by or to the customer.Footnote 1 The purpose is to understand how such competition affects the mail-order/internet sellers’ choice of how to charge for delivery. Despite the relative novelty of internet retailing, its operation is quite similar to that of traditional mail-order retailers: catalogs are distributed (on paper through the mail, or on line), orders taken (via mail, phone, or the Web), and product sent to the customer using package delivery services. Thus, for parsimony and notational simplicity, I refer to both mail-order and internet sellers as “mail-order” firms, and “bricks-and-mortar retailers” simply as “retailers.” An important aspect of competition between mail-order and retailers is the issue of competitive pricing, and the related issue of how customers pay shipping charges. Traditionally, mail-order firms chose between two pricing policies: either “uniform spatial pricing,” where the firm charges the same delivery charge to any customer, or “mill pricing,” where the customer pays the actual shipping charge. For example, Lands’ End has always set a uniform spatial charge for delivery (that was dependent upon dollar sales but not on location), and L.L. Bean has set a similar policy.Footnote 2 However, some mail-order firms such as Dinnerware Depot (fine china), Bikesmart (bicycle parts), PartsAmerica (spare parts for appliances), and Vegetarian Store (specialty foods) bill the shipping charge. In contrast to uniform spatial pricing are other pricing regimes such as mill pricing (sometimes called f.o.b. pricing where the customer pays the freight costFootnote 3), and spatial discriminatory pricing, (sometimes called “delivered pricing” where customers are charged a price that includes both the good and delivery service). In this paper I study and contrast the strategic choice of uniform pricing versus mill pricing for mail-order sellers. I explicitly neglect other patterns of spatial delivered pricing, as discriminatory spatial pricing patterns have been declared illegal under the Robinson-Patman Act (and related laws in Europe). Scherer (1980) presents the long history of government action against firms employing spatial pricing that is not directly tied to cost, particularly distribution/transport cost. This paper contributes to the literature by developing an analytic model to explain why some mail-order firms use uniform pricing and others do not. Intuition behind the model shows that there are two countervailing factors, one of which tends to favor mill pricing and another that can favor uniform pricing. To demonstrate these effects, I assume demand at all spatial markets is described by a linear demand function up until the price where demand becomes zero, that is demand = [constant−price]+. The first factor is that as price rises at a market point eventually demand will fall to zero. This demand truncation causes the demand function to be convex, which we show causes mill pricing to generate higher profit than uniform pricing, all other things being equal. I refer to this as the demand truncation effect. See Fig. 1.
 The truncated linear demand function q = [a−p]+ is a convex function The second factor is caused by the existence of distant competing retail firms. I assume that customers buy from the firm offering the lowest delivered price. For linear demand functions and any mill price set by the mail-order firm, there is an associated uniform price that earns the mail-order firm identical total profit on the market points where the mill price generates positive demand. But at this uniform price, the mail-order firm always penetrates a larger market area. If the mill price is high enough, then the new market area captured creates additional profit for the mail-order firm. See Fig. 2(a). However, if the mill price is low enough, then the new market area captured by the corresponding uniform price is unprofitable and reduces profit. See Fig. 2(b). I call this the market penetration effect. I show that if customers’ willingness to pay is high enough, then the market penetration effect causes uniform prices to be more profitable than mill prices.
 (a) Uniform pricing yields higher profit than mill pricing if parameter a is high enough. Suppose the mail-order firm faced by retailer price p

r
 sets its profit maximizing mill price, \( p_M^* \). The mail-order chooses its uniform price equal to\( p_U^\prime = p_M^* + \overline t \), where \( \overline t \) is the average transportation cost in the interval [x,1−x]. We assume that under mill prices all customers in the interval [x,1−x] are served with positive quantity of good. Under this assumption, Smithies (1941) shows that profit restricted to the interval [x,1−x] using the uniform price and the mill price are identical. When the uniform pricing firm earns positive contribution on the intervals [y,x] and [1−x,1−y], uniform pricing yields greater total profits than mill. We will show in Proposition 3.1 that the optimum mill price is strictly monotone in parameter a and the uniform pricing firm earns positive contribution on the intervals [y,x] and [1−x,1−y] when parameter a is high enough. (b) Mill pricing yields higher profit than uniform pricing if parameter a is low enough. For some small value of a, the mail-order firm’s profit maximizing uniform price against retailers’ price p

r
 is \( p_U^* \). Note that outside of the interval [z,1−z], uniform pricing yields negative profit. Now define a mill price: \( p_M^\prime = p_U^* - \overline t \)where \( \overline t \) is the average transportation cost in the interval [z,1−z]. As shown by Smithies (1941), the profits for this mill price and the optimal uniform price restricted within the interval [z,1−z] are identical. But it is clear that outside this interval, the mill pricing firm earns additional contribution, but a uniform pricing firm loses contribution Thus, demand truncation and market penetration effects can create contradictory forces. How they resolve is explained in this paper using analytical results. To show the generality of results I present two cases of pricing policy choice: when retailers do not react to mail-order prices, and when they do. In the first case, the matter is to find the profit maximizing pricing policy. In the second, competitive case, I study competition using a game-theoretic model seeking to characterize the Nash equilibrium set. In the first case, I show that if customer maximum willingness to pay is high enough, the market penetration effect is more important than the demand truncation effect, and causes uniform pricing to be profit maximizing (and vice versa). In the competitive case, I show that a subgame perfect Nash equilibrium always exists and is essentially unique. I also characterize all Nash equilibria, (a larger set than subgame perfect equilibria) showing that either uniform or mill pricing can be the equilibrium choice for some parameter values. Thus this paper shows that the mail-order firm’s choice of uniform or mill pricing depends on market parameters, and that profit maximizing pricing policies may be different in different markets. The initial analysis leads to corollary results and generalizations. Specifically, the effect of production cost, market size, transportation rates, and degree of competition on the mail-order firm’s price policy choice can be understood. These corollaries all derive from consideration of how these factors affect the market penetration and demand truncation effects. Next is a brief literature review. There is a small but significant literature on uniform pricing in the economics literature. Several authors present conjectures about why mail-order firms use uniform pricing. Among them is Norman (1981) and Cheung and Wang (1996), who hypothesize that uniform pricing occurs because of low customer billing cost. However, this hypothesis does not explain why some firms find billing for shipping “complex” and others find individual billing simple enough to execute. For example, why does Lands’ End set uniform prices while PartsAmerica uses mill pricing when the latter is a much smaller firm with (presumably) higher billing cost? In the general business literature others have suggested that the low cost of transportation per unit value explains the use of uniform pricing. Clearly, low transport cost makes charging for shipping economically unimportant. However, in some retail categories uniform pricing is used for very heavy goods having a relatively large shipping cost-to-weight ratio. For example, web retailers of furniture overwhelmingly use uniform pricing. I show in Section 6 how this empirical observation can be explained by the theories developed in this paper. An economically compelling argument for uniform pricing is offered by Smithies (1941).Footnote 4 That paper assumes a linear bounded market with uniformly distributed customers having elastic demand for a single good, and a single monopolist with a fixed location selling this good. Smithies shows that convexity or concavity of each customer’s demand function makes uniform or mill pricing more profitable. Smithies states (page 64, Proposition 3): 
“If the monopolist is considering the alternatives of f.o.b. selling and of a uniform delivered price, and is subject to constant marginal cost, he will adopt the former if the demand curve is convex,
Footnote 5
and the latter if it is concave. It the demand curve is linear, it is indifferent to him which of the two policies he adopts.”
 These results must be qualified by the condition that all customers in the market are served with positive demand.Footnote 6 Also, the proposition can be generalized to non-uniform distributions of customers. See Lederer (2009). Smithies’ work yields an immediate insight. When demand is linear but demand is not allowed to fall to zero, say, assuming local demand function, D(p) = [a−p]+, Smithies’ results predict that mill pricing will yield higher profit because the demand function is now convex. This is what I called the demand truncation effect in the last subsection. See Fig. 1. Competition between two profit maximizing firms both free to choose between mill or uniform pricing policies is studied by Kats and Thisse (1993). A two-staged game is modeled where first, firms pick price policy and then, second, prices. The market is a circle populated by uniformly distributed customers that have the same reservation price and demand one unit. Most important is the following result: when the common reservation price is high, and both firms choose mill pricing, neither wishes to switch to uniform pricing. This result is exactly the opposite found here, and is caused by an important assumption made by the authors. Unlike in this paper, a uniform pricing firm can not only choose its prices, but it can also choose its customers. The latter option is so powerful that price reaction to it results in very low equilibrium prices when uniform prices are chosen by either or both firms. The power comes from the ability to penetrate markets avoiding losses due to serving distant and thus, transport-costly customers, with uniform prices. Other significant papers in this literature includes Anderson and Neven (1990) who show that a uniform price pattern can arise as the result of two perfectly symmetric firms competing à la Cournot in a linear market. Thisse and Vives (1988) show that delivered pricing always dominates uniform or mill pricing. dePalma et al. (1987) show that a Bertrand equilibrium cannot exist when two firms sell identical goods and use uniform prices. They also show that a location-price equilibrium can exist when the goods are differentiated. Cheung and Wang (1996) study the relative output and social welfare of mill and uniform pricing. Norman (1981) shows that a monopolist with the objective to maximize sales revenue will prefer uniform pricing as long as demand elasticity rises with price. Hanjoul et al. (1990) study the effect of uniform pricing on the location of firms. There is scant empirical work on the use of uniform pricing. Most relevant to mail/web retailing, Dinlersoz and Li (2006) report that internet booksellers almost all use uniform pricing. Less relevant due to focus on industrial markets are two papers: Phlips (1983) presents examples of uniform pricing in Europe in the cement and plasterboard industries, and Greenhut (1981) shows that in industrial markets uniform pricing is widely used. Somewhat related to this paper is work on price competition between traditional suppliers and new entrants via the internet. For example, Kalvenes and Keon (2008), study consumer choice of videos from new video-on-demand providers versus traditional movie rental and cinema alternatives. The rest of the paper is organized as follows. Section 2 presents the basic model of this paper. Section 3 studies the problem when retailers do not react in price to mail-order price changes. Section 4 analyzes two competitive situations: price competition between mail-order and retailers 1) when the mail-order chooses mill, and 2) when the mail-order chooses uniform pricing. Existence and properties of the Nash equilibrium are shown in both cases. In Section 5, the complete model of competition is presented and analyzed: a mail-order firm that can choose its pricing policy competes against retailers that can compete in price. Existence and properties of all Nash equilibria and all subgame perfect Nash equilibria are presented. Section 6 closes the paper with empirical observations and generalizations implied by the analysis.",5
11.0,2.0,Networks and Spatial Economics,12 August 2010,https://link.springer.com/article/10.1007/s11067-010-9141-8,The Morning Commute Problem with Coarse Toll and Nonidentical Commuters,June 2011,Feng (Evan) Xiao,Zhen (Sean) Qian,H. Michael Zhang,,,Unknown,Mix,,
11.0,2.0,Networks and Spatial Economics,04 September 2010,https://link.springer.com/article/10.1007/s11067-010-9147-2,Robust Optimization Model for a Dynamic Network Design Problem Under Demand Uncertainty,June 2011,Byung Do Chung,Tao Yao,Andreas Thorsen,,,Male,Mix,,
11.0,3.0,Networks and Spatial Economics,17 September 2011,https://link.springer.com/article/10.1007/s11067-011-9165-8,Preface Special Issue on Latin-American Transport Research,September 2011,Víctor Cantillo,José Holguín-Veras,,Male,Male,Unknown,Male,,
11.0,3.0,Networks and Spatial Economics,27 August 2010,https://link.springer.com/article/10.1007/s11067-010-9143-6,On the Treatment of Repeated Observations in Panel Data: Efficiency of Mixed Logit Parameter Estimates,September 2011,María Francisca Yáñez,Elisabetta Cherchi,Juan de Dios Ortúzar,,Female,Male,Mix,,
11.0,3.0,Networks and Spatial Economics,29 May 2010,https://link.springer.com/article/10.1007/s11067-010-9134-7,Econometric Effects of Utility Order-Preserving Transformations in Discrete Choice Models,September 2011,Francisco Javier Amador,Elisabetta Cherchi,,Male,Female,Unknown,Mix,,
11.0,3.0,Networks and Spatial Economics,03 February 2009,https://link.springer.com/article/10.1007/s11067-008-9097-0,A Hierarchical Gravity Model with Spatial Correlation: Mathematical Formulation and Parameter Estimation,September 2011,Louis de Grange,Angel Ibeas,Felipe González,Male,Male,Male,Male,"In this work we present a hierarchical gravity model for trip distribution that can incorporate many different spatial correlation structures. Our approach is completely different from those based on traditional spatial econometric techniques (Anselin and Bera 1988; Arbia 2006). The model’s design begins with the specification of a multi-objective optimization problem and its corresponding substitute or equivalent problem. From this mathematical formulation an alternative hierarchical specification is derived that incorporates spatial correlation, which we call the hierarchical gravity model. The parameters of the model are estimated using the traditional maximum likelihood method applied sequentially (Ortuzar and Willumsen 2001). Our main conclusions are that spatial correlation is strongly present in trip distribution and that addressing this phenomenon in gravity-type models by means of a hierarchical structure significantly improves their explanatory and predictive powers. Spatial trip distribution models are a fundamental tool in the planning processes of urban and interurban transportation systems. They are used to estimate trip matrices representing system travel patterns that can then be assigned to a transportation network in order to determine route demand levels (flows in arcs) and service levels (such as travel times). The best known and most basic trip distribution model is the so-called transportation or Hitchcock problem (Hitchcock 1941), which consists in supplying inputs produced at given origins to a series of destinations at minimum cost. This formulation was extended with the development of a doubly constrained adaptation of the classical gravity model (Wilson 1970). Using the concept of entropy, and given certain trip generations and attractions, this approach determines the most likely trip distribution matrix. Other distribution models based on the intervening opportunities concept of destination choices (Stouffer 1940; Schneider 1959) have not proven to be an advance on the classical entropy versions. Using Wilson’s entropy design as a starting point, and adding the Wardrop equilibrium conditions in the form of a Beckman transformation (Beckman 1956), Evans (1976) developed an equilibrium model of combined trip distribution and assignment. Other entropy distribution models include Fotheringham (1983, 1986), Fang and Tsao (1995) and Thorsen and Gitlesen (1998). They are similar to the Wilson’s standard model but with certain more sophisticated features. Common to all of these formulations, however, is that none takes spatial correlation into account. Traditionally, cross-sectional models incorporating spatial correlation are designed and calibrated using spatial econometric techniques enabling various types and formulations of correlation to be specified in analogous fashion to time series models (Anselin and Bera 1988; Arbia 2006). Spatial correlation can be simply defined as the impact on the explained variable for a given geographical sector (in our case, the number of trips for each origin-destination pair) of the explained and explanatory variables of contiguous or neighboring sectors. Spatial econometrics creates a contiguity matrix that indicates whether a given observation is spatially correlated with any other observation. In this study the observations are trips between different origin-destination pairs, or in other words, the elements of an origin-destination trip matrix. It should be noted that the definition of the spatial correlation matrix is arbitrary and depends on the modeler. Various types of spatial correlation can be defined based on the matrix (see Arbia 2006, for an up-to-date treatment of the subject). The approach to incorporating spatial correlation we propose here differs from the spatial econometric one in that it is based on the formulation of hierarchical logit distribution models. Such models are able to capture correlations between the choice alternatives or observations in cross-sectional models (Ortúzar and Willumsen 2001, pp. 228–233). In the case of trip distribution, the choice alternatives are the various origin-destination pairs transit system users can travel between. The classical linear regression models that have been developed for capturing spatial correlation (Anselin and Bera 1988; Arbia 2006) cannot be used for estimating the doubly constrained gravitational model due to the endogeneity problem generated by the balancing factors (De Vries et al. 2000; De Grange et al. 2008). We therefore propose a new gravity-type formulation with a hierarchical structure that can capture certain trip destination correlation patterns, which can be defined as spatial correlation structures. The approach we will take to modeling these patterns requires bi-directionality by virtue of the model’s construction in the sense that the variance and covariance matrix is symmetric. Although this constitutes a limitation compared to more general definitions of spatial correlation that can be incorporated in linear regression models, it represents a significant advance in doubly constrained gravitational models of trip distribution, which cannot be correctly estimated using linear regression but can be with maximum likelihood. The remainder of this paper is organized in the following manner: In Section 2, we set out the analytical derivation of the hierarchical gravity distribution model, the approach to parameter estimation and the specification of the hierarchical model. In Section 3 we introduce an application of the hierarchical model and provide a comparative analysis with the standard gravity model that involves contrasting the modeled trip matrices obtained using the two models with the observed matrix. The two models are also employed to make an out-of-sample prediction. In Section 4 we posit a series of extensions to the hierarchical model, mainly regarding the way it can be incorporated into combined transportation models. Finally, in Section 5 we report the main conclusions and recommendations based on our study’s findings.",18
11.0,3.0,Networks and Spatial Economics,23 May 2010,https://link.springer.com/article/10.1007/s11067-010-9132-9,Special Issue on Latin-American Research: A Time Based Discretization Approach for Ship Routing and Scheduling with Variable Speed,September 2011,Ricardo A. Gatica,Pablo A. Miranda,,Male,Male,Unknown,Male,"Ocean shipping is by far the cheapest transportation system for large quantities of cargo, and consequently, the most important transportation mode for international trade. In the last 25 years, ocean shipping has experienced a significant increase due to globalization, rapid growth of Asian economies, deregulation of international trade, and development of the shipping industry. Some illustrative statistics are given in Agarwal and Ergun (2008). General shipping industry statistics are available in publications by the Institute of Shipping Economics and Logistics (www.isl.org), and the Astrup Fearnley Group (www.fearnley.com). This rapid growth in ocean shipping has derived in a very competitive industry, characterized by its sensitivity to the evolution of global economy, and a high volatility in both shipping rates and ship renting costs. This environment, in an industry with major capital investments and high operating costs, suggests that companies may achieve significant economic benefits by a proper fleet management. Specific areas of potential improvement are fleet sizing, maritime supply chain design, and ship routing and scheduling, among others. See Christiansen et al. (2004) for a recent survey on fleet management and related problems. Shipping companies usually operate their fleets under one of three general modes (Christiansen et al. 2004; Ronen 1993): liner, tramp or industrial. Liners offer services based on regular itineraries with predetermined routes, frequencies, and ports arrivals/departures. Tramp fleets engage in contracts to transport specified (usually large) volumes of cargo between two ports within a period of time. Alternatively, they engage in contracts to make one or several trips, each trip having specified origin and destination ports and time windows for picking and delivering the cargo. Tramp is usually the operation mode selected to transport liquid and dry commodities, or cargo involving a large number of units (e.g. vehicles). Finally, a fleet is said to operate under an industrial mode if the cargo owner controls the fleet. In the simplest cases, the industrial fleet operation is similar to the tramp fleet operation. In more general cases, however, an industrial fleet operation becomes much more complex and unique. This is particularly true when ocean transportation is part of a larger supply network and trips are not pre-specified, but must be determined according to the time-dependent supply and demand functions associated with the different nodes of the network. Another useful classification is that of deep-sea shipping v/s short-sea shipping. Deep-sea shipping involves long (typically intercontinental) trips, in which traveling times are much longer than time windows at picking and delivery ports. Short-sea shipping involves short (typically regional) trips, in which traveling times are likely to be shorter than time windows, and therefore port schedules and service restrictions become much more important for fleet management. There are three main types of costs in ocean shipping: capital and depreciation, running, and operating costs. Capital and depreciation costs are related to the loss of ships’ market value respect to the initial investment. Running costs are fixed in the sense that they are independent of traveled distance, cargo type and amount, maritime routes, and visited ports. They include maintenance, insurance, crew salaries, and overhead costs, among others. Operating costs are directly related with ships’ daily operations. They depend on each trip’s characteristics and conditions (traveled distance, navigation speed, maritime routes, etc.), and include fuel consumption (which can be, to a large extent, controlled by navigation speed), port and customs expenses, tolls paid at canals (e.g., Suez and Panama), etc. In this paper we develop a network based model for the routing and scheduling of a heterogeneous tramp fleet. A distinctive aspect of the model is that time windows for picking and delivering cargoes are discretized. This allows for the consideration of a broad variety of features and practical constraints by simply adding/deleting arcs or modifying the corresponding cost parameters, which has the advantage of preserving the network structure. In particular, we consider problems in which navigation speed can be used to control fuel consumption. This may have a significant impact in the quality of the solution, since fuel consumption is an approximately cubic function of speed (Ronen 1993). Other factors that can be easily incorporated in the model are soft time windows, cleaning times or incompatibilities between successive cargoes, incompatibilities between cargoes and ships, maximum waiting times at ports, alternative maritime routes, etc. We want to highlight that several of these features may be hard to include in an exact continuous equivalent optimization model.",23
11.0,3.0,Networks and Spatial Economics,07 January 2011,https://link.springer.com/article/10.1007/s11067-010-9151-6,New Models for Commercial Territory Design,September 2011,María Angélica Salazar-Aguilar,Roger Z. Ríos-Mercado,Mauricio Cabrera-Ríos,,Male,Male,Mix,,
11.0,3.0,Networks and Spatial Economics,11 July 2009,https://link.springer.com/article/10.1007/s11067-009-9107-x,An Experimental Economics Investigation of Shipper-carrier Interactions in the Choice of Mode and Shipment Size in Freight Transport,September 2011,Jose Holguín-Veras,Ning Xu,Hedi Maurer,Male,,Female,Mix,,
11.0,3.0,Networks and Spatial Economics,14 January 2010,https://link.springer.com/article/10.1007/s11067-009-9125-8,Lumpy Investment in Regulated Natural Gas Pipelines: An Application of the Theory of the Second Best,September 2011,Dagobert L. Brito,Juan Rosellón,,Male,Male,Unknown,Male,"The timing of lumpy investment with stochastic demand for pipelines is not a solved problem.Footnote 1 It is not a problem that is conceptually difficult, but the information needed is not available. The technology of gas pipelines requires lumpy investment. Once the pressure limits on a pipeline are reached, the only way to add capacity is to add pipe or add pumping stations to increase throughput. The market is not a good guide to the allocation of resources in pipeline capacity. It can take as long as 3 years lead time to increase pipeline capacity, so it is necessary to rely on forecasts of future demands for the purpose of planning investment in pipeline capacity. If not most of the time wrong, these forecasts are at best uncertain. Some of the stochastic functions and parameters are short term such as weather and others, such as the price of gas, are long term and can reflect macroeconomic conditions. Such forecasts either do not exist or they are subject to such errors that they are not very useful. In theory, the unsolved problem of investing in pipelines can be formulated as a dynamic program,Footnote 2 but the solution then depends on such stochastic demand functions and parameters that are either subjective or cannot be estimated. Further, computing an ideal first-best efficient solution—where capacity investment is guided through some sort of efficient regulatory mechanism (if it existed)—may not very useful if consumers did not want to face any periods of congestion. The elasticity of the demand for gas is such that small amounts of congestion can cause large fluctuations in price. Inasmuch as many consumers do not have access to complete markets, these fluctuations might result in substantial wealth transfers. In this paper, we formulate the problem in a manner that consumers can choose between an increase in cost of transporting gas against the reduced risk of wealth transfers due to congestion of the pipeline. These investment strategies are not optimal in the strict sense of the word. There is a well-known result in the network literature that an optimal investment policy involves some periods where the congestion is binding. However, in a second-best world, consumers, who ultimately pay the full cost of congestion in pipelines, may prefer to bear the cost of excess capacity rather than the risk of transfers created by binding constraints.Footnote 3 These investment policies are C-efficient if the consumers of the gas bear the cost of moving the gas in the pipeline.Footnote 4
 Another issue in the regulation of gas pipelines is the rate structure. The technology of pipelines is such that marginal cost pricing will not cover average costs during a substantial part of the investment cycle. A theoretical solution to the non-lumpy version of this problem might well be based on two-part tariff regulation. However, investment in gas pipelines tends to be lumpy by nature. Since the demand for gas is very inelastic (especially in natural-gas distribution areas), the welfare losses associated with average cost pricing are small. In particular, this implies that a gas pipeline system could be regulated with a reasonably simple set of rules that regulate investment and rates without any significant loss of welfare. The resulting system can be transparent and a good candidate for some institutional arrangement in which there is substantial incremental private investment in gas pipelines.Footnote 5
 The organization of the rest of the paper is as follows. In Section 2 we review the literature on regulatory mechanisms that promote the expansion of networks, both in general as well as applied to natural-gas pipelines. In Section 3, a discussion on average-cost pricing, and its impacts on welfare loss when the demand for gas is inelastic, is presented. Our model on different strategies to invest in buffer pipeline capacity is developed in Section 4. We also derive in this section the cost of building the extra capacity which consumers are willing to pay so as to never face periods of congestion. This cost is compared with the cost of congestion to consumers in Section 5 so as to derive the optimal timing of investment in buffer capacity. In Section 6, we integrate into a comprehensive welfare model the derivation of the optimal investment timing suggested in the two previous sections. Section 7 concludes.",6
11.0,3.0,Networks and Spatial Economics,09 March 2010,https://link.springer.com/article/10.1007/s11067-010-9128-5,"“Special Issue on Latin-American Research” Maritime Networks, Services Structure and Maritime Trade",September 2011,Laura Márquez-Ramos,Inmaculada Martínez-Zarzoso,Gordon Wilmsmeier,Female,Female,Male,Mix,,
11.0,4.0,Networks and Spatial Economics,19 February 2011,https://link.springer.com/article/10.1007/s11067-011-9155-x,Guest Editorial: New Frontiers in Accessibility Modelling: An Introduction,December 2011,Aura Reggiani,Juan Carlos Martín,,Female,Male,Unknown,Mix,,
11.0,4.0,Networks and Spatial Economics,15 April 2010,https://link.springer.com/article/10.1007/s11067-010-9129-4,Introducing a Method for the Computation of Doubly Constrained Accessibility Models in Larger Datasets,December 2011,John Östh,,,Male,Unknown,Unknown,Male,"The spatial mismatch hypothesis (Kain 1968) has for four decades been used as a scientific framework for the understanding of the geographical mismatch between the jobs and potential job searchers. In his seminal work Kain noted that the suburbanization process of job opportunities in the US metropolitan areas were coupled with increasing disadvantages for the African American workers and job seekers, predominately located in the city center areas. The main reason being that as the commuting distances grew, transportation became less affordable, and in many cases not accessible at all. Since the first article a vast amount of research has been conducted and the hypothesis has been adapted to encompass spatial mismatch relationships between other disadvantaged groups of job seekers and job opportunities in different types of locations. As a consequence of the development of the understanding of the hypothesis, today, a group of workers can be said to suffer from spatial mismatch if they experience worse access to jobs than others groups of workers, and if the creation of new jobs take place at locations that are less available either due to long distances or due to other barriers hampering their labor market participation. Due to the extensive number of articles written on the subject the main results and trends in research may be hard to grasp. However, several reviews of the progress in research serve as good introductions to the field see for instance (Kain 1992; Ihlanfeldt and Sjoquist 1998; Preston and McLafferty 1999; Ihlanfeldt 2006). Over the decades the hypothesis has been used more extensively to interpret mismatch in areas outside the city centre (Gottlieb and Lentnek 2001), including other ethnical groups and female breadwinners (Kain 1992; Blumenberg 2004). The spatial mismatch hypothesis has also, in its extended form, been used to understand the unequal distribution of jobs in countries outside the US (Fieldhouse 1999; Lau and Chiu 2003; Åslund et al. 2009). Due to the gradually more encompassing development of the hypothesis, it has been increasingly intertwined with related hypotheses (Zhang and Bingham 2000), describing the effects of discrimination and class (Kasarda 1989; Wilson 1990) and hypotheses related to gender and domestic work (Hanson and Pratt 1988; Johnston–Anumonwo 1992; Hanson and Pratt 1995). From the empirical perspective, this involves the possibility of using a wide range of hypotheses that partly or fully interrelate, and that the hypotheses can be used together or as subjects of comparison (Teitz and Chapple 1998; Fernandez and Su 2004). Bundled together, the hypotheses function as a scientific platform, from where a great deal of the matching process between jobs and potential workers can be examined. The key feature, functioning as the link between these hypotheses, is the expression of the accessibility to jobs. However, accessibility can also be perceived as the key reason why no cohesion in the interpretation of results using the hypotheses is possible. This is because no natural measure of job accessibility exists and as a consequence, used models can only be considered to point towards accessibility but not represent actual accessibility. As a result, there has been no means to fully describe and understand job access, and any analyses using accessibility models have had to be considered as more or less credible as a result of the models used to denote accessibility. This article is devoted to the problems of accessibility models. A discussion of the modeling assumptions and functionalities of the commonly used accessibility models used in the mismatch-related literature is conducted in the first part of the article. This discussion is complemented with a review over the biases produced through the use of these accessibility models. In an attempt to minimize bias and increase the overall usability of accessibility models, a new method for the computation of doubly constrained accessibility is being introduced, the ELMO-model. In order to test the efficiency of ELMO and other accessibility models, the models are empirically tested using a statistical data-base material from a Swedish labor market region. Three groups of tests are conducted using the different accessibility models. The article is composed of three parts. In the first part the different accessibility models are presented and discussed and the prerequisites for the empirical tests are introduced. In the second part of the article, the models are subjected to various tests determining the functionality of different accessibility models. Finally, some discussions concerning the use of the tested accessibility models in the field of mismatch research are given.",11
11.0,4.0,Networks and Spatial Economics,17 November 2010,https://link.springer.com/article/10.1007/s11067-010-9149-0,Accessibility and Network Structures in the German Commuting,December 2011,Aura Reggiani,Pietro Bucci,Giovanni Russo,Female,Male,Male,Mix,,
11.0,4.0,Networks and Spatial Economics,09 February 2011,https://link.springer.com/article/10.1007/s11067-011-9153-z,Place Rank: Valuing Spatial Interactions,December 2011,Ahmed El-Geneidy,David Levinson,,Male,Male,Unknown,Male,"Transportation practice aims to move people and goods safely and efficiently. The barometers used to measure efficiency attributes include hours of delay, speed of traffic, and number of cars in congestion. These statistics have become standard performance measures used to compare conditions within cities, and regions within cities, over time. Newspapers around the United States wait eagerly for the well-known annual rankings from the Texas Transportation Institute (Schrank and Lomax 2005) to relay to their residents how well (or in a perverse sense of pride, how poorly) their city is performing. Similarly various cities around the world generate annual congestion indicators. Measures of congestion, however, have limited utility. They provide a snapshot of only a select dimension of a city’s transportation system: the ability of residents to transport themselves under certain conditions (e.g., free flow travel times). Measures of mobility are merely concerned with the ability to move, but not with where one is going. In many respects, such measures fail to adequately capture other essential dimensions of a city’s entire transportation environment - that is, how easy it is to get around. Land use practice on the other hand deals with controlling the density and arrangement of activities. There are great debates about what constitutes the best arrangement, and there is no clear goal comparable to what underlies transportation engineering practice, rather it is a multi-objective problem (Matthews et al. 2006). However the success of a city is determined by its accessibility, cities with more accessibility are more valuable (in toto and per unit) than those with less accessibility. Accessibility theory argues transportation systems should aim to help people participate in activities distributed over space and time. Accessibility indicates the performance of how well combined transportation-land use systems serve communities, and is shaped by both land use (Levinson 1998; Scott and Horner 2008) and transportation (Axhausen 2008). The concept of “accessibility” has been coin in the planning field for over five decades. Improving accessibility is a common element in the goals section in many transportation plans in the United States and globally (Handy 2002). However, the term “accessibility” is often misused and confused with other terms such as “mobility”. Mobility measures the ability to move from one place to another . The word accessibility is derived from the words “access” and “ability”, thus meaning ability to access, where “access” is the act of approaching something. The word derives from the Latin accedere “to come” or “to arrive.” Here we concern ourselves with the ease of reaching valued destinations or activities rather than ease of traveling along the network itself. One of the first definitions of accessibility in the planning field was suggested by Hansen (1959), who defines accessibility as a measure of potential opportunities for interaction. Alternative measures are reviewed in Handy and Niemeier (1997) and Geurs and van Wee (2004) and a use-based measure appears in Ottensmann and Lindsey (2008). This paper, extending El-Geneidy and Levinson (2006), introduces and explores a new flow-based measure: place rank, and compares it to three traditional (destination and travel time-based) accessibility measures (cumulative opportunity, gravity-based, and inverse balancing factor). The differences are several. First, place rank focuses on the implicit value of destinations more than the ease of reach, while other measures value all destinations of a type equally, subject to travel time, or require exogenous ratings. Second place rank directly employs flow data, while other measures use travel time and land use data. Place rank can be used in cases where only flow data is available. Currently home and work locations are recorded through data collected by labor agencies, (e.g. in the US, the Longitudinal Employment Household Dynamics survey, orchestrated by the US Census), but this data does not include mode or journey time of transport. Several databases are present for market analysis this data include where people shop for certain goods without the knowledge of the mode being used or travel time of the trip. Similarly such data is available through health care providers or health insurance agencies, where the home and place of treatment are known while the mode of transport is also unknown. While other sources can be used to estimate travel time, these estimates are historically not very accurate, and often based on shaky assumptions such as shortest path (Zhu and Levinson 2010). Unlike utility-based measures of accessibility, place rank does not require the presence of a regional travel demand model to compute. Longitudinal studies conducted over several months show that travel behavior of individuals is largely habitual, identifying fixed activity spaces for individuals (Schoenfelder and Axhausen 2010). Individual activity space contains 90–95% of all their potential places of interest. This implies that the difference between actual and potential accessibility may not be that great. Accordingly a measure derived from actual activity can be used in understanding this relationship. The next section defines those measures in turn as well as competition measures. Then the data for the application of these measures are presented, and a comparison of the access under each method is shown across several case studies. Statistical correlations between the various measures are provided. The conclusion summarizes the paper and suggests directions for practice and research.",40
11.0,4.0,Networks and Spatial Economics,01 April 2011,https://link.springer.com/article/10.1007/s11067-011-9158-7,Special Issue on New Frontiers in Accessibility Modelling: The Effect of Access Time on Modal Competition for Interurban Trips: The Case of the Madrid-Barcelona Corridor in Spain,December 2011,Concepción Román,Juan Carlos Martín,,,Male,Unknown,Mix,,
11.0,4.0,Networks and Spatial Economics,01 September 2010,https://link.springer.com/article/10.1007/s11067-010-9144-5,Does Accessibility Affect Retail Prices and Competition? An Empirical Application,December 2011,Juan Luis Jiménez,Jordi Perdiguero,,Male,Male,Unknown,Male,"Accessibility is a widely used concept in many spheres of transport economy. During the last 40 years it has been increasingly used as a reference term in the fields of transport economics and land planning, and increasing the accessibility levels has become nowadays a common objective for these plans (van Wee et al. 2001).Footnote 1
 In broad terms, accessibility measures—through different methods—the ease of reaching valued destinations; more consumers will reach those firms located in areas with higher accessibility in the same amount of time, thus increasing their business opportunities.Footnote 2 For this reason, transport planning has traditionally focused on the provision of infrastructures, since a direct relationship was assumed to exist between the quality of infrastructure, accessibility and welfare. However, this premise could bring about a vicious circle with considerable long-term environmental repercussions. A parallel branch of the literature has instead analysed the relationship between retailing prices and accessibility. Kaufman et al. (1997), for example, study for the US the relationship between food prices in different types of establishments and their accessibility in terms of family incomes. They show that low-income families with a low probability of living in the vicinity of supermarkets may have to buy the same highly priced products as families with greater purchasing power. According to this idea, MacDonald and Nelson (1991) demonstrate that the prices of a selected basket of goods are 4% lower in suburbs than in city centres. By using a database that includes the locations of fast-food restaurants, Stewart and Davis (2005) determine how differences in location affect their final prices. These examples show that accessibility can be analysed from a different perspective of direct transport economics.Footnote 3
 This paper attempts to demonstrate the direct relationship between accessibility and the level of competition in the retail petrol market. In fact, a possible motivation is that petrol consumption is unlikely to be a programmed decision and, for this reason, they can exercise market power by applying high prices, and they may even create “social exclusion” (if we consider petrol to be a “basic commodity”). This is more serious for car users in remote areas where public transport is not normally as frequent as in city centres.Footnote 4
 This paper combines the classic analysis of accessibility with a closer approximation of the industrial organization methodologies. This second approach analyses how accessibility to the different stations changes the market structure and influences the prices set by firms. There is an extensive amount of literature analyzing the influence of market structure on the prices set by firms. However, not as many articles consider the role that the distance to the other competitors plays in business strategies. One of the first articles to take into account the effect that the distance between the competitors has on their pricing strategies is that of Spiller and Huang (1986). They show how wholesale gasoline markets in the northeast U.S. that are closer are more integrated because proximity lowers the cost of arbitration. Those wholesalers that are further apart and therefore not connected to any relevant market have a greater ability to exercise market power and would be candidates for analysis by the competition authorities. The wholesale market in the U.S. has also been analysed by Pinske et al. (2002). The authors propose a semiparametric estimator that allows them to discern whether the behaviour of the various operators is close to global competition models, where all the operators are competing with everyone, or models of local competition, where the players compete only with their neighbours. The econometric results show how competition in this industry is basically local. Therefore, agents only compete with closer rival operators, and not with those who are geographically distant. Contrary to previous studies show empirical evidence that wholesale gasoline markets are local, being the geographically closest competitors that significantly affect the pricing strategies of firms, Bromiley et al. (2002) note several points at which the gasoline retail market would also have a strong local character. This fact can explain why there are service stations offering different prices even within the same city. One of the main reasons the author put forward for the relevant market in which competition is reduced is that petrol stations have high search costs to be borne by consumers in finding a cheaper station. If these high search costs are added to the individual savings that are normally expected, gas stations only have incentives to compete strongly against their closest rivals. The proposals outlined by Bromiley et al. (2002) seem to find empirical evidence in the study by Barron et al. (2004), where they show that a greater number of competitors within a distance of a mile and a half leads to a lower level of prices in the market and a lower price dispersion. Therefore, the more competitors there are within a close distance, the lower the price fixed by the operators. When taking into account the distance to the nearest competitor for the number of competitors within a mile and a half, the results did not vary significantly. The closer the competitors, the lower the price and the lower its dispersion. The econometric results show that the petrol retail market in the United States is affected mainly by the local competition. However, the authors do not analyse whether this effect of local competition may have some kind of domino effect, spreading the competitive effect to a wider geographical area. Precisely one of the objectives of the article by Atkinson et al. (2009) is to analyse the existence of domino effects in price movements of the stations in Guelph (Ontario). These effects seem to dominate when there are price reductions, but are not seen when the movements are on the rise. The results also show that, although the station only seems to react to the movements of a small group of competitors, it is not always the closest geographically.Footnote 5
 The goal of this paper is to analyse how the difference in accessibility, measured through the distance to other petrol stations affects the prices charged by retailers in the market. That is, to what extent do service stations whose consumers have easy access to stations of rival brands suffer increased competitive pressure and are forced to set a lower price? The paper also examines whether service stations whose consumers have better access to stations of the same brand have greater market power and thus set a higher price. Although there is evidence in the literature on the relationship between the market structure and the prices set by companies, and more specifically the geographical proximity of rivals, this article presents significant contributions. Firstly, it analyses the competitive effect exerted by rivals in terms of geographical distance. In previous articles the authors assume a certain distance to define the geographical market in which companies compete, to analyse subsequently the effect of the number of competitors in that market. Our approach nevertheless allows us to analyse how the effect varies in the number of opponents depending on location and to verify the extent to which rivals located further away exert less competitive pressure. This evidence may be useful for defining the relevant geographic markets in which operators compete. Secondly, the article also analyses how the presence of establishments of the same brand at different geographical distances allows operators to increase their market power and thus impose higher prices. To our knowledge, there is no empirical evidence in the gasoline market that shows the effect of own brand petrol stations in the market to the prices fixed by the retailer. Thirdly, and taking into account the effects generated by rivals and the stations of the same brand in different geographical markets, we can analyse the effect of a monopoly by a single company in a particular market, that is, how prices would vary if a company acquired all the service stations within a certain radius. This result may be of broad utility to anticipate the possible effects of mergers in the market. The main results of the econometric analysis show us how rivals that are geographically distant exert lower competitive pressure. Similarly, stations of the same brand make it possible to increase the market power and thus the price, although it again decreases as it becomes further away geographically. The service stations will increase their margin by more than 5% if they monopolize the service stations within 17 or so minutes of the posting. The paper is structured as follows. In Section 2 we present the data used for the empirical application and the main descriptive statistics, which we develop and solve econometrically in Sections 3 and 4. Finally, in Section 5 we show the main conclusions.",12
11.0,4.0,Networks and Spatial Economics,30 March 2010,https://link.springer.com/article/10.1007/s11067-010-9130-y,Adverse Weather and Commuting Speed,December 2011,Muhammad Sabir,Jos Van Ommeren,Piet Rietveld,Male,Male,,Mix,,
11.0,4.0,Networks and Spatial Economics,30 March 2010,https://link.springer.com/article/10.1007/s11067-010-9131-x,New Routes and Airport Connectivity,December 2011,Renato Redondi,Paolo Malighetti,Stefano Paleari,Male,Male,Male,Male,"Privatisation and deregulation have strengthened the competitive behaviour of airports. This trend has inspired a growing literature on airport benchmarking and performance evaluation (Oum and Yu 2004; Graham 2005; Kinkaid and Tretheway 2006; ATRS 2007; Martin and Roman 2006). In particular, connectivity is gaining in importance relative to other variables employed to evaluate airport performance. Since 2000, the IATA has explicitly evaluated the level of airport connectivity in its performance analysis (IATA 2000). A recent contribution (Malighetti et al. 2008b) also suggests a relation between airport connectivity and efficiency. Thus, it appears that growing competitive pressure is pushing airports to develop a new strategy based on fostering connectivity. The connectivity issue is also of interest to local authorities who wish to improve the level of service for their territories. Governments usually support airport connectivity through incentive schemes based on non-discriminatory principles (EC 2005). However, even without an economic incentive scheme, airports can recommend desirable new routes to airlines.Footnote 1
 The prevalence of hub-and-spoke systems and their overlap with point-to-point networks has made measuring airport connectivity a challenging task. While several improvements have recently been made in this regard (Burghouwt and Veldhuis 2006; Cronrath et al. 2008; Guimerà et al. 2005; Malighetti et al. 2008a), little has been done to help airports understand which new routes would enhance their connectivity the most. In this paper, we suggest that module identification can help airports estimate the connectivity gain of a new route. We apply simulated annealing to the European network and conduct a simulation to measure the connectivity gains generated by new routes. The next section analyzes the literature on airport connectivity and module identification techniques. We then describe the methodology employed and the results of our empirical analysis.",21
11.0,4.0,Networks and Spatial Economics,14 September 2010,https://link.springer.com/article/10.1007/s11067-010-9146-3,The Pareto-optimal Solution Set of the Equilibrium Network Design Problem with Multiple Commensurate Objectives,December 2011,Dung-Ying Lin,Chi Xie,,Unknown,,Unknown,Mix,,
12.0,1.0,Networks and Spatial Economics,16 May 2010,https://link.springer.com/article/10.1007/s11067-010-9133-8,Optimization of Number of Operators and Allocation of New Lines in an Oligopolistic Transit Market,March 2012,Zhi-Chun Li,William H. K. Lam,S. C. Wong,,Male,Unknown,Mix,,
12.0,1.0,Networks and Spatial Economics,10 July 2010,https://link.springer.com/article/10.1007/s11067-010-9140-9,A Voronoi-Based Heuristic Algorithm for Locating Distribution Centers in Disasters,March 2012,Wilfredo F. Yushimito,Miguel Jaller,Satish Ukkusuri,Male,Male,,Mix,,
12.0,1.0,Networks and Spatial Economics,12 August 2010,https://link.springer.com/article/10.1007/s11067-010-9142-7,A Spatial Interaction Model for the Representation of the Mobility of University Students on the Italian Territory,March 2012,Giuseppe Bruno,Andrea Genovese,,Male,Female,Unknown,Mix,,
12.0,1.0,Networks and Spatial Economics,14 September 2010,https://link.springer.com/article/10.1007/s11067-010-9145-4,A Model of Weekly Labor Participation for a Belgian Synthetic Population,March 2012,Cinzia Cirillo,Eric Cornelis,Philippe L. Toint,Female,Male,Male,Mix,,
12.0,1.0,Networks and Spatial Economics,05 October 2010,https://link.springer.com/article/10.1007/s11067-010-9148-1,A Large-Scale Spatial Optimization Model of the European Electricity Market,March 2012,Florian U. Leuthold,Hannes Weigt,Christian von Hirschhausen,Male,Male,Male,Male,"Electricity markets around the world are still in a state of flux, even two decades (the UK market), one decade (for some U.S. markets) or a couple of years (continental Europe) into the reform process. In Europe, the reform momentum has accelerated in the second half of this decade. In fact, the “Acceleration Directive” (2003/54/EC) has been followed by a more coherent attempt of moving toward a single European market. Yet central reform steps such as vertical unbundling, incentives for cross-border transmission investment, and the integration of large-scale renewable electricity into the network are still in the making. Evidence of this process is provided by the discussions of the “3rd Energy Package” of the European Union, providing energy policy guidelines for the next decade. In order to understand the impact of different reform proposals and to simulate diverse development scenarios, the Chair of Energy Economics and Public Sector Management (EE2) has developed a model of the European electricity market(s) based on a DC Load Flow model, called ELMOD (Fig. 1). The model was initiated by Leuthold et al. (2005) for the German electricity market. Weigt et al. (2006) continued this work and extended the model by including France, Benelux, Western Denmark, Austria and Switzerland. Weigt (2006) broadened the scope to a timeframe of 24 h to simulate variable demand and wind input as well as unit commitment, start-up and pumped storage issues. The model was subsequently extended to cover the entire European UCTE (Union for the Co-ordination of Transmission of Electricity) electricity markets (essentially Central and Western Europe). Today, this is one of the larger engineering-economic models, with a very high granularity, allowing differentiated spatial price and flow analysis.
 ELMOD representation of the European high voltage grid. Source: own presentation While unit commitment decisions can be included in ELMOD, it is not a typical day-ahead unit commitment model. ELMOD is intended to represent supply and demand behavior on a typical day of a season, assuming that consumers have had time to adjust to a certain market mechanism. ELMOD then calculates the market outcomes under different market assumptions and can be used as analysis tool for researchers and policy makers. The model is unique in that it combines a large number of specifics from the electricity sector with a large-scale spatial representation of the pan-European high-voltage electricity network. Thus, in addition to the modeling formulation, the results are directly applicable to a large variety of lines (more than 2,000) and nodes (more than 4,000). The flexibility of the model can thus be used to obtain an optimal degree of detail vs. computational speed. This paper summarizes the model and provides an in-depth description of model assumptions and specifics. We start out with an overview of the literature on network modeling of electricity (Section 2), and then proceed with the technical and economic details of ELMOD (Section 3). Section 4 presents the data used, the underlying assumptions, sources, et cetera. In Section 5 we discuss various applications of the model, including congestion management issues, wind integration, and generation capacity expansion. Section 6 concludes and sketches out topics for further research.",153
12.0,1.0,Networks and Spatial Economics,08 January 2011,https://link.springer.com/article/10.1007/s11067-010-9152-5,Network Development Under a Strict Self-Financing Constraint,March 2012,André de Palma,Stef Proost,Saskia van der Loo,Male,Male,Female,Mix,,
12.0,1.0,Networks and Spatial Economics,03 February 2011,https://link.springer.com/article/10.1007/s11067-011-9154-y,A Cooperative Coalitional Game in Duopolistic Supply-Chain Competition,March 2012,Cheng-Chang Lin,Chao-Chen Hsieh,,Unknown,Unknown,Unknown,Unknown,,
12.0,1.0,Networks and Spatial Economics,15 March 2011,https://link.springer.com/article/10.1007/s11067-011-9156-9,Dual Toll Pricing for Hazardous Materials Transport with Linear Delay,March 2012,Jiashan Wang,Yingying Kang,Rajan Batta,Unknown,Unknown,Male,Male,"Hazardous materials (hazmat) transportation, traffic congestion and traffic safety are the three main transportation issues cited by the National Conference of State Legislatures (NCSL). Traffic safety is an important public health issue for many people, including state legislators. Each year, more than 40,000 people are killed in motor vehicle crashes and thousands more are injured (Movassaghi et al. 2007). Traffic crashes have been found to be a leading cause of death for all age groups in the United States and cost our society an estimated $230.6 billion each year. Traffic congestion and hazmat transportation are generally recognized as two main causes of many traffic safety issues. Traffic congestion does not only delay travel time, but also is the trigger of vehicles crashes and often enlarges the consequences of traffic accidents. But the most costly consequences come from hazmat transportation. Hazmat accidents are well known as low-probability, high-consequence (LPHC) incidents. On one hand, the number of accidents caused by hazmat shipments are rather low. In USA, in 1998, there were roughly 15,000 accidents related to hazmat transportation, and only 429 of them were classified as serious incidents (Kara and Verter 2004). On the other hand, hazmat accidents can result in a large amount of damage to the population and the environment. In 2003, the U.S. Department of Transportation received 14,660 reports of incidents from carriers and shippers, with about 87% involving trucks, with a total of $338,136,186 in damages (Gallamore et al. 2005). Gasoline was by far the most common material involved, as might be expected given the prevalence of tank trucks on the highways. Together, gasoline, other flammable liquids, and corrosives account for 57% of serious incidents. Even though most reported incidents do not involve serious consequences, the annual statistics reveal some highly costly consequences. For instance, the 1996 figures include the chlorine gas escaping from a tank car damaged in a train derailment in Alberton, Montana, resulted in the evacuation of more than 1,000 people, more than 700 injuries, and 1 fatality. The 1998 statistics include the deaths of five people from gasoline that spilled and ignited during the unloading of a tank truck in Biloxi, Mississippi. The incident also resulted in the evacuation of 80 people and the closing of an Interstate highway. Because such major incidents are rare, they stand out and tend to dominate the safety data when they do occur. Due to the inherent transport risks, municipal units usually try to separate the hazmat flow from normal traffic flow, especially high-density traffic flows. Currently the transportation of hazmat is regulated under the Federal Hazardous Materials Transportation Act (which was amended by the Patriot Act in 2001) in the United States and under the Federal Transportation of Dangerous Goods Act in Canada. In North America and Europe, government agencies do not have the authority to dictate routes to hazmat carriers for moving their shipments. These agencies mitigate the hazmat transport risks by imposing (permanent or time-based) curfews on the use of the road segments under their jurisdiction. An example of this is the ban on trucks carrying non-radioactive hazmat on certain road segments (e.g., Texas Department of Transportation, 2009, has a list of prohibited roads for Texas). These kinds of policies are usually categorized as network-design (ND) policies and much work has been done in this field (e.g., Kara and Verter 2004). An ND policy can effectively restrict hazmat shipment from high traffic flow. However, it is also criticized as being rigid since it does not consider the carriers’ priorities and wastes the usability of certain road segments. Besides, only restricting certain road segments sometimes cannot rationally adjust the hazmat flows to less-risk areas. Researchers on hazmat transportation has recognized the necessity of a more flexible restriction policy of hazmat shipments. Recently, an alternative policy tool, toll-setting policies (TS), was proposed by Marcotte et al. (2009) to deter (but not prevent) hazmat carriers from using certain road segments via toll-pricing. An example given in Marcotte et al. (2009) is helpful to understand that TS policy is a more flexible regulation tool for hazmat than ND policy, and ND policy may be infeasible in certain cases. By imposing tolls on certain road segments, the hazmat shipments are expected to be channeled on less-populated roads according to the carriers’ own selection (due to economic considerations) rather than by governors’ restriction. This model put forward an attractive policy to network regulators with more flexible solutions, and more acceptable choices to carriers, thereby creating a win-win scenario which is likely to lead to successful implementation. Toll pricing has been a regular method to control traffic congestion for a long time. When it is infeasible to increase the capacity of the transportation network, imposing appropriate tolls on roads can reduce traffic congestion because tolls can encourage travelers to detour or to travel during a less congested period. Besides, transportation agencies have commonly used tolls to generate revenue to offset infrastructure construction and maintenance cost. Hazmat carriers not only are hazmat traffic flows, but also belong to, and cannot be separated from the regular traffic flows. Therefore, they are managed under more regulations than other kinds of vehicles. That means, the hazmat carriers need to carry pricing on both hazmat and regular tolls. In this sense, this paper proposes a dual-pricing model on both hazmat and regular tolls, trying to channel different kinds of traffic flows to avoid the delays and costs caused by traffic congestion, as well as the probabilities and consequences of the unavoidable vehicle crashes. By separating the hazmat shipments from the heavy-congestion traffic flows, it tries to mitigate the severe accident risks and to avoid peak-time traffic congestions. The objective of this paper is to develop a dual toll pricing framework to control both regular and hazmat traffic flows for the public safety. The rest of this paper is organized as follows. The next section reviews some popular risk models in hazmat transport and classic congestion models. In Section 3, we present the mathematical formulation of the dual toll pricing model. Section 4 develops an exact solution procedure to solve the dual toll pricing problems and discusses the properties of our model. Section 5 contains our computational experience, based upon data from a real-life hazmat routing situation in the Albany district of New York State. Finally, Section 6 provides conclusions and future research suggestions.",31
12.0,1.0,Networks and Spatial Economics,22 March 2011,https://link.springer.com/article/10.1007/s11067-011-9157-8,Dynamic Traffic Assignment under Uncertainty: A Distributional Robust Chance-Constrained Approach,March 2012,Byung Do Chung,Tao Yao,Bo Zhang,,,Male,Mix,,
12.0,2.0,Networks and Spatial Economics,30 September 2009,https://link.springer.com/article/10.1007/s11067-009-9110-2,Introduction to the Special Issue on Funding Transportation Infrastructure,June 2012,André de Palma,Robin Lindsey,Stef Proost,Male,,Male,Mix,,
12.0,2.0,Networks and Spatial Economics,31 October 2009,https://link.springer.com/article/10.1007/s11067-009-9109-8,Risk in Transport Investments,June 2012,André de Palma,Nathalie Picard,Laetitia Andrieu,Male,Female,Female,Mix,,
12.0,2.0,Networks and Spatial Economics,07 October 2009,https://link.springer.com/article/10.1007/s11067-009-9111-1,Special Issue on Transport Infrastructure: A Route Choice Experiment with an Efficient Toll,June 2012,John L. Hartman,,,Male,Unknown,Unknown,Male,"In recent years, experiments have been used to explore human behavior on various traffic networks. These experiments pay subjects based on their performance in the experiment, with better performance resulting in higher payouts. The experiments listed below use modifications of the Pigou-Knight-Downs paradox. Before discussing the findings of these experiments, it is important to emphasize that only one of these experiments uses tolls as a mechanism for improving average travel time. A brief summary of the congestion experiments described below, along with a few others, is found in Table 1. Note that all of these experiments have subject profiles that are the same for each person, and that total demand is typically inelastic for traveling through the route network described.
 Selten et al. (2007) modify the two-route network described by the Pigou-Knight-Downs paradox by allowing both routes to be congestible. In this experiment, 18 subjects must travel between two points on either a “main” road or a “side” road, where the side road requires more travel time than the main road if the number of subjects traveling on both routes is the same. Subjects then receive a payout in each round as a function of travel time, with higher travel time resulting in a lower payout. These choices are repeated over 200 rounds, with theory predicting a user equilibrium of 12 subjects on the main route. On average, subject route choices come very close to the theoretical predictions. However, since the population in this experiment is homogeneous and no tolls are charged, any subject’s route choice can be part of an equilibrium as long as 12 subjects travel the main route, since theory only predicts the number of people on each route. With thousands of equilibria possible, any subject could be on either route in equilibrium, leading to substantial fluctuations of the number of travelers on each route from round to round, even in rounds after equilibrium. Although such fluctuations reject the predictions of pure-strategy equilibrium, the mean number of travelers on each route comes close to this equilibrium. Chmura and Pitz (2004a, b) modify the payoffs by using a minority game structure.Footnote 3 Although the minority game framework is useful in many economic settings, it may not be the best way to model the payoffs in a transportation network because commuters typically do not “win” or “lose,” but rather incur one of many possible commuting costs. Two results from the Selten et al. (2007) and Chmura and Pitz (2004a, b) experiments are worth highlighting. First, a person’s payout in one round is negatively correlated to the likelihood that the same person will switch routes in the following round. This implies that many subjects think that the “other” route will be the better choice after a relatively bad payout. In other words, many people believe that a relatively bad payout follows another relatively bad payout if they remain on the same route from one round to the next. Second, subjects who switch routes frequently over the course of the experiment tend to have worse overall payouts than those who switch less frequently. These results shed some light on how subjects react when faced with a coordination problem, and how their reactions affect overall payoffs.",23
12.0,2.0,Networks and Spatial Economics,30 September 2009,https://link.springer.com/article/10.1007/s11067-009-9112-0,"Public-Private Cooperation in Infrastructure Development: A Principal-Agent Story of Contingent Liabilities, Fiscal Risks, and Other (Un)pleasant Surprises",June 2012,Luc E. Leruth,,,Male,Unknown,Unknown,Male,"With increasing pressure on physical infrastructure and limited resources, most governments have turned to various forms of collaboration with the private sector to help finance, build, and/or operate public assets. While the benefits of such joint efforts are potentially numerous, they also entail major risks that are hard to assess adequately and can lead to high economic costs. This paper first discusses the economic and financial rationale for partnerships between the public and the private sectors when it comes to developing infrastructure in the transport, or indeed in almost any, sector. I review a number of examples of public-private partnerships (PPPs), stressing the need to carefully measure, and adequately account for, risks, so as to allocate them to the party best placed to handle them. Indeed, there are certainly risks associated with such undertakings, but sharing risks is a key reason for entering into partnerships. The paper then briefly describes a framework that, I hope, will help researchers conceptualize some key issues related to PPPs. The second part of the paper will therefore be more exploratory and will draw on several related pieces of research, including a recent IMF working paper.Footnote 1 It also includes a brief survey of the recent literature on theoretical approaches to PPPs.",19
12.0,2.0,Networks and Spatial Economics,01 October 2009,https://link.springer.com/article/10.1007/s11067-009-9113-z,Forecasting and Evaluating Network Growth,June 2012,David Levinson,Feng Xie,Norah M. Oca,Male,,Female,Mix,,
12.0,2.0,Networks and Spatial Economics,10 October 2009,https://link.springer.com/article/10.1007/s11067-009-9116-9,Discussion Paper: Who Should Take Responsibility for Unexpected Interest Changes? Lesson from the Privatization of Japanese Railroad System,June 2012,Yoshitaka Fukui,Kyoji Oda,,Male,Male,Unknown,Male,"The JNR reform in April 1987 mainly consisted of the following five specific measuresFootnote 1:
 JNR was broken up into seven Japan Railway (JR) companies which consist of six regional passenger rail companies and one freight rail company. Each company has been given limited (for-profit) company status, but was initially wholly owned by Japanese National Railways Settlement Corporation (JNRSC), a wholly government-owned entity. In order to ensure the managerial autonomy of the rail companies, the Japanese Government was expected to privatize each rail company as soon as possible. JNR operated a rail service in the three islands of Hokkaido, Shikoku and Kyushu, as well as Honshu, the mainland. Three regional passenger companies, Hokkaido Railway Company (JR Hokkaido), Shikoku Railway Company (JR Shikoku) and Kyushu Railway Company (JR Kyushu) have been set up for the three islands (hereafter Island Companies). Because the mainland is much larger than the three islands—and includes the three largest metropolitan areas, Tokyo, Nagoya and Osaka—a rail service in the mainland was further divided into three regional passenger companies with headquarters located in these three mega-cities. They are called East Japan Railway Company (JR East), Central Japan Railway Company (JR Central), and West Japan Railway Company (JR West) respectively (hereafter Mainland Companies). The newly established JR companies inherited minimally necessary assets from JNR to operate their rail and related business, and assumed a reasonable amount of JNR’s debts so as not to impair the financial stability of the new rail companies. The remaining non-operating assets and liabilities were transferred to JNRSC, which was expected to repay as much debt as possible by selling inherited non-operating assets and the shares of the new rail companies. The remaining amount which JNRSC could not repay was to be transferred to the general account of the Japanese Government. The new JR companies continued to hire a vast majority of ex-JNR employees, the number of which was about 20% more than required for operating the existing rail lines efficiently. The remaining ex-employees belonged to JNRSC temporarily, and were to be given support to find new jobs for 3 years. Because the Shinkansen lines operated by JNR were not uniform in their profitability, the assets in their entirety were to be held by the wholly government-owned Shinkansen Holding Corporation (SHC), and the three Mainland Companies leased the facilities paying usage fees determined by SHC according to traffic volume. However, the Mainland Companies decided to purchase the Shinkansen facilities from the government-owned SHC 4 years after the new regime started instead of continuing to pay usage fees indefinitely. Extra financial funds called Management Stabilization Funds were set up for the three Island Companies. They were expected to cover their loss in rail operations with interest income from the funds. The five measures reflect a distinctly Japanese situation concerning rail transportation. In Japan, private non-JNR passenger rail companies have extensive networks in Tokyo and Osaka, and their combined traffic volume is about 40% of ex-JNR companies’ and exceeds that of the entire French National Railways (SNCF). They have been profitable for years despite the burden of construction cost, which may astound some readers because urban rail operations are generally money-losing outside Japan (Winston 2000). In addition, since their networks were partly connected with JNR’s, numerous trains ran directly thorough JNR and non-JNR (including subway) lines before the reform, not to mention the fact that they do after the reform. Because of the existence of fully integrated profitable private rail companies as well as an exceptionally high demand for passenger rail service in general, the Japanese Government expected that ex-JNR rail operators would be self-financing as a whole even if vertically integrated. Therefore it decided not to separate the provision of transport services from the operation of infrastructure as practiced in European countries (Council of European Communities 1991), where passenger and freight revenues alone may not cover the cost of infrastructure. Instead the government divided the national rail network horizontally when JNR was dissolved. Because an airplane is a dominant choice for a longer domestic trip in Japan, passenger rail service is mostly for short- to medium-distance trips. Therefore, it was a consensus that negative effects of a nationwide centralized organization outweighed positive externalities of a larger integrated network when the JNR reform was contemplated. That is, the larger the network, not necessarily the better. The basic philosophy of the Japanese reform has some affinity with the idea of an optimal currency area (Mundell 1961). The most debated part of the reform was how to divide the national network in order to maintain positive externalities and simultaneously realize more customer-oriented management as much as possible. While there was no question that each of the three islands was a natural unit, countless patterns of the mainland division were considered. Finally, the mainland operation was divided into three geographically integrated and operationally independent units. Consequently, the adopted division would enable more than 95% of passengers to complete their trips within the boundaries of each regional unit (East Japan Railway Company 1995, p. 12), beyond which nonetheless many direct trains would run under the new regime as in JNR’s days. Another aspect uniquely Japanese is that the reform concentrated on passenger service while treating freight service as an appendage. It has never been clear why freight service was separated from the six regional companies. One factor may be that—unlike in North America and Europe—the rail share of total freight traffic volume was insignificant in the mid-1980s. Most of the major industrial centers are located on the coast, and virtually all fuels are imported. Coastal shipping carries what would be done by rail in other countries, and splits nearly equally the Japanese freight market with trucking. Since its inception, Japan Freight Railway Company (JR Freight) has been paying usage fees based on avoidable (short-term marginal) cost to the passenger companies which own rail lines. Although this structure has brought about many distortions and inefficiencies both for passenger and freight companies, we will not delve into problems concerning a freight rail service in this paper partly because its size is minuscule (less than 5%) compared to that of a passenger service in terms of revenues. In a sense, the spirit of the Japanese reform bears some resemblance with reform in North America. One part of the network is considered profitable enough to maintain infrastructure on its own if designed properly, while the other part is not deemed to be capable of financial self-sufficiency. The crucial difference is that in Japan it is passenger service which plays a vital role whereas in North America it is freight. Although each of the five measures was instrumental in realizing the JNR reform, the fifth and last one is directly relevant to our main concern, inter-regional profitability adjustment. When JNR was divided into six regional passenger rail companies, the three Mainland Companies, which were given areas of high traffic volume, were expected to make profits from rail operations if they would be able to maintain the traffic volume when JNR was dissolved. On the other hand, the other three Island Companies must operate in areas of low traffic volume and were believed to have no chance to break even, let alone make profits, in rail operations. In order to tackle this profitability differential, three possible schemes were considered: (1) giving governmental subsidies to compensate for a loss from rail operations every year; (2) cross-subsidizing the Island Companies with the profits of the three Mainland Companies; (3) setting up a one-time extra financial fund for the Island Companies to cover an annual operating loss with interest income. The third mechanism was adopted in the end, and accordingly a 1.3 trillion yen fund was established and added on to the debts of JNRSC. No one then anticipated that this seemingly well-structured scheme would pose a totally unexpected problem to the JR companies and the government later.",2
12.0,2.0,Networks and Spatial Economics,05 November 2009,https://link.springer.com/article/10.1007/s11067-009-9117-8,Discussion Paper: Airport Privatization in India,June 2012,Manuj Ohri,,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Networks and Spatial Economics,15 December 2009,https://link.springer.com/article/10.1007/s11067-009-9122-y,Motorway Provision and Management in France: Analyses and Policy Issues,June 2012,Alain Fayard,David Meunier,Emile Quinet,Male,Male,Male,Male,"France has a long experience both in motorway building and in motorway management, especially as regards concessions. As a matter of fact, it has developed many organisational procedures, and its use of concessions has evolved. From this extensive experience, lessons for the future can be drawn. The aim of this section, which draws on Fayard et al. (2005), is to provide some of these analyses by looking at the evolution of the French motorway system in connection with economic and institutional changes. The motorway network in France amounts to 10,800 km of which 8,150 are tolled. The main free motorways are located in urban, semi-urban or landlocked areas, while almost all intercity motorways charge toll. Based on either population or area, the density of France’s network is a bit above the European average. For the former 15 European countries, the figures for 2000 are given in Table 1.
 This network has been built since 1960 according to the pace recorded in Table 2.
 While these figures show a steady rate of implementation, the reader should not think that a stable institutional framework has been maintained throughout the whole period. In fact several changes have occurred, which are now described. In the 1950s car-ownership began to increase rapidly. In order to cope with this growth, the Government sought to increase funds for roads. By 1951 it had established a special dedicated road fund (FSIR) that was to receive a percentage of the motor fuel tax receipts, but competing budgetary pressures prevented the Government from funding the FSIR in full. Thus, in 1955 a law was passed allowing the toll financing of motorways. Public control was maintained by granting concessions, without any competitive process, to public companies in which public interests had all the shares. In this framework, five public companies were set up. Nevertheless, the initial concessions were for only short portions of motorways, of 50 to 100 km. The Government provided initial financial assistance by guaranteeing the loans of the public firms and providing cash and advances that were fairly significant (averaging 30% to 40% of construction costs). Throughout the 1960s these public firms were little more than paper organisations. At this stage their main goals were to make tolls acceptable to users, to earmark these new resources and to provide a better flexibility in project management than in the government’s financial and administrative framework. In fact the equity was virtually nonexistent (e.g. 1 million French francs, i.e. roughly 150,000€), so loans had to be guaranteed. The staff was limited to the chairperson and toll receivers. Construction engineering and operation were provided by government staff and paid back to the government by the motorway companies. Concession companies were commercial registered firms but fully owned by public authorities; they were a tool of the general infrastructure policy as also was the government owned national railway company. They were in fact public services, and the relation to the Minister was hierarchical. The tolls were explicitly fixed by the administration at a level that would fund a significant part of the investment while not diverting too much traffic towards the free highways (Quinet 1998). As a whole, the system coped with a sharp increase in traffic without taking too much public money. Indeed, it had a major advantage in that it could deal with traffic growth and even benefit from it, when the public budget constraint was especially tight. During the 1960s, the links were isolated, without connection, as is shown by Fig. 1. Although tolls were accepted by the public, full privatisation of the motorway firms would not have got support from the political decision-makers.
 The network in 1960 and 1970 Two features of the French system have endured:
 The concessions were, and still are, designed on the Build Operate Transfer (BOT) scheme: the concessionaire has to build and operate the motorway and, at the end of the concession, transfer management of the motorway to the State, which is the owner of the investment from the start. The decision to build a link is, and has always been, made by the State. As in every country, it is motivated by classical welfare concerns. These are estimated through economic return, public finance concerns and constraints, regional balanced development and equity issues, and politics. In the framework of these permanent features, several changes happened through the years. At the end of the 1960s, only 1,125 km of intercity motorways and 435 km of urban motorways were in service, traffic was steadily increasing and congestion was growing. A reform was set up to accelerate motorway provision. Two means were used: (a) to allow private companies to compete for new concessions; and (b) to strengthen the existing public firms in order to give them more autonomy and responsibility. Between 1970 and 1973, after a competitive tendering, four private toll road companies were awarded contracts for stretches of 300 to 500 km of motorways each. All four new concessionaires were consortia of major French public works companies. No investors were interested in investments with such a long payback period, and banks became involved more because they wanted to support contractors that they were linked with rather than because they wanted to invest. The Government was less generous with assistance for concessions granted in the 1970s than it had been in the 1960s; nevertheless, significant financial aid, amounting to about 50% of the total cost of the investment, remained available to both private and public concessions. For example, for the first private company COFIROUTE, 10% of the funds were covered by equity, 10% by in kind advances from State, 65% by State-guaranteed loans and 15% by loans without guarantee. That is to say that 75% of the funds were brought or backed by the Government. In the same period, the public firms gained more independence. They created their own consultancy firm (previously, they used the consultancy services of the State) and their own maintenance services. This de facto shift was followed by slight changes in the concession contracts so that they became de jure suited to the situation. Comparisons of investment costs between private companies (in this case public work consortia) and public firmsFootnote 2 led to the conclusion that private costs were roughly 5% to 10% lower. These figures should be interpreted with caution, as they did not take into consideration the changes in maintenance costs and the possible synergies between investment and maintenance operations. Control over service quality is often considered a major point of franchises; in this case it did not prove to be a problem, in comparison with railways for instance. This is due partly to the technical characteristics of road transport, and perhaps also partly to the fact that the motorway firms, anticipating a huge development of future concessions, had to build their reputation. Extensions to the concessions (see Fig. 2) were granted to existing firms, without tendering. This was done through the cross-subsidisation of new links, for which the collective surplus was positive but the financial profitability was negative, by older segments which had become profitable over time, and by extending the dates at which the older and more lucrative sections of the concessions expired. The public status of most motorway companies made this process easier, since no shareholder claimed additional profit: under public management all the money is devoted to network development.
 The network in 1980 During this period, tolls were regulated by the Ministry of Finance and fixed yearly. These procedures can be explained either as a ploy that allowed the Ministry of Transport to avoid the pressure faced by the Ministry of Finance, or as a way to lower the burden of taxes and to avoid the cost of public funds; it is not easy to disentangle these two reasons. For several reasons the motorway system faced serious problems at the beginning of the 1980s. The first one was the oil crisis, which induced both a slackening of traffic growth and an increase in investment costs. This increase was due, not to technical underestimations, but mainly to economic factors such as higher petrol prices and interest rates. The second, and more important, problem arose from large errors in traffic forecasts for some of the new concessions—two of them had been overestimated by 200%! The previous concessions dealt with isolated and rather short links that were parallel to untolled highways, meaning that the route choices were much more simple and casual. Consequently, the traffic was easier to forecast, and the experience was more transferable from one case to another. Faced with such events, the firms had no way to adapt their behaviour, other than to change the tolls. This remedy proved to be quite insufficient, so there seemed to be little room for a commercial management initiative in the motorway business. The State took over three out of the four private companies through mergers with the old public firms, which, having the revenues of an old and already extended network, suffered less from the new situation. It indemnified the shareholders through a soft enforcement of the forfeiture clause. As a whole, the system proved to be subject to a soft budget constraint, especially through the Autoroutes de France cash pooling system. From the mid-1990s, new private concessions began to appear, especially in urban areas, and the network became more and more meshed (see Fig. 3). The first intra-urban motorway was, in 1993, the Prado-Carénage tunnel inside the city of Marseille, Marseille, connecting the centre of the city to the eastern part of the agglomeration through a tunnel that already existed and just had to be reshaped. The tunnel was franchised to a private company, which runs it successfully. In Lyon, the “Tangentielle Est Ouest” (TEO) scheme (1997, about 10 km long) was not so successful. The toll motorway was auctioned and franchised to a private consortium, but in order to obtain its financial return, it levied a relatively high toll and it also needed restrictive actions on the parallel and adjacent network so as to capture more traffic. After demonstrations and protests by the users, the municipality cancelled the franchise at a great expense and operated the link with a much lower toll. Another urban toll link was built in 1998 in the Ile de France agglomeration (A14, between La Défense and Orgeval, about 20 km long), and it worked successfully. Currently, a toll motorway link (A86, about 20 km long), which will complete a second ring road around Paris, is under construction (its first section was opened mid-2009).
 The network at the end of the nineties It must be noted that these concessions, located in urban areas, raise new problems. Environmental concerns, which increased progressively since the mid-1970s, grew enough to increase the construction costs. Further, the increased congestion on untolled roads compared to the fluid traffic on tolled motorways leads to suboptimal situations and gives an example of the conflict between profit maximisation and collective surplus maximisation (see for instance Brossier and Leuxe (2000) for figures and evidence that, in France, tolled motorways are overpriced and non tolled motorways are underpriced for most categories of vehicles, due to the high petrol taxes). Besides this change in the exogenous environment, the concessions became less and less tied to the State. As from 1995, multi-year contracts that balance investments in renewal and lane extension on one hand, and toll increases on the other, were implemented. These 5 year contracts replaced the previous annual announcement in both toll level and investment. As such, this change has given concessionaires much better foresight over a 5-year horizon. As evident in Fig. 4, an interesting observation is the relationship between the physical connections of different motorway concessions. Each concessionaire has been given a distinct area of the country by the public authority.
 The geographical areas of the concessionaires Without appropriate regulation, the outcome of such a situation would be strong market power with double monopoly margin on adjacent complement links operated by distinct concessionaires. The regulator should be especially cautious about this point, and all the more when concessionaires get privatized. More recently, the toll motorway policy has experienced important changes in order to keep in line with European Directives,Footnote 3 whose general orientation is towards competition and efficiency (through transparent and competitive auctions and the elimination of discriminatory procedures). Concessions are now granted after a public competition process and they are no longer directly backed by collateral existing motorways. As well, the accounting regime of the present concessions has been modified so as to be more in line with the common rule (the core question was the depreciation process), and the State has given up the guarantee for liabilities to existing concessions. In compensation, the duration of the concessions was increased. While cash flows have not changed, assets are now linearly depreciated over a longer period, the consequence being that the companies have been profitable, paid income tax, and distributed dividends. The main public motorway companies were consolidated into three main groups in order to benefit from geographical consistency and financial viability. In 2002, two of them were introduced on Euronext, the European stock exchange, and they ended up being privatised through an auction process. In the same period, the motorway firms adopted a more aggressive commercial policy, based on tariff differentiation (discount fares, season tickets, subsidies from local authorities for discount rates to the local users ...). In other words, the motorway system, while progressively adapting its regulatory regime, is more and more transforming into the textbook case of a concession regulated by the State and aiming at maximising its profit through the usual devices of private firms. Concessions are granted under an auction process, where the choice is based on a multi-criterion assessment that puts the most important weights on the level of subsidies and tolls requested, and on technical and financial ability. While this last evolution is too recent to be assessed, it is interesting to note that the auction process brought in an extra 3 billion Euros [15 billion (actual)−12 billion (expected)]. The difference may represent the gain in cost efficiency and demand management that the auctioneers hope to achieve compared to the previous standards, or it may represent the hope that public regulation will not be efficient at controlling profit growth. As a whole, French motorway policy has experienced important management changes during the past 50 years. These changes resulted from pragmatic rather than dogmatic considerations, in line with the evolution of the issues to be addressed. In the first stage, tolls were introduced in order to develop the motorway network. In the second stage, delegated management (going well beyond just tolling) and autonomy for government-owned companies and private sector-owned companies were introduced along with competition for concession awarding. The third stage was the implementation of general cash pooling as a way to ease a financial crisis for any single company, and to a lesser extent, a general crisis of the whole system. The fourth stage corresponded to a quieter period, with the implementation of the EU rules for awarding concessions. The last stages were the partial and—after some hesitation—complete privatisation of the companies. The geographical network has evolved from a juxtaposition of isolated links to a fully connected and integrated network. In the beginning, the management was fully public with a hierarchical organisation, the executive of which was the head of the Ministry of Transport. Concession contracts were at first relatively simple and essentially concerned with financial elements and the definition of the infrastructure. The very close relationship between companies and the State allowed unplanned actions to be decided, or to be oriented according to public objectives. Additional expenses incurred were mainly covered through an increase of the concession’s duration. Gradually, the management evolved towards an organisation where independent and private concessionaires were chosen through a bidding process and who operated through a long-term contract with the Ministry of Transport. Nevertheless, it appears that these contracts show a high degree of incompleteness, with, for instance, a toll regulation renegotiated every 5 years. Having experienced different types of concessions, the French system is now open to new solutions, for instance, with broader long term public–private partnerships (PPP). The role of concessionaires is now more complex; it includes many dimensions of services, the possibility of toll differentiation, and so on. Both the nature of the concession and the relationship with the State have changed completely. The importance of contractual rules is paramount given that the government has to clearly specify its objectives, and to design and negotiate contractual rules ensuring that the concessionaire’s actions keep in line with public objectives. Conclusions can be drawn from this short historical review on the grounds of economic analysis. Clearly, the system of toll motorway had, and still has, a financial purpose; the point was to raise funds outside of the ordinary public budget. In fact, calculations show that for intercity motorways, tolls were, and still are, higher than the short run marginal cost of infrastructure, which on average is roughly equal to the petrol taxes (see for instance CGPC (1991), Brossier and Leuxe (2000), and Meunier (2009)). The situation is different in urban areas, where short run marginal costs are very high due to congestion costs (Roy 2001). This disconnection of tolls from the marginal cost induces inefficiency. Tolls also have effects on investment costs. Several comparisons show that a toll motorway is about 10% to 20% more expensive than a free motorway, both for construction and for operations. The difference of costs is obviously linked to the implementation of a fee collection system (toll booth construction, system of vehicle identification, data bases and transmission, toll calculation and cashing, enforcement). Besides, as previously seen, private investment and maintenance costs are 5% to 10% lower than their public counterpart. The French experience, confirmed by many other records, shows clearly that the main risk in motorway planning is related to traffic forecasts (see for instance Flyvbjerg et al. (2006) and more specifically SETRA (2008)). This risk is composed of a possible over-estimation bias in some cases, and a rather large standard deviation. Uncertainty on costs exists too, but the collective and individual experience of firms, in general, can limit this risk more effectively than for traffic forecasts. The financial and economic crisis has indeed introduced increased uncertainty for financing, but this is also true for traffic forecasts: is it transitory or will long-lasting trends be modified? nobody knows yet. Political risks, relative to the control of tariff levels, have decreased progressively during the French experience. No business plan can reverse the course of a structurally unprofitable concession, as shown by the bankruptcies experienced in the late 70’s. The reason is that investment in motorways is highly specific. Once completed, it is not possible to change it; you can only work with the tolls, and the effectiveness of this tool is limited. A characteristic of motorways is that they generate congestion externalities, both internal (congestion on the motorway) and external (congestion on other links of the network). These effects induce substitution or complementarities between each link and the adjacent links, whether in complicated networks such as urban systems or in intercity motorways as soon as the links become connected into a network, as happened in France after 1980. This increased complexity makes the situation more difficult both for the concessionaires, and for the public entities in charge of the regulation. Information asymmetry between the operator and the regulator exists, but to a moderate extent, at least ex-ante. Ex post, once the concession has been granted, the asymmetric information is much in favour of the concessionaire, who has a better knowledge of maintenance and investment costs, and has also a better knowledge of operational traffic management conditions. Incentives for the concessionaire to get precise information on traffic appear naturally, for instance, in order to optimise the toll booth operations and the organisation of maintenance operations. The technical quality of the infrastructure seems rather easy to assess by the regulator; visual inspection and technical measures are easy and reliable in the highway sector. Also, whenever the concessionaire has a global long term contract and financial accountability, he can implement an efficient optimisation of the long term global cost.",6
12.0,3.0,Networks and Spatial Economics,05 February 2009,https://link.springer.com/article/10.1007/s11067-009-9099-6,Modelization of Time-Dependent Urban Freight Problems by Using a Multiple Number of Distribution Centers,September 2012,David Escuín,Carlos Millán,Emilio Larrodé,Male,Male,Male,Male,"As time goes on and because of population increase in large cities, the problems generated by urban freight distribution are getting more and more complicated due to traffic flow, traffic congestion, illegal parking, just-in-time delivery, time constraints, e-commerce and, above all, pollution and environmental impact. Although the literature on solving routing and scheduling problems is very extensive nowadays, almost no models exist that take hubs into account, and so, from the point of view of research, it is still necessary to find ways of resolving the negative effects caused by the above-mentioned points, through an analysis of new delivery strategies and algorithms. The aim of the paper is to model urban distribution vehicle routing problems by means of hubs in large cities. Hubs are very well known in the literature; they are often used in many scheduling problems and strategy models, like air traffic models, logistic models, etc. Over the last few years, new distribution centers (called Urban Distribution Centers, UDCs or DCs) have appeared within the cities. Finnegan et al. (2005), present a study evaluating sustainable freight distribution in the city center of Dublin, focusing particularly on urban distribution centers and managing the last mile delivery. The idea behind the urban distribution center is to provide buffer points where cargo and packages which are to be delivered to shops and businesses, can be stored beforehand. At these centers, there will be other kinds of routing problems corresponding to a fairly similar distribution problem. The comparison between both distribution systems is shown in Fig. 1.
 Direct Delivery vs Delivery using DCs One of the main objectives of these centers is related to reducing traffic congestion (caused by the large number of delivery trucks on the streets and because it is not possible to create enough parking places), in zones where problems such as illegal parking lead to reductions in traffic flow. As shops and businesses demand shorter and shorter delivery times, vehicle routing and scheduling problems become harder for distributors. It is recognized that the traditional system based on fixed routes does not fulfil the expectations of trade and may, in some cases, be quite inefficient for distributors. In this work, a new vehicle routing model (based on the known Time-Dependent Vehicle Routing Problem with Time Windows, TDVRPTW, Huey-Kuo et al. 2006) has been developed and a change in the traditional approach is proposed, by adopting a system in which some customers are served by urban distribution centers (to be more specific, by using, for example, hybrid vehicles) while the remaining customers are served by traditional routes. This study is also motivated by recent developments in real time traffic data acquisition systems, as well as national and international policies aimed at reducing concentrations of greenhouse gases in the atmosphere emitted by traditional vans. Due to the fact that the density of shops differs greatly in central districts of a city compared to the outskirts, not all shops are serviced by routes starting at the hub. For this reason, it is suggested that the DCs be located in areas where there is a high density of shops and that in other areas, deliveries be made directly through conventional distribution methods (Fig. 1). The method used consists of extending the traditional VRPTW by giving further consideration to total delivery costs and the influence of arrival times at each DC. The paper is organized as follows: after this introductory section; a review of time dependent models is presented in the next section; then the model formulation is introduced in two parts—a problem description and a mathematical model. After introducing the model, which is the focus of this paper, the solution algorithm is presented once the concept of latest possible departure time is explained in detail. The general scheme of the solution procedure is shown as well. At the end of this paper, in section 5, a case study involving a pharmaceutical distribution is presented to show the method and computational results. Finally, several findings and future work are discussed.",23
12.0,3.0,Networks and Spatial Economics,12 April 2011,https://link.springer.com/article/10.1007/s11067-011-9159-6,Comparison of Methods for Path Flow Reassignment for Dynamic User Equilibrium,September 2012,Malachy Carey,Y. E. Ge,,Male,Unknown,Unknown,Male,"Models used to predict or optimize traffic flows and travel times on road networks are usually referred to as traffic assignment models and when travel demands and traffic flows are varying over time the corresponding models are referred to as dynamic traffic assignment (DTA) models. If the predicted flows are such that no user can unilaterally reduce their origin to destination travel time or cost, then the flow pattern or assignment is said to be a dynamic user equilibrium assignment (DUE). Both analytical and microscopic simulation approaches have been developed for DTA and DUE modeling (e.g. Friesz and Bernstein (2000) review the former and Peeta and Ziliaskopoulos (2001) reviewed both approaches). In this paper we are concerned with the analytic flow based approach but the issues discussed are also relevant to a simulation or microsimulation approach. The solution process for a DUE model or problem is most commonly formulated as iterating between two components namely dynamic network loading (DNL) and path inflow reassignment or, for brevity, path reassignment. We could use other phrases instead of ‘path inflow reassignment’ and some readers may prefer that, however, for consistency we use the latter phrase throughout (see Appendix 2 on alternative terminology). The DNL component takes the inflow to each spatial path at each point in time as given and “loads” all of these path inflows onto the network over time, to obtain the flows and travel times on all time-space paths (see Appendix 1 on the origins of this terminology). The path reassignment component then updates the spatial path inflows in the light of the travel times generated by the DNL component. Iteration between these two components continues until the DUE conditions or stopping criteria are satisfied. A further brief note on generic algorithms for DTA is given in Appendix 3. We do not consider departure time choice or mode choice. In this paper we focus mainly on investigating the path reassignment component of this solution approach, partly because this component has been very little investigated compared to DNL: there is a substantial literature on models and methods for DNL and on properties of these. Most papers on DNL do not discuss or even mention how the DNL solution can be used to solve for a DUE, that is, they do not discuss how path inflow assignment, which they take as given, is obtained. That is somewhat surprising, since the allocation of traffic to paths is otherwise unexplained or arbitrary and presumably will need to be adjusted, through a reassignment process, in order to be consistent with the DNL. A further reason for investigating path inflow reassignment here is that, even though it takes much less time and computing resources than DNL, it has more direct influence than the DNL on the speed of convergence to a solution (the number of iterations) and on whether it converges to a user equilibrium. That is because a DUE is defined in terms of equating travel times on utilized paths, and it is the path inflow reassignment step that reassigns path flows in order to achieve this equality—the DNL is used only to compute the resulting path travel times. Though this paper is mainly concerned with path reassignment, and how this affects solving DUE problems, it is of course essential to include a DNL model or component in order to complete the DUE algorithm. Dynamic network loading (DNL) models and properties have been developed and investigated in many articles. For example, following the DUE model developed in Friesz et al. (1993), which assumed link travel time functions of the form \( \tau ({x_{{jt}}}) = {f_j}({x_{{jt}}}) \) where x

jt
 is the occupancy of link j at time t, various authors developed and investigated DNL methods based on assuming such link travel time functions, e.g. Wu et al. (1998b); Xu et al. (1998, 1999); Zhu and Marcotte (2000). In a quite different approach, Daganzo developed two DNL models that are finite difference approximations to fluid flow model or kinematic wave model of traffic flow, also referred to as the LWR model (Lighthill and Whitham (1955) and Richards (1956)). The first of these is the cell-transmission model or method (CTM) of Daganzo (1994, 1995a), which has been used by many later authors (e.g. Lo and Szeto (2002); Szeto and Lo (2004)). It assumes a triangular or trapezoidal flow-density function. The second (Daganzo (1995b)) is a finite difference approximation method (FDAM) that is similar to the CTM except that it assumes a general nonlinear flow-density function. The latter FDAM can be used for network loading in the same way as the cell-transmission model for networks in Daganzo (1995a). When comparing different path reassignment methods, for comparability we must use each of them in conjunction with the same DNL method. In this paper we used two of the above DNL methods, namely the well-known CTM and the FDAM. We chose these two DNL methods partly because they use a fine discretisation of time and space (link lengths) which makes the path reassignment method all the more important in order to obtain an efficient solution method. Since both of these methods are well-known, and described in the above Daganzo articles, further details are not given in this paper. When the dynamic network loading has been obtained, as above, it is fairly straightforward to compute the path travel time for the traffic entering at each point in time. These path travel times are needed for the path reassignment step. They are computed in the well-known way, for each OD pair, by comparing the cumulative inflow curve at the origin with the cumulative outflow curve at the destination. The path travel time for traffic entering at time i is obtained by noting the cumulative inflow up to time i and noting the time at which this cumulative outflow has just exited at the destination. The difference between these two times is the path traversal time. Note that if the inflow to a path in time interval i is greater than the receiving capacity of its first link or cell, then the excess traffic is held over to the next time interval, in which case there is a queue for entry to the first link of the path. Any waiting times of traffic for entry to a path were added into the corresponding path travel times. are briefly stated below and are set out in more detail in Section 2. For simplicity in outlining these methods we refer to single origin–destination pair of nodes. 
A method of successive averages (MSA). This appears to be the most widely used method for path inflow reassignment in DTA. MSA was introduced by Robbins and Monro (1951) for a quite different type of problem. They suggested using a predetermined step size α

n
 = 1/n where n is the iteration number. The method was later used in transportation modeling (e.g. in Powell and Sheffi (1982)), has been widely used since then and the step size used has generally been α

n
 = 1/n. We here experiment with several other definitions for the MSA step size. In the present context, for path inflow reassignment, MSA consists of removing a fraction α
n
 of the inflows from each of the currently used paths and adding this amount to the inflow for the currently shortest path for each OD pair. 
MSA with constant step size method. In the MSA method referred to above, and in the other methods considered in this paper, the step size α

n
 varies with the iteration number. We here instead choose a predetermined fixed α

n
 and let it remain constant over all iterations. This is done for comparison and because for some other types of problem a constant step size was very effective (e.g. in Bar-Gera and Boyce (2006)). 
A simple travel-time responsive (ttr1) method. In this method, at each iteration n, we take some of the given OD demand from paths that currently (iteration n–1) have relatively higher travel times (costs) and reassign this inflow to paths that currently have lower travel times, so as to move closer to a DUE solution. Such a method is used in, for example, Smith and Wisten (1995), various papers by Smith and Mounce, Huang and Lam (2002); Szeto and Lo (2007) and the method used in this paper is similar. The method is implemented here as pairwise swapping but can be very easily be applied using more general path inflow reassignment than just pairwise. 
A travel-time responsive method (ttr2) of Wu et al. (1998a); Zhang et al. (2001); Rubio-Ardanaz et al. (2003). These authors presented a method for path inflow reassignment that is a projection method in which the new path inflows at each iteration are chosen by minimizing a gap function from Fukushima (1989). They use this in conjunction with a DNL model that is based on applying a whole-link travel time model to each link. As already noted, for consistency in comparing their path inflow reassignment method with others, we will instead use it in conjunction with DNL based on the CTM and FDAM. 
An alternating direction (AD) method from Lo and Szeto (2002). These authors formulated the DUE problem as a variational inequality (VI) problem and to solve this VI formulation they employed an alternating direction method proposed by Han and Lo (2002) for co-coercive VI problems. The method involves iterating between path inflow reassignment and a DNL based on the CTM method. Here we also introduce a slightly modified version of their alternating direction method (in Section 2.4) and compare their original method with the modified version. When comparing the convergence speeds for the various methods we should remember that, in the papers that originally presented these methods, the focus was typically not on speed of convergence, or on finding faster algorithms, or even on methods for path inflow reassignment. The focus was typically on investigating other aspects or properties of the DTA or DUE problem, particularly methods for dynamic network loading. Hence, when comparing these methods based on speed of convergence, we are not suggesting that is the only or most important contributions of the research or papers that proposed these methods. Also, there is not space in this paper to compare all promising approaches to computing a DUE by iterating between a DNL model and a path inflow reassignment model. For example, we do consider computation via a fixed point perspective (e.g. Friesz and Mookherjee (2006); Kaufman et al. 1998). In the methods ttr1 and ttr2 above, the redistribution of inflows amongst paths at iteration n is based on comparing the path travel times obtained from the previous iteration n–1, and in view of that we here refer to these two methods as “travel time responsive methods”. The alternating direction method uses the path travel times in computing a step size which is then applied equally to all paths, so that redistribution of inflows amongst paths does not depend on their relative travel times. Similarly, in the MSA, the redistribution of inflows amongst paths at iteration n does not depend on their relative travel times: a predetermined fixed proportion is reallocated from each path to the shortest path. Using the above path inflow reassignment methods in conjunction with a network loading (DNL) algorithm yields several different DTA algorithms. To compare the performance of the path inflow reassignment methods we applied each of them, in a DTA algorithm, to various data sets and scenarios for three simple example networks, which are set out in Sections 4.1 to 4.3. In each DTA algorithm we use the same measures of convergence and stopping rules, which are set out in Section 3. The convergence measures indicate the degree to which the DUE conditions are satisfied. Since some methods or algorithms may converge more slowly at first and then faster, or vice versa, we report the computing cost or time (or number of iterations) for three progressively finer convergence targets. Section 6 considers and chooses the values of the various parameters (for step size, etc.) for the various methods. For each method we try to choose the parameter values that, in test examples, cause the algorithm to converge fastest. This is important so as to ensure a fair comparison between the various methods. Sections 7 and 8 report numerical comparisons of the methods when applied to the second and third example networks. They also consider how computing costs may be affected by higher levels of congestion, caused by higher levels of travel demand and or by reductions in capacity. Concluding remarks are in Section 9.",31
12.0,3.0,Networks and Spatial Economics,19 April 2011,https://link.springer.com/article/10.1007/s11067-011-9160-0,Combining Energy Networks,September 2012,Jan Abrell,Hannes Weigt,,Male,Male,Unknown,Male,"In the coming decades, energy markets around the world face a multitude of challenges, such as ongoing restructuring process, emission restrictions, support for renewables, reliability, smart grid technology, and security of supply. Electricity markets are the linkage between different fuel markets due to fuel substitution. Consequently, decisions about future market developments, such as the projected ENTSO-E Ten-Year Network Development Plan, or increased imposition of carbon regulations, directly impact the upstream fuel markets, while decisions such as the projected increase in LNG import capacities in Europe directly impact the downstream electricity market since they influence the availability of fuels and change the price levels. Investment and market decisions are further complicated because most fuel markets rely on network infrastructure, e.g. pipeline, sea routes, and railways, for transmission and local distribution. Similarly, electricity markets must account for the physics of power-flows. The different networks are characterized by a substitution relationship, e.g. either the fuel is imported to generate electricity locally, or the electricity is imported. However, this substitution is bounded by the capacity of the transmission grid, meaning that congestion effects typical for grid-bounded transportation must be considered when analyzing the interaction of energy markets and the energy system as a whole. The objective of this paper is to develop a modeling framework that accounts for the interaction of fuel and electricity markets while simultaneously respecting their network character. The connection of energy markets via electricity generation which relies on fossil fuels as the production input and the grid-bounded transportation of most fuels have given rise to two numerical modeling approaches. Energy system models tend to highlight the price interaction between single energy markets, including a detailed description of the value chains such as extraction, transmission, and final demands, but they also tend to simplify transmission modeling by abstracting from networks like natural gas pipelines or electricity grids. In a partial equilibrium settingFootnote 1 the models are formulated in an optimization framework, e.g. MARKAL (Loulou et al. 2004), POLES (Kouvaritakis et al. 2000), MESSAGE (Grübler and Messner 1998), or in an equilibrium format, e.g. LIBEMOD (Aune et al. 2001). Macroeconomic-oriented models are represented in a general equilibrium framework either in a computable general equilibrium format, e.g. MIT-EPPA (Paltsev et al. 2005) and GEM-E3 (Capros et al. 1997), or as intertemporal welfare-maximizing Ramsey-type models, e.g. REMIND (Bauer et al. 2008). Technically-oriented simulation and optimization models look at material flows, optimal fuel usage, or generation mix, e.g. PERSEUS-EEM (Möst and Perlwitz 2009), which incorporates a natural gas pipeline model in a cost-minimizing inter-regional long-term model. They approximate electricity transport by net-transfer capacities which do not explicitly model loop-flows unlike the approach discussed in our paper. On the other hand, single energy models tend to emphasize on the detailed representations of the technological details of grid-bounded transmission in a specific market with particular focus on the role of imperfect competition. Mathiesen et al. (1987) show that the European natural gas market is best described by a Cournot duopoly. Gabriel et al. (2005) and Egging et al. (2008) present a Nash-Cournot framework of the US and European natural gas markets, including pipeline and Liquefied Natural Gas (LNG) transportation. EWI Cologne has produced a series of linear optimization models of which the TIGER model provides the most detailed dispatch model for Europe and is best suited for identifying congestion (Perner and Seeliger 2004; Lochner and Bothe 2007). Holz (2009) discusses these different model families in detail. Network-oriented electricity market models include power-flows along different lines. In contrast to natural gas pipelines, the power-flow in an electricity network is physically determined by the injections and withdrawals at nodes. Smeers (1997) and Ventosa et al. (2005) provide an overview of numerical modeling approaches. As natural gas models, network-oriented electricity models examine the effects of imperfect competition (e.g. Hobbs 2001; Neuhoff et al. 2005). Network models applying the DC load-flow approach are commonly used for economic market analyses (e.g. Stigler and Todem 2005; Green 2007; Leuthold et al. 2008). To our knowledge, the interconnection of energy transmission networks has not received attention in the energy-economic literature, although the engineering literature does discuss the challenge of combining grid-bounded electricity and natural gas transportation. An et al. (2003) present a simplified model combining networks at a single node. Their combined natural gas and electric optimal power flow problem (GEOPF) considers natural gas transportation at a disaggregated distribution stage, i.e. the direction and flow of natural gas in the network are controlled via compressors. Thus, the GEOPF results in an integer problem regarding the natural gas flows.Footnote 2 The approach has been extended by e.g. Unsihuay et al. (2007) for multiple interconnection points. Arnold and Andersson (2008) present a decomposition approach for the GEOPF in order to allow larger dimension applications. While the GEOPF approach represents technological details in great detail, it is limited from an economic point of view since the usage of integer variables leads to problems representing price variables (O’Neil et al. 2005). In this paper, we present a general framework to combine energy markets, including detailed network characteristics using the mixed complementarity problem format. The model is written in the partial equilibrium framework to allow easy incorporation of further market elements. Concentrating on the natural gas transmission stage, i.e. assuming directed pipeline flows but avoiding integer modeling problems, our approach provides a more detailed analysis of energy systems on a large scale. Similar, focusing on electricity transmission, we derive a flow-based market representation applying the DC load-flow approach. The coupling of both markets is conducted via the demand market clearing price in the natural gas market and the electricity generators’ cost function considering the endogenous gas price. We test the approach on a stylized model of the European energy market with two scenarios: 1. the reduction of natural gas imports from Russia, and 2. the imposition of a carbon emission constraint on the European electricity sectors. The interaction demonstrates the importance of a combined market assessment. The remainder of this paper is structured as follows. Section 2 describes the modeling framework. The single market models are presented followed by a description of the combined model. In Section 3, we parameterize the modeling framework to a European Union test case, describe the scenarios and present the results. Section 4 summarizes our work and discusses our conclusions.",33
12.0,3.0,Networks and Spatial Economics,15 May 2011,https://link.springer.com/article/10.1007/s11067-011-9161-z,A Two-Stage Urban Bus Stop Location Model,September 2012,José Luis Moura,Borja Alonso,Francisco José Ruisánchez,Male,Male,Male,Male,"From the point of view of good urban transport planning it would be beneficial to consider the influence of public transport on overall traffic flows within a city. One of the more specific and contentious issues is the influence of the bus stops which not only represent the places where the users access the transit system they are also a determinant factor in the average bus speed. Urban bus stop distribution and location has been the subject of several research projects which were mainly carried out on a macroscopic level and used analytical models to look at either a particular bus line: Lesley (1976), Wirasinghe and Ghoneim (1981) or more recently Furth and Rahbee (2000), Saka (2001), Sankar et al. (2003) or Chien and Qin (2004); or at the overall network: Kuah and Perl (1988), Van Nes (2000), Van Nes and Bovy (2000), dell’Olio et al. (2006) or Ibeas et al. (2010). Nevertheless, the importance and complexity of well operated bus stops has meant that more detailed, exclusive research has appeared over recent years (Fernandez, 2001, 2003) with the design of applications for micro simulation models like PASSION (Fernandez 1993; Fernández and Planzer 2002), IRENE (Gibson et al. 1989 Gibson 1996) MISTRANSIT (Cortés et al. 2007) or BusSIGSIM (Silva 2001). Wong et al. (1998) evaluated the effect of a bus stop close to traffic lights with no designated area to pull over so the buses occupied a lane and interrupted traffic flow. They proposed a new expression to calculate delay based on the classic formulation of Webster and Cobbe (1966) but added terms to simulate the effect of the bus stop. More recently, Koshy and Arasan (2005) looked at the influence of bus stops on traffic flow taking into account their composition and established parameters for changing the type of bus stop. They developed the HETERO-SIM simulation model. Furth and San Clemente (Furth and SanClemente 2006) analysed how ramps and slopes increased delays in public transport showing that the effects become noticeable after 3% and that far-side locations were preferable. Fernandez et al. (Fernández et al. 2007) concentrated on analysing the interaction between traffic and bus stops when they are located close to a signal controlled junction, their simulation included a user behaviour model. Finally, Zhao et al. (2008) used a cellular automata model to study the effect on traffic of positioning a bus stop between two nearby intersections. In the proposed two-stage model, all the bus stops in an urban area are macroscopically distributed over the public transport network, these positions are then microscopically adjusted along the main urban corridor. The macroscopic location of bus stops is based on a bi-level optimization model with an upper level which minimizes a cost function for the overall system (social cost) and a lower level which includes a modal split assignment model, the development and results of which can be found in Ibeas et al. (2010). The microscopic positioning of stops based on their macroscopic distribution is supported by a micro simulation model which provides the optimal bus stop locations along an urban corridor and maximises the commercial speed of the transit buses running along it. The proposed model is described and defined in the next section, followed by the presentation of the solution algorithm and its application to a real case (Santander city, in Spain). Sensitivity analysis studies the effect of variations in bus stop location and finally the main conclusions drawn from this investigation are presented.",20
12.0,3.0,Networks and Spatial Economics,05 July 2011,https://link.springer.com/article/10.1007/s11067-011-9162-y,An Efficient Hybrid Particle Swarm Optimization Algorithm for Solving the Uncapacitated Continuous Location-Allocation Problem,September 2012,Abdolsalam Ghaderi,Mohammad Saeed Jabalameli,Ragheb Rahmaniani,Unknown,Male,Male,Male,"Since Weber (1909) first published his landmark book on location theory, there have been many researchers, such as Hakimi (1964) and Cooper (1963, 1964), who studied location problems. Based on Weber’s study, the general location model can be extended into various location models. The simplest form of location problems is the single facility location problem, which is the base of all the other models. The uncapacitated continuous location-allocation problem (UCLAP), or multi-source Weber problem, is also another popular model in location studies. In addition, the p-median problem is one of the most widely applied models. In this paper, some problems and algorithms are used in the framework of a proposed hybrid particle swarm optimization (HPSO) algorithm. The objective of location-allocation problems is to locate m facilities and to simultaneously allocate n customers to those facilities to minimize the total transportation costs. n customers are located at fixed locations with associated discrete demands. These problems occur in many practical settings where facilities provide a homogeneous service, such as the location of plants, warehouses, retail outlets, and public facilities (Jabalameli and Ghaderi 2008). Location-allocation problems are subject to various assumptions such as distance measure, capacity of facilities, and fixed cost. In the continuous version of the location-allocation problem, the objective is to generate m new facility sites in \( {\Re^2} \)to serve the demands of n customers or fixed points in a manner that minimizes the total transportation (or service) costs. Here, we consider the uncapacitated version in which the model may be formulated as follows (Love et al. 1988):
 where the following notation is used: 
w

ij
 is the quantity assigned from facility j to fixed point i; 
X

j
 = (x

j
, y

j
) is the coordinate of the new facility j; 
P

i
 = (a

i
, b

i
) is the coordinate of a customer at fixed point i; 
d (X

j
, P

i
) is the Euclidean distance from the location (coordinates) of facility j, (x

j
, y

j
), to the location of a customer at fixed point i, (a

i
, b

i
); 
w

i
 is the demand or weight of customer i. In its most general form, the location-allocation problem may involve the determination of the following parameters (Liu et al. 1994):
 
m: the number of new facilities, 
W: the allocation matrix, and 
X

j
: j = 1, 2, …, m, the location of new facilities. In this study, the number of new facilities is assumed to be known in advance. Cooper (1963) proved that, in the uncapacitated version of the location-allocation problem, the demand at each point is satisfied by the nearest facility at a minimum cost. Many extensive studies of location-allocation problems were developed after Cooper’s research. In the general case, most single-facility location problems are convex, and the optimal solutions can be developed through either optimal algorithms or some heuristics (Cooper 1964). However, multi-facility location problems are non-convex and nonlinear, and known algorithms cannot solve large scale problems optimally. In addition, Cooper (1963) proved that the objective function (1) is neither concave nor convex and may contain several local minima. Hence, the multisource Weber problem falls in the realm of global optimization problems. The literature on facility location theory is rich, and, since Weber’s work, many papers have been published that provide admirable introductions and reviews of the development in this field. One may refer to survey papers in this area such as Klose and Drexl (2005), Melo et al. (2009) and ReVelle et al. (2008). In the following, some related works are mentioned. The literature of UCLAP has focused on solution methods because this problem has many local optima. Brimberg et al. (2004) were able to obtain 272, 3,008 and 3,363 local optima from 10,000 random restarts of Cooper’s alternating heuristic in a problem instance with n = 50 and m = 5, 10 and 15, respectively. The authors further observed that the worst deviations from the optimal solution were, in the respective cases, 47%, 66% and 70%. The optimal solution was obtained 690 times for m = 5, 34 times for m = 10 and only once for m = 15 (Brimberg et al. 2006). Thus, the main difficulty in solving this class of problems relates to a highly non-convex objective function and the existence of multiple local minima. Since 1963, when Cooper introduced the LA problem, researchers have continued to investigate the problem from different perspectives, and many methods including exact, heuristic and metaheuristic algorithms have been proposed in the literature to solve the problem. Table 1 shows useful information about these methods, such as the type and name of algorithms, the year of publication and the author(s). In this table, only papers that introduce UCLAP with similar assumptions are listed, except for some studies that are otherwise related to the work here. Furthermore, in the table, A1 and A2 refer to the continuous location-allocation problem in the uncapacitated and capacitated states, respectively. A3 refers to the p-median problem (discrete location-allocation problem).
 As has been shown, many methods have been proposed to solve the UCLAP so far. These solution techniques can be classified into two major categories: exact and heuristic methods. The first category, exact methods, mainly includes enumeration methods, branch-and-bound algorithms, etc. With these methods, the optimal solutions can be obtained by exact techniques; however, the size of the applied problems is limited because these methods require a great deal of computational time to achieve the solutions, even on small-scale problems. The problem dimensions that are solved by these type of algorithms are small. For instance, problems with m = 5, n = 30 and m = 6, n = 25 (m, n are the number of facilities and customers, respectively) were solved by Rosing (1992). The second group is heuristic methods, which include heuristic, metaheuristic and hybrid algorithms. These algorithms generate good solutions; however, they may or may not produce near-optimal or optimal solutions. This group of methods can handle large-scale problems and has been shown to be a very good way to tackle larger non-polynomial hard (NP-hard) problems. The alternate location-allocation algorithm (ALA), introduced by Cooper (1964), was the first heuristic algorithm for solving the LA problem. Even thought this heuristic algorithm cannot guarantee global optimality, it provides a local optimum solution and is very powerful in solving this problem because of its structure. Another method, proposed by Teitz and Bart (1968), is based on swapping facilities and is used for solving location-allocation problems in discrete space (well known as the p-median problem). A fast version of this method was proposed by Resende and Renato (2007). This algorithm obtains the local optimum faster than the previous approaches to the p-median problem. Lorena and Senne (2003) presented Lagrangean/surrogate local search heuristics for capacitated p-median problems. These heuristics are based on location-allocation procedures that swap medians and vertices inside the clusters, reallocate vertices, and iterate until no improvements occur. Another type of algorithm is metaheuristics, which have been implemented on LA problems and are able to give better solutions in comparison to the previously mentioned algorithms. The main difference with the previous algorithms is their ability to search space where heuristic algorithms often become trapped in a local optimum. These algorithms can escape from local optima, which increase the chances of avoiding local optimality. Metaheuristic algorithms include several methods: simulated annealing (SA), tabu search (TS), variable neighborhood search (VNS), genetic algorithm (GA), variable neighborhood decomposition search (VNDS), and neural networks (NN), which have been implemented in the multisource Weber problem. Brimberg et al. (2000) compared various heuristic and metaheuristic methods, i.e., ALA, projection method, TS, GA and several versions of VNS. At this paper, it has been found that VNS consistently gives better results on average. The last type of algorithm that is used for solving LA problems is hybrid algorithms. Recently, these methods have been increasingly used to obtain near optimal solutions for different problems. Karu (1997) proposed a hybrid approach, combined with global optimization and branch-and-bound, which leads to the exact solution of instances with 287 customers and 2 to 100 facilities. These solutions have frequently been used for comparison of heuristics in the literature. Jabalameli and Ghaderi (2008) proposed three hybrid algorithms to solve UCLAP that combine elements of several traditional metaheuristics (GA and VNS) and local search (LS) algorithms to find near-optimal solutions. Proposed hybrid algorithms provide some better results in comparison to the best methods in the literature (GA and VNS). In another study, Neema et al. (2011) developed two hybrid algorithms combining the GAs with different replacement procedures and a traditional local search heuristic. Computational results showed that the hybrid methods obtained much better solutions with less computational effort. Because PSO has good performance to solve large-scale continuous problems, in this paper the PSO algorithm is proposed and examined for UCLAP. Thus, the purpose of this paper is to develop an efficient hybrid PSO algorithm subject to two local search heuristics for solving large-scale UCLAP. To construct the hybrid algorithm, PSO was combined with two well-known local search methods: Cooper and interchange heuristic CH. Computational results show that, as expected, the hybrid approach is a powerful tool for solving large-scale instances of UCLAP. This has important implications in practice, where we see the need for planners and managers to obtain high-quality solutions in reasonable computing time to ever larger problem sizes (see, e.g. discussion in Brimberg et al. 2000). The remainder of the paper is organized as follows: in Section 2, the PSO algorithm is described. In Section 3, a hybrid PSO algorithm is proposed. Section 4 presents the computational results and the effectiveness of this method, including a comparison with the best existing algorithms in the literature. In the last section, the findings are summarized.",32
12.0,3.0,Networks and Spatial Economics,26 July 2011,https://link.springer.com/article/10.1007/s11067-011-9163-x,Hybrid Evolutionary Metaheuristics for Concurrent Multi-Objective Design of Urban Road and Public Transit Networks,September 2012,Elnaz Miandoabchi,Reza Zanjirani Farahani,W. Y. Szeto,Unknown,,Unknown,Mix,,
12.0,4.0,Networks and Spatial Economics,05 August 2011,https://link.springer.com/article/10.1007/s11067-011-9164-9,Behavioral Calibration and Analysis of a Large-Scale Travel Microsimulation,December 2012,Gunnar Flötteröd,Yu Chen,Kai Nagel,Male,,Male,Mix,,
12.0,4.0,Networks and Spatial Economics,22 September 2011,https://link.springer.com/article/10.1007/s11067-011-9166-7,A Generalized Nash Equilibrium Model of Market Coupling in the European Power System,December 2012,Giorgia Oggioni,Yves Smeers,Siegfried Schaible,Female,Male,Male,Mix,,
12.0,4.0,Networks and Spatial Economics,12 October 2011,https://link.springer.com/article/10.1007/s11067-011-9167-6,Optimal Path Problems with Second-Order Stochastic Dominance Constraints,December 2012,Yu (Marco) Nie,Xing Wu,Tito Homem-de-Mello,,,Male,Mix,,
12.0,4.0,Networks and Spatial Economics,04 November 2011,https://link.springer.com/article/10.1007/s11067-011-9168-5,A Cumulative Perceived Value-Based Dynamic User Equilibrium Model Considering the Travelers’ Risk Evaluation on Arrival Time,December 2012,Li-Jun Tian,Hai-Jun Huang,Zi-You Gao,,,,Mix,,
12.0,4.0,Networks and Spatial Economics,05 November 2011,https://link.springer.com/article/10.1007/s11067-011-9169-4,A New Multi-objective Competitive Open Vehicle Routing Problem Solved by Particle Swarm Optimization,December 2012,N. Norouzi,R. Tavakkoli-Moghaddam,A. Salamatbakhsh,Unknown,Unknown,Unknown,Unknown,,
12.0,4.0,Networks and Spatial Economics,12 January 2012,https://link.springer.com/article/10.1007/s11067-011-9170-y,A New Formulation Approach for Location-Routing Problems,December 2012,Hunkar Toyoglu,Oya Ekin Karasan,Bahar Yetis Kara,Unknown,Female,Female,Female,"Organizations in today’s complex environments, whether civilian organizations in competitive markets or military organizations in hostile war arenas, need to operate at their full potential to survive. In order to achieve this potential, they must make proper strategic decisions that affect the long-term direction of the entire organization, tactical decisions that focus on intermediate-term issues, and operational decisions that concentrate on day-to-day activities within the organization. In the special case of transportation organizations, Crainic and Laporte (1997) identify these decision levels, review the literature, and provide the common mathematical models from an operations research perspective. According to them, strategic decisions include the design of the physical network and the location of main facilities. They refer to these issues as Logistics System Design which, consists of location, network design, and regional multimodal planning models. Tactical decisions concern mainly the route choice, service type, etc. They categorize these issues into two groups of which the first is long distance (less-than-truckload or rail) and the second is short distance (several pick up and deliveries mainly by truck) transportation. They refer to the former as Service Network Design and to the latter as Vehicle Routing Problems (VRP). In daily planning, operational decisions are taken in a dynamic environment, where time is an important factor (e.g. vehicle or crew schedules, time windows), and involve uncertain factors, such as demand. Therefore, they investigate these issues under two headings, dynamic and stochastic models (mainly VRP models). Strategic, tactical and operational problems can be collectively identified as Distribution Network Design Problems (DNDP). Within the context of DNDP the Location-Routing Problem (LRP) merges facility location and vehicle routing into a single problem where strategic location and tactical/operational routing decisions are taken simultaneously. Salhi and Rand (1989) evaluate the effect of ignoring routing when locating facilities and clearly show that separating facility location from vehicle routing may lead to suboptimal decisions. This interdependence between the location of facilities and vehicle routing necessitates the combination of such decisions (the number and location of the facilities and the routes emanating from the facilities to serve multiple demand points), which in turn leads to LRPs. The LRP has been studied since the early 1970s and there are several surveys on it (see, e.g., Berman et al. 1995; Laporte 1988; List et al. 1991; Min et al. 1998; Ahipasaoglu et al. (2004, unpublished); Nagy and Salhi 2007). According to the recent classification of Toyoglu et al. (2011) the majority of the LRP literature considers delivery or pickup of a single product, uses deterministic and hypothetical data, includes two layers and locates uncapacitated multiple facilities at one layer with a discrete solution space, utilizes a capacitated homogeneous vehicle fleet, considers a single planning period with no time restrictions, has a single objective function, allows a customer to be supplied by only a single vehicle, incorporates no inventory and uses heuristic methods for solution. The LRP accepts many different formulations, of which the most widely used are vehicle-flow and commodity-flow formulations (see, e.g., Laporte 1988), where the latter explicitly considers the quantity of commodities traveling in the system whereas the former only considers the vehicle circulation. Complex LRP models (see, e.g., Hansen et al. 1994; Yi and Ozdamar 2007) in the literature generally use commodity-flow formulation, since it enables the inclusion of more details and facilitates modeling of real world applications. For example, one of the most realistic LRP models is due to Ambrosino and Scutellà (2005) where they model a four-layer (plants, central depots, regional depots, and customers) LRP with inventory considerations and introduce dynamic version of the model. In general, the LRP is NP-hard (see, e.g., Laporte 1988; Min et al. 1998; Nagy and Salhi 2007). Although there exist several exact solution methodologies for LRPs (see, e.g., Laporte and Nobert 1981; Laporte et al. 1986, 1988; Laporte and Dejax 1989; Belenguer et al. 2011; Baldacci et al. 2011; Karaoglan et al. 2011), almost all reviews or surveys urge the use of heuristics and a significant amount of studies resort to heuristics due to the complexity of LRPs. Recently, Nagy and Salhi (2007) classify LRP heuristics into four groups, namely; sequential, clustering-based, iterative, and hierarchical methods. In general, all methods decompose an LRP into its major components, which is location, allocation, and routing. They then solve these parts either repeatedly, iteratively, or simultaneously. In particular, sequential methods (see, e.g., Or and Pierskalla 1979; Nambiar et al. 1989; Srivastava and Benton 1990) usually first solve a location problem to decide which depots to open and how to allocate customers to open depots. Then, given the locations of the open depots a vehicle routing problem is solved. Clustering-based methods (see, e.g., Billionnet et al. 2005; Schwardt and Dethloff 2005; Barreto et al. 2007) first group the customers into clusters such that each cluster contains one potential depot or vehicle. Then, for each cluster a VRP is solved either after or before locating a depot. Iterative methods (see, e.g., Perl and Daskin 1985; Salhi and Fraser 1996; Wu et al. 2002; Prins et al. 2007; Duhamel et al. 2010; Tavakkoli-Moghaddam et al. 2010) usually construct two or more subproblems each one including one or two of the major components. Then, these subproblems are solved repeatedly such that a subproblem provides some input to the next subproblem in an iterative manner. Hierarchical methods (see, e.g., Nagy and Salhi 1996; Albareda-Sambola et al. 2005; Melechovsky et al. 2005; Bozkaya et al. 2010) treat the location subproblem as the main problem and the routing subproblem as the subordinate problem that is embedded into the main problem. A hierarchical method then solves the location problem while in each step of the location problem it solves a routing problem which in turn provides information to the location problem. Our main objective in this study is to provide a modeling approach different from the common ones existing in the literature. To the authors’ knowledge, this is the first reported study in the operational research literature to provide such a mathematical formulation for modeling LRPs. To introduce our modeling approach, we consider the variant of the state of the art LRP model defined by Toyoglu et al. (2011). Their LRP model, in contrast with the majority of the LRP literature, utilizes a capacitated heterogeneous vehicle fleet, considers capacitated facilities, has three layers, includes two-sided time windows, locates facilities at two different layers, distributes multiple products and allows multiple sourcing. The main difference between the study of Toyoglu et al. (2011) and this study is the attempt to reduce the number of constraints and variables in the mathematical model by applying a new node-based formulation approach. The foremost idea is to reduce the solution time purely by modeling, and therefore the contribution of this paper lies in its mathematical formulation. The remainder of this study is structured as follows. We define the problem in Section 2. We introduce our modeling approach and develop a mathematical model in Section 3. We compare the computational performance of our model with previous results in the literature in Section 4. We introduce a heuristic solution methodology in Section 5 and test it in Section 6. We conclude with Section 7.",17
13.0,1.0,Networks and Spatial Economics,17 May 2012,https://link.springer.com/article/10.1007/s11067-012-9171-5,A Generalized Nash–Cournot Model for the Northwestern European Natural Gas Markets with a Fuel Substitution Demand Function: The GaMMES Model,March 2013,Ibrahim Abada,Steven Gabriel,Olivier Massol,Male,Male,Male,Male,"Quantitative studies and mathematical models are necessary to understand the economic and strategic issues that define energy markets in the world. In that vein, the study of natural gas markets is particularly interesting because most of them, particularly in Europe, show a high dependence on a small number of producers exports. According to Mathiesen et al. (1987), this market structure can be analyzed with strategic interactions and market power. This market power can be exerted at the different stages of the gas chain: by the producers in the upstream market or the local intermediate traders in the downstream market. The European markets are also characterized by long-term contracts established between the producers and the intermediate local independent traders. These long-term contracts were initially designed as a risk-sharing measure between producers and local traders. They are usually analyzed, in particular, as a tool to mitigate the producers’ market power. The combination of strategic interactions and long-term contracts makes the study of the natural gas markets evolution particularly subtle and rich. The economic literature provides an important panel of numerical models whose objective is to describe the natural gas trade structure. As an example, we can cite the “World Gas Trade Model” (Baker Institute) (Rice University 2004), the “EUGAS” model (Cologne University) (Perner and Seeliger 2004), the “GASTALE” model (Energy Research Centre of the Netherlands) (Lise and Hobbs 2008) or the “World Gas Model” (University of Maryland) [Egging et al. (2010), an extension of the work developed in Gabriel et al. (2005a, b)]. Other works include Gabriel et al. (2003), Aune et al. (2009), Boots et al. (2004), Egging and Gabriel (2006), Holz et al. (2008) and Brito and Rosellón (2010). However, most of these models present some necessary simplifying assumptions concerning either the description of the market economic structure or the demand function. For instance, the “EUGAS” model assumes pure and perfect competition between the players and thus neglects market power to allow a detailed description of the infrastructure. The “GASTALE” and “World Gas Model” depict strategic interactions between the players via a Nash–Cournot competition and the latter model also uses exogenous long-term contracts. However, the former model does not include investments in production or in pipeline and storage infrastructure. Besides, the demand representation for all these previous models does not explicitly take into account the possible substitution between different types of fuels (natural gas, oil, and coal, for instance). All these drawbacks have been analyzed in detail in Smeers (2008). The model we develop, named GaMMES, Gas Market Modeling with Energy Substitution, tries to address some of the limitations proposed in Smeers (2008). It is also based on an oligopolistic approach of the natural gas markets. The interaction between all the players is a Generalized Nash–Cournot competition and we explicitly take into consideration, in an endogenous way, the long-term contractual aspects (prices and volumes) of the markets. Our representation of the demand is new and rich because it includes the possible substitution, within the overall primary energy consumption, between different types of fuels. Hence, in our work, we mitigate market power exerted by the strategic players: they cannot force the natural gas price up freely because some consumers would switch to other fuels. We study both the upstream and downstream stages of the gas chain, while modeling the possible strategic interactions between all the players, through all the stages. The production side is detailed at the production node level and we choose a functional form derived from Golombek et al. (1995) for the production costs. We assume, in our representation that the producers sell their gas through long-term contracts to a set of independent traders who sell it back to end-users, where the Nash–Cournot competition is exerted. Storage and transportation aspects are taken care of by global regulated storage and transportation operators. Producers also have the possibility to directly target end-users for their sales. Both producers and independent traders share market power. The long-term contracts are endogenous to our model and this property (among others) makes our formulation a Generalized Nash–Cournot game. The introduction of non-symmetric independent traders that can exert market power in the spot markets and contract in the long-term with the producers, and are in an oligopolistic competition with them in the downstream induces a rich, double layer economic structure. This is a new feature of the description of the natural gas trade. It allows us to represent long-term contracts and mitigate the producers’ market power. The demand side is also detailed. We use a system dynamics approach (Abada et al. 2011) in order to model possible fuel substitutions within the fossil primary energy demand of a consuming country, between the consumption of coal, oil, and natural gas. This approach allows us to derive a new and interesting mathematical functional form for the demand function that includes naturally the competition between these. This particular new feature of the gas markets description that we have introduced in our model induces a flexibility in the gas demand representation. It allows us, for example, to study the sensitivity of gas consumption and prices over the oil and coal prices. We include all the possible investments in the gas chain (production, infrastructure, etc.) and make the long-term contracts’ prices and quantities endogenous to the model using an MCP (mixed complementarity problem) formulation. The remaining parts of the paper are as follows: the first part is a general description of the chosen economic structure representation. All the players are presented and are divided into two categories: the strategic and the non-strategic ones. The strategic interaction is also detailed in this part. The second part presents the notation used and a brief description of a system dynamics approach to model the consumers’ behavior investment in coal, oil or natural gas so that their utility is optimized. The third part is dedicated to the mathematical representation of the markets: the optimization programs associated with all the strategic and non-strategic players are presented and discussed. We also explain in this part how we make the long-term contracts’ prices and volumes endogenous to our model. The next part is an application of our model to the European natural gas trade where the calibration process and the results are discussed. A comparison between our model, a more standard one where the demand does not take into consideration fuel substitution and the European Commission natural gas forecast is carried out in order to compare between the results. The last part summarizes the work.",68
13.0,1.0,Networks and Spatial Economics,22 May 2012,https://link.springer.com/article/10.1007/s11067-012-9172-4,"A Model for Locating Park-and-Ride Facilities on Urban Networks Based on Maximizing Flow Capture: A Case Study of Isfahan, Iran",March 2013,Amir Khakbaz,Ali S. Nookabadi,S. Nader Shetab-bushehri,Male,Male,Unknown,Male,"Facility location is a central problem in real-world, strategic decision-making processes. Location models have long been used to support various planning contexts (see ReVelle et al. 1970; Tansel et al. 1983). Traditional facilities location models, such as the covering problem, the p-center and p-median problem can solve location problems of facilities such as hospitals, fire stations and so on, whose customers are located at nodes of the network. That is, people residing at nodes on the network travel to facility locations to obtain service (e.g., weekly shopping). Alternatively, service providers located at network nodes travel to consumer locations to provide service as requested (e.g., ambulance, police, and maintenance services) (Zeng et al. 2008; Toyoglu et al. 2012; Boyles and Waller 2011). However, the main clients of facilities such as gasoline stations and park-and-ride systems are not located at nodes, but are the customer flow along the planned paths (Hakimi 1964; Daskin 1995). One of the main problems facing large cities is traffic congestion, which is caused by lack of parking spaces and failure to use public transportation facilities. Establishing park-and-rides is one of the best approaches to solve the problem. They provide out-of-town parking spaces from which people may continue their trip into central areas using public transit systems (Carrese et al. 1996). They also offer a choice to those who prefer to use public transportation (e.g., the bus) but also need their own cars for portions of their intra-city trip. Most journey makers use park-and-rides for business trips to CBDs as they save on the high car park charges in central parts of large cities (Garcia and Marín 2001). Park-and-ride facilities have been in use since 1930s (Noel 1988). Studies have concluded that park-and-rides have been successful in reducing traffic congestion and its associated impacts (Bolger et al. 1992; Cairns 1997; Horner and Groves 2007). They provide benefits in terms of traffic congestion relief, reduced energy consumption, safety and inner – urban parking demand. Moreover, establishing park-and-ride systems may bring societal benefits such as reduced air pollution (Noel 1988). Park and ride also has great potential to integrate with the urban network and to provide benefits for the people of a wide range of travel-cost reductions (Lam et al. 2001). Park-and-ride systems can thus have two principal objectives: First, by facilitating modal shifts, they make it possible for journey makers to use public transportation where it is advantageous, but to drive where it is quicker and more convenient. Second, by encouraging people to use public transport for part of their journeys, traffic congestion and other adverse external effects of private vehicle travel will be relieved (Wang et al. 2004). However, the placement of these facilities is critical to the effectiveness of the transportation system. If a goal of park-and-ride systems is to attract private vehicles with low occupancy level and to reduce traffic, then system facilities should be established in a manner that maximizes the opportunities for capturing vehicles en route to their destinations (Ferrari 1999; Lam et al. 2001). Our work is grounded in the literature on network flow capture (Hodgson 1990; Zeng 2004; Berman and Wang 2010), which locates a set of facilities at nodes on a network such that the number of consumers who encounter at least one facility along their travel path is maximised. In particular, it builds on the work of Horner and Groves (2007), which adapted flow capturing models for locating park-and-ride facilities so that the maximum number of vehicle flows has an opportunity to change travel modes to public transit. Our work extends the work of Horner and Groves (2007) by relaxing the assumption of only one CBD zone, as well as by applying a genetic algorithm for solving the problem. By locating a network of park-and-ride facilities where they can intercept the greatest possible flow of vehicles along their journey to CBD zones, this model can help plan a more sustainable multi-modal transportation system to coordinate the smooth transfer of travelers from private vehicles to public transportation. Genetic algorithm has been shown to be a powerful tool for solving large-scale optimization problems (Jaramillo et al. 2002), therefore we propose the use of GA for solving this large size problem. In the next section we give a literature review of prior related research, section 3 presents our model and GA, section 4 is a case study applying our model to park-and-ride location for the city of Isfahan, Iran, and in the final section conclusions drawn from this research are presented.",26
13.0,1.0,Networks and Spatial Economics,17 June 2012,https://link.springer.com/article/10.1007/s11067-012-9173-3,A Branch and Bound Algorithm for Bi-level Discrete Network Design Problem,March 2013,Hamid Farvaresh,Mohammad Mehdi Sepehri,,Male,Male,Unknown,Male,"The network design problem (NDP) concerns with modifying a transportation network (infrastructure) configuration by adding new links or improving existing ones, so that certain social welfare objectives (e.g. total travel time over the network) are maximized. How to locate new links and how to increase the capacity of existing links are motivating problems. The overall objective is to minimize the total system costs under limited budget, while accounting for the route choice behavior of network users. NDP can be roughly classified into three categories: the discrete network design problem (DNDP) dealing with adding new links or road-way segments to an existing road network; the continuous network design problem (CNDP) dealing with the optimal capacity expansion of a subset of existing links; and the mixed network design problem (MNDP) combining both CNDP and DNDP in a network (Yang and Bell 1998). The importance of the NDP problem has been highlighted in both academic and practical literature. From a practical view, NDP becomes more important as increasing populations coupled with economic growth produce travel demand exceeding the existing capacity of transportation infrastructures. Meanwhile, resources available to enhance capacity remain limited. Therefore, enhancing capacity needs a great deal of investments that could be hardly met in developing countries. Accordingly, it is urgent to study how to efficiently allocate limited investment to improve transport efficiency and maximize the social and economic benefits. From an optimization point of view, the NDP can be viewed as a hierarchical decision making problem that includes system planners in the upper-level and users in the lower-level. System planners try to influence users’ choices by adding or expanding some links to minimize total system costs. Total costs of the system are affected by decision variables of both system planners and users (LeBlanc and Boyce 1986). The partition of the control over the decision variables between two hierarchical levels requires the formulation of the NDP as a bi-level programming problem (BLPP). In the resulted BLPP the system planner makes decisions about network configuration to improve the performance of the system, and the network users make choices about the routes of their travel in response to the upper-level decision. Since users are assumed to make their choices to maximize their individual utility functions, their choices do not necessarily align, and often conflict, with the choices that are optimal for the system planners. Lower-level problem is generally described by a user equilibrium model. Previous studies give practical evidence implying that even small and experimental scale NDPs are difficult to solve (Gao et al. 2005; Magnanti and Wong 1984). There is also theoretical evidence supporting these observations since the BLPP is NP-hard even in its linear form. Because of the intrinsic complexity of NDP in the form of bi-level formulation, the problem has been recognized as one of the most difficult ones, yet challenging problems for global optimality in transportation. This often leads to computation times too high for practical purposes. Due to the practical importance of NDP, many heuristic/meta-heuristic algorithms to tackle it have been employed. Meta-heuristic algorithms have enjoyed a considerable popularity in the last decades (Poorzahedy and Abulghasemi 2005; Poorzahedy and Rouhani 2007). The guarantee of finding an optimal solution in heuristic/meta-heuristic algorithms is sacrificed for the sake of a good solution in a significantly reduced amount of time. Although good progress and algorithms have been achieved during past decades, currently the most promising solution methods dealing with realistic scale problems are heuristics/meta-heuristics which do not guarantee the optimality or near-optimality of the solution. On the other hand, some global optimization approaches have been proposed based on converting the bi-level form to a single-level one and exploiting decomposition methods (Gao et al. 2005; LeBlanc 1975). These approaches need a lot of computation time. This paper proposes a new branch and bound algorithm for the bi-level DNDP being able to provide an optimal solution for the real-scale network design problems. To cope with explicit or implicit path enumeration, the link-node network representation with multicommodity flows is employed. The main motivation is presenting an algorithm being able to find a global optimal solution for medium to large-scale DNDP while guarding against Braess’ paradox. The remainder of this paper is organized as follows: Section 2 reviews the literature on network design problems. The proposed formulation is then described in Section 3. In Section 4, the numerical results on four test problems are reported. Finally, concluding remarks are offered in Section 5.",58
13.0,1.0,Networks and Spatial Economics,13 May 2012,https://link.springer.com/article/10.1007/s11067-012-9174-2,A Joint Replenishment Inventory-Location Model,March 2013,Francisco Silva,Lucia Gao,,Male,Female,Unknown,Mix,,
13.0,2.0,Networks and Spatial Economics,16 May 2012,https://link.springer.com/article/10.1007/s11067-012-9175-1,Finding Reliable Shortest Paths in Road Networks Under Uncertainty,June 2013,Bi Yu Chen,William H. K. Lam,Zhixiang Fang,,Male,Unknown,Mix,,
13.0,2.0,Networks and Spatial Economics,22 May 2012,https://link.springer.com/article/10.1007/s11067-012-9176-0,Renewable Portfolio Standards in the Presence of Green Consumers and Emissions Trading,June 2013,Yihsu Chen,Lizhi Wang,,Unknown,Unknown,Unknown,Unknown,,
13.0,2.0,Networks and Spatial Economics,25 May 2012,https://link.springer.com/article/10.1007/s11067-012-9177-z,A Hybrid Route Choice Model for Dynamic Traffic Assignment,June 2013,Zhen (Sean) Qian,H. Michael Zhang,,,Unknown,Unknown,Mix,,
13.0,2.0,Networks and Spatial Economics,21 July 2012,https://link.springer.com/article/10.1007/s11067-012-9178-y,An SOS1-Based Approach for Solving MPECs with a Natural Gas Market Application,June 2013,S. Siddiqui,S. A. Gabriel,,Unknown,Unknown,Unknown,Unknown,,
13.0,3.0,Networks and Spatial Economics,15 July 2012,https://link.springer.com/article/10.1007/s11067-012-9179-x,Vortex-Based Zero-Conflict Design of Urban Road Networks,September 2013,David Eichler,Hillel Bar-Gera,Meir Blachman,Male,Male,Male,Male,"In many types of networks the connections between links are critical design points. Particularly, in road networks, conflicts among traffic movements are a major cause for delays and accidents. In a standard four-leg intersection there are twelve turning movements (defined as a mapping between an ingoing leg and a different outgoing leg at a given intersection) with sixteen primary conflicts where two movements cross each other, and eight secondary (merging) conflicts. At high-volume intersections conflicting movements are separated in time by traffic signals; yet signals do not completely resolve the safety concern, and they can aggravate delay. There are several well known options for conflict reduction, such as left turn prohibitions or one way streets, yet primary conflicts exist in both of these options. The main options used so far to eliminate primary conflicts completely are grade-separated interchanges and roundabouts. Both of these options are widely regarded as providing substantial safety improvements, but each of them has its own limitations. The grade separation at interchanges is costly, land consuming, and does not integrate so well with urban tissue, especially when the latter is already in place. Indeed, most freeways and interchanges are in rural areas; some reach into city limits, but very rarely they are seen in the middle of downtown. At-grade roundabouts offer a better fit to urban areas. On the other hand, short-distance merging and weaving create enhanced conflicts at roundabouts that lead to reduced capacity. In addition, roundabouts suffer from slowdown of through traffic, and lack of control over traffic priorities. These effects may lead in some cases to substantial delay. In this paper we suggest novel operational urban networks with at-grade intersections but without traffic conflicts. The elimination of traffic conflicts imposes restrictions on the network design. On the other hand, our scope is less restrictive than conventional designs in terms of permitted travel directions, as traffic can travel on the right side in some roads, and on the left side in others. We divide the exposition of possible designs into two stages. In Section 3 and in the Appendix, we examine the possibilities for zero-traffic-conflict (ZTC) intersection designs that could be used in such a network. In Section 4, we discuss how to integrate these components into an operational network, and the role of vortices in the design process. Conflict-minimizing designs have several consequences. The main one is the effect on shortest distances within the network. A comparison of the distance implication of the proposed design as well as several other conflict reducing options, the focus of our evaluation, is presented in Section 5. It is clear that considerable work, beyond the scope of this article, remains before the novel network designs proposed here can be implemented with confidence. In Section 6 we present our point of view on additional implications and considerations that bear further examination. As a brief background for evaluating the designs discussed here, some relevant key facts and figures as well as related research are presented in Section 2.",14
13.0,3.0,Networks and Spatial Economics,03 August 2012,https://link.springer.com/article/10.1007/s11067-012-9180-4,The Role of Spatial Interaction in Social Networks,September 2013,Johannes Illenberger,Kai Nagel,Gunnar Flötteröd,Male,Male,Male,Male,"Research on social networks has made great advances in understanding the structure and dynamics of networks in the last decade. An increasing availability of proxy-data sets from which social networks can be inferred (for instance movie actors (Amaral et al. 2000) and co-authors (Newman 2001)) allow for the insight into large-scale networks. While extensive research is conducted on the organisational and social structures of networks the focus has just recently shifted to another dimension: the spatial structure. Among sociologists the relationship between physical space and social structure has already been identified in the middle of the 20th century (Festinger et al. 1963). However, detailed analysis on the spatial structure of social networks has just begun with the spatial analysis of networks in general, such as with transport and communication networks (Gastner and Newman 2006; Schintler et al. 2007; Erath et al. 2009). The lack of research in this area might have just pragmatic reasons: The above mentioned proxy-data sets usually do not involve any spatial information. And even if they provide spatial information, it often comes only with a coarse resolution at the level of municipalities or cities. The literature agrees that distance plays an important role in social networks (Latané et al. 1995; Larsen et al. 2006; Mok et al. 2007; Daraganova et al. 2012). This is rather unsurprising because face-to-face meetings, which are required to maintain a social contact, involve travel for at least one actor. The costs of travel usually scale in distance making the maintenance of long distance contacts more expensive. The literature also agrees that electronic information and communication technologies do not fully replace the need for physical contacts but rather act as a complement (Mokhtarian 2002; Mok et al. 2007; Larsen et al. 2008). Thus, understanding the role of the spatial dimension in social networks is also of importance for the forecasting of travel and communication demands. Realising that spatial proximity influences the occurrence of social contacts raises the question how much it contributes to the explanation of general network structures? Moreover, does the spatial distribution of individuals, which can be quite inhomogeneous in the real world (rural versus urban areas), have any impact? Considering the first question, results of theoretical studies appear to diverge from the observations of empirical studies. Models involving a spatial interaction between individuals can, under certain configurations, explain the emergence of social network structures (Barrat et al. 2005; Hoff et al. 2002; Butts 2002). However, results of empirical studies suggest that a geographical process is not the only process that governs the organisational layout of a network (Daraganova et al. 2012; Lambiotte et al. 2008). This may indicate that the configurations considered in theory are rarely observed in reality. Considering the second question, the theoretical studies are usually based on random distributions or lattice-like layouts. Real-world distributions have gained little attention (see for instance Liben-Nowell et al. 2005 and Butts and Carley 2001). This paper contributes to the answer of both questions. The results of studies presented in this article provide further evidence that distance is a crucial factor concerning tie formation, but it is not the dominating variable explaining the topology of a network. We analyse empirical data describing a network of leisure contacts and show that local network structures, specifically the degree distribution and transitivity, are not determined by the spatial layout of the network. A model for large-scale social networks involving a spatial and social interaction between individuals is proposed. Simulation studies are conducted using a synthetic population generated from real-world data as input. The results show that the model is capable of reproducing the spatial structure of the observed network, but, however, fails to reproduce the topological characteristics. The remaining article is organised as follows: Section 2 provides an overview of empirical studies on social networks and models for spatially embedded networks. In Section 3 a detailed analyses of the empirical data is presented. We turn to the description of the simulation model in Section 4 and present the simulation studies in Section 5. The paper is closed with a discussion of the results of the empirical analysis as well as the simulation studies in Section 6 and a conclusion in Section 7.",41
13.0,3.0,Networks and Spatial Economics,21 September 2012,https://link.springer.com/article/10.1007/s11067-012-9181-3,Optimizing Network Flows with Congestion-Based Flow Reductions,September 2013,Douglas R. Bish,Edward P. Chamberlayne,Hesham A. Rakha,Male,Male,Male,Male,"Models that optimize network traffic flows over time are useful in many traffic planning applications; examples include dynamic traffic assignment and regional evacuation planning (our main area of interest). The Cell Transmission Model (CTM) introduced in Daganzo (1994, 1995) and incorporated into a linear program (LP) in Ziliaskopoulos (2000) is such a model. This framework has been used or studied in numerous papers, including those that study dynamic traffic assignment (Li et al. 2003; Ukkusuri and Waller 2008), ramp metering and traffic signal control (Lo 2001; Lin and Wang 2004; Gomes and Horowitz 2006), and evacuation modeling and planning (Tuydes and Ziliaskopoulos 2006; Sbayti and Mahmassani 2006; Liu et al. 2006; Chiu et al. 2007; Yazici and Ozbay 2008; Xie et al. 2009; Yao et al. 2009; Chung et al. 2011, 2012). Traffic flow is complex (see Kerner and Klenov 2006; Nagel et al. 2003; Schnhof and Helbing 2007, for instance) and optimization-based traffic flow models must include many simplifications in order to be tractable. The CTM is a first-order macroscopic traffic stream model that conforms to the hydrodynamic traffic flow and density relationships proposed by Lighthill and Whitham (1955), Richards (1956) (commonly referred to as the LWR model). The CTM provides a promising modeling framework for traffic flow optimization because it models the spillback propagation and speed reduction caused by congestion, and can be incorporated, in an approximate manner, within an LP (see Ziliaskopoulos 2000), which are generally computationally efficient. To gauge whether a particular modeling framework is appropriate, we can examine whether important higher level metrics are appropriately developed. In this case, we can do this using empirical data and higher fidelity microscopic traffic simulators. Examining the CTM from this perspective, we find a significant problem, which is that the CTM fails to capture the reduction in the flow discharge rate after the onset of congestion at a bottleneck. Traffic stream models typically have flows that initially increase with density until a critical density is reached, after which the flow decreases as density increases, eventually reaching a flow of zero at jam density (May 1990). The fact that congestion upstream of a bottleneck reduces the flow discharge rate from the bottleneck has been extensively shown in various empirical studies (Banks 1990; Hall and Agyemang-Duah 1991; Cassidy and Bertini 1999; Chung et al. 2007) and in traffic simulations (Chamberlayne 2011). In Papageorgiou (1998), the author states that LWR-based models like the CTM will seek to create congestion in order to maximize the network outflow. In this paper we present a generalization of the CTM that captures bottleneck flow discharge rate reductions after the onset of congestion. When incorporating the CTM into an LP (see Ziliaskopoulos 2000) the resulting solution often exhibits traffic holding, which is an artifact of linearizing the CTM. This linearization results in an under-constrained problem, which in turn allows the model to have an unrealistic level of control and restrict flows (i.e., traffic holding) at locations where it is not realistic or reasonable. This problem will be discussed in more detail in Section 3.1, and is especially relevant to our topic because traffic holding is significantly influenced by flow reductions at bottlenecks. To use optimization-based models to develop traffic management strategies, it is important that they model the effects of congestion sufficiently and do not allow traffic holding to produce unrealistic solutions. For instance, when considering network flows during an emergency evacuation, congestion can become severe, lasting for hours, and causing large reductions in the network flow capacity. For obvious reasons, this type of congestion should be avoided since it is not only an inconvenience but can threaten lives during an emergency evacuation. Furthermore, if a solution depends on traffic holding, yet there is no way to enforce this behavior, it will not be effective. This paper advances the understanding of this difficult problem of network flow over time with congestion by making the following specific contributions:
 The development of a generalized CTM that models the reduction in flow discharge rates at a bottleneck after the onset of congestion. The implementation of the generalized CTM within a mathematical program that eliminates unwarranted traffic holding, along with a discussion of traffic holding and congestion-based flow reductions. Improvements to the computational performance of the mathematical program incorporating the generalized CTM. Numerically demonstrates the generalized CTM and the mathematical program incorporating the generalized CTM, using two common freeway bottleneck examples. This paper seeks to capture the net effect of congestion within a mathematical program for the purposes of network flow optimization; it is not an attempt to advance traffic flow theory but is intended for use in transportation network planning and to better understand optimal traffic flows. The remainder of this paper is structured as follows. Section 2 reviews the CTM and proposes a generalized CTM that models the impacts of congestion more realistically, along with an illustrative example. Section 3 reviews a linear programming implementation of the generalized CTM, similar to the one introduced in Ziliaskopoulos (2000), discusses the problem of traffic holding, and proposes a mixed binary programming formulation to eliminate traffic holding. Also included is a freeway merge example and methods for reducing the computation effort required to solve this problem. Section 4 presents the research conclusions.",7
13.0,3.0,Networks and Spatial Economics,15 November 2012,https://link.springer.com/article/10.1007/s11067-012-9182-2,Solving Discretely-Constrained Nash–Cournot Games with an Application to Power Markets,September 2013,Steven A. Gabriel,Sauleh Ahmad Siddiqui,Carlos Ruiz,Male,Unknown,Male,Male,"This paper considers a Nash–Cournot game between energy producers in which each player solves a discretely-constrained optimization problem. Typically, such an optimization problem maximizes producer profits subject to operational and investment decisions. By taking the Karush–Kuhn–Tucker (KKT; Bazaraa et al. 1993) conditions to each player’s problem and combining them, perhaps with additional market-clearing conditions in some cases, results in a mixed complementarity problem (MCP; Cottle et al. 1992) which has recently seen a lot of applications in energy and other issues (e.g., Bard 1983; Bard 1988; Bard and Moore 1990; Karlof and Wang 1996; Labbé et al. 1998; Luo et al. 1996; Moore and Bard 1990; Wen and Huang 1996) and more recently (e.g., Bard et al. 2000, Fuller 2008, 2010 (personal communication); Gabriel et al. 2010; Gabriel and Leuthold 2010; Hu et al. 2009; Marcotte et al. 2001; O’Neill et al. 2005; Scaparra and Church 2008). Equilibrium problems, in general, have been well formulated and studied in power markets (García-Bertrand et al. 2005; Leuthold et al. 2012; Metzler et al. 2003; Oggioni et al. 2011; Smeers 2003) as well as natural gas markets (Abada et al. 2012; Siddiqui and Gabriel 2012). Crucial to expressing the Nash–Cournot game between two or more energy producers as an MCP is the assumption that the KKT conditions can be formulated. When for example some of the variables are integer-valued (e.g., binary go/no go decisions), the KKT conditions are not valid. In this paper we show a new approach that provides a compromise between complementarity and integrality. This is done by first relaxing the discretely-constrained variables to their continuous analogs, taking KKT conditions for this relaxed problem, converting these conditions to disjunctive-constraints form (Fortuny-Amat and McCarl 1981), and then solving them along with the original integer restrictions re-inserted in a mixed-integer, linear program (MILP). The integer conditions are then further relaxed, but targeted using penalty terms in the objective function. This MILP relaxes both complementarity and integrality but tries to find minimum deviations for both and as such is an example of bi-objective problem (Cohon 1978). When an equilibrium solution exists that additionally satisfies the integrality conditions, we show that it can be found. In Section 2 we first provide the general form of this problem which we call discretely-constrained mixed linear complementarity problem (DC-MLCP) and which was originally stated in Gabriel et al. (2012, 2013) but not specialized to the current context. The MILP that is used to solve the DC-MLCP is shown to have a solution under mild conditions. Then, we specialize the DC-MLCP to the current context of a discretely-constrained Nash–Cournot game between energy producers. Nash–Cournot problems without discrete variables have long been studied and it is well known that they can be expressed either as nonlinear complementarity or variational inequality problems (Facchinei and Pang 2003). Allowing for discrete variables makes the problem more realistic as in some settings, for example, production can only occur in discrete amounts or there are binary decisions about starting up/shutting down or investing in some generation or transmission capacity. We show a correspondence between the solution set to the discretely-constrained Nash game and integer solutions to the continuous relaxation. Section 2 provides the mathematical formulation of the considered model and describes the proposed solution technique. Section 3 provides numerical examples that validate the proposed approach followed by conclusions and extensions in Section 4 and an Appendix with specific key formulations.",41
13.0,3.0,Networks and Spatial Economics,30 December 2012,https://link.springer.com/article/10.1007/s11067-012-9183-1,Modeling Parking Behavior Under Uncertainty: A Static Game Theoretic versus a Sequential Neo-additive Capacity Modeling Approach,September 2013,Liya Guo,Shan Huang,Adel W. Sadek,Female,,Male,Mix,,
13.0,3.0,Networks and Spatial Economics,24 March 2013,https://link.springer.com/article/10.1007/s11067-013-9184-8,Modelling Route Choice Decisions of Car Travellers Using Combined GPS and Diary Data,September 2013,Katrien Ramaekers,Sofie Reumers,Mario Cools,Female,Female,Male,Mix,,
13.0,4.0,Networks and Spatial Economics,05 April 2013,https://link.springer.com/article/10.1007/s11067-013-9185-7,Regional Air Quality Conformity in Transportation Networks with Stochastic Dependencies: A Theoretical Copula-Based Model,December 2013,ManWo Ng,Hong K. Lo,,Unknown,,Unknown,Mix,,
13.0,4.0,Networks and Spatial Economics,19 June 2013,https://link.springer.com/article/10.1007/s11067-013-9186-6,Inferring Contagion Patterns in Social Contact Networks with Limited Infection Data,December 2013,David Fajardo,Lauren M. Gardner,,Male,,Unknown,Mix,,
13.0,4.0,Networks and Spatial Economics,18 May 2013,https://link.springer.com/article/10.1007/s11067-013-9187-5,Linear Programming Formulation for Strategic Dynamic Traffic Assignment,December 2013,S. Travis Waller,David Fajardo,Vinayak Dixit,Unknown,Male,Unknown,Male,"With the continuing progress of deployable Dynamic Traffic Assignment (DTA) approaches, researchers are asking fundamentally new questions with regard to model realism and predictability. The core behavioral assumption within many DTA models is that equilibrium route choice exists. This assumption provides substantial descriptive capabilities but also a potential weakness due to the rarity of observed traffic patterns to be in an equilibrium state. Furthermore, this observation is often employed as the key criticism of DTA approaches to an even greater degree than to traditional static techniques due to the ability of the dynamic models to generally represent traffic in a more realistic and comparable manner to observable conditions. Numerous approaches have emerged to address this criticism, often referred to as disequilibrium, stochastic, day-to-day, or transient modeling. An approach that could potentially enhance the behavioral capabilities of DTA models and mitigate the aforementioned criticisms regarding observable equilibria is the concept of strategic assignment, first introduced by Marcotte and Nguyen (1998). In strategic assignment approaches, the resulting model does not attempt to optimize path (or link) flow directly, but rather to discern strategies that are applied following the realization of some uncertain variable. Often, the goal of the new strategic approach is to equilibrate an expected condition as opposed to a deterministic cost equilibration. For the current analysis, we focus on a priori approaches to develop a model that provides superior insights into how traffic volatility emerges in the presence of uncertainty. Specifically, this paper expands the linear programming System Optimal Dynamic Traffic Assignment (SODTA) modeling framework developed by Ziliaskopoulos (2000), which embeds the cell transmission model (Daganzo 1994) for traffic propagation. While system optimal conditions are modeled (i.e., marginal cost equilibration) due to the specific capabilities of the LP approach, substantial analytical tools become utilizable, such as stochastic linear programming. The novelty of the presented approach is the development of a two-stage stochastic variation where trip demand is uncertain but represented as a random variable that gives rise to multiple potential future scenarios (characterizing different days none of which are in equilibrium/optimality when viewed myopically). In the first stage, routing strategies (i.e., flow proportions) are developed to minimize expected system cost and then employed in the second stage after the trip demands are realized to produce scenario-dependent dynamic flows and densities. This work is organized in the following manner. Section 2 discusses previous literature on the topics of dynamic traffic assignment, linear programming formulations of DTA, and efforts at modeling daily volatility of traffic flow, followed by an introduction to the idea of strategic assignment. Section 3 briefly reviews the LP DTA formulation from Ziliaskopoulos for completeness, then Section 4 discusses the model properties, followed by Section 5 containing the problem formulation for the strategic SO (StrSO) DTA problem. Section 6 examines computational results and realized flow variability and this paper concludes in Section 7 with a discussion of future research directions.",24
13.0,4.0,Networks and Spatial Economics,01 June 2013,https://link.springer.com/article/10.1007/s11067-013-9189-3,Solving the p-hub Median Problem Under Intentional Disruptions Using Simulated Annealing,December 2013,F. Parvaresh,S. A. Hashemi Golpayegany,B. Karimi,Unknown,Unknown,Unknown,Unknown,,
13.0,4.0,Networks and Spatial Economics,30 May 2013,https://link.springer.com/article/10.1007/s11067-013-9190-x,A Stochastic Vehicle Routing Problem with Travel Time Uncertainty: Trade-Off Between Cost and Customer Service,December 2013,Junlong Zhang,William H. K. Lam,Bi Yu Chen,Unknown,Male,,Mix,,
13.0,4.0,Networks and Spatial Economics,11 June 2013,https://link.springer.com/article/10.1007/s11067-013-9196-4,Integrated Inventory Control and Facility Location Decisions in a Multi-Echelon Supply Chain Network with Hubs,December 2013,Mehrdad Shahabi,Shirin Akbarinasaji,Rachel James,Male,Female,Female,Mix,,
14.0,1.0,Networks and Spatial Economics,30 June 2013,https://link.springer.com/article/10.1007/s11067-013-9197-3,Multi-product Capacitated Single-Allocation Hub Location Problems: Formulations and Inequalities,March 2014,Isabel Correia,Stefan Nickel,Francisco Saldanha-da-Gama,Female,Male,Male,Mix,,
14.0,1.0,Networks and Spatial Economics,16 July 2013,https://link.springer.com/article/10.1007/s11067-013-9199-1,Maximising the Worth of Nascent Networks,March 2014,P. W. Heijnen,A. Ligtvoet,P. M. Herder,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Networks and Spatial Economics,02 August 2013,https://link.springer.com/article/10.1007/s11067-013-9200-z,Estimation of Bid Functions for Location Choice and Price Modeling with a Latent Variable Approach,March 2014,Ricardo Hurtubia,Michel Bierlaire,,Male,Male,Unknown,Male,"Land use models are an increasingly used tool for forecasting the evolution of cities and evaluating the potential effects of urban interventions such as real estate developments, modifications to the transport system and changes in urban or transport policy. They are of particular relevance for the field of transport modeling, since long-term travel demand is explained in a large amount by the spatial distribution of agents and activities in a region. The distribution and agglomeration of agents (households and firms) describes trip generation and attraction in spatially disaggregated terms and can be used to generate origin-destination trip matrices or, in the case of agent-based travel models, can be used to define the location of the activities involved in the tours of traveling agents. Besides this, the spatial distribution of agents and activities in a city is one of the main sources of a wide variety of externalities such as congestion, pollution or social segregation and, simultaneously, is one of the main factors that affect the value of land and real estate goods. Modeling the location choice of the different agents that interact in a city is, therefore, one of the main objectives of any land use model. Forecasting location choice requires forecasting prices and vice versa. It is widely accepted (Alonso 1964; Fujita 1989; Fujita et al. 1999; Glaeser 2008) that location choice depends heavily on the prices of locations while, simultaneously, real estate prices are determined in part by the location preferences of agents. This endogenous interdependence makes the modeling of prices and location a particularly complex task. Location choice and real estate prices have been traditionally modeled under two different main assumptions regarding the way the market operates: the choice approach and the bid-auction approach. Under the choice approach (McFadden 1978; Anas 1982), agents select the location that maximizes their utility under the assumption that they behave as price takers. In most cases, under the choice paradigm, prices or rents are modeled exogenously through a hedonic model (Rosen 1974). The bid-auction approach (Ellickson 1981) assumes that real estate goods are traded in an auction market, where the best bid for a particular location determines both the located agent and the price or rent of the good. In the field of urban economics, the bid-auction model has been used mostly as an alternative to hedonic models for the estimation of prices and marginal willingness to pay for attributes of real estate goods. The original model proposed by Ellickson (1981) considered an Extreme Value distribution of the willingness to pay that each agent has for a particular location. This generates a Logit model, conditional on the location, that can be estimated via maximum likelihood. The estimation process assumes that every located agent was the best bidder for the location. However, since the under-determined nature of the Logit model does not allow to find absolute estimates of the willingness to pay, Ellickson’s model is only able to estimate relative rents and relative willingness to pay for groups of homogeneous agents. Improving on Ellickson’s work, Lerman and Kern (1983) proposed a method that maximizes the likelihood of an agent being the best bidder for his observed location while, simultaneously, maximizing the likelihood of his bid being equal to the observed transaction price. This method solves the original problem of under-determination in Ellickson’s approach, generating absolute estimates of rents or prices and the associated willingness to pay for the location attributes. However, implementing Lerman and Kern’s approach requires information that, in general, is not easy to collect: the actual price or rent paid for a particular real estate good, together with the attributes of the best bidder and the location or good itself . Moreover, as in the case of Ellickson’s approach, the method imposes a simplification of the bid function, aggregating agents into homogeneous groups of bidders and estimating a single, linear in parameters, bid function for each of them. The simultaneous location choice and price estimation method of Lerman and Kern has been applied, among others, by Gross (1988), Gross et al. (1990), Gin and Sonstelie (1992), McMillen (1997) and Chattopadhyay (1998) to estimate bid-rent functions in several urban case studies. This literature shows that, in general, accounting for the location preferences of consumers (as it is done in the bid-auction approach) generates better results than hedonic price models, thanks to the possibility of estimating the willingness to pay of different groups of agents and, therefore, providing information about consumer behavior. Despite this, the bid-auction approach has not been extensively applied due to a more complex estimation process than standard hedonic models and the already mentioned expensive data requirements. Moreover, the emphasis has been put in estimation of prices and marginal willingness to pay, giving little attention to the location choice distribution and with scarce validation of the resulting model when forecasting prices or locations. Muto (2006) analyzed location choice results when using Lerman and Kern’s method, finding significant and systematic deviations in the results when compared with observed location distributions for the city of Tokyo. This result suggests that, while Lerman and Kern improve over Ellickson’s model by estimating absolute rents, it does so at the cost of worse location forecast capabilities. The bid-auction approach is particularly attractive for location choice modeling since it provides an explicit explanation of the market clearing process that generates the transaction prices (or rents in the case of the rental market) of real estate. This has motivated the development of several land use models that base their location choice process on the bid-auction approach. Examples of this are RURBAN (Miyamoto and Kitazume 1989), MUSSA (Martínez 1996), IRPUD (Wegener 2008) ILUTE (Salvini and Miller 2005) and, to some extent, UrbanSim (Waddell et al. 2003). In these models, the bid-auction approach has been applied with a focus on modeling the spatial distribution of agents (households and firms) in a city, most of the times using Ellickson’s approach to find the relative willingness to pay of different households for the attributes of a location. In these models, if done, the adjustment of the bid functions to absolute levels is done in the context of a market clearing process, separated from the original estimation. Besides the theoretical appealing, the bid-auction approach is attractive for location choice modeling from an econometric point of view, because it does not have the price endogeneity problems usually found when using the choice approach. Endogeneity occurs because the price is highly correlated with unobserved attributes of the location, therefore complicating the estimation of parameters. In the worst case, if descriptive attributes of the location are omitted, price endogeneity may lead to wrong estimates of the price elasticity and proper estimation will require the use of correcting mechanisms like the Control Function method (Guevara and Ben-Akiva 2006). Because the price of the location does not enter the bid function as a variable, the bid-auction approach does not present price endogeneity issues. The relevance and advantages of the bid-auction approach motivates the search for estimation methods that allow for consistent estimation of both location choice (maximum bid probabilities) and price distributions without the need of individual level price data. At the same time it is interesting to explore the possibility of estimating bid-rent models where the bidding agents don’t have to be aggregated in homogeneous groups or regimes and where bid functions are not constrained to be linear in parameters. This paper proposes a method for the estimation of bid functions that maximizes the likelihood of the observed maximum bids while simultaneously adjusting the bid levels to observed average prices or zonal price indicators. The main assumption behind the proposed method is that, as observed many times in practice, real estate goods are traded in auctions that don’t take place explicitly. This implies that the outcome of the auction (the expected maximum bid) is a latent construct that can not be observed but is, however, structurally related to the transaction price. This assumption implies that the potential bid of all agents affects the final price of a real estate good, regardless if they are active in the market (currently looking for a location) or not. The structure of the proposed model is inspired by the Generalized Random Utility Model (Walker and Ben-Akiva 2002) and defines structural and measurement relationships for two latent variables: the bid of each agent and the expected outcome of an unobserved auction (that we call auction price). The bid is, as usual, structurally dependent on characteristics of the decision maker and attributes of the location and it is measured by a non-linear regression over observed location choices. The auction price is structurally defined as the expected maximum bid in the auction and therefore depends on the bids of all agents. It is measured through a linear regression over observed average zonal prices. The paper is organized as follows: Section 2 describes the bid-auction approach to location choice modeling. Section 3 reviews the literature on estimation of bid-rent function and analyzes the advantages and drawbacks of the different existing methods. Section 4 describes the method proposed in this paper and Section 5 describes a case study where the method is implemented, validated and compared with other methods. Finally, Section 6 concludes the paper and identifies future lines of research.",13
14.0,1.0,Networks and Spatial Economics,14 September 2013,https://link.springer.com/article/10.1007/s11067-013-9202-x,Location-Dependent Lane-Changing Behavior for Arterial Road Traffic,March 2014,HongSheng Qi,DianHai Wang,YiMing Bie,,Unknown,,Mix,,
14.0,1.0,Networks and Spatial Economics,12 October 2013,https://link.springer.com/article/10.1007/s11067-013-9208-4,Robust Facility Location Problem for Hazardous Waste Transportation,March 2014,Paul G. Berglund,Changhyun Kwon,,Male,Unknown,Unknown,Male,"Hazardous wastes are generated by a large variety of commercial and industrial processes, on both small and large scale, dispersed throughout developed areas. They vary from the small amounts of waste generated by urban businesses such as dry cleaners and auto repair shops, to larger amounts produced by chemical plants and other heavy industries. For the most part, the producers of waste are responsible for arranging to dispose of it at appropriate processing or long-term storage facilities. The shippers of such waste typically make their own choices about which routes to follow to transport waste to the processing site. These decisions are therefore out of the control of planners, though route choices may be influenced by tolls or road bans. In this paper, we formulate a mathematical optimization model that combines location and routing decisions for hazardous waste facilities under uncertainty. The literature on location and routing problems related to hazardous materials is extensive. An earlier survey of the area is Erkut and Neuman (1989). Alumur and Kara (2007) also include a fairly substantial literature review, though their work is not a survey as such. There are many different ways of approaching problems related to what are generally termed “obnoxious” facilities. Such facilities are necessary, but the facilities as well as transport going to and/or from them are undesirable, potentially dangerous or both. Because of this it will be undesirable to locate these facilities in certain areas. However, for a facility to be useful as well as to reduce the undesirable effects of transporting hazardous materials over long distances, it must be within a reasonable distance of the need it serves. As a result, locating such facilities represents a trade off between the desire to avoid risk and “obnoxiousness”, which could result in the facility being located in some very remote area, and the desire for the facility to be cost effective, which would mean locating it in reasonable proximity to the demand it serves. For obnoxious facility location and routing models to address these conflicting objectives, they may optimize a weighted combination of cost and risk (as well as perhaps other objectives such as fairness), they may minimize risks subject to keeping cost below a given threshold, or minimize cost subject to keeping risk below some maximum acceptable level. There are also a variety of approaches when both facility location and routing are concerned in the context of hazardous materials (hazmat). Some studies assume facility locations are given and solve a routing problem, and some solve a combined location and routing problem. The first study on combined location-routing decisions may be Zografos and Samara (1989), who propose a goal-programming approach to minimize multi-objectives of travel time, transportation risk, and disposal risk. ReVelle et al. (1991) choose sites and routes so as to minimize a convex combination of distance traveled and population exposure. List and Mirchandani (1991) consider equity for a combined location-routing problem, and Jacobs and Warmerdam (1994) use a linear programming based model for both the location of processing sites and the routing of hazardous waste for a single type of hazardous waste. Stowers and Palekar (1993) propose a combined model that minimizes the population exposure from transportation and long-term storage of hazardous wastes. Current and Ratick (1995) propose a multiobjective, mixed integer program considering minimization of cost and risk, and maximization of equity, and Giannikos (1998) presents a goal programming model considering minimization of operating cost and perceived risk, and equitable distribution of risk and disutility. Nema and Gupta (1999) create a model for the location of sites and the routing of waste with a composite objective function weighting risk and cost for multiple waste types. Cappanera et al. (2003) propose a discrete combined location-routing model that is referred as the Obnoxious Facility Location and Routing (OFLR) model and solved by a Lagrangean relaxation. Alumur and Kara (2007) use a multi-objective model (cost and exposure) for a deterministic problem where they find optimal sites for waste processing and disposal, and optimal routes between waste generation points, processing sites and disposal sites. There are also related hazmat network-design problems that consider routing when the locations of origin-destination pairs are given. Kara and Verter (2004) assume that origin and destination points for hazardous material shipments are known, and address the problem of minimizing exposure risk by banning hazardous shipments from traveling on certain network arcs. Garrido (2008) and Marcotte et al. (2009) approach similar problems not by a road ban method but by road pricing, and Wang et al. (2012) extend the concept with dual-toll pricing policy. Erkut and Alp (2007) create a minimal road network (a tree or set of trees) for hazmat transport, then use a greedy algorithm to augment the network with links which will decrease cost and exposure risk. Erkut and Gzara (2008) provide a bi-level optimization approach for hazmat network design and propose a heuristic method. Bianco et al. (2009) consider a similar bi-level approach and Gzara (2013) provides a cutting-plane algorithm for hazmat network design. Bianco et al. (2013) overview the hazmat network design literature. Hazmat network design problems are often referred as global routing problems as opposed to local routing problems that determine a safe path between a single origin-destination pair. Literature on local hazmat routing is abundant. We refer readers to a handbook chapter by Erkut et al. (2007) and references therein. A key to any local routing method is how to assess risk in a path. Various risk measures are introduced and corresponding optimization methods are devised. Erkut and Verter (1998) and Erkut and Ingolfsson (2005) discuss axioms that risk measures in hazmat routing are recommended to satisfy. For risk-averse routing, Bell (2006) considers mixed route strategies, and advanced risk measures such as value-at-risk and conditional value-at-risk are also considered in Kang et al. (2013) and Toumazis et al. (2013). A robust measure considering the worst-case is also introduced in Kwon et al. (2013). This paper inherits the modeling components of the OFLR model of Cappanera et al. (2003). However, the proposed model of this paper has a unique modeling component that is not found in other combined location-routing models: independent behavior of hazmat carriers. While other combined models simply assign a path to each shipment, we assume that hazmat carriers are independent entities from the hazardous waste facility operators. When carriers and operators are the same, assigning a path to each shipment is possible. On the other hand, when the two groups are independent, such assignments are inappropriate or often impossible. Therefore we assume that the hazmat carriers select the shortest-path based on distances only. This modeling approach is common in the above-mentioned, more recent hazmat network design literature (Kara and Verter 2004, Erkut and Alp 2007, Erkut and Gzara 2008, Marcotte et al. 2009, Bianco et al. 2009, Gzara 2013). Therefore the proposed model in this paper has a similar model structure including bi-level mixed integer programming. In the non-hazmat context, the location-routing problems are also well-studied. Earlier problems are reported in Franca and Luna (1982) and Laporte (1988), and comprehensive reviews are provided by Min et al. (1998) and Nagy and Salhi (2007). Stochastic location-routing problems are studied in Laporte et al. (1989) and in Albareda-Sambola et al. (2007) where the location decision and a priori routing decision are made in the first-stage and the routing recourse are determined in the second-stage. More recent studies on the combined location-routing problems include Shen (2007), Klibi et al. (2010), Bozkaya et al. (2010), and Toyoglu et al. (2012). We also refer the readers to Snyder (2006) for a review of facility location problems under uncertainty. In hazmat location and routing problems, methods based on stochastic programming are less effective. This is because historical data are rarely sufficient to construct meaningful probability distributions for modeling parameters such as accident consequences. Although there are many accidents involving hazmat in the entire network, hazmat accidents are still rare events for each road segment or for each region. In addition, the consequences depend on various uncertain factors such as weather conditions, the number of people involved at the accident, the evacuation effectiveness, and the severity of the accident. Such uncertain factors make a probabilistic estimation of accident consequences difficult. This property makes a robust optimization approach more appropriate. To the best of our knowledge, this paper is the first attempt to solve a facility location and routing problem involving uncertainty by an robust optimization approach in hazardous waste management. A noxious facility location problem under uncertainty (Killmer et al. 2001) has been studied, but not combined with routing. Robust optimization approaches to location problems, which are not combined with routing decisions, in the non-hazmat context include Averbakh and Berman (1997, 2000a, b), Carrizosa and Nickel (2003), Baron et al. (2011), and Gülpınar et al. (2013). However for the combined problems there seems to be no research using robust optimization reported in the literature. It may be because applying robust optimization methods to combined problems considering both demand uncertainty and transportation cost uncertainty requires treatment of multiplicative uncertain parameters, and an appropriate approach has only recently been proposed by Kwon et al. (2013). In addition, most robust optimization approaches consider only demand uncertainty. While demand uncertainty is the primary source of uncertainty in many location and transportation problems, transportation risk (transportation cost in general) is also a significant source of uncertainty in the hazmat context. In most cases, hazmat facility location-routing problems are formulated as a 0-1 mixed integer program and solved by commercial optimization solvers like CPLEX or LINDO. One notable exception is the OFLR model by Cappanera et al. (2003) who solve the problem by a Lagrangean relaxation. In this paper, we provide a 0-1 mixed integer programming formulation that is solvable by CPLEX when the problem size is small. Sometimes we find that CPLEX fails to solve the problem even when the problem size is as small as 90-node. For such cases, we propose a genetic algorithm. While Lagrangean relaxation methods are popularly used for location problems and for the OFLR model, it seems inappropriate for the problem in this paper as the problem structure involves two additional levels: one to model carriers’ behavior and the other to consider robustness. In summary, the unique characteristics of the proposed model and the contributions of this paper are as follows: 
 This research studies a hazardous waste facility location problems under demand and risk uncertainty. This research applies a robust optimization methodology for a combined location-routing problem. This research models independent route-choice behavior of hazmat carriers. This research provides a single-level mixed-integer program reformulation of the proposed location-routing model that is solvable by a commercial solver when the problem size is small. A genetic algorithm is proposed for large-scale problems. For the present work we will assume that the origin points for hazardous waste are known, but the destination points (waste treatment or disposal sites) are not. We will then find the set of locations for waste processing sites which minimizes a linear combination of fixed facility cost and the risk posed by the shipment of hazardous wastes (e.g. due to spills). We will assume that the shipment of the waste will be done by third-party carriers who seek to minimize their own costs (that is, that they will take the shortest path to the nearest waste processing facility), and that they are not affected by tolls or road bans. (Or rather, if tolls or road bans do exist they are a fait accompli reflected in the existing arc costs of our network rather than something we need to calculate ourselves.)",44
14.0,1.0,Networks and Spatial Economics,29 January 2014,https://link.springer.com/article/10.1007/s11067-013-9217-3,Solving a Location Problem of a Stackelberg Firm Competing with Cournot-Nash Firms,March 2014,Paul G. Berglund,Changhyun Kwon,,Male,Unknown,Unknown,Male,"The choice of locations for manufacturing facilities is an important strategic decision. Such choices involve large expenditures of capital and are difficult and expensive to change. A poor choice will involve significant negative consequences as the manufacturer either suffers the expense of relocating a facility or else lives with ongoing disadvantages brought about by the suboptimal location of that facility. Because of the importance of the problem, it has been extensively studied and the associated literature is vast. A good overall survey is Hale and Moberg (2003). A significant portion of the literature is concerned with competitive facility location, i.e., locating facilities so as to maximize profits or market share in a competitive situation. Some relevant research articles and surveys of the competitive facility location literature are Friesz et al. (1988), Eiselt et al. (1993), Serra and ReVelle (1999), Plastria (2001), Plastria and Vanhaverbeke (2007), Santos-Peñate et al. (2007), and most recently Kress and Pesch (2011). However much of the literature dealing with facility location subject to competition deals with the location of retail or similar facilities where customers are distributed in some way and will choose a conveniently located facility following some rule e.g., a “gravity model” (A review of such literature is found in Dobson and Kamarkar (1985)). Models appropriate to the location of manufacturing facilities are necessarily different. Rather than placing retail or similar facilities in proximity to demand points, so as to maximize utilization or market share (essentially competing on the basis of proximity), the firm places its manufacturing facilities so as to be able to compete on the basis of cost. That is, facilities are located so as to enable the firm to deliver its products to market as cheaply as possible, by controlling both manufacturing costs, which may vary from candidate location to candidate location, and transportation costs from the manufacturing facility to the markets it serves. Another factor which must be taken into account is market competition. If the market for a particular commodity is controlled by a small number of competing firms (oligopoly) then a firm must take into account the reactions of its competitors to any planned move. (In a monopolistic situation there is no competition to consider, and where the number of competitors is sufficiently large the actions of competitors do not need to be taken into account.) A firm must expect that its competitors will react to whatever it does (in such a way as to maximize their own profits), and by their actions change market conditions, especially prices, by increasing or decreasing the supply of goods available at different markets. In order to evaluate a potential strategy, a firm must calculate how the competition will react to that strategy. This calculation may be done by computing an equilibrium, termed a “Cournot-Nash-Stackelberg” equilibrium (Sherali et al. 1983). The firm choosing a site for a new facility (referred to as the “Stackelberg player”) first makes its choice of sites for establishing manufacturing facilities, what production levels to maintain at each facility and where to sell the materials so produced. Its competitors (“Cournot players”) react, maximizing their own profits, until an equilibrium is reached. In order to find the best possible solution to the manufacturing facility location problem, a firm must therefore solve an optimization problem which incorporates a set of equilibrium constraints to model the response by the competition. See Miller et al. (1996) for more discussion on this topic. The basic structure of the manufacturing facility location problem is therefore to choose locations for manufacturing facilities such that the firm’s profits are maximized at the resulting market equilibrium. The literature on the subject of market equilibrium modeling and its computation and analysis is fairly substantial. Among many research papers on the oligopolistic competition models since Cournot (1838), we want to mention two particular papers. Miller et al. (1991) considered a variational inequality formulation of an oligopolistic competition where firms compete with production quantities and and shipping amounts between markets and production facilities. Miller et al. (1991) assumed that the market price is determined by an inverse demand function, as in Cournot (1838). Later, Friesz et al. (2006) extended the model to consider dynamic environments and formulated the equilibrium problem as a differential variational inequality. On the other hand, when we consider spatially separated markets for a single product, there is another approach to determine the market price of the product. It is based on the spatial price equilibrium models of Samuelson (1952) and Takayama and Judge (1971). Variational inequalities, or some other equivalent forms, are typically used for modeling the oligopolistic equilibrium. Among many approaches that use variational inequalities for analysis and computation, there are Dafermos and Nagurney (1984), Harker (1986), and Nagurney (1987) in this category. The existence and uniqueness results were provided for the variational inequality models, and the computational results are also provided. See Nagurney (1998) for review. The relevant literature that considers location decisions in the form of Stackelberg-Cournot-Nash games begins with Sherali et al. (1983), who considers firms that compete on production quantities. They extended the classic Cournot games by introducing a Stackelberg firm who determines the production quantity by explicitly considering the other firms reaction. In their model, the market price was determined by an inverse demand function based on the total market production quantity. The firm who introduces a new facility is regarded as a leader, and all other market oligopolists are followers; therefore, Stackelberg-Cournot-Nash games are often called leader-follower games. In the line of the location decision model of Sherali et al. (1983) and the spatial model of Miller et al. (1991), Tobin et al. (1995) considered a Stackelberg-Cournot-Nash game where a leader firm introduces a new facility and other firms compete with production quantities and shipping decisions to markets. Later, Miller et al. (2007) further extended to consider a multi-period decision making problem. All three papers modeled the market prices using inverse demand functions. This category of problems is of this paper’s interest. In particular, we will use the modeling framework of Tobin et al. (1995), and propose a heuristic algorithm based on simulated annealing. While the underlying oligopoly models are well studied both analytically and computationally, the location decision problems are generally very hard to solve. Usually, such a problem is modeled as a mathematical program with equilibrium constraints (MPEC) (Luo et al. 1996). Both Tobin et al. (1995) and Miller et al. (2007) used a full enumeration of all possible locations to determine an optimal solution that maximizes the leader firm’s profit resulting from the competition. In this paper, we will investigate if an optimal solution could be obtained more efficiently. There are also location-problem counterparts for the spatial price equilibrium models. Tobin and Friesz (1986) built a location decision making problem based on the spatial price equilibrium of Friesz et al. (1984). Friesz et al. (1989) provided existence results, and Miller et al. (1991) proposed a heuristic method that approximate the spatial price equilibrium processes, which uses sensitivity analysis of the market equilibrium to determine an optimal location. The method requires solving a nonlinear integer programming problem in each main iteration. A summary of the above mentioned relevant models is provided in Table 1. As we have already discussed above, we consider two factors. The first column in the table represents research that uses inverse demand functions to model market prices depending on the total supply to the markets, while the second column represents research that uses spatial price equilibrium models using the Takayama and Judge (1971) approach. The first row is the Cournot-Nash model by Cournot (1838), and the second row provides Cournot-Nash models with spatially separated markets and shipping decisions. The third row is the Stakelberg-Cournot-Nash extension of Cournot (1838), and the last row provides those including shipping decisions.
 It is worth mentioning Konur and Geunes (2012) that consider a location game between firms. While only one firm decides a location all in the above-mentioned research, all competing firms decide their own locations considering the resulting oligopolistic competition. Konur and Geunes (2012) consider spatially separated markets and shipping between markets and facilities in the presence of traffic congestion. They used inverse demand functions to model the market prices. Their model is not a Stackelberg model, as there is no leader. However, to find an equilibrium, they used a two-stage approach, where the first stage determines a location equilibrium and the second stage determines a production-shipping equilibrium. This is structurally similar to the Stackelberg-Cournot-Nash models in Table 1 in the sense that, for any given first stage result, i.e., locations, Konur and Geunes (2012) use a variational inequality model to determine the second stage solution. To determine the best response for the first stage location decision, they use a full enumeration for each firm. As is in Tobin et al. (1995), Miller et al. (2007), and Konur and Geunes (2012), the full enumeration is a popular choice of methods to find an optimal location to place a new facility. When the number of all location candidates is small, full enumerations work fine. However, when there are many location candidates, a full enumeration would be computationally very heavy, because a variational inequality problem, whose size would be also very large, needs to be solved for each location candidate. In this paper, we attempt to reduce the number of variational inequality problems by search the set of locations by simulated annealing. The remainder of this paper is organized as follows. In Section 2, we describe the Stackelberg-Cournot-Nash model we consider for locating a facility in the form of a mixed integer program with equilibrium constraints, where the lower-level equilibrium problem is formulated as a variational inequality. In Section 3, we first consider a branch-and-bound method for the KKT conditions of the lower-level equilibrium problem and present a full enumeration approach using a fixed-point algorithm for solving the lower-level variational inequality problem. In Section 4, we propose a simulated annealing algorithm and test it in a test network. In Section 5, we conclude this paper with some remarks.",13
14.0,2.0,Networks and Spatial Economics,06 October 2013,https://link.springer.com/article/10.1007/s11067-013-9209-3,Design of Sustainable Cordon Toll Pricing Schemes in a Monocentric City,June 2014,Zhi-Chun Li,Ya-Dong Wang,Keechoo Choi,,,Unknown,Mix,,
14.0,2.0,Networks and Spatial Economics,13 October 2013,https://link.springer.com/article/10.1007/s11067-013-9214-6,A Sampling-Based Stochastic Winner Determination Model for Truckload Service Procurement,June 2014,Bo Zhang,Hongwei Ding,Tao Yao,Male,Unknown,,Mix,,
14.0,2.0,Networks and Spatial Economics,26 November 2013,https://link.springer.com/article/10.1007/s11067-013-9215-5,Sensitivity Analysis of User Equilibrium Flows Revisited,June 2014,Byung Do Chung,Hsun-Jung Cho,Tao Yao,,Unknown,,Mix,,
14.0,2.0,Networks and Spatial Economics,29 January 2014,https://link.springer.com/article/10.1007/s11067-013-9216-4,A Rolling Optimisation Model of the UK Natural Gas Market,June 2014,Mel T. Devine,James P. Gleeson,David M. Ramsey,,Male,Male,Mix,,
14.0,2.0,Networks and Spatial Economics,23 January 2014,https://link.springer.com/article/10.1007/s11067-013-9218-2,A Dual Approach for Solving the Combined Distribution and Assignment Problem with Link Capacity Constraints,June 2014,Seungkyu Ryu,Anthony Chen,Keechoo Choi,Unknown,Male,Unknown,Male,"As an integral component of urban transportation system, various types of side constraints are typically required in order to improve realism in the travel demand forecasting procedure. For example, to consider the queuing effect at signalized intersections or behind bottlenecks (e.g., bridges, tunnels, or when two or more roadways merge into a single roadway section), imposing physical link capacity constraints may be a better choice compared to modifying the link travel time functions. On the other hand, to reflect the environmental requirements imposed by the government, certain environmental constraints can also be applied to restrict vehicles entering certain areas (e.g., central business district). Hearn and Ribera (1980) suggested that modifying the traffic assignment procedure by adding side constraints (e.g., link capacity constraints) is a good approach to obtain reasonable traffic flow pattern in a road network. Side constraints, in general, describe the limited supply of certain resources (e.g., link capacities) in a road network which are shared by several activities (e.g., link flows), or certain natural or technological restrictions (e.g., signal control, emission restraints, etc.). By imposing side constraints, the effect of different traffic control policies can be described, the existing traffic equilibrium models can be improved, and the flow restrictions that the central authority wishes to impose upon the network users can be described (Larsson and Patriksson 1999). According to Larsson and Patriksson (1999), side constraints can be distinguished into two principally different types in the traffic assignment models: prescriptive (hard) and descriptive (soft). The prescriptive side constraints are typically imposed upon the users of traffic system. It strictly constrains the traffic flows and it is used for traffic management and control policies (e.g., traffic signals with fixed cycle times). On the other hand, descriptive side constraints are introduced to refine additional traffic flow restrictions into the model (e.g., joint capacities in roundabouts, merging capacities, etc.), to better model congestion effect, and to incorporate some a priori knowledge of traffic flow (e.g., replicating observed flows on some links and/or intersections within certain error bounds). Modeling side constraints in the traffic assignment problem has been studied in various areas as summarized below: Queuing and congestion effects (Bell 1995; Yang and Lam 1996; Larsson et al. 2004; Chen et al. 2011) Link toll pricing (Ferrari 1995; Yang and Lam 1996; Yang and Bell 1997) Restraining traffic flows (Ferrari 1995; Yang and Bell 1997; Chen et al. 2011) Replicating observed flows (Bell et al. 1997; Chen et al. 2005, 2009, 2010, 2012a; Chootinan et al. 2005) Environmental side constraints (Ferrari 1995; Chen et al. 2011) Although side constraints have been incorporated into various applications, most studies focused on the modeling of side constraints in the traffic assignment problem following Wardrop’s first principle (Wardrop 1952). In contrast, only a few studies considered side constraints in the combined distribution and assignment (CDA) problem in the literature (e.g., Tam and Lam 2000, 2004; Li et al. 2007). Note that CDA models have several applications in transportation systems analysis. Among others, the following applications need to use CDA models to capture travelers’ route choice and destination choice behaviors simultaneously. 
Network design problem (NDP). For example, in the bi-level program (BLP) developed by Lin and Feng (2003) for the land use-NDP, a CDA model was used to simulate travel demands according to the land use distribution and network layout. Also, in the BLP developed by Yim et al. (2011) for the reliability-based land use (including residential and job allocations) and NDP, a CDA model was used to map the land-use pattern to the link-loading pattern in the network. 
Analysis of network capacity, capacity reliability, and capacity flexibility. For example, Yang et al. (2000) used a CDA model as the lower-level subprogram in a BLP model to determine the network capacity and level of service. Chen et al. (2013) and Chen and Kasikitwiwat (2011) further used the CDA model in modeling capacity reliability and capacity flexibility, respectively. 
Taxi operation modeling. For example, Wong et al. (2001, 2008) developed a BLP model for taxi movements in congested road networks. A CDA model was used to describe the simultaneous movements of vacant and occupied taxis as well as normal traffic for a given total customer generation from each origin and a given total attraction to each destination. For additional information of the combined network equilibrium models, readers are directed to Boyce and Bar-Gera (2004) and Hasan and Dashti (2007) on the multiclass combined models, Briceño et al. (2008) on an integrated behavioral model of land-use and transport system, de Grange et al. (2010) on the calibration and spatial aggregation of combined models, Chen et al. (2007) on the network-based accessibility measures using the combined travel demand model, Siefel et al. (2006) on the comparison of urban travel forecasts between the sequential four-step model and the combined travel demand model, and Boyce (2007) on a general review of network equilibrium models for travel demand forecasting. In a few existing studies considered side constraints in the CDA problem (e.g., Tam and Lam 2000, 2004; Li et al. 2007), travelers’ route choice behavior (trip assignment) is characterized by the deterministic user equilibrium (DUE) model, whereas the destination choice behavior (trip distribution) is governed by a multinomial logit model (or an entropy model). There exist some behavioral inconsistencies between these two travel choices. In terms of solution algorithm, the penalty method was adopted to handle the side constraints (see Li et al. 2007). As is well known, the penalty method may have difficulty converging if inappropriate penalty value is used. Motivated by the above observations, we consider the CDA problem with link capacity constraints by extending Bell (1995)’s logit assignment model to a hierarchical logit-based distribution and assignment model (i.e., both destination choice and route choice are governed by the logit model in a hierarchical structure) with link capacity constraints, and further developing Bell’s approach based on the dual formulation as a solution algorithm that explicitly makes use of the optimality conditions to analytically adjust the dual variables and update the primal variables (i.e., route flows and O-D flows) along with a column generation scheme. Specifically, the logit-based CDA model based on random utility theory resolves the behavioral choice inconsistencies of the existing CDA models and enhances the realism of the CDA model by explicitly incorporating physical capacity restrictions as side constraints. Different from directly solving the SC-CDA problem (e.g., penalty method), a dual MP formulation is developed as a solution algorithm for solving the SC-CDA problem. Due to the entropy-type objective function, the dual formulation of the SC-CDA problem has a simple nonlinear constrained optimization structure, where the feasible set only consists of nonnegative orthants. The proposed solution algorithm consists of an iterative balancing scheme and a column generation scheme. The iterative balancing scheme explicitly makes use of the optimality conditions of the dual formulation to analytically adjust the dual variables and update the primal variables, while a column generation scheme is used to iteratively generate routes to the working route set as needed to satisfy the side constraints. Two numerical experiments are also conducted to demonstrate the features of the SC-CDA model and the computational performance of the solution algorithm. The remainder of the paper is organized as follows. After the introduction, the next section provides the MP formulation of the SC-CDA problem. Next, the dual MP formulation of the SC-CDA problem is provided to develop a solution algorithm for solving the SC-CDA problem. Finally, numerical results are presented along with some concluding remarks.",23
14.0,2.0,Networks and Spatial Economics,24 January 2014,https://link.springer.com/article/10.1007/s11067-013-9219-1,A Stochastic Optimization Model to Reduce Expected Post-Disaster Response Time Through Pre-Disaster Investment Decisions,June 2014,Lili Du,Srinivas Peeta,,Female,Male,Unknown,Mix,,
14.0,3.0,Networks and Spatial Economics,23 December 2013,https://link.springer.com/article/10.1007/s11067-013-9222-6,Spatial Science and Network Science: Review and Outcomes of a Complex Relationship,December 2014,César Ducruet,Laurent Beauguitte,,Male,Male,Unknown,Male,"A rapid surge of interest in networks in the late 1990s throughout natural and social sciences has witnessed the emergence and diffusion of new concepts and measures. The sudden interest by physicists in network analysis in the late 1990s principally provided models of networks based on two main dimensions: the small-world network (hereafter SWN), based on average distance path and density of neighborhoods (Watts and Strogatz 1998; Watts 2003), and the scale-free network (SFN) based on the hierarchy of hubs (Barabási and Albert 1999). SFN and SWN have quickly spread across various disciplines and scientific fields (Newman et al. 2006). While physicists have increasingly integrated the spatial dimension in their works (Barthélemy 2010), geographers and regional scientists seem to have so far paid rather limited attention to complex networks research. Far from interpreting this state of affairs as a weakness, this paper proposes a review of existing applications. This review takes its inspiration from earlier ones by Borgatti et al. (2009) and Crossley (2005, 2008) about sociology as well as Alderson (2008) on operations research and Ducruet and Lugo (2013a) on transport networks and systems of cities. We examine how the most popular theoretical models of networks proposed by physicists (i.e. scale-free and small-world) have been integrated in the works of geographers and regional scientists, assess what have been the benefits and speculate whether such concepts are likely to increase their influence in further works. In a first attempt to evaluate the benefits of these approaches in geography, Rozenblat and Mélançon (2007) noticed that “this type of empirical approach combining a conceptual approach of ‘small world theory’ and dedicated tools has not been developed in geography”. Although more recent contributions from regional science do support the necessity for further bridging “spatial economic science” with “network science” (Reggiani 2011), little has been done in classifying existing applications of complex network theory in geographical research. The structure of the paper is as follows. The first section briefly recalls the evolution of network analysis in geography and regional science. Secondly, we provide a short review on the emergence of complex network research, followed by a more scrutinizing look at the influence of space in the works of physicists. The third section examines how geographers and regional scientists have used these measures and concepts in their works either independently or through collaborations with physicists and computer scientists. In conclusion, we discuss differences in respective approaches and potential paths for further research.",133
14.0,3.0,Networks and Spatial Economics,18 April 2014,https://link.springer.com/article/10.1007/s11067-014-9223-0,Capacity Consumption Analysis Using Heuristic Solution Method for Under Construction Railway Routes,December 2014,Masoud Yaghini,Mohammadreza Sarmadi,Mohsen Momeni,Male,Unknown,Male,Male,"In railways, calculation and utilization of the capacity is very important for the different operational tasks. Geometrical configuration of the tracks, line and stations and their layouts, features of signaling systems, movement rules and corresponding minimum distance between trains, operation and maintenance planning and quality measures have direct influence on the volume of the capacity. Difficulties include the numerous interacting/interrelated factors, the complex structure of the railway layout, and the magnitude of terminology required (Kontaxi and Ricci 2011). Therefore, different definitions of capacity are used in the railway literature. Theoretical capacity is the number of trains that could run over a route during a time window, in a strictly perfect, mathematically generated environment with the trains running permanently and ideally at minimum headway time. Practical capacity is the practical limit of representative traffic volume that can be moved on a route at a reasonable level of reliability. Used capacity is the actual traffic volume occurring over the network. It reflects actual traffic and operations that occur on the route. It is usually lower than the practical capacity. Available capacity is the difference between the used and the practical capacity (Abril et al. 2008). Several methods have been developed to calculate and evaluate railway route capacity. They can be classified into four categories of analytical, simulation, parametric and optimization methods (Abril et al. 2008; Pachl and White 2004; Khadem-Sameni et al. 2010). Analytical methods are conducted by calculating headway time from the infrastructure and timetable characteristics to determine and describe the capacity of a railway route (UIC Leaflet 405-1 1983; UIC Leaflet 406 2004; Burdett and Kozan 2006; Landex 2009). Analytical methods are quick but provide limited information for capacity evaluation since the only output is capacity value. Simulation methods, such as RailSys, simulate the running operation and evaluate the capacity (Khadem-Sameni et al. 2010). Simulation models are more realistic but data intensive and computationally difficult (Pachl and White 2004; Khadem-Sameni et al. 2010). Because the detailed running operation data such as length of line sections, velocity of trains, movement rules and geometrical configuration considered as inputs. In these models, timetables are required. The parametric methods measure the capacity of line segment by predicting the train delay vs. capacity curve of a line segment (Krueger 1999). Parametric methods (Krueger 1999; Lai and Barkan 2009; Lai and Barkan 2011) model capacity by parametric relationships between infrastructure, traffic and operating parameters. Parametric models that are use the results of simulation running for creating of train delay vs. capacity curve, come in between analytical and simulation models, are very sensitive to parameter inputs and train mix variations and are not based on timetables. Optimization methods are based on obtaining optimal compressed timetables. These optimal timetables are usually obtained by using mathematical programming techniques (Abril et al. 2008). Optimization methods such as Carey (1994), Carey and Lockwood (1995), Higgins et al. (1996), D’Ariano et al. (2007), Caimi et al. (2009), D’Ariano and Pranzo (2009) and Mu and Dessouky (2011) are designed to provide more strategic perspective for solving the railway capacity problem and consider various parameters that affect railway network capacity but the main input of optimization methods is an initial timetable. Train dispatching models belongs to this category of optimization methods. This method obtains route capacity by scheduling a maximum number of additional train services in a timetable (Abril et al. 2008). According to the actual infrastructure and timetable, the UIC 406 leaflet gives a method to measure the capacity consumption of line sections. This method defines a methodology to measure the capacity consumption based on compressing timetable graphs (UIC Leaflet 406 2004). A detailed timetable for under construction routes does not exist. Landex (2008) suggests evaluating the span of capacity consumption for railway routes without exact infrastructure and/or timetable to be done by also examining the minimum and maximum capacity consumptions by using simulation software. Simulation models need intensive data, which is not usually available in the planning stage. In the planning stage, for infrastructure managers, the value of maximum utilization of the route is important issue. Therefore, it is necessary to develop methods to calculate railway capacity consumption for under construction routes using data usually available to planners. The main contributions of this paper are (1) presenting an optimization model based on multicommodity network design problem for calculating the railway capacity consumption of under-construction routes, (2) proposing a heuristic solution method to solve the proposed model, (3) extending application of UIC 406 method in the proposed model, and (4) applying the proposed model and solution method for two case studies in Iran Railways. In the model, the aim of the capacity problem analysis is to maximize the number of trains over a rail route in a specific reference time. The model used in this paper does not require a timetable. The proposed solution method is based on the local branching algorithm. The parameters of the solution method are justified using design of experiments approach. The paper is organized as follows. A review of the related literature is presented in Section 2. The proposed railway capacity model is explained in Section 3. In Section 4, the local branching algorithm and the parameter tuning using design of experiments approach is discussed. The capacity consumption of two routes in Iran Railways are calculated and evaluated as case studies and the results are reported in Section 5. Section 6 provides the conclusions.",11
14.0,3.0,Networks and Spatial Economics,18 April 2014,https://link.springer.com/article/10.1007/s11067-014-9229-7,Welfare Effects of Subsidizing a Dead-End Network of Less Polluting Vehicles,December 2014,Antje-Mareike Dietrich,Gernot Sieg,,Unknown,Male,Unknown,Male,"Many governments, including those of Germany and France, have committed to reducing anthropogenic greenhouse gases. Because the transport sector is one of the largest producers of greenhouse gases, governments attempt to reduce vehicle emissions. To meet the European Union’s goal regarding climate change, namely a 20 percent reduction in greenhouse gas emissions by 2020 (compared to 1990), they advocate green technologies. However, whereas the French government introduced buyers’ premiums to promote certain green drivetrain technologies, such as electric vehicles, the German government is focusing on different research initiatives, emphasizing the possibility of future technological improvements. This article identifies possible causes of such different policy actions. The automobile industry is developing several alternatives to the established gasoline-driven internal combustion engine, such as fuel cells, battery-driven electro motors and biofuel-driven engines. So far, none of these alternative power trains has entered the mass market. Because the usability of a vehicle depends on the network of available service stations, there is a large lock-in effect favoring the established technology. Even if there were decreasing marginal production costs due to economies of scale, there would be a lack of infrastructure, leading to a technological lock-in situation. In many European countries, there are taxes on gasoline that, arguably, address greenhouse emission in Pigouvian style. Because a Pigou tax that internalizes external environmental effects does not account for the lock-in advantage of traditional technology, additional governmental intervention is often necessary to induce the adoption of new technologies (Sartzetakis and Tsigaris 2005). However, any new green technology, such as the battery-driven electro motor, can be replaced entirely by another and better innovation at a later time. Likewise, at the end of the nineteenth century, steam- and battery-driven vehicles dominated the nascent automobile market before the internal combustion engine superseded them. Therefore, even if the technical or environmental advantages of the current battery-driven technology are substantial, it may still constitute a dead-end technology, due to the future development of a better one. Consequently, the question arises as to whether it is reasonable to implement a technology that is dead-end. Would it not be sensible to simply wait for a better technology that may even be compatible with the established network of service stations? To answer this question, we analyze the interaction of service station networks, greenhouse gas emissions, and uncertain technological progress. We consider a scenario that sooner or later, a technology that is environmentally more sound and, furthermore, compatible with the established network will enter for the market. This could entail a technological leap in the internal combustion engine or a new generation of biofuels, for example, but any other innovation that is compatible with the traditional service station network is possible. Therefore we refer to the currently available clean technology as transitory or dead-end. From Sartzetakis and Tsigaris (2005), we know that for an open-ended technology, governmental intervention is socially desirable, if the environmental externality of the green technology is small relative to that of the established technology and/or if the network effect of the installed base of service stations is small. In this paper, we analyze not an open-ended, but a dead-end technology and show that even the implementation of a dead-end technology may be socially desirable. In other words, the argument that the available new technology may be only a transitory improvement, should not per se prevent its implementation. Furthermore, compared to the open-end case where only the reduction of emissions and the network effects count, for dead-end technologies, the social valuation of future payoffs also matter. If consumers and politicians discounting of future payoffs is high, the implementation even of dead-end technologies is sensible. Then, it is better not to wait, but to act now. Our methodical analysis relates to the literature on the economics of networks (Economides 1996; Birke 2009). In particular, our model is based on Farrell and Saloner (1986) and in particular follows Sartzetakis and Tsigaris (2005). The former demonstrate that, due to an installed base, network effects can lead to excess inertia so that a superior technology is not adopted. Sartzetakis and Tsigaris (2005) amend the aspect of environmental externalities and apply the model to the automobile sector. They analyze the conditions for a first-best Pigou solution for a framework in which a new technology reaches its matured network size. We supplement this literature by considering dead-end technologies. We compare a scenario with a new green and open-ended technology, which means that no further technology appears subsequently, to a scenario with a dead-end one, which is replaced by a better one. We find that only in the scenario with a dead-end technology, do preferences for future payoffs affect the implementation decision. Furthermore, our analysis relates to the wide range of literature on the technological transition to alternative-fuel vehicles (Nishihara 2010; Köhler et al. 2010; Schneider et al. 2004; Schwoon 2007; Struben and Sterman 2008). Whereas some authors, such as Bento (2010) only consider the importance of network effects for the adoption of a new technology, others also consider environmental externalities. For example, Conrad (2009) analyzes the optimal path of investment chosen by a service station owner, and Greaker and Heggedal (2010) model the adoption decision of consumers and loading station owners. Unlike these studies, we argue that even if there were economies of scale on the production side, there would still be a large lock-in effect on the consumer side. Here, we focus on indirect network effects and stress the risk of failing to use a welfare-enhancing technology. We clearly address the trade-off between network effects and environmental externalities for the government’s decision to support a new green drivetrain technology. Our results also add the Stern (2006)–Nordhaus (2007) argument that the discounting of future payoffs is essential for efficiently tackling climate change to the discussion on the technological transition to alternative-fuel vehicles.",1
14.0,3.0,Networks and Spatial Economics,22 April 2014,https://link.springer.com/article/10.1007/s11067-014-9230-1,An Analysis of Shipping Agreements: The Cooperative Container Network,December 2014,Simone Caschili,Francesca Medda,Claudio Ferrari,Female,Female,Male,Mix,,
14.0,3.0,Networks and Spatial Economics,22 April 2014,https://link.springer.com/article/10.1007/s11067-014-9231-0,The Accumulation of Empty Containers in Urban Areas: Policy Implications from a Stochastic Formulation,December 2014,Noel Pérez-Rodríguez,Jose Holguín-Veras,,Male,Male,Unknown,Male,"The volume of containerized import to the United States (US) exceeds the amount of export containers shipped from the US to foreign locations. The trade data show that, in 2008, while an estimate of 18.9 million 20 ft equivalent units (TEUs) were imported to the US, only 8.6 millions of TEUs were exported abroad (UNCTAD Secretariat 2009). This, in turn, produces a major imbalance in the flow of empty containers, with more empty units leaving the US than empty containers coming in. This imbalance is made more severe by the significant concentration of demand in the East Coast of the US that is the primary destination of approximately two thirds of the import. The predominantly consumer nature of East Coast attracts a huge influx of loaded containers that come through the gates of the Port of New York and New Jersey and Chicago (that serves as an intermodal connector to container flows coming from Asia through Los Angeles/Long Beach and Seattle/Tacoma) (Mongelluzzo 2000). Over the years, how this flow of empty containers is handled has experienced some significant changes. In the 1990s, for instance, a significant number of empty containers were sent from the East Coast to locations in the Midwest and the South, from where they departed loaded with low priority goods (such as cotton waste, recycled paper, metal scrap and other discards of post-industrial society) towards the West Coast and Asia (Holguín-Veras and Walton 1996). More recently, reflecting the declining interest from the Asians in these waste materials, some trans-Pacific carriers rejected this low-value cargo, as they prefer to send the containers back to Asia empty to facilitate a quick refill with better-paying shipments for the U.S. import market (Mongelluzzo 2005). The trade imbalance in containerized cargo has logistical implications, as it creates imbalances of marine containers, shortages of domestic trailers and chassis, and added costs for ocean carriers repositioning empty containers. The storage of empty containers in port facilities also increases labor costs and congests marine terminals. Container carriers have attempted to mitigate these problems by advance planning and better coordination in the use of empty containers. These strategies enable a carrier to borrow a container from a partner company, thus avoiding hauling of an empty container from a remote location. These approaches have greatly benefited from Internet based bulletin boards to speed up the process of identifying and exchanging containers. Such coordination efforts have also been applied within individual companies, with great success. Container operators try to optimize the repositioning of empty containers so that the equipment is ready to move the cargoes in the next time period, on the basis of the anticipated demand patterns. Although the majority of empty containers are promptly repositioned to other locations, a significant number of them are stored in areas surrounding the major US ports (Holusha 2001). This is not a recent or transient problem. For example, the number of empty containers in the US doubled in just 2 years, reaching as many as 500,000 units in 2001 (Machalaba 2001). A similar situation was found in Australia, Brazil, Uganda, Panamá, and The Netherlands, among many other countries, illustrating the global nature of this problem (Bromby 2003; Eksten 2003; Lloyd’s List 2003a, b; Urquhart 2007; Business Monitor International Staff 2009; JOC Staff 2013). In 2007, 2.5 million containers, or about 10 % of the global inventory, were being stored empty in locations around the world (Rodrigue et al. 2009). It has been reported that port authorities worldwide, in an effort to ease terminal congestion due to the large number of empty units, have limited the number of containers and/or the maximum storage time for containers in port facilities. For example, the container terminal at Southampton, one of Britain’s largest ports, imposed a limit on the number of empty containers that shippers could store at the port, after the trade imbalance of Britain’s trade with Asia left Southampton with 25,000 empty containers rather than the normal 12,000 (O’Connell 2005). Another example is observed in Nigeria, where the Minister of Transport imposed a fine of $200 per container per day to shipping companies failing to remove their empty containers from the ports in 48 h (Ugwoke 2005). Such restrictions, instead of solving the problem, tend to simply shift the problem to another port. The restrictions implemented in Southampton and Hamburg created a crisis at the port of Rotterdam which, in turn, had to implement restrictions to avoid a collapse in its operations (Urquhart 2007). Although the economic turmoil of the 2008–2013 period and the ensuing economic recession reduced the severity of the problem, all signs indicate that the empty container accumulation problem will resurface as soon as economic conditions improve. Different reasons have been offered to explain the accumulation of empty containers. Generally speaking, there is no agreement about the root causes of the problem. The paper discusses the three most frequently cited reasons: (1) low storage costs; (2) low purchase prices (of new containers); and (3) that storing empty containers in urban areas helps the container industry to deal with sudden shortages of empty containers. Regardless of the reason, it seems that the container operators are willing to outbid other types of businesses for the available land in the vicinity of major container ports. This phenomenon has been reported in the news. For instance, in Newark, New Jersey, a developer interested in leasing a 13-acre site to build a 330,000 square foot distribution center lost the bid to a container storage company. Leasing the site for container storage benefited the property owner in other ways because it did not require the costly cleanup required for the distribution center proposal (Strunsky 2003). Leasing relatively expensive land makes financial sense to container storage companies because the high density of storage (empty containers are routinely stacked eight, and even ten, high) significantly reduces the storage cost per container, making it of no consequence when compared to the reposition costs. The implications of this are further discussed later in the paper. Although undoubtedly beneficial to container operators, these storage areas produce economic externalities that, in addition to impacting the surrounding communities, manifest themselves in terms of depressed land values. For that reason, they have turned into a political issue, because local communities consider them an aesthetic nuisance, a safety threat, and an obstacle to what they regard as more beneficial uses of the land. In essence, the stored empty containers depress land values; which in turn make storing empty containers even more financially attractive. The empty container distribution problem refers to the modeling of the optimal repositioning of empty containers, i.e., finding the optimal way to reposition empty containers from points of supply where they sit empty to points of demand where containers will be reused. The literature on this subject is abundant, though most publications have focused in particular aspects of the distribution cycle. In general, the areas that have received more attention are the storage and handling of containers and their ground transportation, e.g., Crainic et al. (1993) and Erera et al. (2009). Other areas of interest include resource allocation, fleet sizing and optimal pricing (Gao 1994; Holguín-Veras and Jara-Díaz 1999; Hwan and Bae 1999; Kozan and Preston 1999; Gambardella et al. 2001; Li et al. 2007). The literature review conducted as part of this paper found no technical papers dealing with the accumulation of empty containers in urban areas. Trying to help fill this void is the main objective of the paper. The paper specifically focuses on modeling the accumulation of containers and the effectiveness of alternative policies to ameliorate the problem. To this effect, the paper conducts a critical examination of the potential causes. The paper is organized as follows. In the next section, a description of the container distribution cycle is presented. Then, a multiperiod stochastic programming formulation is proposed to model the storage, leasing and reposition of empty containers. This model is applied to solve a simplified problem resembling basic features of a container network. This is followed by a section in which the policy implications are discussed. The paper ends with a synthesis of the most important findings.",4
14.0,3.0,Networks and Spatial Economics,22 April 2014,https://link.springer.com/article/10.1007/s11067-014-9236-8,A Distributionally Robust Joint Chance Constrained Optimization Model for the Dynamic Network Design Problem under Demand Uncertainty,December 2014,Hua Sun,Ziyou Gao,Fangxia Zhao,,Unknown,Unknown,Mix,,
14.0,3.0,Networks and Spatial Economics,18 April 2014,https://link.springer.com/article/10.1007/s11067-014-9237-7,Dynamic Vulnerability Analysis of Public Transport Networks: Mitigation Effects of Real-Time Information,December 2014,Oded Cats,Erik Jenelius,,Male,Male,Unknown,Male,"Public transport is a vital component of urban transport systems. In tackling the challenges of increasing congestion and negative environmental impacts, shifting trips from personal cars to public transport options is generally seen as one of the most important strategies. For public transport to be an attractive option for travellers, the system needs to be efficient as well as robust. Efficiency means that travel should be fast, convenient, affordable and comfortable under normal operating conditions. Robustness means that the system should be able to withstand or quickly recover from disturbances such as infrastructural and vehicular malfunctions. In order to ensure that the system is robust, it is first necessary to analyse the system-wide impacts of potential disruption scenarios for travellers and operators. This enables the identification of problematic scenarios, for example expressed as a set of important network links where disruptions would be the most severe. When the scenarios have been identified, appropriate actions can be taken to reduce the problems and improve the robustness. The research field concerned with the risk of severe transport network disruptions for the society is commonly called vulnerability analysis (Berdica 2002). Until now, most work in transport network vulnerability analysis has focused on degradations of the physical infrastructure and major incidents, in particular for the road network and personal vehicle travel (Scott et al. 2006; Jenelius and Mattsson 2012; Taylor and Susilawati 2012). Both theoretical analysis and numerical applications have increased the understanding of how supply and demand together determine vulnerability, through the redundancy of the network and the travel patterns of the users. Much less is known about the vulnerability of public transport networks (PTN), where services are superimposed on roads and railways. Some studies have looked at how degradations of physical infrastructure links in a particular modal network affect connectivity and distances between stations (Angeloudis and Fisk 2006; Criado et al. 2007). PTN configuration plays a key role in determining the impacts of prospective service disruptions. Graph theory provides alternative measures of link importance that were applied on PTN worldwide (von Ferber et al. 2009, 2012). These studies considered the number of immediate connections (node degree) and the betweenness centrality measure which corresponds to the share of shortest paths between nodes which go via a certain node. They concluded that network vulnerability in terms of the size of the largest connected subset is more sensitive to betweenness centrality than to node degree. The same conclusion was reached when vulnerability was defined in terms of the speed in which the system becomes fragmented (Colak et al. 2010). Notwithstanding, PTN varied with respect to the impact of various attack scenarios (von Ferber et al. 2012). It was suggested that robustness of metro systems corresponds to the number of cyclic paths available in the network (Derrible and Kennedy 2010). While some general conclusions can be drawn, such analyses cannot capture many features of PTN that we believe are essential in order to describe their vulnerability properly. The analysis of PTN vulnerability considers disruptions that imply a substantial reduction in the capacity of system components and hence their incapability to fulfil the purpose of the system. Disruptions of PTN need not be caused by degradations of the underlying physical infrastructure, but can also arise from degradations of the services, for example crew strike or limited infrastructure capacity (stops or tracks). PTN are characterized by greater complexity than road networks due to the importance of transfers, multi-modality, transport hubs and the intermediate walking links. These network characteristics suggest that PTN are made up of links that belong to distinguished sets. The connectivity of the PTN is lower than that of road networks in both the spatial and the temporal domains: PTN are less dense than road networks, and the level of service varies non-continuously according to the time tables within the day and between days of the week. Together, these factors imply that PTN are more dependent on few critical network elements and hence possibly more vulnerable. At the same time, the multimodality of PTN can potentially allow alternative modes to provide redundant capacity. The aim of this paper is to develop a dynamic and stochastic notion of PTN vulnerability. The granular nature of public transport services requires a more refined model for emulating system supply. Supply dynamics imply that the travel costs associated with alternative paths are time-dependent, which influences adaptive passengers’ path decisions. The spatial distribution of network vulnerability may therefore vary over time due to variations in service capacity and reliability. Hence, the dynamics of both public transport supply and demand are modelled as well as their interactions in order to evaluate the impacts of disruptions. None of the evaluations carried out in previous studies have taken into consideration these underlying system dynamics. A dynamic notion of public transport service disruption takes into account its accumulated effect on system performance. A service disruption implies that public transport vehicles can neither progress along nor enter a disrupted network element. The disruption has a certain duration after which the system is expected to gradually recover back to normal conditions. These conditions could be defined in terms of the flow of supply (vehicles) or demand (passengers). Compared with the case of road networks, service disruption in the PTN has wider direct implications. While service disruption is associated with the immediate effect on the disrupted network element, the dynamic nature of public transport supply results in escalating impacts on service availability and capacity further downstream. Depending on its duration, it may also impact service availability upstream and even on other lines due to its impact on vehicle scheduling. The impacts of service disruptions depend on local crowding levels as well as on how the demand reacts to changes in supply. Although previous studies have not stated their behavioural assumptions, they all share the assumption that all passengers have perfect knowledge of system conditions and that they always choose the shortest path available. These assumptions are relaxed here by adopting a more realistic behavioural representation. A probabilistic path choice process is used in order to model passenger decisions. The evaluation of alternative paths depends on passenger’s preferences and perceptions. The latter is determined by prior knowledge and traveller’s access of real-time information (RTI) on system conditions. In this study, a dynamic public transport operations and assignment model, BusMezzo, is used as the evaluation tool. The model represents the interactions between traffic dynamics, public transport operations and traveller decisions. The different sources of public transport operations uncertainty including traffic conditions, vehicle capacities, dwell times, vehicle schedules and service disruptions are modelled explicitly. A dynamic path choice model considers each traveller as an adaptive decision maker. Travellers’ progress in the public transport system consists of successive decisions based on anticipated downstream attributes. Factors such as timetables, transfers and walking distances are used for the predictions of passenger loads under various scenarios. Advanced public transport systems (APTS) have the potential to improve system robustness. This includes real-time control and management strategies. The public transport system may also become more robust by informing passengers on downstream conditions. Previous studies demonstrated the effects of RTI on passenger decisions under service disruption scenarios. However, the analysis also highlighted that as passengers are more informed, passenger loads are subject to more fluctuation and therefore may counteract system robustness (Cats et al. 2011a). The remainder of the paper is organized as follows. The proposed methodology for public transport vulnerability analysis is described in Section 2. Section 3 describes a case study for the high frequency PTN of Stockholm, Sweden. Section 4 presents the application results followed by a discussion of the benefits of vulnerability analysis in planning, operations and management and concludes the paper.",112
14.0,3.0,Networks and Spatial Economics,28 May 2014,https://link.springer.com/article/10.1007/s11067-014-9244-8,"A Multilane Traffic Flow Model Accounting for Lane Width, Lane-Changing and the Number of Lanes",December 2014,Tie-Qiao Tang,Yun-Peng Wang,Hai-Jun Huang,,,,Mix,,
14.0,3.0,Networks and Spatial Economics,01 June 2014,https://link.springer.com/article/10.1007/s11067-014-9246-6,A Traffic Breakdown Model Based on Queueing Theory,December 2014,Xiqun (Michael) Chen,Zhiheng Li,Qixin Shi,Unknown,Unknown,Unknown,Unknown,,
14.0,3.0,Networks and Spatial Economics,14 August 2014,https://link.springer.com/article/10.1007/s11067-014-9247-5,Dynamic Resource Allocation Problem for Transportation Network Evacuation,December 2014,Xiaozheng He,Srinivas Peeta,,Unknown,Male,Unknown,Male,"Contemporary research in evacuation planning has been largely dedicated to improving the performance of ground transportation during the evacuation processes. Most of these studies address the problem as a network design problem, which selects an optimal configuration for a transportation network to increase the evacuation network capacity, alleviate evacuation traffic congestion, accommodate the demand surge, and meet the needs of special services and responders during evacuation. As emphasized by Sheffi et al. (1982), transportation network evacuation performance is sensitive to three main aspects: (i) network topological design, such as adding new lanes (Patil and Ukkusuri 2007) or investing in link maintenance (Peeta et al. 2010); (ii) intersection design, such as reducing crossing and merging conflicts by blocking and restricting conflicting movements at intersections (Cova and Johnson 2003; Xie and Turnquist 2011); and (iii) traffic management strategies, such as evacuation shelter location allocation and path guidance (Sherali et al. 1991), signal optimization for evacuation traffic (Lo 2001) and law enforcement personnel deployment strategy (Jabari et al. 2012). These evacuation network design and management strategies primarily deal with the optimal distribution of resources such as budget, personnel, traffic control devices, and time, which are constrained during the evacuation processes. Since time itself is a scarce but crucial resource in evacuation, the complexity of optimally allocating movable resources in the time dimension is inevitable and critical to evacuation operations. In a planning context, there is the need to explore whether the usage of resources by dynamically reallocating them can improve the effectiveness of evacuation operations. To the best of our knowledge, most existing evacuation network design and management strategies have been established in the sense of static resource allocation. Namely, the resource location (or design pattern) does not change during the evacuation process. The resources are assigned time-invariant to the critical locations (road segments or intersections) of the evacuation network. However, the importance of a road/intersection to the traffic evacuation process can change with time. For instance, bottlenecks in the evacuation network may shift from one intersection to another depending on the underlying traffic dynamics. Ignoring the mobility of resources would result in a sub-optimal solution for evacuation planning. In general, it is reasonable to consider static allocation of resources for long-term purposes such as the addition of new lanes to strengthen an evacuation network. However, some resources such as well-trained personnel (law enforcement) and movable devices (portable message sign systems) can be reallocated quickly. Allocating such resources dynamically can enable evacuation management agencies to improve the system performance in the evacuation time horizon. This study is motivated by the need to develop models to dynamically assign resources to the needed locations based on the evolving traffic characteristics while factoring practical spatiotemporal constraints on the resources for efficient evacuation planning and operations. Traditionally, evacuation network design and management strategies are modeled using a bi-level program structure, with an upper level program determining the optimal resource allocation plan and a lower level program describing the underlying evacuation traffic flow. The lower level program has been formulated as a deterministic user equilibrium problem (for example, Kongsomsaksakul et al. 2005) or a stochastic user equilibrium problem (for example, Liu and Luo 2012). With the advancement of dynamic traffic assignment (DTA) techniques (Peeta and Ziliaskopoulos 2001), DTA has been incorporated for modeling evacuation network design problems to account for the effect of traffic dynamics (for example, Janson 1995). However, the complex structure of a bi-level program makes the evacuation network design problem rather intractable (Jeon et al. 2006). Single-level linear program (LP) models, which preserve the problem tractability, have been proposed by Waller and Ziliaskopoulos (2001) and Ukkusuri and Waller (2008) for evacuation network design. The simple LP structure is due to the adaptation of an LP formulation for single destination DTA proposed by Ziliaskopoulos (2000), which incorporates the cell transmission model (CTM) proposed by Daganzo (1994, 1995). However, akin to other previous evacuation network design studies, these LP models only consider time-invariant resource assignment during the evacuation process. Sbayti and Mahmassani (2006) propose an evacuation operational scheduling problem where the time-varying evacuation operation is considered in terms of departure time, route, and destination choices. They use a bi-level framework to solve for an optimal flow pattern under a system optimal objective using the DYNASMART-P assignment-simulator. Their assumption of full compliance of evacuees allows the optimal operational scheduling to be solved simultaneously in the dynamic traffic assignment platform. However, the focus of their study is on dynamically scheduling evacuees, and the question of how to dynamically distribute movable resources into an evacuation network remains unanswered. This paper proposes a mathematical model for dynamic resource allocation (DRA) under transportation network evacuation, whereby a limited number of resources are available to be dynamically assigned to critical intersections. The focus of this study is on the scheduling of the moveable resources to improve the evacuation efficiency, but not on the determination of the impacts of the assigned resources. The proposed mathematical model is formulated as a mixed integer linear program (MILP) built upon the CTM to describe the traffic dynamics associated with the evacuation process. The proposed single-level MILP formulation enables problem tractability. In contrast to static network designs for evacuation, the proposed model addresses the questions of when, for how long, and where to assign a movable resource in a traffic network during evacuation. It is formulated from an evacuation planning viewpoint as the goal of the study is to highlight the importance of dynamic resource allocation compared to static resource allocation for no-notice evacuations. The solution to the proposed analytical formulation can potentially provide pointers for more effective real-time evacuation operations. This paper advances the modeling of transportation network evacuation planning and operations by making the following specific contributions. First, an MILP model is developed for allocating moveable resources dynamically to improve transportation network evacuation efficiency in both spatial and temporal dimensions. Second, a set of spatiotemporal constraints on resource assignment is introduced into the analytical formulation, which enables realism for practical evacuation planning and operations. Third, the proposed MILP model is reformulated into a two-stage optimization problem to reduce the problem complexity. Finally, an efficient heuristic solution method is developed based on the two-stage optimization reformulation. The remainder of the paper is organized as follows. The next section provides preliminaries including notation, the CTM, and a system optimal (SO) single destination DTA program, for modeling the DRA problem. Section 3 describes the MILP model for the DRA problem. Section 4 presents a two-stage approximation approach to the MILP model. Numerical experiments are used in Section 5 to illustrate the effectiveness of the proposed DRA approach. The final section summarizes this study and identifies potential venues for future research.",18
14.0,3.0,Networks and Spatial Economics,25 June 2014,https://link.springer.com/article/10.1007/s11067-014-9253-7,An Improved Stirling Approximation for Trip Distribution Models,December 2014,Louis de Grange,Felipe González,Sebastián Raveau,Male,Male,Male,Male,"Many demand models used in transport system planning are based on maximum entropy optimization problems (Wilson, 1970; Anas, 1983; Fotheringham, 1983, 1986; Fotheringham and O’Kelly, 1989; Fang and Tsao, 1995; Abrahamsson and Lundqvist, 1999; Florian et al., 1999; Boyce and Bar-Gera 2003, 2004; Ham et al., 2005; Boyce, 2007; Hasan and Dashti, 2007; De Cea et al., 2008; Briceño et al., 2008; De Grange et al., 2010a; De Grange et al., 2010b; Donoso and De Grange, 2010, Donoso et al., 2011; De Grange et al., 2013; Kitthamkesorn et al., 2013). In trip distribution models, for example, the entropy function used to estimate the most likely trip matrix takes the following functional form (Wilson, 1970): where T is the total number of trips in the system and T

ij
 are the trips between origin i and destination j. The values of T

ij
 that maximize E in (1) are the estimates of the most likely trip matrix and in this case are positive integers. To obtain an expression approximating (1) in which the T

ij
 values are continuous, Stirling’s first-order approximation (Hazewinkel, 2001) is applied to the denominator. Unfortunately, this approximation technique produces serious errors at small values of T

ij
. The hypothesis of this study is that alternative methods such as Stirling’s second-order approximation (Feller, 1966) or Burnside’s formula (1917) can be used to mitigate these errors and thus improve the predictive capacity of trip distribution models based on maximum entropy. The application of these two methods engenders two new model forms distinct from Wilson’s classic gravity model formulation, which uses Stirling’s first-order. Our comparisons of the three formulations will demonstrate that the two using the Stirling’s second-order and Burnside approximations, respectively, do indeed boost predictive capacity. The remainder of this paper consists of five sections. Section 2 reviews the first-order Stirling approximation and its use with the gravity model, introduces the second-order Stirling method and compares its approximation error to that of the first-order version, and then uses the former to specify a new trip distribution model. Section 3 sets out Burnside’s formula, compares it to the first-order and second-order Stirling approximations, and uses it to derive a new trip distribution model. Section 4 describes an approach for estimating the parameters of the two new models based on the maximum likelihood method but with certain modifications required as a result of the incorporation of the second-order Stirling and Burnside approximations. Section 5 gives an empirical analysis of the second-order Stirling and Burnside based models, comparing their results with the classic gravity model using data from the Santiago, Chile metro system. Finally, Section 6 presents our conclusions and some ideas for future research based on the results obtained.",2
14.0,3.0,Networks and Spatial Economics,07 September 2014,https://link.springer.com/article/10.1007/s11067-014-9254-6,A Reliable Budget-Constrained FL/ND Problem with Unreliable Facilities,December 2014,Davood Shishebori,Lawrence V. Snyder,Mohammad Saeed Jabalameli,Unknown,Male,Male,Male,"Managers of service systems, as well as supply chains for goods, are continuously looking for ways to reduce total costs while also improving the performance of their systems in order to stay competitive in today’s business landscape. At the same time, many companies face disruptions and other unexpected events throughout their supply chains, and these can lead to disastrous financial losses. Combined with the worldwide economic downturn in recent years, this risk necessitates the use of proactive strategies for mitigating the effects of disruptions. We present a model that combines facility location and network design decisions under the risk of disruptions. Thus, our model optimizes strategic decisions that account simultaneously for the need for efficiency (i.e., low costs) and for reliability (i.e., disruption resilience). (Other papers discuss operational approaches for mitigating disruptions; see, e.g., Tomlin (2006), or see Snyder et al. (2010) for a review.) Our model minimizes the total transportation cost assuming no disruptions take place while imposing a budget constraint on the fixed cost of building facilities and links in the network. It also ensures the reliability of the resulting network by enforcing an upper bound on the total cost that may result when disruptions occur. Facility location problems choose the locations of facilities and, often, the allocation of customers to them, in order to optimize some objective function, such as minimizing the operating cost or maximizing the demands covered. Based on their objective functions and constraints, facility location problems are categorized into several problem classes, such as the p-median and p-center problems (Hakimi 1964), the uncapacitated facility location problem (Kuehn and Hamburger 1963), the maximum covering location problem (Church and ReVelle 1974) and the set covering location problem (Toregas et al. 1971). In contrast, in network design problems, the basic goal is to optimally construct a network that enables some kind of flow, and possibly that satisfies some additional constraints. The nodes usually are given and the problem must make decisions about which links (edges) to choose from among a set of potential edges. Our work combines aspects of facility location and network design by making open/close decisions on nodes, as well as on the edges used to travel among the nodes. Often these two problems (facility location and network design) are solved independently, but we would argue that it is more realistic and effective to model and solve them simultaneously. All of the aforementioned classical facility location models locate facilities on a predetermined network. However, the topology of the underlying network may profoundly affect the optimal facility locations. Joint facility location/network design problems have many applications in industries and services, and some studies clearly illustrate the value of solving them simultaneously; see for example, Melkote (1996), Melkote and Daskin (2001), whose facility location/network design models underlie our own. Another significant aspect that can affect facility location and network design is the reliability of the system. Assuming that facilities are always available and never disrupted is typical in classical studies. Although most companies would like to assume that disruptions rarely happen, and that even if they occur, their supply chains will be reliable enough, in practice, some unexpected disruptions happen and some companies are vulnerable and therefore easily disrupted. The terrorist attacks of 9/11, the catastrophic devastation caused by Hurricane Katrina (Barrionuevo and Deutsch 2005; Latour 2001; Mouawad 2005), the huge finespaid by the Boeing company in compensation for postponing the delivery of the Dreamliner 787 (Bathgate and Hayashi 2008; Peng et al. 2011) andthetragic earthquake and subsequent tsunamiin Japan in 2011 (Bathgate and Hayashi 2008; Clark and Takahashi 2011) are among the most obvious examples of these kind of disruptions. It can be difficult for companies to remove (or even reduce) the causes of disruptions, since often the causes—such as equipment failures, natural disasters, industrial accidents, power outages, labor strikes, and terrorism—are out of companies’ control and cannot effectively be avoided by precautionary actions. Although some disruptions are short-lived, they can still cause serious long-term negative financial and operational outcomes. Some studies have quantified these negative effects of disruptions empirically; for example, the abnormal stock returns of firms that have been affected by disruptions can reach approximately 40 % (Hendricks and Singhal 2005). Similar findings are described by (Peng et al. 2011; Hicks 2002). When facility disruptions occur, customers may have to be reassigned from their original facilities to the other available facilities, in which case the transportation costs will surely increase. Moreover, the facility locations that are chosen when the disruption risks are ignored may not be good locations to respond to disruptions; therefore, it is important to incorporate the risk of disruptions when making facility location and network design decisions. That is the primary focus of our study. The initial model for the facility location/network design problem (FLNDP) was introduced by Daskin et al. (1993). They demonstrated the importance of optimizing facility locations at the same time asnetwork design and developed a mathematical model to do so. Subsequently, Melkote (1996) developed three models for the FLNDP in his doctoral thesis, including uncapacitated and capacitated versions (UFLNDP and CFLNDP, respectively) and the maximum covering location-network design problem (MCLNDP). These models were also described by Melkote and Daskin (2001). In another doctoral thesis, some efficient approaches were developed to solve the static budget-constrained FLNDP by Cocking (2008). Also, Cocking (2008) developed some useful algorithms to find good upper and lower bounds on the optimal solution. The main heuristics that were proposed by Cocking are simple greedy heuristics, a local search heuristic, meta-heuristics such as simulated annealing (SA) and variable neighborhood search (VNS), and a custom heuristic based on the problem-specific structure of FLND. In addition, a branch-and-cut algorithm using heuristic solutions as upper bounds and cutting planes to improve the lower bound of the problem were developed. The method reduced the number of nodes which were needed to approach optimality. Drezner and Wesolowsky (2003) proposed a new network design problem with potential links, each of which can be either constructed at a given cost or not. Also, each constructed link can be constructed as either a one-way or two-way link. Bigotte et al.(2010) studied a version of the FLNDP in which multiple levels of urban centers and multiple levels of network links were considered simultaneously in developing a mixed integer mathematical model. Their model determines the best transfers of urban centers and network links to a new level of hierarchy in order to improve the accessibility of all types of facilities. Jabalameli and Mortezaei (2011) proposed a bi-objective mixed integer programming formulation as an extension of the CFLNDP in which the capacity of each link for transferring the demands is limited. Contreras and Fernandez (2012) reviewed the relevant modeling aspects, alternative formulations and several algorithmic strategies for the FLNDP. They studied general network design problems in which design decisions to locate facilities and to select links on an underlying network are combined with operational allocation and routing decisions to satisfy demands. Contreras et al. (2012) presented a combined FLND problem to minimize the maximum customer-facility travel time. They developed and compared two mixed integer programming formulations by generalizing the classical p-center problem so that the models consider the location of facilities and the design of the underlying network simultaneously. Rahmaniani and Ghaderi (2013) developed a mixed-integer model in order to optimize the location of facilities and related transportation network simultaneously to minimize the total transportation and operating costs. They assumed that there are several types of transportation links in which their capacity, transportation and construction costs are different. Also, Ghaderi and Jabalameli (2013) proposed a mathematical model for the dynamic version of uncapacitated facility location–network design problem with a constraint on investment budget for opening the facilities and constructing arcs at each time period during the planning horizon. This model determine the optimal locations of facilities and also the design of the underlying network simultaneously. Table 1 presents an overview of the literature on the FLNDproblem. The literature related to system reliability in facility location problems demonstrates that, in light of the huge investment required for facility location, the attention paid to system failures in facility location has increased in recent years (Qi and Shen 2007; Qi et al. 2010).Drezner (1987) was one of the first researchers who proposed mathematical models for facility location with unreliable suppliers. He studied the unreliable p-median and (p, q)-center location problems, in which a facility has a given probability of becoming inactive. In subsequent research, Snyder (2003), 2005, 2007) proposed several mathematical programming formulations for the reliable p-median and fixed charge problems based on level assignments, in which the candidate sites are subject to random disruptions with equal probability. Berman et al. (2007) formulated a p-median problem with disruptions that relaxes the equal-probability assumption made by Snyder and Daskin (2005). Their model is highly non-linear, and they focus on structural properties and special cases. Shen et al. (2009) also relaxed the assumption of uniform failure probabilities, formulated the stochastic fixed-charged facility location problem as a nonlinear mixed integer program, and proposed several heuristic solution algorithms, as well as an approximation algorithm for the equal-probability case. Lim et al. (2009) proposed a reliability continuum approximation (CA) approach for facility location problems with uniform customer density in which facilities can be protected with additional investments. They demonstrated the impact of misestimating the disruption probability in facility location problems in the presence of random facility disruptions. Matisziw et al. (2010) studied the effects of disruptions on network services and proposed a multi-objective optimization approach for network restoration during disaster recovery. The model evaluates a tradeoff between two objectives, minimization of system cost and maximization of system flow over a planning horizon within budgetary restrictions and repair scheduling prioritization issues. Berman and Krass (2011) studied the effect of line segment failure on then-facility median problem. They assumed that customers have some information about the status of each facility ahead of time and thus travel directly to the closest operating facility. They proposed a linear combination of deterministic median problems for minimizing the expected travel distance. The problem can be solved for any number of facilities and also valid when the failures are correlated. In another study, Berman et al. (2013) assumed that the facilities are not perfectly reliable and failures may be correlated. Moreover, customer information regarding the state of facilities may be incomplete. Therefore, they studied the impact of failure probabilities, correlated failures, availability of information, and the median and center objectives on the optimal location patterns for location and reliability problems on a line line. They derive explicit expressions for facility routes as functions of the model parameters in order to obtain managerial insights. Hanley and Church (2011) developed a facility location–interdiction covering model for finding a robust arrangement of facilities that has a suitable efficiency under worst-case facility losses. They formulated a MIP model in which all possible interdiction patterns are considered, and a second, more compact bi-level model in which the optimal interdiction pattern is implicitly defined in terms of the chosen facility locations. Peng et al. (2011) studied the effect of considering reliability in logistic networks design problems with facility disruptions and illustrated that applying a reliable network design is often possible with small increases in total location and allocation costs. They considered open/close decisions on nodes but not on arcs of the commodity production/delivery system. By applying the p-robustness criterion (which bounds the cost in disruption scenarios), they simultaneously minimize the nominal cost (the cost when no disruptions occur) and reduce the disruption risk. Recently, Liberatore et al. (2012) proposed a tri-level mathematical model for the problem of optimizing for tification plans in capacitated median distribution systems with limited protective resources in the face of disruptions that involve large regions. They illustrated empirically that considering correlation effects in a system plays an important role in reducing the suboptimal protection plans and subsequently decreasing the non essential growth in the system cost when disruptions happen. Jabbarzadeh et al. (2012) studied a supply chain design problem in which distribution centers may have partial or complete disruptions. The problem was formulated as a mixed-integer nonlinear program which maximizes the total profit of the system while taking into account different disruption scenarios at facilities. Alcaraz et al. (2012) proposed two hybrid meta-heuristics to solve the Reliability p-median problem, a genetic algorithm and a scatter search approach. They proposed several modifications to improve these algorithms in order to solve the problem. Kim and O’Kelly (2009) and Lei (2013) considered reliable hub-location problems, which have some similarity to our model in the sense that arcs connect hubs (facilities) to each other and not only to demand nodes, though these models do not make open/close decisions about arcs, as our model does. Also, in another study, Shishebori et al. (2013) considered the reliable facility location/network design problem considering system reliability (the same problem we consider in this paper). They present a mixed integer non-linear programming (MINLP) formulation for the problem and present a relatively small case study but do not propose a solution method. In contrast, we formulate the problem as a mixed-integer linear programming (MILP) problem andpropose an efficient heuristic to solve it. Moreover, some other recent studies were done at the facility location problems area regarding to system disruptions (e.g., Yushimito et al. (2012), Chen et al. (2013), Aydin et al. (2013), Maheshwari et al. (2014), Correia (2014), Berglund and Kwon (2014)). We note that the literature on facility location with disruptions is related to, but distinct from, the literature on backup coverage models (e.g., Hogan and ReVelle (1986), Daskin et al. (1988)). Backup coverage models aim to cover each customer multiple times (that is, to have multiple facilities within a certain radius of each customer) in order to account for congestion at the facilities. However, there is no explicit assignment of customers to “primary” and “backup” facilities, as there is in our model and other disruption models. Table 2 provides an overview of the literature of facility location problems with respect to system reliability. It is evident from the preceding literature review that the existing studies have not considered both network design and system reliability together with facility location. In fact, the literature review illustrates that there is a research gap in facility location regarding more realistic factors such as network design and system reliability to manage practical facility location problems. However, there are numerous examples of practical problems in which simultaneously considering facility location, network design, and system reliability is critical in improving the efficiency, usefulness, and security of the system. These examples include pipelines for gas and water, infrastructure for airline and railroad networks, and systems for delivering services such as health care and education. (In the latter example, link construction may represent establishing routes for medical transport vehicles or school buses, or may represent the construction of new roads to access the facilities, e.g., in under developed regions.) Moreover, our model includes a budget constraint on the fixed cost of locating nodes and links in the network, which reflects a practical constraint faced by many of these systems. We will refer to our model as the reliable budget-constrained facility location/network design problem (RBFLNDP). The main contributions that differentiate this paper from existing ones in the related literature can be summarized as follows: (1) We introduce a new optimization model to consider simultaneously facility location and allocation, network design, system reliability and a budget constraint as a mixed-integer, linear programming (MILP) problem. Our model integrates tactical and strategic decision making, such as determining the optimum locations of new facilities, optimum construction of transportation links, and optimum allocation of demand nodes to located facilities so that total costs as well as system reliability are optimized. (2) Our new mathematical formulation not only takes into account facility location costs, link construction costs, and transportation costs, but also constrains the maximum allowable disruption cost of the system, as well as the investment in facility location and transportation link construction. (3) We develop a new hybrid heuristic solution approach for the RBFLNDP that, to the best of our knowledge, has not previously been proposed for solving facility location problems. The remainder of the paper is organized as follows: In Section 2, the mathematical formulation of the RBFLNDP is developed. In Section 3, the hybrid LP relaxation heuristic solution approach is proposed and described. Then, in Section 4, a numerical example that illustrates the application of the heuristic is demonstrated and, based on it, a sensitivity analysis of the model parameters is reported. Computational results are presented in Section 5 and finally, conclusions and future work are discussed in Section 6.",27
14.0,3.0,Networks and Spatial Economics,11 September 2014,https://link.springer.com/article/10.1007/s11067-014-9255-5,Considering En-Route Choices in Utility-Based Route Choice Modelling,December 2014,Dawei Li,Tomio Miwa,Takayuki Morikawa,Unknown,Male,Male,Male,"Route choice models play an important role in many transport applications. As a crucial part of traffic assignment and simulation procedures, route choice modelling is essential in both transport planning and network simulation tools (Ben-Akiva et al. 2014). As a representation of individual behaviour, route choice models provide an understanding of travellers’ choices under different scenarios. This means that route choice modelling is also essential when analysing the effect of policy (e.g. congestion pricing) (Hartman 2012) or information provision on traffic demand and network traffic conditions. Route choice has been widely investigated from both methodological and practical perspectives. A comprehensive review of the route choice problem can be found in Prato (2009). Much previous research has assumed that drivers make a route choice only at the origin of a trip. Models based on this assumption are referred to as static models (SM) in this research, because they ignore a driver’s changing perceptions of route characteristics during a trip and the possibility of a driver making en-route choices. The route choice behaviour of a traveller is based on the person’s perceptions of route characteristics, tastes for route characteristics and certain decision rules. A driver’s perceptions of some route characteristics (e.g. travel time) are dynamic because traffic conditions are changing all the time, due to varying travel demand and the unpredictability of traffic incidents. The static route choice models can only consider the time-dependent perceiving route characteristics at the origin. The static route choice model can consider drivers’ day-to-day perceptions of route characteristics (Xiao and Lo 2014). However, travellers’ perceptions of route characteristics keep changing during the trip, especially given the increasing sophistication of information systems and the popularity of on-board navigation tools. Travellers’ updating of perceived route characteristics is related to their experiences and the information provision. There are many previous studies that explicitly consider the updating of perceived route characteristics in route choice analysis. For instance, Bogers (2009) propose a joint model to consider the effects of advanced travel information service, habit, and learning on route choice by laboratory simulator experiments. Peeta and Yu (2005) develop a hybrid model based on fuzzy modelling to incorporate en-route updating of perceived route characteristics in both within day and day-to-day contexts. Trozzi et al. (2013) analyse the effects of countdown displays on the public transport route choice behaviour under severe overcrowding. The en-route updating of perceptions of route characteristics is a main reason that for travellers to make en-route choices. For instance, if a driver hears in a radio broadcast that his/her currently chosen route is blocked because of a serious crash, he/she will very likely consider switching to another route. This is also confirmed by some previous studies which are about the effect of information provision on route switch behaviour. For instance, Mahmassani and Liu (1999) find that travellers switch routes if the expected travel time saving exceed a threshold which is related to the remaining travel time to the destination, based on the experiment with a dynamic simulator. Peeta et al. (2000) did an exploratory study on the effect of variable message signs on route switch behaviour. They find that, there is a strong correlation between the message content and drivers’ route switch behaviour. To model travellers’ en-route choices, a policy-based adaptive route choice modeming framework is proposed in previous research (Gao 2012; Gao et al. 2008, 2010). In their research, drivers are assumed to have a plan about the route switching before making a trip, and the alternatives are routing polices, not routes. Different from their research, we assume drivers’ route choice behaviour is not strategic, but completely reactive. Models that take into account en-route choices are referred to as dynamic models (DM) in this research. Besides the en-route updating of a driver’s perception of route characteristics, which is not discussed in this research, the development of a DM still presents two additional problems as compared with static models, as follows: The possibility of making en-route route choices; The dynamic nature of a driver’s taste for route characteristics when en-route choices are made. For the first problem, a natural way to build a dynamic route choice model is to have a sequence of static choice models, one at each decision node, where the characteristics of alternative routes reflect the updated information. Here, a decision node means a network node where a driver can switch to another available route. Some dynamic traffic assignment models, e.g. DynaMIT (Ben-Akiva et al. 2002), employ dynamic route choice models constructed this way, and applied in real time traffic management (Ben-Akiva et al. 2001). This kind of model is referred to as a deterministic dynamic route choice model (DDM) in this paper. Although easily applied in practice, it is obviously not necessary for a driver to make a route choice decision at every decision node. According to some previous analysis using probe vehicle data (Li et al. 2013a, b; Morikawa and Miwa 2006), a SM will sometimes give better numerical results than a DDM in a small network. These authors also proposed models to account for the probability of making a route choice decision at each decision node. This kind of dynamic model is referred to here as a stochastic dynamic model (SDM). In the models proposed by these authors, however, the second problem of the dynamic nature of driver taste is not considered. Previous studies have proved that, some determinants other than travel time and cost will affect drivers’ route choice behaviour. For instance, de Palma and Picard (2005) did a study about route choice decision under travel time uncertainty based on survey data, and found that travellers’ tastes on the level of travel time uncertainty are related to gender, employment status and purpose of the trips. Parkany et al. (2006) explained that attitudinal indicators influence consistency and diversion for both stated and revealed preferences of drivers. According to research carried out by Morikawa and Miwa (2006), drivers’ taste is related to the distances between origin and destination. Li et al. (2013a, b) also prove a relationship between route characteristic taste and familiarity with the origin–destination pairs. Previous studies (Chen et al. 2012; Miwa et al. 2010) has also demonstrated that, the consideration of distance related taste heterogeneity in route choice modelling will significant affect the traffic assignment results. It should be found that, the determinants that affect travellers’ tastes can be categorized to 3 groups: individual specific (e.g. age and gender), origin–destination pair (OD) specific (e.g. distance and familiarity), and trip specific (e.g. travel purpose). If en-route choices are allowed, travellers’ tastes at different decision nodes will be different because of some OD specific determinants, but also correlated because of individual and trip specific determinants. This will be considered in study using mixed logit method. Existing SDMs are latent class models because the en-route choices of drivers cannot be observed. These models were estimated by maximizing the log-likelihood of path observations. However, whether this estimation method can recover the true route choice behaviour has not been discussed. In this paper, we will extend previous work on SDMs and proposed a dynamic model based on a choice process in which the probability of a driver making an en-route choice and a driver’s dynamic taste in different choice situations are taken into consideration. The dynamic choice process is defined as the sequence of choices during a trip, including the route choices (both pre-trip and en-route choices) and the choices of making a route choice again at every decision node. We will also discuss estimation issues with the proposed model. This paper is organized as follows. First, in Section 2, a simple example is used to illustrate the dynamic route choice problem. Then the choice model is specified in Section 3. In Section 4, the estimation issues are discussed. A case study based on synthetic data is carried out in Section 5. The final section presents some concluding comments and discusses possible future directions.",11
14.0,3.0,Networks and Spatial Economics,22 August 2014,https://link.springer.com/article/10.1007/s11067-014-9258-2,"Innovation, Decentralization, and Planning in a Multi-Region Model of Schumpeterian Economic Growth",December 2014,Amitrajeet A. Batabyal,Peter Nijkamp,,Unknown,Male,Unknown,Male,"The general objective of our paper is to study innovation and the resulting Schumpeterian economic growth that this innovation gives rise to in a dynamic model of an aggregate economy consisting of N heterogeneous regions. For each region i, i=1,…,N, our analysis focuses on six issues. First, we define the balanced growth path (BGP) allocations and the ensuing equilibrium that are of interest. Second, we stipulate the form of the so called innovation possibilities frontier that is consistent with balanced economic growth. Third, we derive the economic growth rate of the ith region in the decentralized equilibrium with no governmental or social planning and show that there are no transitional dynamics. Fourth, we solve the social planner's optimization problem and derive the Pareto optimal economic growth rate in the ith region. Fifth, we compare the two preceding growth rates and discuss the circumstances in which there is either too much or too little innovation in (i) the ith region, (ii) an aggregate economy of N>2 regions and (iii) an aggregate economy of N= regions. Sixth, we briefly model and analyze interregional trade and its effects in an aggregate economy of N=2 regions. The general objective stated in the preceding paragraph and the specific issues that our analysis concentrates on are interesting and relevant because of four reasons. First, our analysis formalizes and therefore helps us better understand the observation of researchers such as Fischer and Nijkamp (2009), Baumol (2010), and Batabyal and Nijkamp (2013a) who have emphasized that innovation is a significant driver of regional economic growth and development. In fact, this view is now widely accepted in regional science and hence policymakers understand that “the presence of successful entrepreneurship and of a favourable business and innovation climate will bring high benefits to the host region” (Fischer and Nijkamp 2009, p. 186). Second, our analysis explicitly recognizes that innovative activities and processes are fundamentally competitive in nature and hence this analysis “operationalizes” for regions, a central insight of Joseph Schumpeter who argued that growth processes are characterized by creative destruction in which “economic growth is driven, at least in part, by new firms replacing incumbents and new machines and products replacing old ones” (Acemoglu 2009, p. 458). Third, our analysis helps shed light on several aspects of the growth process including the impacts of the trinity of monopoly distortions, the profit stealing effect, and the replacement effectFootnote 1 on the extent of innovative activity in regions. Finally, the analysis we undertake helps shed light on firm dynamics and the reallocation of resources among incumbents and new R&D conducting entrants in regions. In an early paper, Leahy and McKee (1972) stated but did not explicitly model the idea that change in regional economies can be well understood by adopting a “Schumpeterian view” of the regional economy. Despite the appearance of this statement more than four decades ago, regional scientists have begun to use the ideas of Schumpeter to systematically investigate the nexus between innovation and economic growth in regions only since the early 1980s. Even so, there is now a fairly substantial empirical and case study based literature that has analyzed alternate aspects of Schumpeterian economic growth in regional economies. Van Wissen and Huisman (2002) consider low and high economic growth scenarios and then empirically show that, at the regional level, large growth differentials are likely to be observed between what they call “core,” “ring,” and “peripheral” regions. Lodde (2008) uses sectoral data to compare the utility of the Schumpeterian approach in understanding the relationship between human capital and productivity growth in regions in Italy. He shows that there is qualified support for the Schumpeterian hypothesis. Crespi and Pianta (2008) focus on six nations in Europe and point out that the ideas of Schumpeter are useful in comprehending the variety of innovation and what they call “innovation-performance relationships” in the six countries under study. Qian (2007) uses the passage of national pharmaceutical patent law as a natural experiment to show that the implementation of patents stimulates innovation mostly in countries with higher market freedom. Similarly, Quatraro (2009) uses Italian patent data and shows that Schumpeter’s views about innovation and business cycles can be used to shed light on the diffusion of innovation capabilities in various Italian regions. These and other such studies provide empirical support for the idea that a complementarity exists between patent protection and product market competition in fostering innovation. Aghion et al. (2009) use UK firm level panel data and show that there is empirical support for the idea that more intense competition enhances innovation among what they call “frontier” firms but that this kind of intense competition may actually discourage innovation in “non-frontier” firms. Shi et al. (2010) study the connections between technological externalities, transaction costs, and economic agglomeration. Focusing on innovative firms, Akcigit and Kerr (2010) and Haltiwanger et al. (2010) show that firm size and firm age are positively correlated and that in innovative industries, small firms exit the industry more frequently and hence the surviving firms tend to grow relatively rapidly. Focusing on 2,645 counties in the United States, Hodges and Ostbye (2010) find support for a Schumpeterian growth model because, in their empirical model, bigger firms are needed to carry out effective R&D which then leads to higher economic growth in the localities being studied. Finally, Saunoris and Payne (2011) use United States data from 1960 to 2007 and show that long run increases in R&D expenditures are necessary to offset lower R&D productivity due to the presence of product proliferation. There are very few theoretical studies of the connections between innovation and Schumpeterian economic growth in the context of regions. Recently, Batabyal and Nijkamp (2012) have theoretically analyzed a one-sector, discrete-time, Schumpeterian model of growth in a regional economy. These researchers show that the regional economy they study experiences bursts of unemployment followed by periods of full employment. There are two key differences between the Batabyal and Nijkamp (2012) paper and our paper. First, the Batabyal and Nijkamp (2012) paper studies a single region in discrete time, whereas we focus on a multi-region economy in continuous time. Second, the specific questions we study in our paper are different from the basic question studied by Batabyal and Nijkamp (2012). In particular, Batabyal and Nijkamp (2012) analyze the nature of labor dynamics in the presence of Schumpeterian economic growth. In contrast, the questions we address—see section 1.1—are very different and labor is not even a factor of production in the present paper.Footnote 2
 Our formal analysis departs from and “supersedes” the existing theoretical literature in four ways. First, the basic unit of analysis in our paper is a region and not a country. In this regard, the word “region” refers to a geographic entity that is smaller than a nation. Second, instead of working with labor, we work with human capital as a key factor of production. Third, the model we analyze is a model of multiple regions and not a model of a single country. Finally, unlike the existing literature, we show the impact that the trinity of monopoly distortions, the profit stealing effect, and the replacement effect have on the magnitude of innovations in multiple regions. The remainder of this paper is organized as follows. Section 2 describes our theoretical model of an aggregate economy consisting of N heterogeneous regions that is adapted from Aghion and Howitt (1992) and Acemoglu (2009, pp. 459–472). Section 3 defines the balanced growth path (BGP) allocations and the resulting equilibrium that are of interest. Section 4 specifies the form of the innovation possibilities frontier that is consistent with balanced economic growth. Section 5 derives the economic growth rate of the ith region in the decentralized equilibrium without any governmental or social planning and shows that there are no transitional dynamics. Section 6 first solves the social planner’s maximization problem and then derives the Pareto optimal economic growth rate in the ith region. Section 7 compares the decentralized equilibrium and the Pareto optimal growth rates and discusses the conditions in which there is either too much or too little innovation in (i) the ith region, (ii) an aggregate economy of N>2 regions and (iii) an aggregate economy of N=2 regions. Section 8 concludes and then discusses potential extensions of the research delineated in this paper. Finally, the appendix discusses interregional trade and its impacts in an aggregate economy of N=2 regions.",14
14.0,3.0,Networks and Spatial Economics,18 September 2014,https://link.springer.com/article/10.1007/s11067-014-9263-5,Covering Part of a Planar Network,December 2014,Zvi Drezner,George O. Wesolowsky,,Male,Male,Unknown,Male,"Covering models, for the most part, consider demand concentrated at demand points to be covered by facilities with a given radius. Maximal covering models (Church and ReVelle 1974) have received a lot of attention since the early 1970s. In these models demand is generated at demand points and a set of possible locations for facilities is given. A demand point is covered by a facility if its distance from the facility does not exceed a coverage radius R. The objective is to locate a given number (p ≥ 1) of facilities to cover maximum demand. The maximum covering problem, when demand points are located in the plane and facilities can be located anywhere in the plane, was also investigated (Church 1984; Drezner 1981, 1986; Watson-Gandy 1982). The maximum cover problem on tree networks was investigated in Megiddo et al. (1983). Curtin et al. (2010) proposed a maximal covering formulation with backup covering for design of police patrol areas. Set covering problems (ReVelle et al. 1976) aim to cover all demand points with the minimum number of facilities. Minimum cover problems, modeled for obnoxious facilities, were analyzed and solved in Berman et al. (1996) for the location of one facility on a network. Berman and Huang (2008) studied the multiple facility version of this problem on a network. The objective is minimizing the total demand within a given radius from the facilities subject to a constraint that the shortest distance between each pair of facilities is at least a given distance. The problem that combines minimum and maximum covering by allowing positive and negative “demand” was analyzed in (Berman et al. 2009). For a review of covering problems the reader is referred to Kolen and Tamir (1990), Current et al. (2002), Plastria (2002). Recently, some research was published on different definitions of cover. Rather than coverage within a given radius with abrupt change from being covered to not being covered, gradual, general, and partial coverage are defined. See Berman and Krass (2002), Berman et al. (2003), Karasakal and Karasakal (2004), Eiselt and Marianov (2009) for models on a network and Drezner et al. (2004; Drezner et al. 2010) for location on the plane. The cooperative cover defined as cover by cumulative effect of several facilities is discussed in Berman et al. (2010a). For a review of extensions to covering models see Berman et al. (2010b). One of the models in ReVelle et al. (1976) is the cover of links (arcs) rather than cover of demand points. Facilities can be located on nodes. An arc is covered if and only if one facility covers the whole arc. An arc covered jointly by two or more facilities is not considered covered unless either facility alone can cover the arc. Church and Meadows (1979) removed the restriction of joint cover. An arc can be covered jointly by more than one facility. Church and Meadows (1979) also considered partial coverage of arcs. Current and Schilling (1994) investigated the problem of finding a tour connecting p nodes with a minimum length and access to nodes not on the tour. Several papers investigated coverage of links rather than nodes of a planar network when the facility can be located anywhere in the interior of the network. Drezner and Wesolowsky (1996) sought the location of an obnoxious facility in the interior of a planar network, maximizing the minimum distance to all links of the network. Drezner et al. (2009) assumed that the nuisance generated by the facility declines by the square of the distance from the facility and sought the location of a facility in the interior of a planar network that minimizes the total nuisance inflicted on the network by the facility. Yin (2006, 2008) and Curtin et al. (2010) considered locating police patrols for maximal covering of network links. The issue of transporting hazardous material on a network affecting on- and off-network establishments was investigated in many papers: (Karkazis and Boffey 1995; Bell 2006; Wang et al. 2012; Berglund and Kwon 2014). The survivability of a network following a disaster is investigated in Du and Peeta (2014). In this paper we consider two models of covering links rather than nodes of a network. Facilities are to be located in a given feasible area and cover some links or parts of links within a given radius R. The feasible area can be the interior of a planar network (convex hull of the nodes) or any union of convex polygons. For example, if the network has a “U” shape, it can be presented as a union of three convex polygons if the “empty” center of the network is not feasible. One model seeks maximum coverage of link lengths and another model seeks the minimum coverage of link lengths. The maximum coverage model is appropriate for the location of cell phone transmitters or medical helicopter sites to cover as much as possible a network of highways. When the whole network needs to be covered with the minimum number of facilities, the equivalent of the set covering problem can be applied. The minimum covering model is appropriate either for the location of an obnoxious facility or for the location of a facility in the interior of an obnoxious network. In the first case, the nuisance generated by the facility affects the points on the network within a given radius. The obnoxious facility can be a polluting establishment that may occasionally disrupt traffic on freeways. In the second case the facility is affected by nuisance generated by the network within a given radius. You wish to locate a facility such as an hospital, school, small neighborhood, at a point where the nuisance generated by a network (noise, risk, etc.) is minimized. Another example is hazard generated by earthquake faults which are present in an area such as Southern California or Japan. When an earthquake occurs on a fault, serious damage is expected to be caused within a certain distance from the epicenter. A facility is to be located in some feasible region so that the probability of suffering serious damage when an earthquake occurs is minimized. The probability that an earthquake will cause damage is proportional to the length of the covered faults.",3
14.0,3.0,Networks and Spatial Economics,18 September 2014,https://link.springer.com/article/10.1007/s11067-014-9264-4,Inferring Urban Land Use Using Large-Scale Social Media Check-in Data,December 2014,Xianyuan Zhan,Satish V. Ukkusuri,Feng Zhu,Unknown,,,Mix,,
15.0,1.0,Networks and Spatial Economics,18 September 2014,https://link.springer.com/article/10.1007/s11067-014-9265-3,Cooperation with Externalities and Uncertainty,March 2015,Helga Habis,Dávid Csercsik,,Female,Male,Unknown,Mix,,
15.0,1.0,Networks and Spatial Economics,30 September 2014,https://link.springer.com/article/10.1007/s11067-014-9266-2,Agents of Change and the Approximation of Network Outcomes: a Simulation Study,March 2015,Bruno Wichmann,,,Male,Unknown,Unknown,Male,"Modern economic systems are becoming complexly interconnected and economic behavior is often influenced by networks of relationships. For instance, student performance and participation in retirement plans depend on the behavior of peers (Lin 2010; Duflo and Saez 2002). The adoption of new agricultural technology depends on social learning through networks (Conley and Udry 2001). Crime rates are affected by the rates of neighboring areas (Baller et al. 2001). It is important, therefore, to incorporate network structure when modeling economic outcomes that are subjected to social or spatial effects. This paper analyzes economic environments that can be represented by a system of linear equations. In our environment, an agent’s outcome is a function of the average outcome of connected agents. The outcomes of these agents are, in turn, a function of the original agent’s outcome. This formulation introduces the reflection problem (Manski 1993). However, under certain regularity conditions, this system of equations has a stable solution. We study the use of Neumann approximations (see Meyer 2000) of the system’s solution to decompose network effects into: i) the effect of an agent’s own exogenous variables on her outcome; ii) the effect of the agent’s connections on her outcome; iii) the effect of connections of connections, and so on.Footnote 1
 Through simulations we generate economic systems with networks constructed from three models: Erdös-Renyi, Watts-Strogatz, and Barabási-Albert. The simulated data allow us to study the precision of the Neumann approximations. Outcomes are predicted through approximations of orders 1 to 30. We evaluate how precision improves as higher order approximations are considered. The first two random models of network formation allow us to study the effects of density and intransitivity on the prediction errors. This is possible because Erdös-Renyi networks trade off density and intransitivity while with Watts-Strogatz networks we can fix density and vary intransitivity. In addition, we use the Barabási-Albert model to simulate scale free networks. This is important because the vast majority of socio-economic networks exhibit this topology. We find that, for all three network types, a two-order approximation is sufficient to predict economic outcomes with precision of 5 % when the peer-elasticity of outcomes is below 0.3. This is equivalent to an economic system with social (or spatial) multiplier below 1.43, i.e. systems in which the network intensifies the effects of exogenous sources of outcome variation by approximately 43 %.Footnote 2 As the social multiplier increases, higher-order approximations are necessary to achieve the 5 % error threshold. Using the results from the Erdös-Renyi and Watts-Strogatz models, we find that density and intransitivity of networks do not influence prediction errors. We also find that errors are not affected by the scaling parameter (or the power of the preferential attachment) of the Barabási-Albert networks. The simulations indicate that a second-order Neumann approximation coupled with a snowball sampling scheme is a valuable approach for predicting economic outcomes of a group of agents in a networked economic system. We show that sampling the targeted group, their connections, and the connections’ connections is enough to predict economic outcomes with errors below 5 % when social multipliers are less than or equal to 1.43. Therefore, one of the main contributions of the paper is to demonstrate that for several economic environments the data required to study the economic outcome of a group of targeted agents can be significantly reduced to information about the group and those agents with two-degrees of separation from the group. The Neumann approximation applied to this reduced sample is able to produce fairly precise outcome predictions despite the complex network structure connecting the outcomes of all agents in the economic system. This result is robust to changes in network size and in the shape of the distribution of exogenous variation. We also perform tests using real-world spatial and social networks and the results remain unchanged. Our findings suggest that agents with two degrees of separation from a targeted group are instrumental in introducing social changes for this group. Hence, these agents can be thought of as agents of change or opinion leaders for the group. Identification of change agents enables the design of more effective economic policy when the goal of policy is to affect economic outcomes of a group of agents.Footnote 3 If a certain group in a networked system is the target of economic policy, policy effort should concentrate on the group’s agents of change as opposed to the entire network. The paper contributes to a literature about sampling networks. Several papers concentrate on estimation of network properties from sampled data. This is a traditional research area within network analysis that still catches the interest of current scholars. Examples of work in this field include Frank (1979); Erickson and Nosanchuk (1983); Galaskiewicz (1991); Costenbader and Valente (2003); Illenberger and Flötteröd (2012), to name a few. In contrast with this literature, the paper explores how a network sample can be used to estimate outcomes that are influenced by networks, as opposed to estimating networks characteristics. The reminder of the paper is organized as follows. Section 2 presents the model and its solution. Section 3 introduces the Neumann approximation of the solution. Simulations are discussed in Section 4. Section 5 addresses implications of the simulations results. Robustness checks are presented in Section 6. Section 7 offers a discussion and concluding remarks.",2
15.0,1.0,Networks and Spatial Economics,18 October 2014,https://link.springer.com/article/10.1007/s11067-014-9269-z,Heuristic Algorithms for Solving an Integrated Dynamic Center Facility Location - Network Design Model,March 2015,Abdolsalam Ghaderi,,,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Networks and Spatial Economics,03 October 2014,https://link.springer.com/article/10.1007/s11067-014-9270-6,Bargaining Power and Value Sharing in Distribution Networks: A Cooperative Game Theory Approach,March 2015,Roberto Roson,Franz Hubert,,Male,Male,Unknown,Male,"This paper is intended to illustrate a novel methodology for analyzing bargaining games on network markets, by means of numerical models that can be calibrated with real data. Several studies and models are now available for the analysis of market interactions that take place over a network infrastructure, in which flows are distributed according to principles of economic equilibrium e.g., (Roson 1993; Friesz et al. 2001; Lederer 2003). A few applications of game theory to such network markets are also available (Metzler et al. 2003; Altman and Wynter 2004) but, to the best of our knowledge, cooperative games have never been examined in this context. On the other hand, the burgeoning network economics literature does have considered cooperative games as value or cost allocation rules in networks (Myerson 1980; Jackson 2005), but the “payoffs” of the different coalitions are not typically derived from explicit market and network equilibria. This paper bridges the gap and shows that, once payoffs are dependent on given network and market structures, several interesting features, relevant in real-world markets, can be fruitfully analyzed. We focus on specialized physical networks such as oil and gas pipelines, water-supply or irrigation systems, rail transport systems etc. The usage of such systems often requires the cooperation of many partners, each of whom controls only a small part of the network. In such networks the control of dedicated resources such as switches, connections, exit and entry points defines a power structure. This power structure, in turn, is likely to affect the incentives to link up with the network, to develop it by adding new links or by increasing the capacity of existing ones. We start from the assumption that the power of a player in such a network will depend on the value of his resources for other players. For the physical networks mentioned above, it is often possible to assess with a reasonable degree of confidence how they should be used optimally, if some resources were not available. This enables us to define a cooperative game. For a group of players, also called a coalition, we first determine which parts of the network would be accessible. Then we use a network optimization model to calculate the total payoff, achieved by the optimal usage of the sub-network under their control. This payoff is the value of the coalition. Obviously, the Grand coalition of all players has the whole network at its disposal and can generate the largest payoff. By repeating this optimization procedure for all possible coalitions, we obtain the so called value function of a cooperative game, which captures all the economic and technical features of the network. To obtain a measure of the players’ power in the network market, we solve the game with the Shapley Value. While not the only possible theoretical solution, the Shapley value is the most widely used one (Moretti and Patrone 2008) and already well accepted as a power index for voting games (Shapley and Shubik 1954). Footnote 1
 The work is inspired by recent applications of cooperative game theory to gas-pipeline systems. Hubert and Ikonnikova (2011) analyze gas transit to northwestern Europe. Hubert and Cobanli (2014) consider the impact of new pipelines on the power structure in the Eurasian gas market and (Hubert and Orlova 2014) investigate the liberalization of pipeline access within the European Union. These papers consider particular networks calibrated with real world data. This paper examines, instead, an abstract, fictitious network, but enlarges the scope of the analysis to consider a number of variants, which could be highly relevant in many applied settings: market imperfections, outside options, varying demand, external benefits or costs. The discussion of the different model variants highlights the flexibility of the proposed methodology. Furthermore, it points out a typical feature of these models: incentives for the expansion of the network typically differ between large coalitions and smaller sub-coalitions, even when agreement between only a limited number of players is necessary to expand the network. The rest of the paper is organized as follows. The next section describes the basic methodology, which is illustrated in Section 3 by means of a simple numerical example. Section 4 introduces a number of variants in the model, and discusses the effects of some alternative hypotheses on the example network. Section 5 concludes and provides suggestions for future research. An Appendix presents the numerical results obtained by using the nucleolus instead of the Shapley value as an allocation concept.",16
15.0,1.0,Networks and Spatial Economics,13 January 2015,https://link.springer.com/article/10.1007/s11067-014-9271-5,A Logit Model With Endogenous Explanatory Variables and Network Externalities,March 2015,Louis de Grange,Felipe González,Rodrigo Troncoso,Male,Male,Male,Male,"Logit multinomial discrete choice models are frequently used in marketing to model consumer preferences, and are also employed in transport system planning to represent trip demand and ground use. They are traditionally built from random utility models and estimated using maximum likelihood (McFadden 1974; Ortúzar 1982; 1983; Train 2003; Ortúzar and Willumsen 2011). Alternatively, however, they have been derived as the solution to certain constrained entropy maximization problems in which the Lagrange multipliers of the constraints are the model’s parameters (Anas 1983; Donoso and De Grange 2010; Donoso et al. 2011). In discrete choice models, interaction between supply and demand (Berry et al. 1995; Petrin and Train 2010) and the omission of variables unobservable to the researcher (Villas-Boas and Winer 1999) may cause endogeneity. In both cases, the problem can be handled by introducing instrumental variables into the estimation. The existence of endogeneity has also been traced to the definition of the choice set facing the individual (Haab and Hicks 1997; Louviere et al. 2005). In this study, endogeneity is considered to be the result of network externalities or social interactions of the type peculiar to transport systems. More specifically, it emerges from the fact that the attractiveness of a particular alternative depends on the number of persons choosing it (Yamins et al. 2003; Blumenfeld-Lieberthal 2009; Bogart 2009). But whereas the attractiveness of a technology product, for example, often varies positively with user numbers, the choice of a transport mode can be negatively impacted by user numbers because it is influenced negatively by congestion. To deal with this phenomenon we propose a novel specification for logit models that corrects the endogeneity bias introduced by network externalities and social interactions. The approach that will be taken is based on entropy maximization with explicit incorporation of the endogeneity caused by network externalities or social interactions into logit multinomial discrete choice models. An equivalent maximum entropy optimization problem is formulated in which the explanatory variables facing individuals depend on the decisions they make (e.g., with private transport, trip time depends on congestion; with public transport it depends on wait time or crowding at bus stops or metro stations). This interdependence produces endogeneity in the model’s explanatory variables, whose values will therefore depend on the individuals’ decisions. The result of this approach is an alternative version of the logit multinomial model that has the functional form of a fixed-point equation in which the choice probabilities depend on themselves. Estimation is by maximum likelihood, and no additional variables (e.g., instruments) other than the original ones in the model are required, a major advantage of the proposed method. The remainder of this article is organized into four sections. Section 2 introduces the theoretical framework for the proposed methodology and briefly surveys the literature on endogeneity in discrete choice models. Section 3 formulates the Logit model with endogenous explanatory variables. Section 4 presents a numerical example using simulations and real data for a city in northern Chile. Finally, Section 5 sums up the main conclusions and contributions of this study.",11
15.0,1.0,Networks and Spatial Economics,17 December 2014,https://link.springer.com/article/10.1007/s11067-014-9272-4,Integrating Intermittent Renewable Wind Generation - A Stochastic Multi-Market Electricity Model for the European Electricity Market,March 2015,Jan Abrell,Friedrich Kunz,,Male,Male,Unknown,Male,"Electric power industries in Europe have experienced a major restructuring process toward a competitive market environment in which generators face the fundamental task of determining the optimal dispatch of their thermal power plants. In contrast to former monopolistic times, generators now have to recover their costs solely through prices determined in different electricity markets: In a dayahead market, generators offer supply of electricity for all hours of the subsequent day. In the intraday market these offers can be continuously revised reacting to new information available in the market. Finally, the transmission system operator (TSO) needs to ensure that electricity supply announced by the generators is feasible, i.e., the TSO has to ensure reliability of electricity supply. On the other hand, concerns about climate change initiated an ongoing transformation of the electricity system toward less carbon-intensive generation technologies. Therefore, several European countries have implemented special support schemes for renewable energy sources in order to, firstly, not just reduce domestic carbon dioxide emissions but also fossil fuel import dependency in the energy sector. In this context, wind energy, among others, has become a dominating renewable source due to favorable natural conditions, technological progress, and political support. For instance, between 2000 and 2010, 75 GW wind capacity was installed in Europe, resulting in a share of 10 % in the European power capacity mix (EWEA 2011). However, wind generation is characterized by a variable and uncertain generation pattern as it directly depends on meteorological conditions. Hence, it cannot be dispatched in a controlled manner like conventional power plants. Consequently, variability and uncertainty about the residual load left to conventional dispatchable generation technologies is increasing, which imposes additional integration costs as the balance between demand and supply needs to be ensured at every instant of time by conventional generation technologies. In this article, we develop a quantitative framework mimicking the subsequent clearing of different electricity markets. Special emphasis is put on the role of physical restrictions imposed by the electricity transmission system and the intermittent nature of wind power. Furthermore, the framework is applied to assess the impacts of different electricity markets against a hypothetical benchmark of integrated planning. Moreover, we evaluate the costs of intermittent wind power and its dependency on the modeling approach to uncertainty. We find that employing a stochastic programming approach lowers system costs compared to deterministic optimization based on expected values. This is achieved by creating flexibility in the generation mix in two ways: first, using traditional flexible gas-fired technologies. Second, operating rather inflexible coal-fired plant in part-load creates the flexibility to increase generation in the short-run avoiding additional startup of capacity. To address the complex interactions in electricity systems, unit commitment and economic dispatch approaches are applied to determine a secure and economically optimized generation scheduling. As most electricity systems are dominated by thermal generation capacities, the aim of the short-term planning is to determine the least-cost generation mix of different technologies to meet a specified electrical load taking into account operational limitations of thermal units, i.e., minimum ontime, minimum offtime, and ramping constraints.Footnote 1 As the variability and uncertainty of renewable generation influences the short-term operation of the electricity system, elements of stochastic optimization have been introduced into the unit commitment framework.Footnote 2 Recently, a variety of contributions focus on the large-scale integration of renewable wind generation in power systems and both their short- and long-term implications on the electricity system. In the long-term, the appropriate development of transmission as well as generation infrastructure has to ensure a secure and efficient integration of renewable energy sources (e.g. Traber and Kemfert 2011; Weijde and Hobbs 2012; Spiecker and Weber 2014). Regarding the short-term implications, the variability and uncertainty inherent in wind generation is a dominating aspect affecting, in particular, the unit commitment of thermal generation units. Most studies generally focus on optimal unit commitment strategies under stochastic wind generation within a two-stage framework consisting of first-stage unit commitment and a second-stage dispatch (e.g. Bouffard and Galiana 2008; Wang et al. 2008; Ruiz et al. 2009; Pritchard et al. 2010; Wang et al. 2011; Papavasiliou and Oren 2013). While the incorporation of uncertainty generally leads to an increase of system costs, the unit commitment and dispatch of power plants is more robust to short-term fluctuations of intermittent generation, which then can reduce the need for dayahead reserve procurement. However, the structure of the underlying market regime is often condensed to two stages reflecting dayahead and real-time decisions, whereas most market regimes, i.e. Germany, are characterized by a subsequent clearing of daily dayahead and hourly intraday electricity markets. Thus, using a two-stage setting abstracts from the fact that forecasts as well as, henceforth, commitment and dispatch decisions are revised during the day (Papavasiliou and Oren 2013). Within a multi-market setup, the interaction of revised renewable generation forecasts, which improve over time, and the flexibility of committed generation can be explicitly considered. On the one hand, renewable generation forecasts become more accurate closer to real-time. On the other hand, conventional generation is less flexible closer to real-time due to unit commitment restrictions. This trade-off is important to consider when investigating integration aspects of uncertain renewable generation. The successive clearing of the dayahead and intraday markets, as well as the improvement in renewable forecasts is incorporated in Weber et al. (2009). They formulate a stochastic programming model to assess the impact of large-scale wind power generation on electricity systems. A rolling planning procedure is implemented to link the different electricity markets.Footnote 3 The stochastic behavior of wind generation is explicitly taken into account and the model thus allows the assessment of the impact of increased wind generation on reserve needs and usage, power plant operation and system costs. Tuohy et al. (2009) present an updated version of Weber et al. (2009), including a mixed-integer unit commitment. However, both approaches have in common that they abstract from the physical characteristics of electricity flows and the management of congestion by the transmission system operator. In other words, congestion in the physical transmission network which may influence the utilization of thermal as well as renewable capacities is not considered. The management of network congestion is particularly important with increasing renewable capacities (Kunz 2013; Neuhoff et al. 2013).Footnote 4 With respect to electricity transmission, Leuthold et al. (2012) describe a deterministic techno-economic model with a detailed representation of the European high voltage network. Physical characteristics of power transmission are represented by a DC-loadflow approach. In various applications, the impact of wind power generation on the power system, in particular on the physical transmission network, are analyzed (e.g. Leuthold et al. 2009; Weigt et al. 2010), but mostly abstracting from a detailed market representation. The approach presented in this paper combines the characteristics of the different electricity markets as well as the technical specifics of thermal generation with the characteristics of transmitting electricity. Thus, the main contribution of this paper lies in the combination of these two characteristics into one modeling framework. Additionally, the intermittency of wind generation is explicitly taken into account by employing stochastic programming techniques. In this paper we describe a stochastic Electricity Market Model (stELMOD), which is used to investigate the impact of stochastic wind generation on the unit commitment and dispatch of power plants taking into account limitations through physical network congestion. The successive market-clearing process of dayahead and intraday markets is rebuilt considering the arrival of improved information on wind generation forecasts in each optimization step. After clearing of the daily dayahead and the subsequent hourly intraday markets, the final power plant dispatch is determined by the transmission system operator considering network congestion arising from previous market commitments. Uncertainty about wind generation is represented by a two-stage multi-period scenario tree and updated for each intraday market optimization. Our general modeling framework resembles Tuohy et al. (2009). In contrast to Tuohy et al. (2009) we incorporate an explicit representation of transmission flows and congestion management based on Kunz (2013). A DC-loadflow approach is used to determine physical electricity flows in the interconnected transmission network. The model is applied to the German electricity system covering a time frame of 168 hours (one week) in order to investigate the impacts of stochastic wind power availability on the German electricity system. We further contribute to the existing literature by evaluating the effects of a rolling planning approach compared to an integrated central planning perspective. The remainder of the paper is structured as follows. In the next section we briefly describe the daily market procedure of the German electricity market. Based on this framework, three distinct models – a dayahead market, an intraday market, and a congestion management model – are developed and coupled by a rolling planning procedure to reflect the subsequent market clearing. Section 3 presents the data used including the derivation of wind generation forecasts. In Section 4, we compare different approaches of incorporating stochastic supply of wind power to a deterministic case. Furthermore, we evaluate the effects of rolling planning procedure compared to an integrated approach which is optimized over the whole time horizon. Section 5 formulates the conclusions.",32
15.0,1.0,Networks and Spatial Economics,09 November 2014,https://link.springer.com/article/10.1007/s11067-014-9273-3,Removing Cross-Border Capacity Bottlenecks in the European Natural Gas Market—A Proposed Merchant-Regulatory Mechanism,March 2015,Anne Neumann,Juan Rosellón,Hannes Weigt,Female,Male,Male,Mix,,
15.0,1.0,Networks and Spatial Economics,08 February 2015,https://link.springer.com/article/10.1007/s11067-014-9275-1,Optimal Deployment of Alternative Fueling Stations on Transportation Networks Considering Deviation Paths,March 2015,Yongxi Huang,Shengyin Li,Zhen Sean Qian,Unknown,Unknown,,Mix,,
15.0,2.0,Networks and Spatial Economics,17 March 2015,https://link.springer.com/article/10.1007/s11067-015-9283-9,Resilience and Vulnerability of Spatial Economic Networks,June 2015,Simone Caschili,Aura Reggiani,Francesca Medda,Female,Female,Female,Female,,32
15.0,2.0,Networks and Spatial Economics,20 August 2014,https://link.springer.com/article/10.1007/s11067-014-9261-7,Spatial Economic Resilience: Overview and Perspectives,June 2015,Marco Modica,Aura Reggiani,,Male,Female,Unknown,Mix,,
15.0,2.0,Networks and Spatial Economics,18 September 2014,https://link.springer.com/article/10.1007/s11067-014-9267-1,Network Hub Structure and Resilience,June 2015,Morton E. O’Kelly,,,Male,Unknown,Unknown,Male,"Hubs are concentrator nodes that are critical to the continued operation of transportation and communication systems. The potential to disrupt a large amount of traffic by attacking hubs makes them a tempting target. It is clear that these potential disruptions impact businesses that rely on commercial interaction for business-to-business (B2B) and other critical communications (Campbell and O’Kelly 2012). This paper examines the vulnerability of hub interconnection points. The research points to parts of a network that require strengthened defenses to prevent loss or damage on a broader scale. While there have been numerous comprehensive reviews of many aspect of the vulnerability and resilience of complex networks, there are some important connections to be made from the perspective of hub network design. Much research is focused on complexity and physical transport infrastructure (Lordan et al. 2014) but with a broader perspective, it is apparent that significant contributions have been made by clusters of academics with a more general spatial view. These include groups such as Schintler et al. (2005 and 2007); Reggiani and Nijkamp and collaborators (see for example Reggiani and Nijkamp 2009 and 2012); and Murray and Grubesic and collaborators (see for example Murray and Grubesic 2007). In defining concepts related to vulnerability, [e.g. robustness, resilience, redundancy, and reliability] there is a need to tolerate ambiguity and to recognize that what might appear to be end members of a continuum sometimes have subtle differences in interpretation, depending on the field of application. In broad terms vulnerability “is the state of susceptibility to harm from exposure to stresses associated with environmental and social change and from the absence of capacity to adapt” (Adger 2006). In the context of electric power grids, Holmgren (2007) states that “vulnerability is described as a susceptibility (sensitivity) to threats and hazards that substantially will reduce the ability of the system to maintain its intended function.” The key to both views is that any attack that prevents the system from operating smoothly is problematic and the system’s inability to adapt to this attack is at the heart of its vulnerability. Vulnerability and robustness are typically seen as complementary concepts; for example Sullivan et al. (2009) present a review related to the field of network-disruption analysis, including measures of network robustness and vulnerability. Others view robustness as a capability to withstand normal errors whereas vulnerability is a systemic weakness to determined attack (Barabási 2003, 118). Resilience according to the national infrastructure protection plan (Department of Homeland Security 2013), on the other hand, is “the ability to prepare for and adapt to changing conditions and withstand and recover rapidly from disruptions… [it] includes the ability to withstand and recover from deliberate attacks, accidents, or naturally occurring threats or incidents.” In a general sense then vulnerability is a lack of resilience or robustness (see also Holmgren 2007). Reggiani (2013) provides a summary of the important terms such as network vulnerability, robustness, resilience, scale-free network and so on. For example, redundancy can mean wasteful excess but also has a close bearing on the idea of back up coverage and protection from exposure to a single bottleneck. A system with some built-in redundancy is capable of providing a good work-around in the event of a failure. A system with some redundancy can provide resilience (Fiksel 2003) leading to its longer term sustainability. Cardoso and Diniz (2009) show that there is a critical level of additional capacity that can protect vulnerable hubs. The availability of back-up solutions is dependent on the system operator providing something more than a bare bones infrastructure and in this sense redundant alternatives contribute to the system’s resilience. There is a view that a little redundancy (which is the opposite of complete efficiency) gives networks a chance to survive (Reggiani et al. 2002). The paper referenced below has an interesting counter position -- networks evolution rewards modularity, and that connected modules that are sparsely linked to distant modules are easy to repair (Clune et al. 2013). With these terms in hand, it is now possible to consider whether systems can be both vulnerable and resilient, or whether vulnerability itself is an antonym for resilience. Further discussion of this point in the context of actual networks follows below.",49
15.0,2.0,Networks and Spatial Economics,14 September 2014,https://link.springer.com/article/10.1007/s11067-014-9259-1,Analysis of Critical Infrastructure Network Failure in the European Union: A Combined Systems Engineering and Economic Model,June 2015,Olaf Jonkeren,Ivano Azzini,Georgios Giannopoulos,Male,Male,Male,Male,"The Joint Research Centre (JRC), among others, provides technical support to European policies in the domain of Critical Infrastructure Protection. These policies are part of the overall framework of European Programme for Critical Infrastructure Protection (EPCIP). The legislative tool of EPCIP is the Council Directive 2008/114/EC for the identification and designation of European Critical Infrastructures and the assessment of the need to improve their protection. Its goal is to assess and address the needs for improving the protection of CI’s. The relevant sectors and subsectors that are currently covered by the aforementioned Directive are depicted in Table 1. The Directive mentions the economic effects of infrastructure failure as one of the criteria on which an infrastructures’ criticalness is evaluated.Footnote 1 The objective of this paper is therefore to establish a methodology to assess those effects. This methodology is grounded in a model which consists of two components: a Systems Engineering model and a Dynamic Inoperability Input–output model (SE-DIIM for short). The model first applies the systems engineering component to analyse performance degradation and recovery of a disrupted infrastructure network. Next, economic losses are estimated for a system of CI’s and economic sectors using the DIIM component, which leverages information from the SE component. In both components, resilience is included in the analysis where we distinguish between static and dynamic resilience. These measures for resilience are adopted from Rose (2007; 2009) where static resilience refers to the ability of a system to maintain function when shocked and dynamic resilience concerns the speed of recovery of a system.Footnote 2 The relevance of the resilience concept for socio-economic systems was already pointed out by Reggiani et al. (2002). The SE component is based on a modular technique which enables modelling interconnected infrastructures taking into account the functional interdependencies existing between them in a comprehensive way (Filippini and Silva 2014). Its key characteristics are that it is applicable at different scales of CI networks while it requires a limited amount of state variables, thus reducing the overall data needs. For our study, the SE component is applied to the Italian electricity infrastructure network being an interconnection of four major districts: Northern Italy, Central Italy, Southern Italy and Sicily. This enables us to build up a structured representation of 1) the gradual spread of the blackout event, 2) the service restoration process, which took place on different time scales due to the sudden nature of the accident and, 3) the stress level impacting the whole Italian electricity network. The economic component (the DIIM) analyses cascading inoperability – a measure for the level of dysfunction of an infrastructure—that results from interdependencies within a system of infrastructures and economic sectors (Crowther and Haimes 2005).Footnote 3 The importance of the workforce (the household sector) in the context of hazard loss estimation is emphasized several times in the literature (Santos and Haimes 2004; Ferrari et al. 2011). Workforce is therefore included in the DIIM and modelled as a CI sector. In order to include all main actors in an economy (businesses, households and the government) we also close the model with respect to government sales and purchases. In its application in the present study, the DIIM estimates the economic losses resulting from the electricity infrastructure network disruption in Italy in 2003, where cascading losses to other CI’s and economic sectors are included in the estimation. The economic component of the model is chosen specifically for its universality and adaptability. Indeed Greenberg et al. (2012) claim that the Inoperability Input–output Model (IIM) is one of the ten most important accomplishments in risk analysis in the past 30 years. The reason for applying an engineering model in tandem with an economic model is threefold. Firstly, the SE component captures with high accuracy the propagation and reduction of inoperability in one or more infrastructures. Consequently this specific information, which is generally absent in such detail as input data in DIIM applications (see Anderson et al. 2007 for example), is fed into the economic component. Secondly, with a DIIM it is assumed that interdependency levels between infrastructures and economic sectors can be approximated by the level of the monetary value of exchange of goods and services between them. However, especially during unstable states like electricity blackouts, functional interdependencies are likely to predominate (Oliva et al. 2011). As the SE component is able to model those functional interdependencies, its use is paramount in order to tune the DIIM for accurately assessing economic impact of CI disruptions. In some studies (Devogelaer and Gusbin 2004; De Nooij et al. 2007), it is assumed that for infrastructures which have a functional relationship, the transmission of a failure is total. This is another way for correcting the above mentioned deficiency of the DIIM but likely leads to an overestimation of the economic impact because it neglects the presence of resilience measures. Thirdly, the SE-DIIM satisfies the point made by Buldyrev et al. (2010) and Gao et al. (2012) that complex networks have been studied intensively, but research still focuses on the limited case of a single, non-interacting network. In the end, many real-world networks do interact with and depend on other networks. Economic losses can be defined as stock damage or flow losses (business interruption losses). Flows refer to the services or outputs of stocks over time while stocks refer to a quantity at a single point in time. Property damage represents a decline in stock value and usually leads to a decrease in service flows. Flow losses originate only in part from a company’s own property damage and can occur without the presence of property damage Rose (2009). Both, stock damage and flow effects can be of direct or indirect nature. Direct effects are sustained by the sector that is hit by a particular hazard. Indirect effects impact on sectors that are located in the close vicinity of the initially hit sector (indirect stock damage) or that are dependent on the initially hit sector through supply and demand relationships (indirect flow effects). Table 2 summarizes the different types of economic impact. Input–output (I-O) models, and thus also our DIIM, ignore stock damages and only take into account direct and indirect flow losses. Including both stock damages and flow losses would result in double counting. The value of an asset, for example equipment pertaining to an infrastructure, is the discounted flow of net future returns from its operation. So suppose that a machine with a 1-year lifespan is destroyed, and not replaced for a year, then the economic loss is equal to either the value of a replacement machine with a 1-year lifespan or the discounted flow of not produced output for 1 year (Rose and Lim 2002). In the next section, the theoretical framework of the SE and the DIIM components are described. In Section 3, an application of the model to the 2003 electricity blackout in Italy is discussed. The last section reports the conclusions.",23
15.0,2.0,Networks and Spatial Economics,21 September 2014,https://link.springer.com/article/10.1007/s11067-014-9268-0,A Model of Stratified Production Process and Spatial Risk,June 2015,Tatsuaki Kuroda,,,Unknown,Unknown,Unknown,Unknown,,
15.0,2.0,Networks and Spatial Economics,14 September 2014,https://link.springer.com/article/10.1007/s11067-014-9262-6,Emergence and Resilience in a Model of Innovation and Network Formation,June 2015,Rainer Andergassen,Franco Nardini,Massimo Ricottilli,Male,Male,Male,Male,"It is a well established fact that interaction among firms generates technological spillovers and that network topology has a crucial importance in spreading information and knowledge that enable firms to innovate. The purpose of this paper is to determine the resilience of a firms’ spillover-propagating network as it evolves once hit by periodic, random, technological shocks that cause structural shifts in the aggregate capability to innovate. Resilience can be understood either as a static property, namely as the capability of a system to maintain its functions and structure when subject to change, or dynamically as the system’s ability to recover its former operational state after a perturbation involving one of its state variables (Allenby and Fink 2005; Brede and de Vries 2009; Schweitzer et al. 2009a, b). In this study, we adhere to the latter of the two concepts to discuss resilience in terms of the system’s capability to recover its innovative performance after being struck by shocks. The latter can be understood as idiosyncratic, firm-specific innovations that are generated in the economy as an upshot of R&D efforts. In this context, firms that are able to apply, imitate or otherwise adopt them through adaptation stand to reap benefits in terms of higher technological capabilities; firms that do not are subject to obsolescence and therefore lower performance. In this paper, we distinguish two different but definitely complementary and overlapping ways through which R&D occur. The first exploits the spillover potential that lies in a firm’s neighbourhood of other firms and thanks to which gathering innovation-useful information is actually possible. The second rests with the autonomous capacity that a firm possesses in order to carry out in-house innovative research. While these two R&D processes not only coexist but are also reciprocally sustaining, we find it expedient to separate them by integrating a knowledge diffusion mechanism that propagates technological capabilities with an independent stochastic process capturing innovation arrivals due to internal, in-house, R&D. Thus, the paper investigates a spillover searching process that unfolds over a virtual space of interactions; this space rather than being a geographical notion is a locusof knowledge and information exchange (see also Andergassen et al. 2006b).Footnote 1
 It is well established that because of bounded rationality, the gathering of relevant information occurs within the confines of small neighborhoods. This fact owes to firms’ limited ability to explore a given system’s cognitive complexity, i.e. the full range of all the technologies that firms apply. Network evolution depends on how firms assess their performance and the spillovers that they are able to collect. In a bounded rationality framework, they normally resort to a protocol. We address this issue by positing a routinized behavior that adaptively reshapes the neighborhood that firms observe to glean information. The way the specific neighbor-choosing routine is organized determines in a significant way how resilient or vulnerable the economic system is in the face of innovative change. The analysis of the relationship between search routines, the economic system’s resilience and the emerging network architecture is central to our paper. To clarify this relationship, we have thought it expedient to define two distinct but to some extent overlapping neighborhoods which are relevant for firm’s interaction. The first is the neighborhood whose members are observed by each firm and from which contributions to innovative capacity are obtained. We term this neighborhood inward. The second is the one made up by a firm’s observers, i.e. by firms observing and learning from it: it evolves as an active search for new inward members is carried out. This neighborhood is, instead, termed outward. It is shown that this interactive process leads to the emergence of some firms that are observed by most others. It is this few that provide much of the overall technological capability: because of this outstanding role, they are called paradigm setters. More formally, we define them as firms whose technological features set a paradigm for most other firms with positive probability and we investigate the relationship between the emergence of paradigm setters and the system’s resilience. The spreading of knowledge through information flows is a crucial aspect of the network evolution that we discuss. Such flows, however, involve firms of different cognitive capabilities. We define the latter as the set of skills, know-how and expertise that is specific to a given knowledge base. This heterogeneity is taken into account by modeling the economy as being partitioned in areas of homogeneous technological knowledge within which information spreads relatively unhindered whilst it encounters greater difficulty across them. It will be shown that cognitive heterogeneity is an important factor in determining interaction and the system’s overall resilience. We formalize the features stated above by dividing the economy into areas of specific knowledge bases within which information is likely to spread with relative ease whilst encountering hurdles when journing across. Finally, the model is then simulated to determine how search routines in the presence of heterogeneous knowledge affect the economic system’s resilience. We aim to identify (i) the system’s resilience to recover technological capability when struck by random shocks; (ii) under what conditions the emergence of technological paradigm setters occurs, (iii) the pattern of neighborhood formation and (iv) the relationship between the emergence of technological paradigm setters and the system’s resilience. The plan of the paper is as follows: Section 2 discusses the background and relevant literature, Section 3 illustrates the linear model that is used to run simulations according to different searching routines; Section 5 illustrates and discusses results. Section 5 draws conclusions and sets an agenda for further research.",7
15.0,2.0,Networks and Spatial Economics,22 March 2015,https://link.springer.com/article/10.1007/s11067-014-9274-2,An Interdependent Multi-Layer Model: Resilience of International Networks,June 2015,Simone Caschili,Francesca Romana Medda,Alan Wilson,Female,Female,Male,Mix,,
15.0,2.0,Networks and Spatial Economics,31 August 2014,https://link.springer.com/article/10.1007/s11067-014-9256-4,Spatial Autocorrelation in Spatial Interactions Models: Geographic Scale and Resolution Implications for Network Resilience and Vulnerability,June 2015,Daniel A. Griffith,Yongwan Chun,,Male,Unknown,Unknown,Male,"Recent attention has turned to network autocorrelation, one of the previously neglected components of spatial interaction models (e.g., Griffith 2007; LeSage and Pace 2008; LeSage and Fischer 2010; Chun and Griffith 2011; Novak et al. 2011). This component coupled with spatial autocorrelation latent in the geographic distribution of origin and destination characteristics affiliated with flows over space impacts upon global distance decay parameter estimates, and results in substantially better goodness-of-fit measures between predicted and observed flows. To date, this improved model specification outcome, which reflects a dimension of spatial network complexity, is documented for journey-to-work flows within a wide variety of geographic landscapes (e.g., Griffith 2009a, b; Griffith 2011; Chun and Griffith 2011). Both geographic scale and resolution can stretch/compress the geographic landscape of spatial interaction data. Because spatial equilibria need to take into account spatial spillover effects (i.e., spatial autocorrelation), a better understanding of spatial autocorrelation’s relationship with geographic scale and resolution for spatial interaction data furnishes insights into the speed to which network flows can return to a spatial equilibrium after a shock (i.e., resilience), as well as potential amplifiers and dampeners of shock propagations in a network (i.e., vulnerability) (Reggiani 2013). Both are functions of a given geographic connectivity structure that channels flows through space. This paper summarizes connectivity experiments (see Ducruet and Beauguitte 2013) primarily with selected journey-to-work data across scale and resolution, also adding a new landscape to the inventory of those whose network autocorrelation has been studied. The new landscape is for Puerto Rico, and involves 2000 United States (US) journey-to-work flows data for its municipality (equivalent to a county) resolution. Scale increases (paralleling increasing domain spatial sampling) from the San Juan core urban area, to the San Juan metropolitan statistical area (MSA), and then to the entire island. This landscape laboratory is appealing because few workers go to/from the island on their daily journeys to work, resulting in it being nearly a closed system. The second landscape is Germany (also see Patuelli et al. 2007; Reggiani et al. 2011), with a change of scale going from kreise to district, then to province. Estimation is of the parameters of a double-constrained gravity spatial interaction model specification, where network autocorrelation is captured with eigenvector spatial filters (ESFs; see Chun and Griffith 2011). These ESFs represent one feature of complexity in spatial networks. This paper summarizes their estimation for journey-to-work data. Spatial autocorrelation in the geographic distribution of the balancing factors represents another feature of spatial network complexity. This paper presents measurements and portrayals of these spatial autocorrelation sources. Estimation is with Poisson regression, whose specification reveals that network autocorrelation often either inflates (the quantity e—the antinatural logarithm—raised to a positive ESF value, which is a value greater than 1) or deflates (the quantity e raised to a negative ESF value, which is a value between 0 and 1) a predicted flows value, relating locations to network vulnerability. This adjustment arises from the geographic clustering tendency of origins for flows to a particular destination, and of destinations for flows from a given origin. This paper exemplifies such clustering with data from the World Trade Center disaster. This paper makes novel contributions to the literature, including visualization of the ESF predicted flows inflation/deflation by combining a pair of choropleth maps portraying spatial autocorrelation with flows portraying network spatial autocorrelation. Another concerns parameter estimate comparisons across geographic scale (i.e., expanding landscape size) and resolution (changing size of areal units). Because of the nature of spatial autocorrelation, the ESFs are expected to vary across resolution, but not necessarily across scale, although they should show edge effects. Yet another contribution concerns the establishing of relationships between parameter estimates across geographic resolutions. Appropriate parameter estimate implications are drawn in terms of network resilience and vulnerability.",38
15.0,2.0,Networks and Spatial Economics,22 July 2014,https://link.springer.com/article/10.1007/s11067-014-9251-9,Assessing the Demand Vulnerability of Equilibrium Traffic Networks via Network Aggregation,June 2015,Richard D. Connors,David P. Watling,,Male,Male,Unknown,Male,"Real-life transport systems are forever in a state of flux, evolving over different time-scales. Accidents, breakdowns and severe weather may lead to short-term, unplanned reductions in capacity, and special events may lead to unusual levels of demand, all of which lead to unpredictable traffic conditions on a particular day for both travellers and road operators. In the medium-term, routine maintenance of roads/utilities and seasonal changes in demand patterns may all lead to periodic changes to traffic patterns. In the longer-term, capacity improvements and growth/decay in the overall demand for travel will lead to a changing systematic trend in traffic patterns. Looking to the past, traffic authorities worldwide have taken a growing interest in measuring, historically, the extent to which their transportation systems have been able to accommodate such changes while still delivering an acceptable level-of-service. This has led to several schemes around the world for collecting data and monitoring the repeatability of journey times (see the review of Watling and Balijepalli 2012). It has also led to transport planners seeking to design transport systems which are relatively robust to changes that may occur in the future, by somehow taking account of these eventualities well before their occurrence and scale is actually known. The focus of the present paper is on such a future-oriented approach to robust planning. Two broad classes of approach have dominated the transportation literature for assisting planners in addressing this range of problems. Reliability methods are typified by an approach in which: (i) probability distributions are first specified for the causes (e.g. capacity, demand, weather) or impacts (e.g. travel time variability), (ii) a network/behavioural model is assumed for how travellers might respond in such a changing environment, considering the interaction of their choices with congestion patterns, and (iii) the consequential distribution of measures of system performance (or moments or quantiles thereof) are inferred. In vulnerability methods on the other hand, we avoid the need to posit probability distributions, and instead aim to identify parts of the system that are weak or could most contribute to degradation in system performance. A disadvantage of vulnerability methods is that they do not, therefore, provide a prediction of the likely future states of the network, as reliability methods do. On the other hand, the advantage of vulnerability methods is that they effectively remove the need to associate probabilities with the system states identified in element (i) of the reliability approach. Removing the need to specify the probabilities of future events becomes more attractive as the desired robust planning horizon moves further into the future, since the concomitant uncertainty in specifying such probabilities only increases. In the field of reliability analysis, a wide range of tools now exist which vary in terms of the types of variability modelled, the kinds of network/behavioural assumptions and models adopted, the algorithms used for implementation, and the measures of system reliability impact considered. The field was originally motivated by considering variations in link capacity, and this has continued to be a strong theme. The origins can be traced to methods in which the system measure is the probability that an OD pair remains connected by any available path, assuming independent link failures, as described by Bell and Iida (1997), ranging to later work which allows multiple link states, correlated across links, and alternative behavioural models for how travellers might adjust (e.g. Sumalee and Watling 2008). In such methods, the concept of connectivity is generalised such that the measure is the probability of the travel time for an OD movement to exceed some threshold (OD connectivity being obtained in the limit, as the threshold tends to infinity). Taking a somewhat different stance on the system measure of reliability, Yang et al. (2000) and Chen et al. (2002) instead proposed calculating the probability of catering for a given level of demand in the face of stochastic network capacities. In contrast, other papers have sought to highlight the role played in system performance by variations in demand about some mean level, rather than variations in capacity (e.g. Clark and Watling 2002; Shao et al. 2006; Nakayama and Watling 2014), and indeed several methods now exist that allow both demand and capacity to be randomly varying (e.g. Lam et al. 2008; Sumalee et al. 2011; Uchida 2014), including the impacts of weather. In the field of vulnerability analysis, a corresponding wide array of techniques now exist, again primarily focused on the impacts of degradations in capacity (Bell 2000; D’Este and Taylor 2001; Nicholson and Dalziell 2003; Taylor et al. 2006; Jenelius et al. 2006; Szeto et al. 2006; Yang and Qian 2012; Sullivan et al. 2010). As with reliability methods, these papers cover a diverse range of assumptions, techniques and measures of impact, though with the general theme that they aim to find the weakest part of the system by considering how it might perform, when allowing for travellers to readjust to the degraded situation. Similarly to reliability methods, there has also been an emerging interest in considering the role of demand in the vulnerability of congested networks. Berdica (2002) considered interruptions to critical links, general capacity reductions and variability of demand, using equilibrium models to numerically assess the sensitivity of networks to such potential interruptions. Chen et al. (2012) considered the impact of variability in OD demand particularly for large-scale systems. Ho et al. (2013), on the other hand, used a continuum model to explore vulnerability at a regional level. Watling and Balijepalli (2012) considered the twin problem of demand variability and demand growth, developing a link-level indicator to parallel those considered for capacity changes (Knoop et al. 2012). In the present paper, we shall focus on the role of OD demand on network vulnerability. Motivated partly by the observations in Watling and Balijepalli (2012), we shall specifically consider how demand growth might affect the performance of a network. Our interest is not so much in variability (either in capacity or OD demand), which might occur in the short to medium term, but rather in longer term trends in demand patterns. Typically, such patterns may be expected to unfold over many years, although in rapidly-emerging economies we can observe major changes to demand patterns over shorter periods. In essence, our take on vulnerability is to consider to what extent the existing network capacity can accommodate future growth, without travel times/costs growing at an unacceptable rate. The main methodological contribution of our paper is developed from the observation that we may link such an analysis of demand vulnerability to the methods developed in two existing fields of transportation analysis, namely network aggregation and sensitivity analysis. In order to understand such connections, it is useful to first imagine numerically solving a great number of network equilibrium problems for a given network, with the problems differing by making adjustments in some predefined way to the levels of demand in the OD matrix. For simplicity’s sake, let us imagine that the Deterministic User Equilibrium (DUE) is used with separable link travel cost functions, then for each OD demand matrix instance we obtain a unique travel cost (on any used path) at the OD level. If there are \( n \) OD demand movements, then by varying the OD demand we obtain for each OD movement a functional relationship between that movements OD travel cost and the \( n \) OD demand levels; let us call these the OD cost functions, which are clearly non-separable. We could then draw a network in which all OD movements are directly connected by a single link, and associate the corresponding OD cost function with each such link; we refer to this as the ‘aggregate network’. Indeed, as we shall show, this kind of conceptual development is not specific to the DUE model, and in fact we can develop such an ‘aggregate network’ based on the Stochastic User Equilibrium (SUE) model, which turns out to have some mathematical advantages in our subsequent analysis. While this conceptual link to network aggregation is useful in theory, it seems to have two major drawbacks. Firstly, it seems to require that we solve an enormous number of equilibrium problems, which would not seem to be practicable in real-life systems. Secondly, even if we can overcome these numerical problems, we would then present planners with a functional relationship from and to a Euclidean space that is as large in dimension as the number of OD movements, and it is unclear how they might use this information. Therefore, having established the theoretical link to network aggregation (section 2), and the notation to be subsequently adopted (section 3), we numerically explore the nature of these relationships in simple cases (section 4), in order to motivate our approach to both efficiently estimate (section 5) and to illustrate/explore the aggregate relationships in the context of vulnerability (section 6). Our solution approach in section 5 utilises the connection to sensitivity analysis, whereby the gradient vector of the relationship at a single equilibrium solution is seen to be (perhaps surprisingly) sufficient to construct a reasonable approximation to the whole aggregate relationship. In section 6, we further utilise sensitive analysis to develop a simple illustration of point vulnerability through a demand vulnerability matrix, with the idea that such a matrix might be examined at different point of future growth. Finally, in section 7, we draw conclusions from the research and examine future research directions.",14
15.0,2.0,Networks and Spatial Economics,31 August 2014,https://link.springer.com/article/10.1007/s11067-014-9260-8,The Evaluation of Road Network Vulnerability in Mountainous Areas: A Case Study,June 2015,Federico Rupi,Silvia Bernardi,Antonio Danesi,Male,Female,Male,Mix,,
15.0,3.0,Networks and Spatial Economics,18 September 2014,https://link.springer.com/article/10.1007/s11067-014-9257-3,"Special Issue on Dynamic Traffic Assignment, Parts 1 and 2",September 2015,Terry L. Friesz,Satish Ukkusuri,,,,Unknown,Mix,,
15.0,3.0,Networks and Spatial Economics,19 September 2015,https://link.springer.com/article/10.1007/s11067-015-9307-5,"Erratum to: Dynamic Traffic Assignment: Theory, Computation and Emerging Paradigms",September 2015,Terry L. Friesz,Satish Ukkusuri,,,,Unknown,Mix,,
15.0,3.0,Networks and Spatial Economics,31 May 2014,https://link.springer.com/article/10.1007/s11067-014-9239-5,Submission to the DTA2012 Special Issue: Convergence of Time Discretization Schemes for Continuous-Time Dynamic Network Loading Models,September 2015,Rui Ma,Xuegang (Jeff) Ban,Henry X. Liu,Male,Unknown,Male,Male,"Dynamic Traffic Assignment (DTA) has been extensively studied for decades; see Ran and Boyce (1996), Peeta and Ziliaskopoulos (2001) and Friesz et al. (2010) for comprehensive reviews on this topic. Different from its static counterpart, DTA is challenging due to not only its conceptual complexity such as the dynamic evolution of traffic states over a network (i.e., flow propagation), but also numerical difficulties: it is natural to formulate DTA in continuous time, which however usually needs to be discretized in order to obtain numerical solutions. The dynamic network loading (DNL) process is one of the essential components of DTA (Carey and Ge 2012). DNL aims to find the time-varying status of a road network for a given inflow profile (Xu et al. 1999). Given such an inflow profile, network status such as exit flow, queue length, travel times can be calculated by the DNL process. The family of DNL models includes whole link models (Carey and McCartney 2002, Nie and Zhang 2005a), point-queue models (Kuwahara and Akamatsu 1997, Nie and Zhang 2005a), and continuum traffic flow models such as the LWR hydrodynamic model (Lighthill and Whitham 1955, Richards 1956). A whole-link model uses a delay function to describe the travel time on a link as a function of the number of vehicles on the link and/or other variables, such as inflow and exit flow rates. A point-queue model assumes the physical length of any vehicle is zero and the queue is not built up until vehicles are at the exit node of the link. Vehicles are always traveling at the free flow speed before they join the queue. Continuum flow models describe the temporal-spatial evolution of macroscopic flow quantities, with two fundamental relations, conservation of vehicles and the flow-concentration-speed relation (Zhang 2001). In the past, extensive research has been conducted for DNL models such as how to ensure the first-in-first-out (FIFO) property. Nie and Zhang (2005a) provided an extensive review of different types of DNL models and compared their performances. The advantages and drawbacks of all these different types of DNL models have also been well discussed in the literature. For example, the whole-link and point-queue models are relatively simple mathematically, and can be easily integrated with DTA formulations. They, however, suffer from much simplification of traffic dynamics (e.g., point-queue models assume vertical queues instead of physical queues), and thus lack of certain traffic realism. Continuum traffic flow models can capture more realistic traffic dynamics, which are however much more challenging to solve; as a result, integrating those models into DTA formulations is also more challenging. Most DNL models is described in continuous time because time is not discrete but continuous by nature. This generally results in a continuous-time ordinary differential equation (ODE) or partial differential equation (PDE). Ideally there could be analytical solutions to the ODE or PDE if the model is analytically solvable. For example, in Han et al. (2013a) a PDE-type DNL model based on the cumulative inflow and exit flow count is formulated and an analytical solution is obtained so that in Han et al. (2013b) numerical results can be derived from the analytical solution without discretizing any diferential equations. This is, however, not the case for most existing DNL models with variable inflow patterns. Thus, numerical solution methods usually have to be applied to solve continuous-time DNL models. Two critical issues then arise. The first one is how to discretize a continuous-time model into its discrete-time counterpart, i.e., what kind of discretization scheme one should choose to generate and solve the corresponding discrete-time model. The second issue is how to make sure that the solution obtained from the numerical methods (i.e., the solution from the discrete-time model) is convergent to the solution of the continuous-time model. These two issues are the major focus of this paper. There are two commonly used schemes to discretize a continuous-time model: an implicit scheme versus an explicit scheme (Shampine et al. 2003). Implicit and explicit schemes perform differently in key model properties such the convergence as we will show later in this paper; they also require different computational efforts. The former calculates the unknowns at the current time step by solving a discrete problem defined by such unknowns and involves inputs from the known quantities obtained from the previous time steps. One such example is the Euler backward difference which is an implicit scheme for numerically solving ODEs or PDEs. The latter calculates the unknowns at the current time step from the calculated iterates of previous time steps. Euler forward difference is an example of an explicit scheme. See next section for a mathematical description of these two schemes. Convergence of a discretization scheme means that the solution of the resulting discrete-time model approximates a solution of the continuous-time model; more precisely, convergence here means if the discrete time step goes to zero, the discrete-time solution should converge, in some sense, to a solution of the continuous-time model. This is a major priority when numerically solving continuous-time models, such as DNL models. In other words, if convergence cannot be guaranteed, the discretization scheme would fail to solve the original continuous-time problem. For numerical schemes to solving ODEs and PDEs, two other concepts are also closely related to convergence: consistency and stability. The former means the truncation error between the continuous-time and discrete-time models vanishes as the discrete time step goes to zero; the latter means the discrete solution is bounded if the continuous-time solution is bounded. Most existing DNL discretization applied an explicit scheme, since in general, implicit schemes require a higher computational cost. However, implicit schemes also have advantages in ensuring convergence as will be demonstrated later in this paper. The selection of a discretization scheme thus has both theoretical and computational implications on solving continuous-time models. So far in the literature, the issue of selecting a proper discretization scheme for DNL models has not been fully discussed. To the best of the authors’ knowledge, the only two references that focused on these two schemes are Carey and Ge (2007) and Ban et al. (2012a). In Carey and Ge (2007), it was shown that the implicit scheme can always guarantee First-In-First-Out (FIFO) for whole link models, which is not always true if an explicit scheme is used. In Ban et al. (2012a), both the explicit and implicit schemes were used to solve point-queue models, and their impacts to FIFO and convergence were discussed. The importance of consistency, stability, and convergence of a numerical scheme for solving continuum traffic flow models, which are usually formulated as PDEs, has been well recognized and investigated; see Zhang (2001) for a comprehensive review. Interestingly, this seems not to be the case for whole link models and point-queue models, for which the convergence issue (as well as consistency and stability) of a numerical solution scheme has not been well studied or understood; see next section for a more detailed review. Many mentioned about convergence, which however were generally not rigorously investigated. The only known exception is Ban et al. (2012a) which provided direct proofs of the convergence of some implicit schemes when discretizing and solving ODE-type DNL models. Therefore in this paper we illustrate the concepts of convergence (also consistency and stability) using a modified point-queue model recently proposed in Ban et al. (2012a). Notice that a whole link model is usually a regular continuous-time function, while a point-queue model is often formulated as an ODE. This means that both models are conceptually and computationally easier than the PDE-type continuum flow models. We find that by focusing on those simpler-form continuous-time DNL models, the implications of different discretization schemes and the importance of convergence/stability can be more readily illustrated. In this paper, after reviewing the current literature of the whole link and point-queue models, we apply standard ODE analysis techniques to a particular type of DNL model, namely the α-model, in Ban et al. (2012a). In particular, we focus on the discretization schemes of the α-model, and the key concepts of consistency, stability, and convergence. We select the α-model for several considerations. First, the resulting ODE of the α-model is absolute continuous for a “Lipschitz continuous” right-hand side, i.e., there is no additional difficulty due to discontinuities and shocks. This will allow us to apply traditional ODE convergence analysis techniques, such as the Lax’s Equivalence Theorem (Strikwerda 1989), to analyse the convergence and stability issues of the discretization schemes. Although the convergence conditions of an implicit scheme to the α-model has been established in Ban et al. (2012a) using a direct approach, we will provide convergence results for both the explicit scheme and the implicit scheme, extending the results in Ban et al. (2012a). Secondly, the α-model approximates the Linear Complementarity System (LCS) model proposed in Ban et al. (2012a) which corrects the deficiency of the original point-queue model (thus a more preferred point-queue model to use for DNL).Footnote 1 Furthermore, as a parameter of the α-model approaches infinity, the α-model coincides with the LCS model. In this sense, the LCS model may be used as a benchmark to compare different schemes to solve the α-model. We show in this paper that both the explicit and implicit schemes of the α-model are consistent. The implicit scheme is always stable, and thus convergent. For the explicit scheme, however, the stability in only ensured under certain conditions. As a result, the convergence of the explicit scheme can only be achieved under similar conditions. Numerical experiments on a single link are then carried out to compare the stability and convergence properties of the two discretization schemes. A numerical experiment on the DNL process for a diverging network is also conducted to illustrate the propagation of the instability and negativity through network links.",7
15.0,3.0,Networks and Spatial Economics,31 May 2014,https://link.springer.com/article/10.1007/s11067-014-9240-z,Submission to the DTA2012 Special Issue: Approximating Time Delays in Solving Continuous-Time Dynamic User Equilibria,September 2015,Rui Ma,Xuegang (Jeff) Ban,Henry X. Liu,Male,Unknown,Male,Male,"Dynamic user equilibrium (DUE) and its generalized version dynamic traffic assignment (DTA) have been extensively studied during the last several decades; see reviews in Peeta and Ziliaskopoulos (2001), Ran and Boyce (1996), Friesz et al. (2010), and Carey and Ge (2012). Although existing DUE research has primarily focused on discrete-time DUE problems (Peeta and Ziliaskopoulos 2001), recently modeling and solving continuous-time DUE have received much attention (Frieset al. 2010; Ban et al. 2012b), to capture the fact that time is continuous by nature. This is largely motivated by the work of Friesz and his colleagues (Friesz and Mookherjee 2006; Fries et al. 2010) which applied a new mathematical framework, called differential variational inequality (DVI), to study DUE; see Pang and Stewart (2008) for a comprehensive review on DVI. DVI can be considered as (i) a non-smooth ordinary differential equation (ODE) which requires a variational (or complementarity) relation satisfied at each time instant (Pang and Stewart 2008); or (ii) an infinite-dimensional VI whose constraints contain ODEs (Friesz 2010). It provides a unifying framework to simultaneously capture the two distinct features of DUE: variational (or complementarity) relations to model drivers’ choice behavior (such as route and/or departure time choices) and dynamical systems to model traffic dynamics (Ban et al. 2012b). Friesz and Mookherjee (2006) are the first authors to apply the DVI framework to model the DUE with simultaneous departure time and route choice, based on the path-based DUE formulation (Friesz et al. 1993). Recently in Friesz et al. (2010), the DVI framework is further used to model both within-day and day-to-day traffic dynamics, resulting in a dual-time-scale DUE model. The model is solved by a projection-based fixed-point algorithm in continuous-time; convergence is established under the component-wise strong pseudomonotonicity of the path delay operator, which relaxes the conditions in Friesz and Mookherjee (2006). More recently, Ban et al. (2012b) apply the differential complementarity system (DCS), a special form of DVI (their relation is similar to that of the nonlinear complementarity problem (NCP) and variational inequality (VI) in a finite dimension), to model and solve continuous-time, instantaneous DUE problems. The DCS formulation is in a link-node fashion (see Ban et al. 2008), based on modified point-queue models (Ban et al. 2012a) to ensure the consistency and convergence during discretization. The DCS model is solved via time decomposition (in continuous-time) and time-stepping (in discrete-time); convergence is established under rather mild conditions that does not require monotonicity of path or link travel times. One of the key challenges in modeling and solving DUE is the flow propagation that inherently introduces time delays (or time shifts) in the DUE formulation. In Ban et al. (2012b), since the instantaneous DUE (IDUE) simplifies the route choice condition, the resulting model only contains constant time delays, which can be naturally decomposed in time based on these constants (actually the free flow link travel times). After the decomposition, time-delay terms disappear and the resulting problems are a series of smaller-size DCS without time delay that can be solved using standard time-stepping method for ODEs (Shampine and Thompson 2001); see also Pang and Stewart (2008). The general DUE problem however will have to deal with time-varying, state dependent delays, e.g., in the route choice condition, which cannot be decomposed in time. Also there appears to be no mathematically rigorous tools that can deal with such delays properly.Footnote 1 As a result, it seems that approximating the delayed terms in DUE models is very likely the only viable way at the current stage. The idea was first proposed in Friesz and Mookherjee (2006) who approximated the delayed terms in DUE as pure time functions so that the approximated model does not contain any time delay. Such a simple approximation, although very innovative in concept, cannot capture the functional relationships between the original and approximate delayed terms. Kachani and Perakis (2009) proposes two approximations where the exit link flow rate with time delay term can be approximated in linear or quadratic terms of the time delay, while the method to derive the coefficients of the approximations is not fully developed. We aim to develop in this paper an approximation scheme with dynamic coefficients that can capture some of those functional relationships and thus is expected to work better than the simple scheme in Friesz and Mookherjee (2006) and Kachani and Perakis (2009). It is clear from Ban et al. (2012b) that DVI/DCS with constant delays is much easier to deal with. In this paper, we propose a scheme to approximate any variable with time-varying, state-dependent delays in a DUE formulation using variables and/or functions that contain either constant time delays or no delay at all. For example, in a link-node based DUE model (see the general model in Section 2 in Ban et al. 2012b), t denotes the continuous time, (i, j) is a link from node i to node j, s is a destination, and τ

i
j
(t) is the travel time of flow traversing link (i, j) when entering the link at time t. At time t, to check whether link (i, j) is on a minimum cost path from node i to the destination s, one needs to compare the node-to-destination travel time at node i at time t, i.e., \({\pi _{i}^{s}}(t)\), with the minimum travel time via link (i, j), which is the sum of the travel time of link (i, j) at time t and the minimum node-to-destination travel time at node j at time t + τ

i
j
(t), i.e., \({\pi _{j}^{s}} (t+\tau _{ij}(t))\). Here, the minimum node-to-destination travel time \({\pi _{j}^{s}}(t+\tau _{ij}(t))\) contains time-varying, state-dependent delays. Usually τ

i
j
(t) is a function of the traffic states and thus time-varying and state-dependent. Our proposed method will approximate \({\pi _{j}^{s}}(t+\tau _{ij}(t))\) as a function of \(t, {\pi _{j}^{s}}(t+\tau _{ij}^{0})\), and τ

i
j
(t): 
 Here the dynamic coefficient σ

i
j
(t) is the “pseudo derivative” (PD) of the minimum node to destination travel time at time t measured over the travel time of link (i, j), which can be numerically estimated. Such an approximation reduces the time-varying, state-dependent delays to constant delays, which are much easier to deal with as shown in Ban et al. (2012b). The time-stepping method as presented in Ban et al. (2012b) can then be readily applied to solve the approximated DUE (ADUE) model. In this paper, we present detailed investigations on the mathematical properties of the PD and how effective it is to solve a general DUE problem. In this paper we only consider a network with a single destination. It is challenging for the ADUE to disaggregate the variables for all destinations (if a network with multiple destinations is considered) and at the same time not violating the FIFO condition. To make our discussion clear and focus on the key issues of PD and the proposed approximation scheme, we apply this concept to the DUE with a single-destination and given (fixed) demand profiles in this paper. We provide more discussion in the Conclusion Section on how to extend the proposed scheme to deal with a network with multiple destinations. The queuing model that will be applied is the the point-queue model based on linear complementarity system (LCS) (Ban et al. 2012a). We first present a DCS formulation for the DUE problem with single destination, fixed demand, and LCS-based point-queue. We show that the flow conservation at a node may be violated at very specific situations, and propose a procedure to detect and correct if such violation happens. The ADUE model is then presented, based on the PD concept. We show the requirement for PD in order for first-in-first-out (FIFO) to be satisfied and discuss the flow conservation issues at a node for ADUE similar to that of the original DUE problem. We next present the method that is applied to discretize the continuous-time ADUE model, resulting in a discrete-time, finite dimensional nonlinear complementarity problem (NCP). We also provide the approach to numerically estimate the PD of a link, and discuss how it can be integrated into an iterative algorithm to solve the general DUE problem by solving a discrete-time ADUE in each iteration. Although we cannot provide the convergence proof of the iterative algorithm at this stage, numerical results for a small network and the Sioux Falls network do show certain convergence performances of the algorithm.",9
15.0,3.0,Networks and Spatial Economics,30 April 2014,https://link.springer.com/article/10.1007/s11067-014-9234-x,A Continuous DUE Algorithm Using the Link Transmission Model,September 2015,N. Nezamuddin,Stephen D. Boyles,,Unknown,Male,Unknown,Male,"Despite several decades of active research and growing acceptance among practitioners, dynamic traffic assignment (DTA) models have yet to reach their full potential in transforming traffic assignment in the field. While explicitly representing the time-dependent nature of travel demand and congested behavior can model important phenomena more accurately, including traffic control and queue spillbacks, doing so introduces a host of additional modeling difficulties not present in static models. Broadly speaking, two major difficulties concern the nature of the traffic flow model employed, and the nature of dynamic equilibrium itself. These are briefly explained below; readers seeking additional detail can consult Peeta and Ziliaskopoulos (2001) for a more thorough review of DTA approaches. These difficulties can be highlighted by comparing approaches which have been developed to date. The first class of DTA models was based on analytic representation of link delay using “exit functions” (Merchant and Nemhauser 1978a, b). While such functions lend themselves to convergence analysis, properly enforcing first-in first-out (FIFO) discipline results in nonconvex constraints (Carey 1992) which form an obstacle to large-scale application. Another class of DTA formulations attempts to simplify a traffic-flow model so that it can scale tractably to large-scale systems; such efforts often approximate the hydrodynamic model of Lighthill and Whitham (1955) by introducing a piecewise-linear flow-density diagram, and applying a space-time discretization to form the cell-transmission model (Daganzo 1994, 1995), or methods based on cumulative counts (Newell 1993a, b, c), as in the link transmission model (LTM) (Yperman 2007). Questions have been raised about whether discretization should be spatial or trajectory-based (Bar-Gera 2005), or how to ensure consistency between link and path models (Blumberg and Bar-Gera 2009), or whether certain stochastic or adaptive features should be modeled (Qian and Zhang 2013, Waller et al. 2013). These issues are further compounded when questions of traffic control arise, including signals, roundabouts, or stop-controlled junctions. In all cases, a tension exists between accuracy and tractability, and the lack of consensus among researchers likely hinders adoption of DTA in practice. The second, more subtle difficulty is central to the definition of dynamic equilibrium. Many practical DTA packages employ a mesoscopic simulator to represent traffic flow, in which individual vehicles are propagated through the network. From a mathematical standpoint, these models are discontinuous in the representation of vehicles, and often discontinuous in the representation of traffic control as well (in the case of signals, for instance). In such cases, it is not difficult to produce examples where no dynamic equilibrium exists. Even when continuous models are employed, the equilibrium solution need not be unique (see, for instance Nie 2010). These may seem to be technical issues more than practical ones, but are in fact fundamental to the application and interpretation of DTA models in practice—if multiple equilibria exist, or no equilibria, then what should planners design for? This paper describes a continuous-vehicle, continuous-time dynamic equilibrium formulation which addresses these issues. The corresponding traffic flow model is relatively general; for concreteness the demonstrations in this paper use the link transmission model (LTM) developed by Yperman (2007) and further developed by Gentile (2010) and others. The LTM is amenable to representing traffic control, building on established results from traffic operations, such as the Highway Capacity Manual. In particular, continuity can be provided by representing average delay due to traffic control, rather than explicitly simulating gap acceptance and signal control. This reduces computational delay and, as Yperman (2007) argues, travelers presumably base route choice on average control delay anyway, so the loss of “accuracy” is negligible. Continuous vehicle flows are addressed by using node routing parameters, avoiding path enumeration and aggregating travelers by destination. The underlying traffic flow model implicitly maintains first-in first-out (FIFO) discipline and vehicles are assigned to user optimal paths based on their experienced path travel times. Equilibrium-seeking DTA models have to necessarily take into account travelers’ experienced travel time, rather than instantaneous travel time, to be consistent with their route-choice behavior learned in the network over a long period of time (Chiu et al. 2010). This model forms the main contribution of the paper. Its prime advantages are that it (1) uses continuous flows (and thus equilibrium can be assured) while (2) simultaneously introducing node routing parameters (obviating the need to enumerate paths), (3) enables derivative calculations to improve the speed of convergence to equilibrium; and (4) its node models are flexible and can admit a variety of traffic control. Section 2 presents the model and associated notation, and Section 3 defines and proves existence of at least one dynamic Wardrop equilibrium using a fixed-point theorem. Section 4 give two potential solution methods, the well-known method of successive averages, and a method resembling the LUCE algorithm from static traffic assignment, utilizing derivative information from the flow model. These methods are demonstrated on two networks in Section 5 before conclusions are given in Section 6. In these regards, the continuous DUE model presented differs from the earlier continuous DTA models, which typically use optimal control theory or variational inequalities. The DTA models using optimal control theory assign vehicles to paths based on instantaneous path travel times (Friesz et al. 1989, Wie 1991, Ran et al. 1993, Ban et al. 2012), implying that link travel times are invariant during the course of a vehicles journey. The quasi-continuous DTA model developed by Janson and Robles (1995) also falls into this class of DTA models using instantaneous travel times. The other class of continuous DTA models uses variational inequalities (Friesz et al. 1993, Ran and Boyce 1996, Ran et al. 1996, Bliemer and Bovy 2003, Ban et al. 2008) and differential variational inequalities (Friesz et al. 2011, 2013) to recognize the time-varying nature of link travel times. While such models assign demand based on vehicles experienced travel times, the propagation of traffic using link exit functions is limited in its representation of the queue spillback phenomenon, which is critical to dynamic modeling of congested networks. Another continuous DTA model developed by Mounce (2006) uses deterministic queues at link exits to model conditions when flow exceeds capacity. However, this model is only applicable to networks with a single bottleneck per route, which severely limits its application to congested urban traffic network. The continuous DUE model presented in this paper uses experienced path travel times and does not require the use of exit functions. Certain aspects of this model, such as avoiding path enumeration (by using node routing parameters), have been used by earlier researchers. Obviating path enumeration is achieved by using either arc-based formulations (Ran and Boyce 1996, 2012; Wie et al. 2002; Ban et al. 2008) or node routing parameters (Long et al. 2011) However this paper is unique in its calculation and use of travel time derivatives to speed convergence, and in its formulation of equilibrium as a fixed point rather than as a variational inequality.",6
15.0,3.0,Networks and Spatial Economics,24 May 2013,https://link.springer.com/article/10.1007/s11067-013-9188-4,Day-to-day Dynamics & Equilibrium Stability in A Two-Mode Transport System with Responsive bus Operator Strategies,September 2015,Giulio E. Cantarella,Pietro Velonà,David P. Watling,Male,Male,Male,Male,"Traditional, steady-state equilibrium methods allowed us for many years to study the hypothetical behaviour of transport systems under conditions of constant demand, constant travel infrastructure/costs and constant policy measures. The field of study embodying dynamic transportation systems has opened up the possibility to study the more realistic changes that occur in demands, costs and policy responses, over multiple temporal scales. While initially many advances focused on developing dynamic methods for the within-day time-scale, assuming the between-day scale to be constant, a growing body of research is developing on the converse position, namely where the between-day scale is dynamic, assuming the within-day scale constant. The present paper falls within this second body of research. In particular, in the present paper we shall exploit the ability of such day-to-day models in representing a situation where some “parameters” of the transport system are themselves responsive, on a day-to-day scale, to the flows on the transportation system, to reflect the reactions of some ‘agent’ representing a responsible transport authority. The feedback measures of this agent in turn will affect the travel experiences and subsequent decision of the other agents in this system, namely the travelling public, which in turn will affect the responsive parameters in the future. In such a complex, responsive, multi-agent environment, it is natural to question whether a given strategy may be successful in stabilising the transport system (or whether it may indeed lead to greater instability), or in directing the system to towards desirable long-term states. Day-to-day dynamic process models are especially powerful in such a context, providing the opportunity for analysis of theoretical properties of system convergence to different attractors (not necessarily equilibrium or fixed-point) such as existence, uniqueness and stability. This paper presents a day-to-day dynamic analysis of transport mode choice behaviour in a transportation system, which allows us to analyse equilibrium stability and the effects of (and on) transit operator strategies. Quite a large number of papers have now been proposed to deal with different aspects of day-to-day dynamics in single-mode private transport networks, using both deterministic and stochastic models (e.g. Friesz et al. 1994; Hickman and Bernstein 1997; Cantarella and Cascetta 1995; Cantarella and Velonà 2003; Hazelton and Watling 2004; Bie and Lo 2010; Han and Du 2012). All of these approaches assume that the demand for private transport is invariant to the performance and policy measures in the public transport system. Although some results in the above-quoted papers may be extended to mode choice (effectively imagining the routes to be modes), there are several distinctive features of the choice of mode that are not captured by such an analogy, and to the authors’ knowledge there have yet to be any papers specifically on this topic. Typically the design of public transport systems is based on steady state analysis, often considering the public transport mode in isolation (see the review of Guihaire and Hao 2008). While some studies exist that consider both public and private modes, this typically retains an implicit or explicit assumption of a steady state analysis, such as in the recent study of a bi-modal transportation system by Li et al. (2012), with no consideration of the dynamics in travellers adjusting to any new policy measures. Furthermore, in practice it is not only travellers that may make a dynamic adjustment (of their choice, in our case mode choice), but also the transit operators may also make dynamic adjustments, and our work is distinctive in including such operators as an active agent, rather than the typical approach adopted of considering policy measures as some abstract, external force. The dynamics of the system are therefore also affected by the responsive strategies of the transit operators, and in turn the dynamics of the system also shape the long-term nature of their strategies. Again, some limited analogies exist with the study of private transport networks. Watling (1996), developing an example originally due to Smith (1979), explored the day-to-day dynamic evolution of route choice in a network in which the traffic signals were responsive to the traffic flows. It was seen that while two out of the three equilibria were locally stable, one of the stable equilibria was much more likely to evolve from given starting conditions. Similar behaviour was observed in examples with non-monotone cost functions, such as Morlok (1979) established can arise in when bus operators strategies in which frequencies are responsive to the demands. The overall aim of our contribution is to stress that day-to-day dynamic models are useful and often needed to support transportation supply design (e.g. Cantarella 2010). From the motivating background described above, the first objective of the paper is to propose a class of dynamic process models for mode choice evolution over time, in which the dynamics of the transit operator demand-responsive strategies are embedded. According to the theory of dynamical systems, bifurcations from a stable equilibrium, considered as a fixed-point attractor, may lead towards other types of attractors, such as periodic (and then a-periodic) or quasi-periodic, or towards multiple equilibria. This theoretical result supports the conjecture that multi-mode transportation systems may likely present several point equilibria, some of them being non-stable, as shown by some simple numerical example. Our second objective, therefore, is to present a theoretical analysis of the conditions for uniqueness and stability of point and other attractors within such a model framework. Among other things, such an analysis facilitates a study of how the stability and multiplicity of equilibria is affected by on-board crowding, on-street congestion, and transit operator strategies (e.g. in terms of fleet management, fare). Our third and final objective is to make perform and report numerical experiments concerning the bifurcation of equilibrium and transient system states with respect to assumed control measures in our transportation system. In this way, our adopted day-to-day dynamic approach allows to analysis transient and to define policies to move towards a desired equilibrium. For simplicity, reported results in this paper refer to a single OD pair connected by two transport modes: cars and buses sharing common streets, neglecting route choice behaviour. Our analysis will be based on a simple, multi-agent, bi-modal system, with users and bus operators as agents. In our approach, the level-of-service relevant for user mode-choice behaviour is influenced by on-street congestion, monetary cost (such as bus fares, fuel cost, and the like), bus in-vehicle crowding, and service frequency (as determined by bus operator behaviour). The proposed model is meant to be a tactical/strategic planning tool, at such a decision level a frequency-based approach is usually applied, whilst the bus timetable is designed at lower decision levels. The paper concludes by discussing extensions of this work to more general applications, as well to stochastic process models. Our multi-agent approach, from an explicitly (day-to-day) dynamic perspective, is also open to further extension, by including not only the transit operators and travellers, but also a city authority as a third kind of active agent. Such an approach may, for example, build on the work of Friesz and Shah (2001) on disequilibrium network design, if we presume that the city authority may be represented to be acting as if ‘to maximize the net present value of benefits to users of a transportation network over a fixed planning horizon’. The paper is structured as follows: section 2 describes the formulation of the model; this model is applied to a two-mode system in section 3 where some specifications are described. In section 4 first the results of some numerical examples are reported showing that multiple, possibly non-stable, equilibria may exist in multi-mode system; then a sensitivity analysis is carried out through a bifurcation analysis with respect to bus fare as well to demand flow and user behaviour dispersion; finally some consideration about transients are also included. In section 5 major findings are recapped and some research perspectives are outlined.",40
15.0,3.0,Networks and Spatial Economics,27 July 2013,https://link.springer.com/article/10.1007/s11067-013-9193-7,Extending the Cell Transmission Model to Multiple Lanes and Lane-Changing,September 2015,Malachy Carey,Chandra Balijepalli,David Watling,Male,,Male,Mix,,
15.0,3.0,Networks and Spatial Economics,18 April 2014,https://link.springer.com/article/10.1007/s11067-014-9233-y,Submission to the DTA 2012 Special Issue: On the Stability of a Boundedly Rational Day-to-Day Dynamic,September 2015,Xuan Di,Henry X. Liu,Jeong Whon Yu,,Male,,Mix,,
15.0,3.0,Networks and Spatial Economics,27 April 2014,https://link.springer.com/article/10.1007/s11067-014-9232-z,DTA2012 Symposium: Combining Disaggregate Route Choice Estimation with Aggregate Calibration of a Dynamic Traffic Assignment Model,September 2015,Moshe Ben-Akiva,Song Gao,Yang Wen,Male,,,Mix,,
15.0,3.0,Networks and Spatial Economics,04 May 2014,https://link.springer.com/article/10.1007/s11067-014-9243-9,"A Comparison of Dynamic User Optimal States with Zero, Fixed and Variable Tolerances",September 2015,Y. E. Ge,B. R. Sun,Xizhao Zhou,Unknown,Unknown,Unknown,Unknown,,
15.0,3.0,Networks and Spatial Economics,27 September 2013,https://link.springer.com/article/10.1007/s11067-013-9206-6,Investigating Factors for Existence of Multiple Equilibria in Dynamic Traffic Network,September 2015,Takamasa Iryo,,,Male,Unknown,Unknown,Male,"Uniqueness is an important characteristic of network user equilibrium. If uniqueness is guaranteed in a user equilibrium assignment, only the solution obtained in the calculation would be necessary to be considered to estimate traffic conditions in the real world, thus providing a reliable solution that is useful for policy decisions or effective control strategies. The definition of ‘uniqueness’ should be explicitly stated before starting any discussion of this property. In this study, ‘uniqueness’ represents one of two situations: in one, the link travel-time profile is uniquely determined and in the other, the solution set is a convex set. The former definition is useful when the purpose of traffic assignment is to forecast network congestion; the latter is useful because any solution point can be obtained using a linear combination of extreme points of the solution set. Note that, even if a link traffic volume pattern is not uniquely determined in equilibrium, it may be classified as a ‘unique solution’ by either of these criteria (e.g. a parallel network consisting of two links with same free-flow travel time and infinite capacity, in which any feasible traffic flow pattern is equilibrated). Uniqueness in dynamic user equilibrium (DUE) has been shown to exist in certain conditions. Mounce and Smith (2007) proved that, when the bottleneck model is adopted and vehicles’ departure times from the origins are externally given and fixed, the existence of a unique (in terms of the link travel-time profile) equilibrium solution is guaranteed in single-origin and single-destination networks. In addition, Mounce (2007) proved that the solution set is convex if the route travel time is a monotone function of route traffic flow. In the bottleneck model, the link travel time function is a monotone function of link flow given a constant link capacity (Smith and Ghali 1990), implying that route travel time is a monotone function if each route contains only one bottleneck. The monotonicity of route travel time is actually not a general property of the bottleneck model; in fact, cases in which route travel time is not a monotone function of route traffic flow have been found (Kuwahara 1990; Mounce 2001; Mounce and Smith 2007). The analysis of uniqueness introduced above suggests that there may be a case with multiple equilibria in a dynamic traffic network with bottlenecks and a many-to-many origin-demand flow pattern. Indeed, Iryo (2011) found a twofold symmetric network structure and demand pattern in which multiple equilibria (more specifically, a non-convex equilibrium solution set with different link travel-time patterns) exist. The existence of non-unique solutions mentioned above was determined using a special setting in which the bottleneck model, Wardrop’s first principle, and twofold symmetric structure of the network and demand pattern are employed. Obviously, these settings are too special to be realised in the real world. To judge whether the issue of non-uniqueness should be explicitly considered for real-network problems, investigating the possibility of existence of non-unique solutions with general settings is ultimately necessary, while it seems to be too tough to be achieved at this moment. Instead of analysing general cases, investigating the mathematical mechanism that causes non-unique solutions in the example of Iryo (2011) should be helpful to discuss the implication of this special example for real-network problems. To perform this, the following three factors that may be correlated to the existence of the non-unique solutions, that is, Symmetry of the network, i.e. symmetric structure vs. asymmetric structure, Link travel-time model, i.e. the bottleneck model vs. the whole-link model (see e.g. Friesz et al. 1993; Friesz et al. 2001; Carey 2001 for details of this model), Criterion of user equilibrium, i.e. Wardrop’s first principle vs. stochastic user equilibrium (SUE). are investigated in this study. Regarding the structure of the study network, the ‘loopy network’, which is topologically identical to the twofold symmetric network by Iryo (2011), is mainly considered in order to maintain the characteristics of the original example providing non-unique solutions. In addition to the loopy network, this study also considers another network structure, referred to as the ‘reverse-link network’. As explained later, this network structure also derives non-unique equilibrium solutions, whereas the effects of the three factors are different from the loopy-network case. This paper consists of eight sections, including the introduction. Section 2 introduces the demand pattern, link travel-time models (the bottleneck model and the whole-link model) in the loopy network, as well as the equilibrium criteria (Wardrop’s first principle and SUE) employed in this study. Section 3 calculates the equilibrium solution in a loopy network in which the bottleneck model and Wardrop’s first principle were employed. Using this solution, the cases with asymmetric network structures, SUE, and the whole-link model are investigated in Sections 4, 5, and 6, respectively. The analysis of the reverse-link network with the bottleneck model and the whole-link model is shown in Section 7. Conclusions are summarised in Section 8.",15
15.0,3.0,Networks and Spatial Economics,02 July 2014,https://link.springer.com/article/10.1007/s11067-014-9250-x,Advances in Dynamic Traffic Assgmnt: TAC,September 2015,Wen-Long Jin,,,,Unknown,Unknown,Mix,,
15.0,3.0,Networks and Spatial Economics,27 July 2014,https://link.springer.com/article/10.1007/s11067-014-9249-3,Trip-Based Path Algorithms Using the Transit Network Hierarchy,September 2015,Alireza Khani,Mark Hickman,Hyunsoo Noh,Unknown,Male,Unknown,Male,"Finding the shortest path in a network is a traditional problem in transportation planning. There are also other variations of the path problem, such as the weighted optimal path, or the optimal hyperpath. An important application of shortest path algorithm is in the transportation network assignment, where travelers seek their best path (in any form of path and with any cost function) to reach their destinations. Paths are generated between all origins and destinations, and at each departure time in a time-dependent network, several times in an assignment model, which makes the problem computationally expensive. Commonly, the path generation in assignment models is in the form of on-to-all, where demand from a certain origin to all other destinations is assigned to the network as the result of one shortest path run. Trip planning is another application of the path models, which is usually provided by transportation agencies to travelers. In trip planning applications, the shortest path (sometimes a set of paths) is sought in an origin–destination pair only. These requirements form the various implications of a path model in different applications. When the network is time-dependent, the problem becomes more complex and computationally burdensome as the time dimension is added to the problem. In transit networks, particularly where the service is time-dependent, there are limitations categorized into spatial constraints, due to the geographical coverage of the routes, and temporal constraints, based on the service schedule. To model user behavior in choosing a transit path, in addition to travel time, other parameters may be considered: transfer between routes, the inconvenience of walking and waiting times, and crowding (congestion) which may result in a failure to board due to capacity of transit vehicles. To minimize travel time in an unreliable transit network (e.g. frequency-based networks), strategy was introduced in the field of public transit modeling (Nguyen and Pallottino 1988 and Spiess and Florian 1989). Strategies can also be used to deal with capacity constraints in a transit network (Teklu 2008 and Schmöcker et al. 2008 in frequency-based networks and Nguyen et al. 2001 and Hamdouch and Lawphongpanich 2008 in schedule-based networks). A strategy can be modeled by hyperpath, being a set of elementary paths connecting an origin to a destination at a given time (Nguyen and Pallottino 1988). On the other hand, in a model with stochastic user behavior, multiple paths need to be generated as alternative path(s) for a user. Thus, a set of attractive paths has to be found for each origin–destination pair, and this is usually handled by a hyperpath algorithm in the recent models (Nguyen and Pallottino 1998 and Noh et al. 2012a). With these complexities of the transit networks, and multiple parameters involved in passengers’ decision making process, transit path problems should be modeled appropriately to achieve the following goals: to model the complexities of a time-dependent network, to model user behavior more realistically, and to find the path(s) efficiently. As opposed to the road networks in which every node has a static list of adjacent nodes for all periods of time, in schedule-based transit networks the adjacent nodes (stops) vary over time, and the cost of reaching an adjacent node is a function of the available service at a given time. In other words, although stops i and j are connected geometrically, they should be connected in time as well, in order to consider j adjacent to i. This implies that the topology of a transit network differs at different times of the day. This property makes schedule-based transit networks more complicated than road networks. In the last few years, many studies have been conducted to model schedule-based networks, mostly using a time-expanded network [Hamdouch and Lawphongpanich 2008 and Noh et al. 2012a]. In such models, the size of the network increases significantly, and solving a path problem requires significant computational effort. Since the purpose of this study is to increase the efficiency of solving the path problems, we propose an alternative network representation to avoid expanding the network nodes and links in time. In this structure, the topology of the network remains the same as the static network, while embedded in the network elements is information about the departures and arrivals of transit vehicles. We call this network a Trip-Based Network, since transit stops are connected with scheduled vehicle trips instead of links. Using the trip-based network, we propose a trip-based shortest path, a trip-based hyperpath, and a trip-based transit A* algorithm. This paper is organized into 5 parts. After the introduction in part 1, in part 2 we provide detailed information about the trip-based network, including the transfer stop hierarchy and GTFS data as the main data source to build the network. Part 3 is dedicated to the proposed path algorithms. A brief review of the most relevant literature is provided for each model in this part of the paper. An illustrative example is also used to show how the proposed path models work. The application of the models, as well as computational tests, is provided in part 4. Finally, conclusions and future work are mentioned in part 5.",39
15.0,3.0,Networks and Spatial Economics,24 April 2014,https://link.springer.com/article/10.1007/s11067-014-9242-x,Improving the Convergence of Simulation-based Dynamic Traffic Assignment Methodologies,September 2015,Michael W. Levin,Matt Pool,S. Travis Waller,Male,Male,Unknown,Male,"Dynamic traffic assignment (DTA) models have become a widely accepted tool to support a variety of transportation network planning and operation decisions. The ability of these models to produce stable and meaningful solutions is crucial for practical applications, particularly for those involving the comparison of modeling results across multiple scenarios. Although the literature presents a fairly unified approach to define the conditions that characterize equilibrium in the context of simulation-based DTA (SBDTA) (e.g. Lo and Chen 2000; Lu et al. 2009; Chiu and Bustillos 2009), practical implementations differ in the methodology used to attain these. The first goal of this research effort is reviewing and contrasting existing methodological approaches for the equilibration of large-scale SBDTA models. Increased convergence in SBDTA models may improve viability for practitioners. In order to study the convergence pattern of different methodologies under comparable conditions, this work implements several of the surveyed techniques and some novel variations within a common SBDTA platform. There are two main processes which are repeated multiple times during the solution of a SBDTA framework: the simulation of traffic conditions for a given assignment of vehicles to paths, and the search for new shortest paths based on the simulated traffic conditions. Both may involve significant computational effort, depending on the characteristics of specific SBDTA implementations. This paper is focused on providing a better understanding of the characteristics of the convergence process of different algorithms. Both the overall convergence trend and stability of improvement from individual iterations are studied. The results of the numerical experiments, conducted on three real networks with up to 200,000 trips, are described in terms of the number of simulations runs and time-dependent shortest path computations required to achieve an equilibrium solution. The analysis includes a discussion of the properties of the solutions obtained through different methodologies intended to reveal the cause for the observed convergence rate. These properties may play a role in the selection of an acceptable convergence level for practical applications. The computational efficiency of the analyzed techniques is not explicitly described, as it will highly depend on implementation-stage decisions that involve other components of a SBDTA model. Instead, this paper is focused on providing a better understanding of the characteristics of the convergence process of different algorithms. The results of this effort may motivate the development of efficient implementations that can improve the ability of SBDTA models to handle larger networks more efficiently.",28
15.0,3.0,Networks and Spatial Economics,23 April 2014,https://link.springer.com/article/10.1007/s11067-014-9241-y,Evacuation Planning with Endogenous Transportation Network Degradations: A Stochastic Cell-Based Model and Solution Procedure,September 2015,Jian Li,Kaan Ozbay,,,Male,Unknown,Mix,,
15.0,3.0,Networks and Spatial Economics,27 June 2014,https://link.springer.com/article/10.1007/s11067-014-9248-4,Combined Route Choice and Adaptive Traffic Control in a Day-to-day Dynamical System,September 2015,Lin Xiao,Hong K. Lo,,Female,,Unknown,Mix,,
15.0,3.0,Networks and Spatial Economics,19 June 2013,https://link.springer.com/article/10.1007/s11067-013-9192-8,A New Tradable Credit Scheme for the Morning Commute Problem,September 2015,Yu (Marco) Nie,,,,Unknown,Unknown,Mix,,
15.0,3.0,Networks and Spatial Economics,14 June 2013,https://link.springer.com/article/10.1007/s11067-013-9195-5,Clearance Time Estimation for Incorporating Evacuation Risk in Routing Strategies for Evacuation Operations,September 2015,Yu-Ting Hsu,Srinivas Peeta,,,Male,Unknown,Mix,,
15.0,3.0,Networks and Spatial Economics,31 July 2014,https://link.springer.com/article/10.1007/s11067-014-9252-8,Submission to the DTA2012 Special Issue: A Case for Higher-Order Traffic Flow Models in DTA,September 2015,Ranju Mohan,Gitakrishnan Ramadurai,,Unknown,Unknown,Unknown,Unknown,,
15.0,3.0,Networks and Spatial Economics,20 June 2013,https://link.springer.com/article/10.1007/s11067-013-9191-9,A Sustainable Road Network Design Problem with Land Use Transportation Interaction over Time,September 2015,W. Y. Szeto,Y. Jiang,A. Sumalee,Unknown,Unknown,Unknown,Unknown,,
15.0,3.0,Networks and Spatial Economics,21 November 2013,https://link.springer.com/article/10.1007/s11067-013-9207-5,Effects of Countdown Displays in Public Transport Route Choice Under Severe Overcrowding,September 2015,Valentina Trozzi,Guido Gentile,Michael G. H. Bell,Female,Male,Male,Mix,,
15.0,3.0,Networks and Spatial Economics,11 August 2013,https://link.springer.com/article/10.1007/s11067-013-9198-2,Model Representation & Decision-Making in an Ever-Changing World: The Role of Stochastic Process Models of Transportation Systems,September 2015,David P. Watling,Giulio E. Cantarella,,Male,Male,Unknown,Male,"The global economic downturn, the ‘Arab spring’ and the turmoil in currencies are recent reminders that we live in an ever-changing world. Economic and social factors have profound influences on the level and pattern of travel demand and the choices of travelers within a given transport infrastructure. They also impact on the ability of responsible authorities to fund the maintenance and improvement of infrastructure, and to conduct effective travel demand management and control policies. It is just at such stages of major change and uncertainty that those planning future transport policies most need support in making their decisions, but in general this is exactly when most of the modelling tools we adopt fail to offer support, with their assumptions based on either an unchanging world, or one in which the future follows deterministically from the present. Even in periods of relative economic/social stability, such assumptions are increasingly difficult to support; this is most notable in cities where continued demand growth has outpaced the expansion in capacity of the transport infrastructure, with the transport system highly sensitive to daily and seasonal fluctuations in demand and capacities. The question then arises as to how we might develop modelling approaches to better deal with such situations. One approach to such problems is that of ‘worst-case’ planning, whereby the models suggest actions for a planner to take so as to minimize the impacts under a worst-case scenario. The worst-case scenario itself may be user-defined, or for some methods may be itself generated as part of the modelling approach; examples of the latter are methods based on robust optimization (e.g. Ben-Tal et al. 2011), or on game theory in which a fictitious evil entity is at work (e.g. Bell 2000). Such approaches have the advantage that there is no need to define event probabilities for the factors that affect the transportation system performance, and one can continue to use deterministic methods with tractable solution approaches. Such approaches have a particular advantage for representing extreme but rare events, where we may not have sufficient real-life evidence to make reasonable estimates of the component probability distributions. An obvious disadvantage of such methods is that they provide no information on anything but the extreme-most case. For dealing with typical daily and seasonal fluctuations in demand and supply, it seems that a stochastic model would be much more appropriate, where we aim to explore the full probability distribution of network impacts, not just the extreme-most point. This is also arguably the case for longer term trends that have major transportation impacts, such as economic development (e.g. GDP) and oil prices, whereby a stochastic model can be used to capture the uncertainty in such future events. In practice, we believe that a combination of scenario-based deterministic methods and fully stochastic methods are appropriate, depending on the nature of the variation under consideration (as discussed above). In the present paper we shall henceforth focus only on stochastic approaches, on the basis that there is considerable empirical evidence of real-life variation that we already routinely collect but make little use of in our modelling. With this focus in mind, the purpose of the present paper is to raise the profile of a particular class of stochastic approaches to transportation system modelling which was first proposed more than twenty years ago (Cascetta 1987, 1989), but which has since attracted relatively little attention. Indeed it is commonly misunderstood by researchers in the field, as well as being mistakenly described and interpreted in transportation journal papers, and so we feel that it is timely for a paper to clearly set out the approach and its possibilities, in order to raise its profile. This approach is able to deal with many aspects of both (a) dynamic change and (b) uncertainty/variability, representing the time-evolution of all relevant state variables as a stochastic process. It is very different from the well-known Stochastic User Equilibrium model (so called after Daganzo & Sheffi, 1977), though the appearance of the word ‘stochastic’ in both can serve to confuse those unfamiliar with the method. It is also very different from the now growing body of research on deterministic dynamical system models—for recent examples see Bie and Lo (2010), Han and Du (2012) and He and Liu (2012) —which are able to capture the dynamics in (a), but not the aspects of uncertainty/variability to which we refer in (b). In this context we refer the reader to two companion papers by the authors to the present paper, in one of which we focus entirely on deterministic process models (Cantarella and Watling 2013), and in the other we explore the relationships between deterministic process, stochastic process and stochastic user equilibrium approaches (Watling and Cantarella 2013). At its simplest, most ‘stripped down’ level, the Stochastic Process (SP) approach could be said to comprise three main elements for representing the epoch-to-epoch changes in a transport system: A learning model, to describe how travelers learn from their travel experiences in past time epochs. A decision model, to describe how travelers make decisions, given their learnt experiences in 1. A supply model, to describe the experiences of travelers in a particular time epoch. Some or all of these elements are described by probability statements or probability distributions, and when brought together they provide a single, self-consistent framework for representing the mutual interactions between the uncertain components of the transport system. Just as we demand of equilibrium transportation analysis, we can ask to what extent this combination of elements may produce a well-defined and unique ‘output’ (if the long-run is indeed what interests us), but whereas in equilibrium systems we refer to a unique flow state, in the SP approach we refer to a unique probability distribution of flows. That is to say, the result of the modelling approach is to provide the planner with probability distributions, not with single point estimates. The description given above is deliberately rather general, in that it does not specifically say what we mean by a traveler choice (e.g. which route, which departure time, whether to travel, where to live), what we mean by a time epoch (e.g. an hour, a day, a week, a month, a year), and what particular combinations of assumptions might make up the learning, decision and supply models. So, at one extreme we might be considering the year-to-year dynamics of residential location choice and the impact of and on the transport system, and at another we might be considering the day-to-day dynamics of route choice and traffic congestion. However, in keeping with the primary focus of most work on this topic to date, we shall focus heavily in our examples (section 3) on the class of problems concerned with the day-to-day dynamics in route choice (though in section 2 we explain the wider context of this work). In some respects this class defines an especially challenging (and therefore interesting) context, since it is subject to both the wider ‘global’ variations described in the opening of this abstract, and the more ‘local’ variations that are always seen between days-of-the week and seasons. The paper is structured as follows. In section 2 we address a key issue in understanding and applying the approach, namely the possible choices for representing elements of the transportation system, and the implications of these choices, particularly in terms of the state-space representation and the proper specification of probability statements about this system. In section 3 we illustrate these various forms of representation with a range of example models that sit within the stochastic framework. To provide some focus, section 3 considers only the sub-class of problems concerned with driver route choice in networks, in contrast to the rather general treatment of section 2. The transition functions governing the stochastic dynamics are explicitly derived for simple examples of these models. The purpose of these examples is also to illustrate the possibilities for the approach to link to different fields (and philosophies) of transportation modelling, be that behavioral dynamics, dynamic traffic assignment or micro-simulation. In section 4 we address the issue of how such models might be used in a planning environment, either in ‘dynamic’ or ‘stationary’ mode, the latter being an analogue of existing equilibrium methods of planning. For the latter mode, theoretical conditions are set out to guarantee existence and uniqueness of the relevant stationary distributions, as well as indicating efficient computational shortcuts. In section 5, a rather general family of stochastic process models is presented for analyzing the class of day-to-day dynamic route choice problems, and the properties of this class analyzed. Finally, we conclude by identifying future applications, practical issues and research directions.",48
15.0,3.0,Networks and Spatial Economics,12 October 2013,https://link.springer.com/article/10.1007/s11067-013-9213-7,Correcting the Market Failure in Work Trips with Work Rescheduling: An Analysis Using Bi-level Models for the Firm-workers Interplay,September 2015,Wilfredo F. Yushimito,Xuegang Ban,José Holguín-Veras,Male,Unknown,Male,Male,"There is the widespread view that the peak hour/period traffic is nothing but the result of traveler decisions (e.g., Ott et al. 1980; Moore et al.1984). Under this perspective, travelers decide what their preferred departure times and times of travel are in response to expectations of travel conditions, and from that, some sort of equilibrium (static or dynamic) emerges. Implicit in this assumption is the idea that the traveler has the possibility to reschedule, modify, and even cancel the trip. At the heart of this issue are the differences between compulsory and non-compulsory trips. While in the latter, the traveler has great liberty to change travel patterns; the same cannot be said about the former because of the existence of more rigid travel constraints. This is because in the case of compulsory trips, there are powerful external agents (e.g. employers, educational institutes) that mandate the arrival and departure times almost without feedback from their customers (e.g. workers, students). As a result, assuming that travelers could react by changing time of travel decisions, it requires taking proper consideration of the corresponding travel constraints. In the case of non compulsory trips, the constraints are not likely to be very tight; while in the compulsory case, they are likely to severely restrict what this traveler could do in reaction to either traffic conditions or public sector interventions like tolls. This in turn poses a major challenge because in the peak hours and peak periods -where the use of pricing is likely to be justified- the bulk of this traffic (passenger and freight) is of a compulsory nature, i.e., work trips. The behavior of the travelers is therefore controlled by the demand generator while the travelers have little or no means to influence what the demand generators do. In work trips and work related trips, it is the firm who sets up the starting time (which usually is during peak hours) without with little or not consideration of the implications and impositions to the workers (the users of the transportation system). Workers end tied to the firm’s decision suffering from the additional costs of traveling during peak periods. The Evaluation Study of the Port Authority of New York and New Jersey’s Time of Day Pricing Initiative (Holguín-Veras 2005, 2011) seems to support this idea. The study found that the decisions for traveling at certain time are usually constrained by their work schedules (about 87 percent of AM peak users and 67 percent of the PM peak users) with a relatively narrow windows of flexibility (see Table 1) of about 20 minutes for early departure or arrival, and about 14 minutes for late departure or arrival. Moreover, according to the report, about 46.6 percent of the work trip respondents had no flexibility at all in arriving late. Flexibility was higher if workers decided to arrive early, while 19 percent of workers had no flexibility at all in this case. Only roughly 10 percent of trips reported a flexibility of 30 minutes or more. These results are similar to those found by Emmerink and van Beek (1997) who used data collected in 1992 in The Netherlands. They have found even larger percentages: 67 percent of the respondents indicated that their employers do not allow for any flexibility at all.
 This implies that most travelers cannot simply wait for the non-peak period due to arrival time constraints, which are often tight and inflexible. This implication is important because some strategies, such as congestion pricing, can be only applicable to a small percentage of users during peak hours who have certain flexibility in their schedules. On the contrary, they are not generally applicable to most workers as their schedules are not flexible. Flexibility in arrival time is, as pointed out by Emmerink and van Beek (1997), a “...necessary condition for implementing congestion pricing...” and therefore a necessary condition for implementing demand management policies that seek to alter travel behavior patterns. This relatively little or no flexibility found indicates that users have constraints imposed by jobs that make it difficult to shift their time of travel. Firms tend to benefit from having their workers on duty during the same hours as well as at the same time as other companies (competitors or clients), imposing the working schedules to a large portion of their workers. As Jones et al. (1977) noticed, there is an asymmetric firm-worker’s relationship with a firm acting as the dominant agent: “Most choices to travel in the peak period should be viewed as a joint decision of the individual and the employer because the “choice” of travel time is determine by work schedules...Employers are likely to have far more discretion that their employees in scheduling work hours...Most employees face a Hobson’s choiceFootnote 1 between unemployment and work schedule conformance...” And, when such type of asymmetry is present, it can be expected that a market failure arises. A market failure is a situation in which the allocation of goods or services is not efficient. It is usually associated with the presence of public goods, information asymmetry, non-competitive market, or externalities (Ledyard 2008). In such situations the first two theorems for welfare do not hold (Mas-Colell et al. 1995) and the market cannot reach an efficient solution. The evidence presented earlier has shown that the current scheme favors the firm who sets up the starting time (which usually is during peak hours) with little or no consideration of the implications and impositions to the workers, who are the users of the transportation system. If workers are tied to a firm’s decision that sets up standard schedules (fixed or tight), they will suffer from the additional costs of traveling during peak periods. Thus the failure mechanism in this scheme is that the presence of the private utility of the dominant agent (the firm) prevents the society to reach a social optimum that improves social welfare. This private utility does not account for the additional costs imposed in their workers as well as the costs imposed to the rest of the society in terms of traffic congestion and pollution, making this scheme inefficient for society. The presence of this market failure also justifies public sector intervention. Economics offers two types of instruments for governments to address the problem for transport market failures (congestion, road and environmental damage, road accidents, etc): command-and-control and incentive based policies (Santos et al. 2010). The former are regulations imposed by the government to consumers and users to change their behavior. Examples of these regulations are emission standards, parking restrictions, among others. Incentive based models are economic mechanisms in which targeted agents receive incentives to alter directly their private utility or cost that result from their behavior. Examples of these mechanisms are taxes such as congestion pricing, subsidies, and incentives (Santos et al. (2010) provide a comprehensive description of the incentives used in transportation and their policy implications). In the particular case of the peak congestion, the key is to modify the behavior of the dominant agent (the firm) that creates the demand making it more attractive to reschedule workers to off-peak periods. The intervention is therefore more suitable for an incentive focused on the dominant agent rather than imposing charges to the dominated ones. Such type of approach has been applied in the implementation of a shift in freight traffic from regular hours to off-peak hours through incentives (Holguín-Veras et al. 2005). In these cases, financial incentives compensate the inconvenience caused by this practice in receivers as shown in Holguín-Veras (2008, 2011) and Holguín-Veras et al. (2006). The success of such type of approach is well documented in Holguín-Veras et al. (2011). The purpose of this paper is to study the nature of the market failure related to the problem of tight and restrictive arrival times in workers imposed by firms. This is done by modeling the firm-worker interplay providing an alternative work schedule. The aim is to flatten the peak hours by assigning some of the demand to the less congested periods, and therefore reducing the peak demand. Past initiatives and pilot experiments have claimed that fewer work trips are expected during the peak hours (Maric 1977; Guiliano 1990; O’Malley and Selinger 1973; Owens and Warmer 1973). This not only reduces costs in society (e.g., congestion, pollution) but also helps to minimize the physical requirements. However, formal experiments with this policy continued in the 1980s but its use has declined over time. In order to make this strategy sustainable it is necessary to go in depth in understanding the behavior of the agents (firms and workers) involved when work activities are rescheduled as well as in translating such behavior into a mathematical framework with the correct interaction between agents. In the model proposed in this paper, a mathematical framework captures such interaction at a network level. This includes (1) departure time choice, (2) route choice, (3) network interactions (i.e., flow propagation), and (4) a general production effects on the firm model based on arrivals. The main assumption is that the firm, affected by production effects, seeks to maximize their utility. This is addressed using an indirect measure of the productivity based on the “Theory of Marginal Productivity” that has been used in the only two studies that have sought to measure production effects as a function of work starting times (i.e., Wilson 1988; Gutiérrez-i-Puigarnau and Ommeren 2012). Once the firm decides the work schedules, the workers react by adjusting their departure time and route choice behavior balancing them with their actual arrival time until a Dynamic User Equilibrium (DUE) is achieved. The actual arrival time thus affects the firm productivity and the negotiation is repeated with firm the firm acting as the leader and the workers as followers in a Stackelbergh equilibrium. This is set up in a bi-level programming framework that not only helps to show the market failure but also it is useful to evaluate corrective measures based on incentives or based on congestion taxes. The paper is organized as follows: Section 2 presents the analysis of the agents involved (firms and workers) and the mathematical models that captures their behavior. Section 3 presents the bi-level models developed used in the evaluation. Section 4 presents the solution algorithm. Section 5 presents an illustrative example that uses real data to capture the production effects on firms. Finally Section 6 presents the conclusions.",7
15.0,4.0,Networks and Spatial Economics,25 November 2014,https://link.springer.com/article/10.1007/s11067-014-9276-0,Hub Location Problems with Price Sensitive Demands,December 2015,Morton E. O’Kelly,Henrique Pacca L. Luna,Gilberto de Miranda Jr.,Male,Male,Male,Male,"Service networks have grown to become a world-wide phenomenon. In these networks, multi-layered structures called Hub-And-Spoke (HS) systems are often used to control communication costs. HS systems are potentially large-scale networks where hubs are responsible for concentrating the traffic of information, packages, or people. Networks employing the HS paradigm are well known, and include telecommunications, freight, and air-traffic networks. Such networks have specialized their operations, tuning their service-level to best suit their customers. Keeping track of what drives consumer choice is an important component of their business strategy (see Lederer and Nambimadom (1998)). Generally, networks can increase their scope and density by covering more interacting pairs, and to do this, they need to have a good way to attract and retain traffic. The two driving forces, (i) cost reduction by exploiting scale economies and (ii) the need to provide the highest possible level of service to stay competitive, imply a continuous need to improve efficiency. In fact, the profit margins of highly networked businesses have been squeezed by intensive price competition. It is vital to this analysis to understand the sensitivity of flows to true routing costs, as these in turn determine the prices that the carrier must charge for their service. It can also assist in understanding demand reaction to differential services. With the aim of making a minimal set of assumptions about actual data, this paper simply supposes that there is a linkage between the cost of the service and the demand for that service. While some restrictive assumptions are needed, these are both reasonable (non-negativity) and powerful – in other words with a relatively simple formulation, our aim is to show that the model is solvable for large instances, and that these solutions correctly represent several logical aspects of markets. Generally, amalgamating flow through hubs allows the network operations to capitalize on economies of scale. A well-organized HS system which increases density of link flows, could be desirable from a cost point of view, as in Gillen et al. (1990). In the limit, however, the indirect routing through a few major hubs forces flows to be carried over rather circuitous trips. In the short term, assume that origin-destination patterns and destination options are relatively fixed, so the main reaction would be to downgrade the amount of flow between an origin and a destination in response to the cost of the interaction or, eventually, a switch to other services having lower service-levels and associated cost profiles. The complex set of relationships between prices, levels of service, network design, and efficiency is the subject of this paper. Another possibility, also considered here, is that the amount of demand for interaction between a pair of places is diminished if the service cost is high. This can happen if the consumer makes a comparison to some other service, provided that the offered service levels are similar and the price of the second (inferior) service is lower. In this work, an extensive report of computational experiments on the topic of quality of service (QoS) price equilibrium for HS systems with price sensitive demands is provided. These experiments aim to describe how the competitiveness of a market operating under the HS paradigm will affect the demand and price levels at the economic equilibrium, given the use of several interacting network layers. A set of efficient computational tools is also deployed, ensuring that large scale instances stay in reach of personal computers. Several topics from related literature are presented in Section 2 where a theoretical background is established on HS systems, price equilibrium, Benders decomposition and some computational aspects. Further, Section 3 establishes the notation and definitions needed to present the proposed mathematical programming model, the first and most natural one. In Section 4 the proposed model is modified by expanding it to a higher dimensional space, giving rise to a mathematical model that is well-suited to the computational challenges at hand. In Section 5 Benders decomposition is applied to this later model and also enhanced by a specialized cut generation procedure, as detailed in Section 6, resulting in a robust algorithm for the proposed model. Computational experiments are extensively provided and analyzed in Section 7. Finally, in Section 8 the concluding remarks and hints for future research are given.",26
15.0,4.0,Networks and Spatial Economics,13 January 2015,https://link.springer.com/article/10.1007/s11067-014-9277-z,An Excess-Demand Dynamic Traffic Assignment Approach for Inferring Origin-Destination Trip Matrices,December 2015,Chi Xie,Jennifer Duthie,,,Female,Unknown,Mix,,
15.0,4.0,Networks and Spatial Economics,25 November 2014,https://link.springer.com/article/10.1007/s11067-014-9278-y,Analyzing the Maritime Transportation System in Greece: a Complex Network Approach,December 2015,Dimitrios Tsiotas,Serafeim Polyzos,,Male,Male,Unknown,Male,"Maritime networks may be classified among the oldest forms of spatial communication and their architecture can be surely considered that it affected and still affects the further procedure of the worldwide economic evolution (Ducruet and Beauguitte 2013). This indicates that the study of the maritime communication systems and the understanding of their structure, geography, operational mechanisms and evolutionary patterns (Ducruet and Notteboom 2012) suggest a considerable effort for the stakeholders and policy makers of the worldwide economic system. In the recent years, the complexity of modern economies’ spatial and industrial organization favor a multi-actor and multi-modal pattern configuration (Zwier et al. 1995), leading to the establishment of Network Theory (Borgatti and Halgin 2011; Wang et al. 2011; Tsiotas and Polyzos 2013a; Caschili et al. 2014) and particularly Complex Network Analysis (Albert and Barabasi 2002; Schintler et al. 2007; Ducruet and Beauguitte 2013) as a separate scientific sector that constitutes a fundamental tool of geography and spatial analysis, providing a new framework of analytical techniques and methods for spatial economic analysis. In this modern analytical framework, maritime networks employ a fundamental portion of complex network analysis, since they represent spatial communication systems producing voluminous flows of socio-economic interest, such as tourism and trade flows. In terms of trade, an amount of about 90 % of the worldwide trade is transported by sea, fact that it is economically interpreted to a 70 % of the worldwide trade value (Windeck 2013). On the other hand, maritime networks also provide fundamental transportation services to tourism, where large tourist masses reach annually their coastal and insular tourism destinations. Even more, the mean of maritime transportation suggests a tourism resource by itself, constituting a market that is estimated to possess almost the 10 % (Diakomihalis 2007) of the total expenditure of the worldwide tourism. Under the above perspective, this article studies the Greek Maritime Network (GMN), by using measures of complex network analysis and empirical techniques, taking under consideration that the maritime activity in Greece is listed as one of the most important components of its economy. In numeral terms, the Greek maritime passenger system constitutes a transportation system having annual carrying capacity estimated to 55 million passengers, fact that sets Greece into the first place (possessing the highest share of about 30 %) in the list of European maritime transportation and into one of the highest places in the respective world ranking (Tzannatos 2005). Utilizing complex network analysis for modeling spatial networks generally benefits a macroscopic spatial and socioeconomic approach for these physical communication systems (Ducruet and Beauguitte 2013). This article applies for the first time, according to the best of our knowledge, this analytical tool for studying maritime transportation systems at national level and surely this stands for the case of Greece. Works of similar thematic framework conducted until now (Fremont 2007; Hu and Zhu 2009; Ducruet et al. 2010) apply complex network analysis on maritime transportation systems of larger geographic scale, such as global, intercontinental or international, and others that are applicable in the similar geographic scale (Wang et al. 2011; Lin 2012) study other means of transportation, such as aviation networks. The study of maritime transportation systems in national scale is driven by different characteristics than the respective international and global scale studies, fact that renders a different perspective to this article. The first difference is due to scale. In international and global maritime networks, the distances, the size of ports and the volume of flows are greater than these of national maritime networks, inducing economies of scale (Tang et al. 2011) that are not present in the majority of the national cases. This gives to the ports of global maritime networks the opportunity to evolve sometimes faster than the growth rates of their countries, affecting secondarily the hinterland’s development, depending on their position in the global network and the incentives policy that they adopt (shipping margins, infrastructure investments, etc.) (Marquez-Ramos et al. 2011). In opposition, the evolution of national maritime networks seems to be more endogenously ruled, depending basically on the developmental dynamics of each country. In reality, there is an interactive relation between the developmental dynamics of a country and its transportation system (Blumenfeld-Lieberthal 2008; Xie and Levinson 2009; Szeto et al. 2013). In particular, every endogenous alternation in a transportation network (such as the connection of a new place) induces microeconomic effects that through a complex mechanism affect the national and regional economy (Xie and Levinson 2009) and, vice versa, an amount of macroeconomic factors, such as transportation and economic policies, affect the structure of the national transportation networks (Blumenfeld-Lieberthal 2008). Additionally, the topology of transportation networks can be used as an index of the economic development of a country. Within this framework, the study of the topology of a national maritime transportation network obviously provides better insights for the economic identity of a country than the study of an international maritime system that that represents a polysynthetic economic pattern. Next, the global maritime networks seem to “lack of tantamount competitors”, implying that such enormous networks are developed to serve needs that, in the majority of cases, they cannot be served or they are partially being served by other competitive transportation modes, due to the higher cost of conducting distant hinterland transportations (Notteboom 2004). This observation illustrates the high connectivity of such global, international and intercontinental maritime networks, in opposition to the national cases, where the existence of competitive modes of transportation may produce disconnected national networks, consisting of separate components. Also, according to the literature (Notteboom 2004; Fremont 2007; Ducruet and Notteboom 2012; Windeck 2013), it is obvious that the global and intercontinental maritime networks, with the exemption of the international and intercontinental cruises that suggest a particular aspect of tourism, mainly refer to supply network chains that they are specializing to cargo transport and they are producing commodity network flows. This statement implies that the global and the intercontinental maritime networks are more cohesive than the domestic networks and thus they are more resistant to seasonality effects or disease spreading, which mainly describe the passenger transport networks. Within this framework, the study of the maritime transport system in Greece is ruled by both the aforementioned characteristics and by a set of extra peculiarities. The first characteristic is evident from the Greek history and concerns the geostrategic importance of the geographical position of the country in the map. Greece is a country that is located between two continents (Europe and Asia) and among three seas (Black Sea, Aegean and the Mediterranean), setting by default its national maritime transportation system as a point of economic and political interest. The second characteristic concerns the importance that the maritime activity (Tzannatos 2005) and the tourism activity (Polyzos et al. 2013a) have in the Greek economy, which are obviously spatial dependent. Additionally, the complex insular morphology of Greece (Tzannatos 2005), having a large number of islands, suggests an important motivation for studying its maritime system. According to the Greek National Census (ELSTAT 2011), the Greek islands, islets and rocky islands that are recorded in the regional sub-directories exceed the number of 1’350, from which over 230 are inhabited. This, in conjunction with the fact that Greece is a small country that it is placed within an area of 132 thousand square kilometers (m2), illustrates the obvious insular ontology of Greece, implying its significant maritime activity. Another peculiarity of the GMN is the intense seasonality in tourism activity between the summer period and the remainder of the year, which it is even differentiated within the summer period itself. It is remarkable that the population of the island regions, excluding Crete, exceeds at the summer period the amount of 6 million people, including permanent residents and visitors, when at the remainder period of the year it is less than 1 million (Polyzos et al. 2013a). Obviously, such demographic seasonality induces serious consequences to the operational efficiency and consequently to the topology of the network, influencing the total transportation capability of the country. The final peculiarity is that the GMN is a disconnected network, consisting of separate connected components. This is, primarily, due to scale, as it was previously noted, which allows the development of competitive transportation modes (coastal road transportation and insular air-transport) and, further, due to the complex coastal and insular morphology of Greece that favors the development of multi-mode transportation services. Despite the fact that, in the topological space, the GMN constitutes a disconnected network, where the distance between a pair of disconnected nodes is defined as infinite (Koschutzki et al. 2005), in the physical space this maritime system is a spatial network that represents an aspect of the Greek socioeconomic system. This consideration allows interpreting the GMN as a spatio-economic unity that was diachronically ruled by a consistent set of developmental norms, setting a conceptual link among the disconnected GMN’s components, which allows making conventions in order to define a global GMN’s topology aiming to the application of spatial analysis. In general, the research orientation of this article is ruled by the rationale that, diachronically, the spatial communication systems reflect on their architecture and on their topology some information about the society that utilizes these systems and its cultural and cognitive attributes. Within this framework, the main research hypothesis of this paper is whether the GMN embodies, inside its topology, information of the economic and particular of the tourism dynamics of Greece and in what level such thing is done. Also, this paper proposes an empirical technique for detecting how distance affects a network’s structure, suggesting a further topic of empirical research on various networks. Consequently, testing the applicability and the effectiveness of this modern modeling network approach in Greece becomes a more interesting treatment today, since Greece is currently being subjected to an unconventional model of economic crisis, which for many analysts depict structural deficiencies of the European or even the global banking system (Polyzos et al. 2013b). Greece needs more than ever today to utilize both the common and the untried available research methods, in order to acquire a more in-depth knowledge for its developmental dynamics that will reignite the developmental dynamics of the country. The remainder of this article is organized as follows: Section 2 presents the methodology; the Graph Theoretical modeling of the GMN, the available data, the quantitative and statistical tools and the methodological framework of the analysis. Section 3 illustrates the results of the analysis and their interpretation and finally, at Section 4 conclusions are given.",41
15.0,4.0,Networks and Spatial Economics,17 January 2015,https://link.springer.com/article/10.1007/s11067-014-9279-x,Combined Gravity Model Trip Distribution and Paired Combinatorial Logit Stochastic User Equilibrium Problem,December 2015,Ampol Karoonsoontawong,Dung-Ying Lin,,Unknown,Unknown,Unknown,Unknown,,
15.0,4.0,Networks and Spatial Economics,07 March 2015,https://link.springer.com/article/10.1007/s11067-014-9280-4,Inter-School Bus Scheduling Under Stochastic Travel Times,December 2015,Shangyao Yan,Fei-Yen Hsiao,Yi-Chun Chen,Unknown,Unknown,,Mix,,
15.0,4.0,Networks and Spatial Economics,29 January 2015,https://link.springer.com/article/10.1007/s11067-014-9281-3,A Nonlinear Pairwise Swapping Dynamics to Model the Selfish Rerouting Evolutionary Game,December 2015,Wen-yi Zhang,Wei Guan,Jun-fang Tian,Unknown,,Unknown,Mix,,
15.0,4.0,Networks and Spatial Economics,04 February 2015,https://link.springer.com/article/10.1007/s11067-014-9282-2,Memetic Heuristic Approach for Solving Truck and Trailer Routing Problems with Stochastic Demands and Time Windows,December 2015,Seyedmehdi Mirmohammadsadeghi,Shamsuddin Ahmed,,Unknown,Male,Unknown,Male,"Truck and trailer routing problems (TTRPs) are related to transporting manufacturing goods within a plant or between factory floors and delivering products to markets and/or customers. The TTRP is a variant of the conventional vehicle routing problem (VRP). Indeed, VRP is known to have been one of the most studied combinatorial optimization problems of the past few decades, due to the fact that it covers certain areas in practice and, to a reasonable extent, considers complexities (Dantzig and Ramser 1959; Laporte 1992; Vidal et al. 2013). This theory was originally derived from travelling salesman problems (TSPs) (Vidal et al. 2013). Over the last two decades, constraints like time windows, time-dependent (Escuín et al. 2012), travel and service times (Ando and Taniguchi 2006; Zhang et al. 2013) and depot deadlines were added to VRP solutions (Li et al. 2010; Lei et al. 2011). However, such research only considered deterministic demand situations. This paper presents a solution to truck and trailer routing problem(s) under stochastic demand with a time window (TTRPSDTW) constraints. In TTRP, the customers may be serviced by either a single truck or complete vehicle (truck with a trailer). This feature is usually ignored in VRP. However, because of some obstacles that appear in real-life situations, such as road conditions, market locations, government regulations or limited space to manoeuver at the customer’s site, only a single truck may be needed to serve a few workstations and/or customers. These constraints are obvious in many practical situations (Lin et al. 2009; Derigs et al. 2013; Villegas et al. 2013). Several researchers have so far contributed in this area. One instance is that of Gerdessen (1996) working on VRP with a trailer. He demonstrated two real applications pertaining to TTRP. In one case it was the distribution of dairy products in cities where the distributor faced heavy traffic. Due to the fact that manoeuvring a complete vehicle (a truck with a trailer) was difficult in some areas, and that some streets had traffic restrictions not allowing trailers to enter, the trailers were often parked in parking spots from which customers were serviced. The second case was the distribution of animal feed components to farmers. Because most villages have narrow roads or small bridges, different kinds of vehicles were required to deliver the feed to farmers, one of which was called a double bottom, consisting of a truck and a trailer. While the truck made deliveries using sub-tours (some parts of the tour) the trailer parked in a parking area. Gerdessen showed the necessity of considering TTRP by demonstrating real applications. In some real applications, special operational restrictions or requirements may exist, such as customers’ working periods whereby some customers must be serviced during a specified time interval and there can be no delays in servicing those customers. These issues resulted in considering VRPs with time windows. Correspondingly, time window constraints can be seen in TTRP applications. These problems are known as TTRP with time windows. In addition, in practical situations, a dispatcher may not know the demands in advance. Therefore, the company may face the problem of delivering products to customers with random demands. Consequently, unexpected extra costs will be imposed on the company. These issues can be considered in vehicle routing problems with stochastic demands (VRPSD). Moreover, when facing the limitations and restrictions mentioned above, VRPSD cannot cover these issues and needs to consider TTRP with stochastic demands (TTRPSD). The literature survey revealed no papers on TTRPTW (truck and trailer routing problem with time windows) with stochastic parameters. Only a few articles were published on TTRPTW with deterministic parameters. The papers published on stochastic VRP are simply large in number. These concepts need to be considered together to formulate TTRPSDTW. Therefore, this paper classifies the relevant papers into two groups – standard TTRP papers and VRP with stochastic demands. To solve TTRPSDTW, it appears that its solution is computationally more difficult than VRPSD with time windows (VRPSDTW). Indeed, VRP itself is one of the most difficult combinatorial optimization problems. It is generally framed and solved by heuristics approaches (Cordone and Calvo 2001; Baker and Ayechew 2003; Eksioglu et al. 2009). To formulate and solve TTRPSDTW, one can make an effort to reduce it to VRPSDTW. As VRPSDTW is also a complex type of combinatorial optimization problem, it can be solved by heuristics approaches (Tillman 1969; Chao 2002; Scheuerer 2006; Derigs et al. 2013). The purpose of TTRPSDTW is to design a set of routes to cover all customers and optimize the total costs or distance that will satisfy all constraints. Standard TTRP was first proposed by Chao (2002). Scheuerer (2006) applied 0–1 integer programming formulation to solve TTRP. Chao (2002) and Scheuerer (2006) used a two-phase approach to solve TTRP. They used heuristics to construct the initial TTRP solution in the first phase. In the second phase, the Tabu search algorithm was used to improve the initial solution. Chao (2002) followed Fisher and Jaikumar’s (1981) construction to solve VRP. Scheuerer (2006) used Chao’s (2002) model and improved it by using two-construction heuristics, T-Cluster and T-Sweep, and applied a new Tabu search improvement algorithm to solve TTRP. Lin et al. (Lin et al. 2011; Lin et al. 2009; Lin et al. 2010) introduced simulated annealing to solve TTRP and obtained 17 best solutions to the 21 benchmarked TTRPs as reported by Lin et al. (2009). Then they applied time window constraints to the TTRP solution for the first time to bring the model closer to reality (Lin et al. 2011). Villegas et al. (2010) considered a single TTRP with satellite depots (STTRPSD). Variable neighbourhood descent (VND) and greedy randomized adaptive search procedures (GRASP) were proposed by them for solving TTRP. In addition, they applied the GRASP/VND algorithm for multi-depot VRPs and improved the previous analysis. Villegas et al. (2011) improved this solution by considering a hybrid algorithm based on the GRASP/VND and a path relinking (PR) algorithm and proved that the performance of this hybrid algorithm exceeds that of GRASP/VND. Finally, Villegas et al. (2013) proposed a new hybrid algorithm by considering GRASP and an iterated local search (ILS) and found a new solution for benchmarking, which was considered by Lin et al. (2009) for the first time. Derigs et al. (2013) proposed TTRP without load transfer between truck and trailer for the first time. A hybrid algorithm was applied to solve the TTRP by considering the large neighbourhood search (LNS) and local search (LS). In addition, time window constraints were also considered for each customer to bring the model closer to reality. Tillman (1969) introduced stochastic demand in capacitated VRP (CVRP) for the first time. In his study, the multi-depot VRP was considered in which the demands are stochastic with Poisson distribution. Bertsimas (1992) demonstrated critical evidence for the stochastic VRP and compared re-optimization and a priori strategies. Gendreau et al. (1996a) presented a Tabu search algorithm for solving VRP with stochastic demands and customers’ demands following a known distribution and customers were presented with a probability. The integer L-shaped method for CVRP with normal and Poisson demands was improved by Laporte et al. (2002). Lei et al. (2011) extended CVRP with stochastic demand (CVRPSD), and time window constraints were added to the vertices. They solved VRPSDTW by considering discrete and continuous distributions for stochastic demands. In this study, the memetic algorithm (MA) is applied in solving the stochastic TTRP (TTRPSDTW). The MA is a hybrid algorithm which combines the population and LS procedures to find the best solutions. This paper demonstrates that the MA is efficient in solving TTRPSDTW. However, no benchmark instances are available to compare the memetic heuristic solutions that are used in solving TTRPSDTW. To offset this limitation, 54 benchmark instances, derived by Lin et al. (2011) for the TTRPSDTW, are modified. Also, sensitivity analysis is conducted to validate the results.",16
16.0,1.0,Networks and Spatial Economics,13 June 2015,https://link.springer.com/article/10.1007/s11067-015-9296-4,Guest Editorial: Special Issue on Quantitative Approaches to Environmental Sustainability in Transportation Networks,March 2016,W. Y. Szeto,,,Unknown,Unknown,Unknown,Unknown,,
16.0,1.0,Networks and Spatial Economics,28 May 2014,https://link.springer.com/article/10.1007/s11067-014-9245-7,Sustainability SI: Bikeway Network Design Model for Recreational Bicycling in Scenic Areas,March 2016,Jen-Jia Lin,Rong-Ying Liao,,Unknown,,Unknown,Mix,,
16.0,1.0,Networks and Spatial Economics,04 August 2013,https://link.springer.com/article/10.1007/s11067-013-9201-y,Modeling Mode and Route Similarities in Network Equilibrium Problem with Go-Green Modes,March 2016,Songyot Kitthamkesorn,Anthony Chen,Seungkyu Ryu,Unknown,Male,Unknown,Male,"Environmental sustainability is an emerging issue in most major cities. Motorized vehicles generate a large amount of harmful emissions and noise, which have adverse effects not only to the environment but also to human health. Only about 20 % of the world’s town residents enjoy good enough air quality as measured by the levels of emissions (The Economist 1996, 1997), and over 600 million people in urban areas worldwide were exposed to dangerous levels of traffic-generated emissions from congestion (Cacciola et al. 2002). Moreover, these vehicle emissions are the sources of greenhouse gas that contributes to global warming (Cappiello 2002). This trend is truly an obstructer to the sustainable development (Button 1990) that hinders the goals set by the Kyoto Protocol (1997) to prevent further environmental damage. With these adverse effects, the need to encourage the usage of ‘go-green’ modes such as public transit and bicycle to realize the increasing travel demands while keeping the environmental expenses low is therefore evident. In order to evaluate and design an effective go-green promotion strategy, we need a better behavioral model of travelers’ mode choice shift between private motorized mode and ‘go-green’ modes as well as their mode-specific route choices. A popular approach is the network equilibrium model (Boyce and Bar-Gera 2004; Siefel et al. 2006; Boyce 2007; Hasan and Dashti 2007; Briceño et al. 2008; Szeto et al. 2012). The multimodal network equilibrium problem belongs to the combined modal split and traffic assignment (CMSTA) model that would provide behavioral richness, quantitative evaluation of different go-green promotion policies, and computational tractability. The concept of this combined travel demand model is far from new. Florian and Nguyen (1978) provided a mathematical programming (MP) formulation for the CMSTA problem by treating the transit mode as exogenously given. Then, Florian (1977), Abdulaal and LeBlanc (1979), Fernandez et al. (1994), and Gracia and Marin (2005) provided variational inequality (VI) or fixed point formulations to consider the mode shift in the equilibrium process endogenously. To overcome inconsistency between the stochastic mode choice behavior (from the logit-based model) and deterministic route choice behavior (from the user equilibrium model) in these combined models, Oppenheim (1995) and Wu and Lam (2003) adopted the multinomial logit (MNL) model to represent both mode choice and route choice dimensions in congested networks. However, the MNL model cannot handle the shared unobserved utility. According to the independently distributed assumption, the MNL model cannot capture the mode similarity (e.g., physical attributes and operating policies) and the route similarity (or route overlapping) (e.g., Sheffi 1985; Nielsen et al. 2002) which are generally included in urban transportation networks. To overcome this shortcoming, the multinomial probit (MNP) model could be adopted (e.g., Meng and Liu 2012). Nonetheless, due to the lack of a closed-form probability expression, solving the MNP model will require either Monte Carlo simulation (Sheffi and Powell 1982), Clark’s approximation method (Maher 1992), or numerical method (Rosa and Maher 2002). In this study, we provide a new CMSTA model to overcome the shortcomings of the MNL-based combined model by making use of the recent advances in extended logit modeling to develop an equivalent MP formulation that explicitly considers mode and route similarities under congested networks. Specifically, a nested logit (NL) model (Ben-Akiva and Lerman 1985) is adopted to model the modal split problem by accounting for mode similarity among the available modes (e.g., mode shift between the private motorized mode and the go-green modes), and a cross-nested logit (CNL) model (Bekhor and Prashker 1999) is used to account for route overlapping in the traffic assignment problem. Numerical examples are provided to illustrate the features of the proposed combined NL-CNL model for modeling the travelers’ mode shift between private motorized mode and ‘go-green’ modes as well as their mode-specific route choices, and evaluating the effectiveness of different ‘go-green’ promotion policies. The remainder of this paper is organized as follows. The next section provides some background of the NL and CNL models. In Section 3, an equivalent MP formulation of the CMSTA problem is presented. Section 4 and 5 respectively provide numerical examples to demonstrate the proposed model and concluding remarks.",40
16.0,1.0,Networks and Spatial Economics,25 August 2013,https://link.springer.com/article/10.1007/s11067-013-9203-9,Robust Multi-period Fleet Allocation Models for Bike-Sharing Systems,March 2016,Chung-Cheng Lu,,,Unknown,Unknown,Unknown,Unknown,,
16.0,1.0,Networks and Spatial Economics,18 April 2014,https://link.springer.com/article/10.1007/s11067-014-9224-z,Sustainability SI: Exploring Heterogeneity in Cycle Tourists’ Preferences for an Integrated Bike-Rail Transport Service,March 2016,Ching-Fu Chen,Wen-Chieh Cheng,,Unknown,Unknown,Unknown,Unknown,,
16.0,1.0,Networks and Spatial Economics,12 September 2013,https://link.springer.com/article/10.1007/s11067-013-9194-6,Influence of Social Networks on Latent Choice of Electric Cars: A Mixed Logit Specification Using Experimental Design Data,March 2016,Soora Rasouli,Harry Timmermans,,Unknown,Male,Unknown,Male,"Transport significantly contributes to environmental problems such as pollution and noise. Urban planners and transportation engineers have attempted to reduce the contribution of transport to these environmental problems by designing resilient urban and transportation systems. For example, the New Urbanism Movement has advocated that high-density, mixed-use urban environments induce people to use slow modes of transport, implying a reduction in pollution and noise levels. The transportation literature has accumulated a corresponding overwhelming body of empirical knowledge about the relationship between properties of the built environment and travel behaviour (e.g. Ewing and Cervero 2010). Although the results of these studies are mixed, the dominant findings seem to indicate that the strength of the relationship between high-density, mixed-use environments and the use of slow modes of transport is relatively weak. Similarly, the success of high density, mixed-use urban environments in reducing overall mobility levels is modest at best. Several considerations may explain these findings. First, the assumed relationship between density and mobility is, ceteris paribus, almost tautological. If individuals can find the locations for their activities at closer distance, as high density would imply, by definition their mobility in terms of distance travelled should be less. Thus, any statistical relationship between density and travel behaviour does not necessarily reflect people’s preferences and true underlying behaviour. Second, common reasoning ignores the fact that activity locations at close range do not stimulate individuals to behave in efficient ways. Consequently, high density often co-varies with higher travel frequency, which in turn, at least partially, counterbalances the positive effects of closer distance. Thirdly, daily travel is not a need in its own right. Rather, as has been commonly accepted in activity-based analysis (e.g. Timmermans et al. 2002), individuals travel to participate in activities, which in turn are conducted to satisfy needs and desires (e.g. Miller 2005). Individuals have several commitments reflecting a set of life trajectories decisions and pursue different careers such as a housing career, a job career, etc. The utility individuals derive from their long-term life trajectories careers tend to be substantially higher than the disutility of travel. Thus, in organizing their daily lives, most individuals tend to give priority to their job and housing careers and simply take the resulting travel for granted, as long as particular constraints are not violated. This principle has become even more prevalent as the result of an increasing share of women participating in the workforce and the rise of dual-earner households. In many cases, it is a coincidence that spouses can both find a job very close to their home, especially if the jobs are relatively scarce. Under such circumstances, at least one of the spouses may need to travel longer distance. Finally, the general increase of accumulated wealth has implied that fewer constraints act on individual mobility. A large share of the population of Western societies can afford buying a second car and thus will do so, also considering the convenience and flexibility the car brings in organizing daily activities, in particular when faced with full agendas and time pressure. If we move the focus of attention from developed Western countries to emerging societies with high economic growth such as China and India, it has become clear that increased social status and the growth of disposable income that comes with it, are reflected in consumption patterns, similar to those observed in developed countries before. Despite the fact that environmental concerns may even be more severe in such countries and that certainly the general awareness is, increased income and status are manifested in one or more cars. The problem here is that we cannot deny individuals in these countries to act in any other way. It implies that despite national and international urban, transportation and environmental policies, mobility will rapidly increase globally, further enhancing existing environmental problems. These observations do not imply that these policies should be abandoned; it merely means that expectations should not be high. The limited contribution of urban and transportation policies in reducing environmental problems also implies that a true solution should come from technology. In that regard, significant progress has been made recently in developing alternative fuel cars, such as electric and hybrid cars. This new technology based on non-fossil fuels not only means a smaller contribution to the depletion of scarce resources, but also less emission and energy consumption. The question, however, is whether the market will accept this new technology. Popular media and early industry reports seem to suggest that the market share for such new technology is still limited (e.g., Karfisi et al. 1978; Morton et al. 1978; Beggs and Cardell 1980). Concerns have been raised about the general performance of electric cars, relative to conventional cars, and their battery life and recharging limitations. Moreover, there may be issues with regard to the image projected by this type of car. This scepticism is not uncommon to technological innovations and diffusion processes in general. In the beginning, after the introduction of new technology, individuals are often reluctant to accept and buy the technology. Only after a certain threshold has been reached, general acceptance tends to increase rapidly, to slow down again until some saturation point has been reached. In light of this introduction, the aims and objectives of this study are two-fold. First, the aim is to better understand the latent choice of electric cars as a function of (i) their attributes, (ii) context, (iii) the nature of reviews it receives, (iv) the acceptance of the electric car by members of various elements of social networks (relatives, friends, co-workers and peers), and (v) socio-demographic variables. This study is designed to examine the relative importance of these factors in the decision whether or not to buy an electric car as the primary vehicle of the household. In particular, a stated choice experiment is designed to better understand this decision process and derive the relative importance of these factors in the choice of an electric car, with a special focus on social influence. Considering the objective of studying the impact of social influences, the design of the experiment differs from conventional stated preference and choice designs, which typically vary only the attributes of the choice alternatives. The second aim of this study therefore is to extend conventional choice experiments to allow the estimation of social influence on choice behavior. To these ends, the article is structured as follows. First, we will discuss the existing but still limited literature on the acceptance and choice of electric cars. This section serves to put our research findings in a wider context. Next, we will discuss the design of the experiment, followed by a discussion of the sample and its characteristics. The following section is concerned with a discussion of the analyses and results. In particular, the dichotomous choice data will be analysed using a mixed logit model. The dependent variable is the latent choice, whereas attribute levels of the car, context, nature of reviews, various social influences, operationalized in terms of shares of people of that social network owning an electric car, and a set of socio-demographic variables serve as independent variables. In addition, as reflected in the choice of a mixed logit model, unobserved heterogeneity will be captured by estimating distributional effects of some key independent variables. The article will be completed by a set of conclusions and a discussion of policy implications.",79
16.0,1.0,Networks and Spatial Economics,06 October 2013,https://link.springer.com/article/10.1007/s11067-013-9212-8,Sustainability SI: Optimal Prices of Electricity at Public Charging Stations for Plug-in Electric Vehicles,March 2016,Fang He,Yafeng Yin,Yanni Yang,,Unknown,Unknown,Mix,,
16.0,1.0,Networks and Spatial Economics,24 January 2014,https://link.springer.com/article/10.1007/s11067-013-9221-7,The Electric Vehicle Shortest-Walk Problem With Battery Exchanges,March 2016,Jonathan D. Adler,Pitu B. Mirchandani,Minjun Xia,Male,Unknown,Unknown,Male,"The environmental, geopolitical, and financial implications of the world’s dependence on gasoline-powered vehicles are well known and documented, and much has been done to lessen our dependence on gasoline. One thrust on this issue has been the embracing of the electric vehicles (EV) as an alternative to gasoline powered automobiles. These vehicles have an electric motor rather than a gasoline engine, and a battery to store the energy required to move the vehicle. Governments and automotive companies have recognized the value of these vehicles in helping the environment (Hacker et al. 2009), and are encouraging the ownership of EVs through economic incentives. States and cities are assisting owners of electric vehicles by creating charging stations for EVs in busy areas (Senart et al. 2010). For many electric vehicles, such as the Nissan LEAF or Chevrolet VOLT, the current method of recharging the vehicle battery is to plug the battery into the power grid at places like the home or office (Bakker 2011; Kurani et al. 2008). Because the battery has a limited capacity before it requires a recharge, this method has the implicit assumption that vehicle will be used only for driving short distances. EV companies are trying to overcome this limited range requirement with fast charging stations: locations where a vehicle can be charged in only a few minutes to near full capacity. Besides being much more costly to operate rapid recharge stations, the vehicles still take a more time to recharge than a standard gasoline vehicle would take to refuel (Botsford and Szczepanek 2009). These inherent problems, combined with a lack of refueling infrastructure, are inhibiting a wide-scale adoption of electric vehicles. These problems are especially apparent in longer trips, or inter-city trips. Range anxiety, when the driver is concerned that the vehicle will run out of charge before reaching the destination, is a major hindrance for the market penetration of EVs (Jeeninga et al. 2002; Sovacool and Hirsh 2009; Yu et al. 2011). Hybrid vehicles, which have both an electric motor and a gasoline engine, have been successful since they overcome the range anxiety of their owners by running on gasoline when needed. Since hybrids still require gasoline these vehicles do not fully mitigate the environmental consequences (Bradley and Frank 2009; Shiau et al. 2010). Another refueling infrastructure design is to have quick battery exchange stations (BEs). These stations will remove a pallet of batteries that are nearly depleted from a vehicle and replace the battery pallet with one that has already been charged (Shemer 2012). This method of refueling has the advantage that it is reasonably quick. The unfortunate downside is that all of the vehicles serviced by the battery exchange station are required to use identical pallets and batteries. It is assumed here that the developers of these battery pallets will coalesce around a single common standard, as has been the case for other car parts such as tires and wipers. In conjunction to the battery exchange concept, it is required for there to be a viable business model that provides a reasonable profit for companies that establish battery exchange facilities for the public. Battery exchange stations have been tried out by taxi vehicles in Tokyo in 2010 (Schultz 2010). In fact the country of Denmark is investigating the possibility of having sufficient battery exchange locations so that the country relies on none, or very few, gasoline-powered vehicles (Mahony 2011). Although the research team is working on several issues related to the design and operations of the infrastructure for battery exchange or battery charging stations, this paper focuses on only one aspect of the problem: finding the shortest walk from an origination point s to a destination t with minimum cost. This is route referred to as a “walk” as opposed to a “path” since a detour to exchange or charge a battery may include repeat arcs which are normally not included in shortest paths. In fact, this problem applies to any scenario where there only a limited number of places for “refueling” which is true not only for EVs but also other alternative fuel vehicles where a refueling infrastructure still needs to be developed, for example hydrogen powered vehicles (Ogden et al. 1999) where empty tanks or canisters are exchanged for full ones at special stations. Consider the underlying scenario. Taking a trip, especially one through lowly populated areas, requires the driver to plan when the vehicle will need to be refueled. Given the abundance of gasoline stations for standard vehicle, drivers of these vehicles usually consider refueling only when their fuel tank is low. The search for a good refueling point can be further aided by navigation systems and smart phone apps, such as Google Maps, that provide motorists the location of gasoline stations in the vicinity. In the case of EVs, planning refueling is more important than for gasoline vehicles, since there are few places to recharge. Thus, the vehicles will likely have to detour from the most direct route. Likewise, understanding routing would be even more critical for battery exchange facilities, since the infrastructure would gradually involve so, at least initially, the density of battery exchange stations would be very low. Hence, one needs to develop models which look for the shortest routes from origins to destinations that include detouring when necessary.",63
16.0,1.0,Networks and Spatial Economics,01 September 2013,https://link.springer.com/article/10.1007/s11067-013-9204-8,Towards High-Resolution First-Best Air Pollution Tolls,March 2016,B. Kickhöfer,K. Nagel,,Unknown,Unknown,Unknown,Unknown,,
16.0,1.0,Networks and Spatial Economics,24 January 2014,https://link.springer.com/article/10.1007/s11067-013-9211-9,Optimal Charging Strategies under Conflicting Objectives for the Protection of Sensitive Areas: A Case Study of the Trans-Pennine Corridor,March 2016,Astrid Gühnemann,Andrew Koh,Simon Shepherd,Female,Male,Male,Mix,,
16.0,1.0,Networks and Spatial Economics,23 April 2014,https://link.springer.com/article/10.1007/s11067-014-9235-9,Sustainability SI: Logistics Cost and Environmental Impact Analyses of Urban Delivery Consolidation Strategies,March 2016,Jane Lin,Qin Chen,Kazuya Kawamura,Female,,Male,Mix,,
16.0,1.0,Networks and Spatial Economics,29 January 2014,https://link.springer.com/article/10.1007/s11067-013-9220-8,Finding Efficient and Environmentally Friendly Paths for Risk-Averse Freight Carriers,March 2016,Qianfei Li,Yu (Marco) Nie,Tito Homem-de-Mello,Unknown,,Male,Mix,,
16.0,1.0,Networks and Spatial Economics,22 April 2014,https://link.springer.com/article/10.1007/s11067-014-9226-x,Sustainability SI: Bundling of Outbound Freight Flows: Analyzing the Potential of Internal Horizontal Collaboration to Improve Sustainability,March 2016,Tom van Lier,An Caris,Cathy Macharis,Male,,Female,Mix,,
16.0,1.0,Networks and Spatial Economics,18 April 2014,https://link.springer.com/article/10.1007/s11067-014-9227-9,Sustainability SI: Multimode Multicommodity Network Design Model for Intermodal Freight Transportation with Transfer and Emission Costs,March 2016,Yi Qu,Tolga Bektaş,Julia Bennell,,Male,Female,Mix,,
16.0,1.0,Networks and Spatial Economics,11 September 2013,https://link.springer.com/article/10.1007/s11067-013-9205-7,Bicriteria Optimization Model for Locating Maritime Container Depots: Application to the Port of Valencia,March 2016,Antonio Palacio,B. Adenso-Díaz,Salvador Furió,Male,Unknown,Male,Male,"Maritime container transportation has undergone significant growth worldwide. Since competition is intense, efficient inland operations are essential. One of the issues that must be dealt with is the storage of empty containers. Furió et al. (2005) list several reasons that may make it necessary to store empty containers. Among them are: a) The world container fleet is nearly double the total capacity of container ships; b) The lack of balance between import and export operations requires the repositioning of large amounts of equipment used for import and export; c) The difficulties reported for triangulations (i.e. coordinating import and export operations) require the storage of equipment once emptied until it is used again in a ‘match back’ operation; d) The variability in the load and the inability to match quantity, location and time between supply and demand, requires that carriers and leasing companies have stocks of equipment available to satisfy the demands of the different regions in which they operate. As the storage capacity of port terminals is limited and expensive (Holguín-Veras and Jara-Díaz 2006) it becomes necessary to have other places available for storing the empty containers. Container depots are where empty containers are stored after being used and are waiting to be taken for exportation. Furthermore, the containers require intermediate operations before being reused, such as inspection, cleaning and/or repair. Container depots usually offer these services, taking advantage of the empty containers passing through their facilities. It can thus be concluded that deciding on the location of depots in the different hinterlands is a decision of particular importance in maritime logistics operations. Since the location of container depots has an impact on maritime logistics costs, the container depot location problem is usually treated as a cost minimization problem. However, the location and operation of container depots also has an important environmental impact, defined as any change or modification in the environment with negative (or positive) effects produced as a result of the activities, products and services of any organization. Container depots, due to the amount of daily movements performed by trucks, generate a lot of traffic. This is one of the main reasons why container depots have a sizable environmental impact (e.g. traffic congestion; traffic accidents; atmospheric, noise and visual pollution). In addition to the impact caused by heavy transport operations associated with container depots, the environmental impact generated by the existence of the depots themselves should also be considered (e.g. construction of the depot, visual impact, pollution due to the internal operations of the depot). This paper considers the problem of designing a network of container depots in a hinterland. The starting point for our research is the Multicommodity Capacitated Location Problem with Balancing Requirements (MCLB) initially proposed by Crainic et al. (1989). This model assumes that once a ship reaches a port and is unloaded, the incoming containers are delivered to the consignees and, once unloaded, have to be transported to a depot or a terminal to be stored. These containers then remain in the depot until shippers request them for shipping their own products. Gendron et al. (2003) consider limited capacities at depots, where the capacity of a depot is represented by an estimate of the number of empty containers that can be handled there. The problem is defined as finding the best location for the depots to satisfy the demand for empty containers and minimize operating costs. Several methods have been proposed for efficiently solving the MCLB problem, mainly based on branch and bound (Crainic et al. 1993a; Gendron and Crainic 1995, 1997) as well as on tabu search (Crainic et al. 1993b; Gendron et al. 2003). However, although cost minimization is of course a primary objective, the growing importance of environmental responsibility makes it necessary to include also environmental impacts in formal decision models. This is sometimes done by incorporating impacts as explicit objective functions, thus giving rise to multi-objective approaches (e.g. Harris et al. 2009, 2011) or as constraints that bound acceptable environmental impact levels (e.g. Ghaddar and Naoum-Sawaya 2011). We have taken the former path, i.e. considering a bi-objective optimization model. Our approach differs from existing ones in that many different impacts are considered and not just CO2 emissions – which has become the main type of global impact considered in the literature on green logistics. Thus, as mentioned above, not only atmospheric but also noise and visual impacts, and accidents and congestion, will be considered. This has led us to use experts’ opinions on the importance and magnitude of these impacts for the different depot locations. To that end, the well-known Analytic Hierarchy Process (AHP) methodology (Saaty 1980) has been used. The proposed procedure thus consists of augmenting the Multicommodity Capacitated Location Problem with Balancing Requirements (MCLB) of Crainic et al. (1989) with the environmental impacts that result from the transport associated with the depots and to the setting up and maintenance of the depots themselves. It has been applied to the Port of Valencia, which is the largest container port in Spain, using as the solution methodology the ε-constraint method (Haimes et al. 1971; Miettinen 1999). The structure of the paper is as follows. In section 2 the proposed approach is presented. Section 3 reports the results of the application of the proposed approach to the hinterland of the Port of Valencia. Finally, in section 4, the summary and conclusions of this study are presented.",6
16.0,1.0,Networks and Spatial Economics,24 April 2014,https://link.springer.com/article/10.1007/s11067-014-9228-8,The Cost of Environmental Constraints in Traffic Networks: Assessing the Loss of Optimality,March 2016,Xin Lin,Chris M. J. Tampère,Ben Immers,,,Male,Mix,,
16.0,1.0,Networks and Spatial Economics,22 April 2014,https://link.springer.com/article/10.1007/s11067-014-9238-6,Dynamic Modeling of Performance Indices for Planning of Sustainable Transportation Systems,March 2016,Pankaj Maheshwari,Romesh Khaddar,Alexander Paz,Male,Male,Male,Male,"In the recent years, sustainability has become a very important research area in the field of transportation. Many studies have focused on understanding the design and analysis of sustainable transportation systems (Cascetta 2008; Manheim 1979). Issues that have been discussed include the formulations, analysis, design, and computation of solutions to such problems through the use of appropriate policies, ranging from tolls and tradable pollution permits (Nagurney 2000). Li et al. (2013) addressed the design of sustainable cordon toll pricing schemes and the findings suggest the interrelationships among cordon toll scheme, traffic congestion, environmental effects, and urban population distribution. The study also revealed the effects of subsidizing the retrofit of old vehicles on reduction in emissions and determined the optimal subsidy policy for social welfare. Szeto et al. (2013) discussed a sustainable road network design and provided interaction of transportation system with land use over time. Watling and Cantarella (2013) summarized the state of the art knowledge in modeling of transportation systems to conduct effective travel demand management and control policies. Sustainability of supply chains has emerged as a major theme in both research and practice, since the impacts of climate change have made both producers and consumers more cognizant of their decision-making and how their decisions affect the environment. The study of sustainability and supply chains helps understand how business integrates in context with the environment (Linton et al. 2007). Marale (2012) discussed the dimensions of human life and its linkages with the external environment for sustainable development. In addition, he proposed practical tools to solve global environmental problems. Chiabai et al. (2012) discussed the use of stated preference techniques to evaluate the importance of information and communication technology for environmental sustainability in key sectors (climate change, natural resources, energy, and biodiversity). Kitthamkesorn et al. (2013) used mathematical programming formulations to enhance the environmental sustainability through efficient promotion of ‘go-green’ transportation modes which included public transit and bicycle. Nguyen and Coowanitwong (2011) discussed the application of strategic environmental assessment tools for sustainable air quality policies. Their study was robust and helped to integrate the environmental aspects into decision making process. In addition, environmental performance can be looked upon as a source of reputational, competitive, and financial advantage among businesses (Miles and Covin 2000). It is evident that customers and suppliers will punish polluters that violate environmental rules; this is known as a reputational penalty (Klein and Leffler 1981; Klassen and McLaughlin 1996). The use of plug in electric vehicles (PEVs) has increased in recent years due to advances in battery technologies, increased gasoline prices, and increased awareness towards the detrimental environmental effects. Chen and Wang (2013) discussed the renewable portfolio standards in the presence of green pricing programs and greenhouse gas emissions trading. He et al. (2013) explored the use of optimal electricity prices at public charging stations for PEVs. The authors coupled the research with road pricing in order to better manage both power distribution and urban transportation networks. Moura et al. (2010) proposed models for transportation of supplies to large public infrastructure works in congested urban areas. Their idea was to minimize the impact on the environment as well as local transportation users. These studies have identified the environment as a major factor in identifying the performance of any sustainable system. The concept of sustainability in itself is a broad topic, comprising many dimensions and systems. A system of systems (SOS) approach was used by researchers to study the inter-relationships and dependencies between multiple systems (Churchman 1968; DeLaurentis 2005; Parker 2010). The interactions among these systems were evident in economic cycles over time. The concept of economic cycles, also referred to as business cycles, is a theory that attempts to explain changes in economic activity that vary from long-term growth trends. For example, efforts have been made to understand the relationship between the transportation service index (TSI) and the economy (Young et al. 2007). The results from that study suggested that the freight component of the TSI showed a strong leading relationship to the economy. Using dynamic factor models, another study analyzed the business cycle features of the transportation sector (Lahiri and Yao 2004). The results indicated that the transportation cycles peak ahead of the economic cycles. A one-to-one correspondence between cycles in the transportation sector and the aggregate economy has been identified (Lahiri and Yao 2006). In addition, the effects of vehicle miles traveled (VMT) on resource consumption also were studied (Genier 2008). Several indicators involving the transportation system (TS), activity system (AS), and environmental system (ES) have been developed by a variety of researchers (Zheng et al. 2011; Bell and Morse 1999; Bossel 2001; Paz et al. 2013). The indicators provided a necessary tool to understand such systems. The System Dynamics (SD) approach has been useful in understanding the interactions by considering multiple variables and parameters. In order to understand and model the dynamics of transportation system performance, researchers have used the SD approach based on cause-and-effect analysis and feedback loop structures (Wang et al. 2008). In addition, the SD approach was used to analyze the relationship between transportation and land use (Haghani et al. 2003; Pfaffenbichler et al. 2008). However, there is a difference between SD and dynamical systems. SD is primarily focused on the dynamics of system behavior, while dynamical systems study the dynamics of its parts (Ogata 1998). For example, in the context of our problem, the three elements are TS, AS, and ES. Since the behavior of a system is different from the behavior of its elements, the SD and dynamical systems each have a different purpose (Higgins 2002). Also, SD is based on causal loop diagrams (a logic-based description) as opposed to dynamical systems, based on differential equations. Recently, efforts have made to establish the performance indices based on performance measures (Paz et al. 2013). The research tried to understand the interactions by using fuzzy logic techniques to combine multiple performance indices. The results showed that the transportation system performance index (TSPI) and the activity system performance index (ASPI) followed an increasing trend over time, while the environmental system performance index (ESPI) followed a decreasing trend. This had been verified by the growth pattern, with changes in economy and environment. The study was robust, and explained the static nature of the problem. In contrast, the interactions among these systems were dynamic in nature and varied with time. Based on the cited literature and knowledge of the authors, numerous studies have been conducted regarding the principles and applications of dynamical systems in multiple disciplines, including mechanics, thermodynamics, population ecology, epidemics, economic, and population genetics (Luenberger 1979). In dynamical systems, the present output depends on the past input; the output changes with time if it is not in a state of equilibrium (Ljung and Glad 1994). Such dynamical systems currently are being used in evolutionary games (Sandholm 2005, 2011), ecological predator–prey networks (Nagurney and Nagurney 2011, 2012), optimization based sample identification methods (Raschke et al. 2013), and energy policy modeling frameworks (Woolley et al. 2009). The theory of dynamical systems also is being utilized in neuroscience to model the brain, and is being applied to robotics (Girard et al. 2008). Simple deterministic models capture the essence of the epidemic process, and provide a solid starting point for analysis (Kermack and McKendrick 1927). These models improve the general understanding of the behavior of systems, and help make better design and policy decisions at an aggregate level. Hence, it is vital to use a suitable modeling approach that captures the dynamic interactions within the SOS. A method of system of ordinary differential equations is chosen to model the aggregated variables of sustainability and their interdependencies over time. There are many other methods available for modeling of dynamical systems. For instance, we could choose finite state machines, petri nets, cellular automata, partial differential equations etc. or we could also chose stochastic versions of these such as stochastic differential equations, Markov chains, etc. Generally, the researchers choose the appropriate methodology to suit their goals and tasks and also the availability of tools in that methodology. A cellular automaton is also one of such techniques which have been used successfully for modeling many dynamical systems. Generally speaking, cellular automata is used where the system is divided spatially into cells and then the cell properties change based on the dynamics involving interactions between the neighboring cells. It is definitely possible to model sustainability on a geographic area by dividing the space into cells and then apply the cellular automata methodology. As compared to cellular automata modeling and its corresponding simulation, we have chosen our modeling paradigm because it allows mathematical tractability and analysis from a quantitative point of view. However, it would be a great contribution to literature if we develop cellular automata based model for sustainability and also study its mathematical and analytical properties. We hope to pursue this in the future, where it might also be possible to integrate the two techniques. The proposed modeling paradigm allows us to identify equilibrium points, perform stability analysis, and analyze vector field diagrams at a macro perspective. However, the integration of cellular automata with the proposed modeling is possible by selecting a specific geographic region. Therefore, a macro region can be divided into multiple cells (sub-regions) and the properties of the cells change based on the interactions between them. Sustainability of this macro region is partially dependent on its realization at the micro level. Moreover, the sustainability of individual constituents at micro level is useful to achieve robustness in the system by identifying and eliminating the problems at micro level. Hence, it is equally important to perform the analysis from a micro perspective and advance using a bottom up approach. Therefore, in this study, the dynamic interactions were developed, because they have not been well-defined and analyzed in the existing literature. The primary reason behind the SOS approach is to gain insight into the behavior and modeling of such systems. With this as the motivation, the overall objective of the proposed research is to build dynamic models of performance indices that help to understand the behavior of interdependent systems. The paper is organized as follows. Section 2 discusses the data used in this study, and Section 3 describes the methodology. The results and analysis are summarized in Section 4. Section 5 discusses the concept of interconnected networks required for decomposition of large scale dynamical systems. Section 6 provides conclusions and recommendations.",25
16.0,1.0,Networks and Spatial Economics,27 April 2014,https://link.springer.com/article/10.1007/s11067-014-9225-y,Reduced Carbon and Energy Footprint in Highway Operations: The Highway Energy Assessment (HERA) Methodology,March 2016,Natalia Sobrino,Andres Monzon,Sara Hernandez,Female,Male,Female,Mix,,
16.0,1.0,Networks and Spatial Economics,04 October 2013,https://link.springer.com/article/10.1007/s11067-013-9210-x,Optimal 4-D Aircraft Trajectories in a Contrail-sensitive Environment,March 2016,Bo Zou,Gurkaran Singh Buxi,Mark Hansen,Male,Unknown,Male,Male,"Air transportation contributes a small but growing share of global anthropogenic climate change impact. Today, emissions of CO2 from aircraft operations account for approximately 2 % of the global total, and are expected to grow 3–4 % annually (ICAO 2012). The projected growth, in contrast to emission reductions in other industrial and agricultural sectors, will lead to a larger share of emissions from the aviation sector in the future. Royal Commission on Environmental Protection (RCEP) (2002) estimates that by 2050 the aviation sector will be responsible for 6 % of total radiative forcing (RF) from human activities. While CO2 is the most widely known greenhouse gas agent derived from aviation, emissions from aircraft engines include other constituents that contribute, via the formation or destruction of atmospheric constituents, to climate change. Emissions of NOx tend to increase tropospheric ozone and reduce methane. The increase in radiative forcing associated with ozone is largely offset by the methane reduction, resulting in a relatively small net positive NOx impact compared to the CO2 impact (Williams et al. 2003). Of increasing concern is another aviation-induced climatic driver—contrails, which are line-shaped clouds composed of ice particles and formed in the wake of jet aircraft at high altitude where the ambient temperature is very low. Like natural high clouds, contrails affect the atmosphere near the surface in two opposing ways: by reducing the amount of earth-emitted radiation escaping to space, and by increasing the amount of solar radiation reflected. For contrails, the effect of outgoing terrestrial radiation is larger, resulting in a warming at the surface (Williams et al. 2002). Contrails evaporate quickly if the ambient air is dry, but can persist if the ambient air is humid enough. While a number of studies have examined the climate impact of persistent contrails with varying estimated results, the general conclusion is that the magnitude of contrail climate impact is non-negligible comparable to that of CO2 (Penner et al. 1999; Mannstein and Schumann 2005; Schumann 2005). Among a range of operational, technological, economic, and regulatory options that aim to mitigate the climate impact of aviation-induced contrails (Penner et al. 1999; Waitz et al. 2004; Williams et al. 2007), this study will focus on the operational aspect. Specifically, in this paper we will develop a 4-D (three dimensions in space plus the time dimension) aircraft trajectory design model that allows one or multiple flights to fly optimally in the presence of airspace that is favorable to persistent contrail formation. Nevertheless, it is worth noting that exploring the operational possibilities is closely intertwined with considerations on the technological, economic, and regulatory fronts. For instance, decisions on the optimal flight paths depend critically upon aircraft fuel consumption parameters, how climate impacts from CO2 and contrails are valuated, as well as air traffic management standards and norms. Several previous modeling efforts have attempted to address the issue of mitigating aviation-included contrails. Some use simulation tools to consider vertical displacement of flight paths to avoid the persistent contrail formation areas (PCFAs) in the airspace (Williams et al. 2002, 2003; Fichter et al. 2005; Sridhar et al. 2012; Chen et al. 2012). They find altitude adjustment to be an effective strategy to significantly reduce contrail production. Sridhar et al. (2011) present the optimal control approach to investigate the horizontal adjustment of flight paths to minimize contrail-inclusive flying cost. Mixed integer programming techniques are also considered to minimize total aircraft fuel cost while avoiding the formation of persistent contrails (Campbell et al. 2008). These studies yield similar conclusion that travel time through PCFAs can be eliminated entirely or reduced by a substantial amount at a relative small price of fuel consumption increase. Our work extends the above research and makes two important contributions. Firstly, we consider horizontal and vertical aircraft maneuvering in the presence of evolving PCFAs over time, and the difference in contrail impact by time of day. By formulating the problem as a binary integer program, our approach permits greater flexibility in flight path design and enables optimal 4-D flight routing in a dynamic, contrail-sensitive environment. Secondly, different from the previous studies which focus on contrail formation alone, or in conjunction with fuel burn, we take a comprehensive view by quantifying a more inclusive set of climatic and cost components in aircraft operations. We introduce a novel approach that converts flight climate impacts into dollar values, and consider the optimization of all climatic and cost impacts, including CO2 emissions, contrail formation, flight operating expenses, and passenger travel time, in an integrated manner. Results from the analyses offer more explicit insights into the critical factors and tradeoffs in the trajectory design. The remainder of the paper is organized as follows. In Section 2, we conduct a review of the existing aviation-contrail related research. Section 3 introduces a grid structure to represent the airspace and PCFA regions. Building upon this grid structure, a binary integer program is formulated in the ensuing section to identify the optimal trajectory for a single flight. Relevant cost factors used in the program are determined in Section 5, followed by computational analyses in Section 6. The model is then extended to determining trajectories for a sequence of flights. The solution procedure and numerical investigations are presented in Section 7. Section 8 concludes and provides future research directions.",18
16.0,2.0,Networks and Spatial Economics,10 March 2015,https://link.springer.com/article/10.1007/s11067-015-9284-8,A Genetic Algorithm Based on Relaxation Induced Neighborhood Search in a Local Branching Framework for Capacitated Multicommodity Network Design,June 2016,Mohsen Momeni,Mohammadreza Sarmadi,,Male,Unknown,Unknown,Male,"Network design models are extensively used to represent a wide range of planning and operation management. These formulations are often characterized as follows: given a network with arc capacities, it is necessary to send flows in order to satisfy known demands between origin–destination pairs at minimum cost. In doing so, one pays a price not only for transporting flows, but also for using arcs, in terms of fixed costs. In this situation, the fixed-charge Capacitated Multicommodity Network Design (CMND) formulation is achieved. The CMND formulation is one of the common network design problems which are planning tools in many areas such as transportation, telecommunications, manufacturing and logistics, among others (Lin and Liao 2014; Yaghini et al. 2014; Ghaderi 2014; Li et al. 2011; Chung et al. 2011). The goal of the model is to determine the optimal amounts of flows to be transported and the arcs to be used. In other words, the objective of CMND is to identify the optimal design, i.e., to select the links to be included in the final version of the network to minimize the total system cost, computed as the sum of the fixed and routing costs, while satisfying the demand for transportation. The CMND is modeled as a Mixed Integer Programming (MIP) where continuous variables represent the flows and 0–1 variables are used to model the design decisions (the arcs to be used) (Chouman et al. 2003). In this paper, a new algorithm is presented for searching the neighborhood structure. The proposed algorithm is a Genetic Algorithm (GA) based on Relaxation Induced Neighborhood Search (RINS) method that fits within local branching framework. This hybrid method is the LB framework algorithm that uses RINS algorithm to search neighborhood space. The basic idea of the proposed solution method is to use the GA algorithm to explore the search space and the hybrid LB and RINS method to move from the current solution to a neighbor solution. This solution strategy is metaheuristic in nature, though it is designed to improve the metaheuristic behavior by using the heuristic method that is exact in nature to explore the solution space. This paper provides an efficient hybrid algorithm for a difficult network optimization problem and the Design of Experiments (DOE) approach is applied to adjust parameters. The paper is organized as follows. In Section 2, literature review is conducted. In Section 3, different formulations of CMND problem are presented. In sections 4 and 5, the proposed algorithm and the parameter tuning using DOE approach are discussed. In Sections 6 and 7, the experimental results and statistical analysis of the proposed algorithm are described. Conclusions are presented in Section 8.",7
16.0,2.0,Networks and Spatial Economics,16 April 2015,https://link.springer.com/article/10.1007/s11067-015-9285-7,The Multiregional Core-periphery Model: The Role of the Spatial Topology,June 2016,Javier Barbero,José L. Zofío,,,Male,Unknown,Mix,,
16.0,2.0,Networks and Spatial Economics,02 April 2015,https://link.springer.com/article/10.1007/s11067-015-9286-6,Modeling Business Land Use Equilibrium for Small Firms’ Relocation and Consumers’ Trip in a Transportation Network,June 2016,Qian Liu,Chongchao Huang,,,Unknown,Unknown,Mix,,
16.0,2.0,Networks and Spatial Economics,09 April 2015,https://link.springer.com/article/10.1007/s11067-015-9287-5,"Substitutes, Complements and Network Effects in Instant Messaging Services",June 2016,James Christopher Westland,Jin Xing Hao,Siqing Shan,Male,Female,Unknown,Mix,,
16.0,2.0,Networks and Spatial Economics,25 April 2015,https://link.springer.com/article/10.1007/s11067-015-9288-4,Organization Mining Using Online Social Networks,June 2016,Michael Fire,Rami Puzis,,Male,Male,Unknown,Male,"In recent years, online social networks have grown in scale and variety and today offer individuals the opportunity to publicly present themselves, exchange ideas with friends or colleagues, and network more widely. For example, the FacebookFootnote 1 social network has more than 1.32 billion monthly active users, with new users signing up each month (Facebook 2014). According to recent statistics published by Facebook (Constine 2013), on average 655 million Facebook users log onto this site on a daily basis, and more than 4.75 billion pieces of content are shared each day (web links, news stories, blog posts, notes, photo albums, etc.). On the personal level, social networks create new opportunities to develop friendships, share ideas, and conduct business. Many social network users expose personal details about themselves and their social connections via their profile pages (Acquisti and Gross R 2006; Boshmaf et al. 2011), as well as location data, sensitive business information, and details about their place of employment. On the global level, the abundance of information provides oportunities for mining data about almost any entity in our lives. For example, social network data was analyzed recently by Zhan et al. (2014) to infer urban land use. In this study, we analyze publicly available social network data in order to infer the internal organizational structure of six high-tech companies of different scales. A similar analysis has been performed by Tyler et al. (2005) on the Hewlett-Packard organization. However, their analysis was based on protected organizational data, i.e., email logs. We show that it is possible to use only publicly available data, such as from Facebook and LinkedIn,Footnote 2 in order to achieve similar results for multiple organizations. In this work we employ the power of complex network analysis methods (Ducruet and Beauguitte 2014) for mining information about commercial companies. The mining methods proposed in this paper were applied to six well-known high-tech companies of various sizes, ranging from small companies with several hundred employees to large-scale companies with hundreds of thousands of employees. For each company, the mining process included three major steps. First, we acquired the organization’s informal social network topology from publicly available information, as detailed in Section 3. As part of this process, we collected information about the company’s structure as exposed by the company’s employees on Facebook. The presented method for organizational data mining can yield a wide range of organization social network topologies which were not available to the research community in the past. Next, we used different centrality measures to detect the hidden leadership roles inside each organization. In Section 4, we highlight the centrality measures with the highest accuracy in pinpointing the leaders. We additionally used machine learning algorithms to classify management roles in each organization. In the third step, we used a state-of-the-art algorithm to cluster the organization’s social network into disjoint communities, and we cross-referenced the disclosed leaders and communities with information obtained from LinkedIn (see Section 5). This enabled us to derive the roles of many communities within an organization, providing important insights about the organization’s structure and communication patterns. Such insights included, for example, the relationships between divisions, and the assimilation patterns of employees from previously acquired companies. These details also highlight the need for organizations to be aware of their social networking vulnerability and to establish policies to control this exposure as necessary. The contributions of this paper are fourfold: First, we present a method for uncovering an organization’s informal social network topology based solely on publicly available data. The crawled social network topology can later be utilized to investigate a wide range of organizational phenomena, such as to study the diffusion of information inside the organization (Chesney and Fire 2014); to uncover organizational structural problems, such as structural holes (Burt 1995) and fragile structures (Krackhardt and Hanson 1993); and to study how vulnerable organizations are to socialbot attacks (Elyashar et al. 2013; Paradise et al. 2014). Second, we use the organization’s structure to discover hidden leadership roles within the organization. Pinpointing employees with leadership roles and analyzing these employees’ inner organizational links can assist in constructing better working groups and improving the formal organizational structure. Third, we utilize the organization’s structure to identify communities inside the organization. This could identify which communities are dominant or weak, and which ones are functioning well or poorly. Lastly, we perform a qualitative analysis of these leadership roles and communities and demonstrate that it is possible to obtain significant insights into the organization and the role of each community without having any access whatsoever to the organization’s internal data. The remainder of this paper is organized as follows. In Section 2, we provide a brief overview of previous relevant studies on social networks with a special focus on organizational social networks analysis. In Section 3, we describe the methods used to obtain the organizational social network structure, and we show the different organizational datasets obtained. In Section 4, we present methods for identifying an organization’s leadership roles. Next, our methods used to discover the communities’ roles inside each organization are described in Section 5. In Section 6, we discuss our obtained results. Then, in Section 7, we offer future research directions. Lastly, in Section 8, we present our conclusions.",31
16.0,2.0,Networks and Spatial Economics,14 April 2015,https://link.springer.com/article/10.1007/s11067-015-9289-3,Integrated Co-evolution Model of Land Use and Traffic Network Design,June 2016,Tongfei Li,Jianjun Wu,Ziyou Gao,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Networks and Spatial Economics,14 April 2015,https://link.springer.com/article/10.1007/s11067-015-9290-x,The Classical Braess Paradox Problem Revisited: A Generalized Inverse Method on Non-Unique Path Flow Cases,June 2016,Ming-Chorng Hwang,Hsun-Jung Cho,,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Networks and Spatial Economics,10 April 2015,https://link.springer.com/article/10.1007/s11067-015-9291-9,Efficient Data Reporting in Intelligent Transportation Systems,June 2016,Mohammad Hossein Anisi,Abdul Hanan Abdullah,,Male,Male,Unknown,Male,"In recent years, intelligent transportation systems (ITSs) have appeared as an efficient solution for enhancing the performance and safety of transportation systems (Chen et al. 2010; Crainic et al. 2009; Qin and Khan 2012; Simroth and Zahle 2011; Zografos et al. 2009). ITSs integrate advanced electronics, sensors, information technology and communication to improve the convenience and security of transportation systems (Lin et al. 2014; Chen et al. 2013). In ITSs, various important and helpful types of information, such as safety warnings and traffic and traveler information can be retrieved using advanced monitoring technology (Chang et al. 2010; Hickman and Hanowski 2011; Miranda-Moreno and Fu 2006; Rupi et al. 2014). In such systems, using an efficient communication method allows drivers to receive relevant information at the right time. Hence, the drivers can make appropriate decisions based on the relevant situation, which may, for instance, prevent potential accidents and save drivers time (Jin 2014; Xie and Levinson 2009). However, in most ITSs, safety and traffic data are gathered through wired sensors, communication wires and power-hungry devices. The cost and time required for deployment and maintenance of such systems make widespread deployment of traffic monitoring systems a serious issue. Wireless sensor networks (WSNs) (Akyildiz and Kasimoglu 2004; Yick et al. 2008) can be considered an appropriate solution to this problem. In fact, WSNs are an economical and low-power wireless monitoring method that is capable of supporting ubiquitous ITS applications. WSNs are a collection of several small sensor nodes with computational, sensing and wireless communication capabilities. In addition to the advantages of lower costs and easy deployment, they allow automated, flexible, scalable and powerful monitoring for ITSs. WSNs are able to aggregate, fuse and complement the perceived information of sensors through data processing and provide improved dynamic results (Anisi et al. 2012). Moreover, as a distributed monitoring system comprised of several sensor nodes, WSNs are robust against the failures of nodes (Lee and Younis 2010) and can be adapted to the traffic infrastructure. However, depending on the application, different quality of service (QoS) parameters including energy of the sensor nodes, reliable data transmission and delay must be satisfied. On the other hand, less security, lower speed and more complex configuration comparing to wired networks can be considered as some disadvantages which need more improvement (Bhattacharyya et al. 2010). In this area, some research has focused on collecting real-time information in ITSs using WSNs (Coleri et al. 2004; Karpinski et al. 2006; Lee and Ghosh 2008; Tang et al. 2006; Zhao and Guibas 2004). In addition, there are several approaches offering the advantage of a mobile sink in WSNs. Nevertheless, in none of these works, users are in direct contact with the WSN, querying the network and dynamically receiving relevant results. In (Tacconi et al. 2010), the authors proposed an architecture to address this problem. In this architecture, mobile users query the network, and the query responses are sent back to one of the nodes, which acts as an intermediate node between sensor nodes and mobile users (called a vice sink). Then, when the target mobile user reaches the vice sink, it sends the response to the mobile user. However, if this packet transmission is unsuccessful, considerable delays from re-tracking and retransmitting the packet are imposed on the system. In addition, the loss of response packets during data transmission among the sensor nodes can cause the same problem. Moreover, due to the energy constraints of WSNs, such packet overheads consume more energy for the sensor nodes and may cause node failure. In this paper, we have proposed a two-tier architecture for ITSs using WSNs in which data communication between static sensor nodes and mobile objects is separated in each tier. This process eases the tasks of static sensors and helps them conserve their energy. For further energy conservation, the sensor nodes have been organized into several clusters in which data are delivered to the cluster heads using a tree structure. Moreover, to meet the required quality of service in data delivery, a link cost function has been proposed and applied for data transmission between the senor nodes. Using the proposed link cost function, the energy of the nodes is balanced, and delays and congestion are reduced. Furthermore, in the mobile object tier, a reliable data forwarding mechanism has been proposed for mobile objects that can considerably reduce packet loss.",21
16.0,2.0,Networks and Spatial Economics,21 April 2015,https://link.springer.com/article/10.1007/s11067-015-9292-8,"Dynamics in the European Air Transport Network, 2003–9: An Explanatory Framework Drawing on Stochastic Actor-Based Modeling",June 2016,Shengrun Zhang,Ben Derudder,Frank Witlox,Unknown,Male,Male,Male,"The deregulation of the air transport industry in Europe through the implementation of three subsequent deregulation ‘packages’ has greatly affected and transformed the entire sector. As a consequence, over the last decades air transport networks in Europe have undergone major changes in terms of structure, capacity, demand and scale. The structure of European airport network has evolved into a complex, multi-layered network consisting of hub-and-spoke and point-to-point networks (Malighetti et al. 2009). This mixed structure consists of a number of overlapping networks, but also and perhaps above all of a number of parallel networks (De Neufville 2004). Ryanair, for instance, which is known to operate a point-to-point network, offers service between London, Brussels and Frankfurt via London/Stansted, Brussels/Charleroi, and Frankfurt/Hahn rather than through London/Heathrow, Brussels/Zaventem and Frankfurt/Main. In addition, air transport services tend to shift towards second ranked cities due to changes in global production and the use of smaller long-haul aircraft (O’Connor and Fuellhart 2013; O’Connor 2003). The deconcentration of air transport networks may imply opportunities for secondary airports, and pose challenges for large hub airports. Changing structures have coincided with network expansion. Burghouwt and Hakfoort (2001) found that the total seat capacity of European airport network has grown by 59 % between 1990 and 1998, while the growth of intercontinental traffic and intra-European traffic has surpassed 70 %. Fan (2006), examining intra-European flights between 1996 and 2004 found that the number of cities served increased by 40 % (from 94 to 135), while the number of city-pairs surged by 91 % (from 224 to 428). Air travel in Europe continues to grow at a rapid rate, as Eurocontrol forecasts there will be 14.4 million yearly flights in Europe by 2035, which would be 50 % more than in 2012 at a growth rate of 1.8 % per year (EUROCONTROL 2013). These overall figures clearly indicate that the European airport network continues to experience both growth and change. However, to date relatively little attention has been paid to the factors driving the changes in European air transport networks. Exceptions are the studies by Burghouwt and Hakfoort (2001) and Fan (2006). In Burghouwt and Hakfoort (2001), the authors investigate how the capacity of the European airport network changed over the period 1990–1998 at the airport and the route level. Although their distinction between airport and route level change overcomes the drawbacks of previous empirical studies in that the nature of change is better revealed, the study remains descriptive, thus largely ignoring the interdependence between airport (nodal) and route (dyadic) attributes. Fan (2006) provides more evidence on the evolution of inter-city air transport connectivity in Europe, showing for instance that network growth is mostly attributable to the rising importance of low-cost carriers. His analysis, however, does not explore other factors that may be driving the evolution of air transport networks in Europe. Taken together, it is clear that our understanding of the spatial-temporal development of European air transport networks can be enhanced by addressing the mechanisms of change and growth in a more comprehensive way. The objective of this paper is to explore and interpret the factors driving changes in European airport networks. To this end, we apply a stochastic actor-based modeling (SABM) framework. To our knowledge, this is the first time that SABM is applied in a longitudinal analysis of air transport networks. Reviewing recent theoretical and empirical research suggests that SABM has indeed the potential to shed light on the processes underlying network dynamics (Andrew 2009; Buchmann and Pyka 2013; Ingold and Fischer 2014; KINNE 2013; Liu et al. 2013c). The main advantages of applying SABM to examine network dynamics are that (1) the modeling framework encompasses a wide variety of endogenous and exogenous effects on network change, and (2) allows evaluating these effects in the spirit of statistical inference (i.e. by providing parameter estimates that allow for the formal testing of hypotheses regarding potential drivers of network change). The remainder of this paper is structured as follows. In the next section (2), we discuss the conceptual parallels between the analysis of air transport structures and (social) network analysis in which SABM was developed. This is essential as one of the basic assumptions of SABM is that actors control and change their outgoing ties based on their and others’ attributes, their roles in the network and their interactions throughout the rest of the network (Snijders et al. 2010): unless it can be established that this assumption holds in the case of air transport networks, applying SABM would be a mere statistical exercise without much formative remit. The following sections outline the collection of longitudinal network data (3) and the methodological core of SABM (4). We then specify the SABM framework applied in this research by introducing the endogenous network and the exogenous actor and dyadic attributes used in the modeling (5), after which we discuss the main results of our analysis (6). In the final section (7), we present our main conclusions and outline some avenues for further research.",22
16.0,2.0,Networks and Spatial Economics,14 May 2015,https://link.springer.com/article/10.1007/s11067-015-9293-7,Routing Strategies Under Demand Uncertainty,June 2016,Hussein Tarhini,Douglas R. Bish,,Male,Male,Unknown,Male,"In this paper, we study the problem of routing and controlling traffic flows through a network, from origins to destination, under demand uncertainty. The objective is to minimize the sum of the time that each vehicle (from the realized demand) remains in the network before reaching a destination; we refer to this as the Total System Time (TST) which is known as the system optimal problem (as opposed to the other common objective of obtaining a user equilibrium). This important problem is related to dynamic traffic assignment (DTA) and regional evacuation planning problems. Specifically, we study this problem using the cell transmission model (CTM) (Daganzo1994, 1995) to approximate traffic flows. One potential drawback of CTM is that it has a nonlinear flow-density relationship. Ziliaskopoulos (2000) proposed a Linear Program (LP) where traffic flows are governed by a linear version of CTM. This framework optimizes routing and traffic controls (e.g., flow priorities at network merges) to minimize TST, and has been extensively studied in the literature under deterministic demand for both DTA problems (e.g., Lo (2001), Lin and Wang (2004), and Nie (2011)) and similar evacuation planning problems (e.g., Bish and Sherali (2013), Bish et al. (2014), Chiu et al. (2007), Liu et al. (2006), and Tuydes and Ziliaskopoulos (2006)). This framework has also been studied under demand uncertainty in Waller and Ziliaskopoulos (2006), Yao et al. (2009), and Chung et al. (2011, 2012); these papers propose the use of a deterministic demand parameter to represent the stochastic demand in an optimization setting. The optimal traffic flows produced from this strategy (optimal, that is, for the given deterministic problem) are then subject to policies to produce a solution (i.e., a plan) for any realization of the demand. Waller and Ziliaskopoulos (2006) considers a known cumulative distribution function (CDF) of the demand at each origin, and set the deterministic demand parameter based on the probability that the realized demand will be lower, e.g., the demand parameter can be set such that there is a 70 % chance that the realized demand will be less than or equal to the parameter. Using this demand parameter an LP is used to minimize TST. Using this framework, various probability levels are empirically tested to assess their performance under different demand realizations. To transform the LP solution into an actionable plan the following rules are used: 1) if the realized demand is less than the demand parameter, a randomly chosen subset of flows will be implemented, and 2) if the realized demand is more than the demand parameter, the traffic flows of the solution are implemented, and the “extra” demand is routed over randomly selected routes from from the LP solution. Yao et al. (2009), and Chung et al. (2011, 2012) study the routing problem in an evacuation setting using a robust optimization approach based on a deterministic LP where a range is provided for the uncertain demand. Here the objective is to minimize TST with an additional large penalty for evacuees that have not reached a destination by the end of the given time horizon. To transform the LP solution into an actionable plan the following rules are used: 1) if the realized demand is less than the demand parameter, a randomly chosen subset of flows will be implemented, and 2) if the realized demand is more than the demand parameter, the “extra” demand will remain at the destination (and thus incur the penalty). Because of the second rule, the large penalty for not evacuating the system, and the assumption of a known range for the demand, the deterministic demand parameter is set to the high-end of the demand range. This is considered a robust approach. When demand is uncertain, there are various ways of measuring the quality of a solution. For instance, important qualities of a solution include the robustness of a solution (the likelihood that a solution is feasible under various realizations), the expected objective function value (i.e., expected TST), or the solutions worst-case performance, given the possible demand realizations (i.e., the worst possible TST). The solution approaches studied in Waller and Ziliaskopoulos (2006), Yao et al. (2009), and Chung et al. (2011, 2012) consider various of these aspects, but these solutions all have problems directly related to the modeling framework used. One of the main goals of this paper is to illustrate some of the problems. To do so, we develop an optimal policy for problems that have a special network structure (the network used in Waller and Ziliaskopoulos (2006) and Chung et al. (2012), which we also use for illustrative purposes, has this structure). This policy is easily implemented and directly leads to an optimal solution for any realization of the demand (and is thus robust). This allows us to evaluate the solution approaches studied in Waller and Ziliaskopoulos (2006), Yao et al. (2009), and Chung et al. (2011, 2012). Furthermore, for more general networks we provide a heuristic that also outperforms the solution deterministic counterpart approaches from the literature. The remainder of the paper is structured as follows. Section 2 presents the model that we are studying, we then review the approaches used by Waller and Ziliaskopoulos (2006), Yao et al. (2009), and Chung et al. (2011, 2012) to solve this model. In Section 3, we present a class of networks that are studied in the literature in which the optimal solution can be characterized irrespective of the demand level and we will use it to illustrate some of the disadvantages of the methods introduced in Section 2 to solve the model with demand uncertainty. In Section 4, we describe a heuristic that can be used in any network structure, and we compare it to the deterministic counterpart strategies. And finally, Section 5 is the conclusion.",5
16.0,2.0,Networks and Spatial Economics,17 April 2015,https://link.springer.com/article/10.1007/s11067-015-9294-6,Finding Outbreak Trees in Networks with Limited Information,June 2016,David Rey,Lauren Gardner,S. Travis Waller,Male,,Unknown,Mix,,
16.0,3.0,Networks and Spatial Economics,05 June 2015,https://link.springer.com/article/10.1007/s11067-015-9295-5,Technical Efficiency of European Metro Systems: The Effects of Operational Management and Socioeconomic Environment,September 2016,António Lobo,António Couto,,Male,Male,Unknown,Male,"Many public transport systems are managed by the state. State-owned firms do not usually see financial profit as their main objective; rather, they prioritize the social and environmental benefits that a rapid, reliable and eco-friendly transport system can represent in a community. Nevertheless, such systems should not disregard the improvement of their operational performance in order to become less of a burden on public finances (Dodgson 1985; Nash 2000). This has been a matter of concern across the decades for the governments, transport authorities and researchers, which have established policies and directives and developed evaluation and action tools to improve the operational performance of transport systems. Under the scope of the Horizon 2020, the ongoing EU Framework Programme for Research and Innovation, the European Commission (EC) stresses the need for new strategic planning approaches at the local level to achieve sustainable urban mobility, since few transport authorities currently perform a reliable analysis of trends and develop scenarios to support long term policies. Therefore, the EC is promoting actions to enhance the capabilities of local authorities and other stakeholders to plan and implement sustainable mobility measures on the basis of reliable data and analysis, regarding the take-up of the innovative concept of Sustainable Urban Mobility Plans (SUMPs) at the European scale (European Commission 2014). Our study aims to contribute to this goal, providing a tool for the analysis of the operational performance of urban rail transit systems, regarding the evaluation of production efficiency and its main drivers. We apply a stochastic frontier modeling approach to evaluate the technical efficiency of 17 European metro systems and the effects of internal and external production factors on the production. The analysis is based on historical data covering the period from 1990 to 2011, and composed of capital and labor inputs, socioeconomic indicators for the urban areas served by the systems, an output characterizing the service supply (car-kilometers), and an output reflecting the demand (number of passengers). We use a stochastic frontier regression model to process the outputs and the internal production factors, estimating the elasticities of each input. Two separate regressions are performed to establish an optimal production function for each output, from which the technical efficiency of each firm is estimated. Because we are dealing with two outputs separately, we adopt different terms for the technical efficiency. Thus, the technical efficiency associated with the supply-oriented output is simply referred to as efficiency, and is mainly dependent on the strategies applied to operational management. Similarly, the technical efficiency related to the demand characterizing output is termed effectiveness. Effectiveness reflects a transport system’s capability to attract users, relying not only on the characteristics of the transport service but also on the surrounding socioeconomic environment. For this reason, a similar stochastic approach is used in a second stage to perform a regression between the set of both internal and socioeconomic indicators and the number of transported passengers. Ultimately, we are able to compare the efficiency and effectiveness levels of each firm considering the internal production factors, and also to compare the effectiveness scores with and without consideration of the external indicators. The developed models aim to improve the knowledge on the production of European metro systems in terms of its main drivers, efficiency scores achieved by the systems, and also the extent to which the systems are operating in favorable or adverse socioeconomic contexts. Therefore, the outcomes of this study may support the development of policies and actions by the practitioners to promote sustainability in urban transit.",7
16.0,3.0,Networks and Spatial Economics,23 June 2015,https://link.springer.com/article/10.1007/s11067-015-9297-3,An Algorithm for the One Commodity Pickup and Delivery Traveling Salesman Problem with Restricted Depot,September 2016,Lanshan Han,Binh T. Luong,Satish Ukkusuri,Unknown,,,Mix,,
16.0,3.0,Networks and Spatial Economics,22 July 2015,https://link.springer.com/article/10.1007/s11067-015-9298-2,Transportation and Regional Economic Development: Analysis of Spatial Spillovers in China Provincial Regions,September 2016,Xiushan Jiang,Lei Zhang,Ruijun Wang,Unknown,,Unknown,Mix,,
16.0,3.0,Networks and Spatial Economics,02 August 2015,https://link.springer.com/article/10.1007/s11067-015-9299-1,A Train Dispatching Model Under a Stochastic Environment: Stable Train Routing Constraints and Reformulation,September 2016,Lingyun Meng,Xiaojie Luan,Xuesong Zhou,Unknown,Unknown,Unknown,Unknown,,
16.0,3.0,Networks and Spatial Economics,04 August 2015,https://link.springer.com/article/10.1007/s11067-015-9300-z,Investigating the Dynamic Spillover Effects of Low-Cost Airlines on Airport Airfare Through Spatio-Temporal Regression Models,September 2016,Dapeng Zhang,Xiaokun Wang,,Unknown,Unknown,Unknown,Unknown,,
16.0,3.0,Networks and Spatial Economics,09 September 2015,https://link.springer.com/article/10.1007/s11067-015-9301-y,Competitive Location and Pricing on Networks with Random Utilities,September 2016,Dominik Kress,Erwin Pesch,,Male,Male,Unknown,Male,"Ever since the seminal work of Hotelling (1929), competitive location models have been intensively studied in the economic and operations research literature. This is reflected in the large amount of review articles and special issues that have appeared over the past decades, such as Drezner (1995), Eiselt et al. (1993), Eiselt and Laporte (1996), Kress and Pesch (2012b), Plastria (2001), Serra and ReVelle (1995), Friesz (2007) and Santos-Peñate et al. (2007). Essentially, one seeks to locate (physical or nonphysical) facilities in some given space with respect to some objective function, incorporating the fact that location decisions have been or will be made by independent decision-makers (players) who will subsequently compete with each other. A well known competitive location problem, formally introduced by Hakimi (1983), is the (r|X

p
)-medianoid problem. Here, given a network with customers located in the vertices and a predefined set of p leaders’s (or incumbent’s) facilities X

p
, a follower (or entrant) wants to enter the market with a given number of r facilities so that the market share is maximized. When restricting the set of potential facility sites to the vertex set of the network, this problem is sometimes referred to as the maximum capture problem (MAXCAP, ReVelle 1986) or discrete (r|X

p
)-medianoid problem. Obviously, when players compete for market share, the researcher needs to apply some kind of customer choice model. Typically, as in Hakimi (1983), customer choice is assumed to be binary, i.e. it is assumed to be deterministic from the perspective of the players with the total demand of each customer being served by a single facility. For example, one may suppose that customers patronize the closest facility only.Footnote 1Fernández et al. (2007) and Benati and Hansen (2002), among others, deviate from this assumption. In the latter paper, the authors introduce the maximum capture problem with random utilities. Here, probabilistic customer behavior is modeled by random utility functions that are composed of deterministic and stochastic components. They select the multinomial logit approach, which is well established in the economics, marketing and operations research literature (see, for example, Anderson et al. 1992; Hensher et al. 2005; Train 2003), to model the decision process of utility maximizing customers. In their definition of the deterministic component, the authors focus on incorporating effects of distances from customers to facility locations. An overview of other location models utilizing probabilistic choice models can be found in the review papers mentioned above. Braid (1988) and Chisholm and Norman (2004), for instance, consider the choice of locations of two or more (single-product or multiple-product) firms on small chain networks under the multinomial logit model. As Hotelling (1929) considers not only location, but also price decisions, another stream of research focuses on the incorporation of price competition into competitive location models. The majority of these models is concerned with one-dimensional location spaces (see Kress and Pesch 2012b, for a recent overview). For example, de Palma et al. (1985) consider equilibrium locations of two or more firms along a line segment with uniformly spread customers with and without price competition under the multinomial logit model. Another related example is Lederer (2003). Fik and Mulligan (1991), Fik (1991) and Braid (1993) are examples of (economic) models of spatial competition that consider network structures with discrete and continuous (customers are dispersed over the edges of the network) demand distributions. Serra and ReVelle (1999) consider the maximum capture problem on networks under a binary choice rule, where players are allowed to compete in prices after having chosen locations. As proposed by Benati and Hansen (2002), this paper contributes to the literature by applying the idea of competition in prices to the maximum capture problem with random utilities in order to “improve the realism of the model”. Hence, our research is closely related to de Palma et al. (1985). Related models can also be found in the field of product positioning, cf. Choi et al. (1990) and Rhim and Cooper (2005). We additionally contribute to the literature by providing complexity results for the resulting location problem. In order to compute equilibrium prices under multinomial logit demand, we adapt a fixed-point iteration approach that has previously been introduced in the literature by Morrow and Skerlos (2011) (cf. also Morrow 2008). In this context, we present examples of problem instances with fixed location sets of the players, that demonstrate the potential non-existence of price equilibria and the case of multiple local equilibria in prices. Finally, we show that different price sensitivity levels of customers may actually affect optimal locations of facilities, and we provide first insights into the performance of heuristic algorithms for the location problem. This paper proceeds as follows. First, we introduce the basic notation and definitions in Section 2. A detailed problem formulation is given in Section 3 with results concerning the existence of price equilibria and the computational complexity in Sections 3.1 and 3.2, respectively. In Section 4 we are concerned with the aforementioned fixed-point iteration approach (Section 4.1), example instances (Section 4.2) and some computational tests (Section 4.3). Heuristic approaches for solving the location problem are subject of Section 5. The paper closes with a conclusion in Section 6.",19
16.0,3.0,Networks and Spatial Economics,02 September 2015,https://link.springer.com/article/10.1007/s11067-015-9302-x,Exploring Vulnerability and Interdependency of UK Infrastructure Using Key-Linkages Analysis,September 2016,Scott Kelly,Peter Tyler,Douglas Crawford-Brown,Male,Male,Male,Male,"Physical infrastructure systems are integral to the proper functioning of all modern economies. However, the link between infrastructure availability, economic growth and productivity is still the subject of much uncertainty and debate within the literature (Straub 2008). Although it is clear that infrastructure investment is a crucial factor in economic development it is less clear what forms of infrastructure are most important for different forms of economic activity to occur. There are also concerns, including from the UK Treasury itself, that the United Kingdom is under-investing in critical infrastructure (Bottini et al. 2012). Infrastructure such as transport systems, water, sanitation services, energy networks and telecommunications represent a large portfolio of public expenditure ranging from one-third to one-half of total public investment for most developed countries (Kessides 1993). Yet, prior to the 1990’s infrastructure as an analytic concept was absent from most economic thinking, entering only as a curious but inadequate component of the notion of capital (Prud’Homme 2004). While most formal research studying the relationship between infrastructure and the economy since the 1990’s has tended to take a macroeconomic perspective, findings are mixed with some consensus that infrastructure capital has a significant positive effect on economic output and growth (Cain 1997; Démurger 2001; Chakraborty and Nandi 2011; Pradhan and Bagchi 2013). Kessides (1993) suggests the difficulty in designating direct causal links for the economic impact of infrastructure arises because it is hard to attribute any firm conclusions from studies that take such highly aggregated measures attempting to capture all possible externalities and spillover effects that occur as a result of investment in infrastructure. Unlike market goods where total economic contribution is calculated by taking the quantity of units sold multiplied by unit price, the economic contribution of infrastructure is much more difficult to discern. Complicating matters further is the issue of funding and ownership structures that are becoming increasingly complex. In Fig. 1 the source of funding (public, public/private, private) across different infrastructure types is provided for different infrastructure investments between 2013 and 2014. It is interesting to note the significant contribution being met from the private sector in the provision of several critical infrastructure sectors. Future source of funding for different infrastructure categories in the UK . Source: UK National Infrastructure Pipeline (Treasury 2013) Infrastructure systems are also particularly vulnerable to the effects of disasters. It has been observed that both the frequency and intensity of natural disasters has been increasing, with costs now rising year on year (The Economist 2012; New Scientist 2012). With increasing risks from extreme weather events caused by the onset of climate change and a concentration of populations now living in vulnerable coastal cities, river deltas and along earth quake fault-lines, the risks of damage to infrastructure systems is now an acute issue. In the event of a disaster, direct infrastructure failure may have cascading effects on other economic systems (Zhang et al. 2005; Lian et al. 2007). Therefore, understanding the interconnectedness of infrastructure with the rest of economy is critical for assessing the effects of disasters and developing resiliency strategies (Caschili et al. 2015b; Andergassen et al. 2015; O’Kelly 2015). Key-linkages analysis is a rigorous economic approach that allows the interdependencies between different economic sectors to be quantitatively determined and the wider systemic effects estimated. This article shows how key-linkages analysis can be used to understand the role and purpose of nine independent infrastructure sectors within the UK economy.Footnote 1 It identifies the sectors of the UK economy that are most dependent on infrastructure for the provision of goods and services and estimates the economic contribution that different infrastructure sectors provide to the UK economy when both direct, indirect, employment and income effects are considered together. This research therefore compliments existing multi-layer infrastructure network models (Zhang et al. 2005; Caschili et al. 2015a). The next section provides an introduction to key-linkages analysis followed by a description of the methods and mathematical derivations used in this paper. The remaining sections describe the data and the results of the analysis. The paper ends with a discussion on the implications of these findings.",15
16.0,3.0,Networks and Spatial Economics,28 September 2015,https://link.springer.com/article/10.1007/s11067-015-9303-9,Shadow Prices in Territory Division,September 2016,John Gunnar Carlsson,Erik Carlsson,Raghuveer Devulapalli,Male,Male,Unknown,Male,"Dividing a given territory into pieces is a fundamental geographic problem with many application areas, including logistics, economics, and natural resource allocation. A simple mathematical formulation of such a problem is as follows: suppose that R is a geographic region in the plane which we are to partition among n agents, that is, we are to select n sub-regions R
1, … , R

n
 of R such that R

i
 ∩ R

j
 = ∅ for all pairs and \(\bigcup _{i}R_{i}=R\). In this paper, we will assume that R is a connected, polygonal region with non-empty interior. Letting u

i
(⋅) denote a “utility density” function associated with agent i, we can represent the overall utility of agent i as the integral \(\iint _{R_{i}}u_{i}(x)\, dA\), where R

i
 denotes the sub-region assigned to agent i. In order to generalize this model further, let us also assume that f(⋅) is a given probability density function (representing population or distribution of a natural resource, for example) on R, so that the overall utility of agent i is the integral \(\iint _{R_{i}}f(x)u_{i}(x)\, dA\). This problem has been previously considered in many different domains for various particular forms of f(⋅) and u

i
(⋅) which we will discuss shortly. The key issue in the preceding problem is how to construct the sub-regions in a balanced, or equitable, fashion. One way to partition the region is to maximize the overall utilities of the agents while imposing constraints on the amounts of f(⋅) that are contained in them. That is, our problem can be written as
 where the q

i
 are given constants. A second way to partition the region in a balanced way is to maximize the minimum utility of all of the sub-regions:
 In this paper we show that the boundaries between the optimal sub-regions to problem (1) are curves of the form
  and that the boundaries between the optimal sub-regions to problem (2) are curves of the form
  provided that either u

i
(x) > 0 for all i and x or u

i
(x) < 0 for all i and x (except for possibly a set of measure zero). Although this turns out to be a simple and immediate consequence of complementary slackness in vector space optimization, it allows us to obtain very concise, constructive proofs to well-known existing results in equitable partitioning. Moreover, our proof technique reduces both balanced partitioning problems to n-dimensional convex optimization problems, which allows us to actually solve both problems efficiently in practice by computing a set of shadow prices associated with the agents. It turns out that these boundary curves are often equivalent to the market regions that would be described by a spatial pricing scheme (Lederer 2003), where the prices are the dual variables for problems (1) and (2). For example, the conditions on the masses q

i
 in Eq. 1 can be interpreted as seeking a price vector that “clears” the spatial market consisting of the region R; we will make this statement more concrete in Sections 3 and 6. The remaining contributions of this paper are as follows: in Section 4, we give fast algorithms for solving (1) and (2) by showing how to compute a subgradient vector for either problem, which enables us to use (for example) a cutting plane method to find the optimal partition. Section 6 gives two economic applications of these principles to compute a market-clearing price vector in an aggregate demand system or a variation of the classical Fisher exchange market. Section 9 then considers a dynamicproblem in which the density function f(⋅) varies over time (simulating population migration or transport of a resource, for example) and we derive a set of partial differential equations that describe the evolution of the optimal sub-regions over time. Problems (1) and (2) have already been studied for specific forms of the functions u

i
(⋅). The case of problem (1) where u

i
(x) = −∥x − p

i
∥2 for fixed points p

i
 ∈ R was first analyzed in Aurenhammer et al. (1998) and later in Pavone et al. (2009, 2011); the former gives a fast algorithm for optimal partitioning for the case where f(⋅) is an atomic distribution and the latter gives a control scheme that converges to an optimal partition for smooth f(⋅). The case of Eq. 1 where u

i
(x) = −∥x − p

i
∥ and f(⋅) is a uniform distribution was analyzed in Aronov et al. (2009) (whose analysis also extends cleanly to general distributions f(⋅)) who also give an approximation algorithm for simultaneously locating the points p

i
 and designing partitions. Our paper (Carlsson and Devulapalli 2012) re-derives this analysis for problems (1) and (2) using complementary slackness techniques, and further shows how to find an exact solution to their problem (as opposed to an approximation). Problems (1) and (2) belong to the class of so-called districting or zoning problems (Curtin et al. 2010; de Grange et al. 2010; Haugland et al. 2007; Salazar-Aguilar et al. 2011) in which the goal is to divide a given geographic territory into pieces in some optimal or efficient way. One of the major difficulties in problems of this type is the reconciliation of workload allocation objectives, such as minimizing workloads or maximizing service quality within the regions, with geometric shape constraints, such as requirements that regions be compact, convex, or connected. In our problems, we do not enforce shape constraints explicitly, but rather, for many choices of objective functions u

i
(⋅), such properties turn out to be attributes of the optimal solution. In this paper we adopt the standard notation of vector calculus; in particular, we let ∂(⋅) denote the boundary operatorand we let ∇⋅ denote the divergence operator. We use a double integral \(\iint _{R}f(x)\, dA\) to denote integration over a planar region whereas we use a triple integral \(\iiint _{R}f(x)\, dV\) to denote integration over a domain of arbitrary dimension.",13
16.0,3.0,Networks and Spatial Economics,30 August 2015,https://link.springer.com/article/10.1007/s11067-015-9304-8,Supply Chain Network Designs Developed for Deteriorating Items Under Conditions of Trade Credit and Partial Backordering,September 2016,Yu-Chung Tsao,Vu Thuy Linh,,Unknown,,Unknown,Mix,,
16.0,3.0,Networks and Spatial Economics,26 August 2015,https://link.springer.com/article/10.1007/s11067-015-9305-7,A Methodology for Assessing the Regional Economy and Transportation Impact of Introducing Longer and Heavier Vehicles: Application to the Road Network of Spain,September 2016,Andres Felipe Guzman,Jose Manuel Vassallo,Alejandro Ortega Hortelano,Male,Male,Male,Male,"In continental Europe, freight by road is by far the most important mode. For instance, in Spain the road share accounts 95 % of the tonne-km of the domestic market (EUROSTAT 2012). In order to mitigate the negative effects caused by road transportation, there are different measures to make freight transportation by road more sustainable and cleaner. The first group of measures intends to make freight more efficient by implementing cleaner technologies for engines, use of new fuels, and increasing the maximum weights and dimensions of trucks. A second group of measures aims at encouraging the adoption of other, cleaner modes of freight traffic, such as short sea shipping, inland waterways, and rail, through setting charges to Heavy Goods Vehicles (HGVs), and improving the connectivity from the road network to these other modes (European Commission EC - Directorate-General for Mobility and Transport 2012). Finally, the aim of the third group of measures encompasses the separation of trucks by considering truck-only lanes and truck tollways. De Palma et al. (2008) listed the benefits gained, such as alleviation of congestion, reduction of accidents, predictability of travel times, less frequent need for braking, accelerating, and overtaking although these facilities have not been built yet. With regard to the relaxing weights and dimensions of HGVs by considering Longer and Heavier Vehicles (LHVs), it is noteworthy the valuable experience around the world in countries such as the United States (US), Canada, Mexico, Brazil, and Australia. They have permitted LHVs by considering strict rules for its operation. For example, LHVs in the US are allowed in states where they were in operation before June 1, 1991 for specific cargo types (Clayton et al. 2007). In the European Union (EU) some countries have introduced LHVs, other countries have implemented pilot projects or theoretical studies to evaluate their performance, and other countries have shown no interest. EU regulations concerning truck size and weight ensure interoperability across nations but they are free to set their own limits for domestic traffic. The main benefits due to LHVs introduction are the reduction of transport costs per tonne-km, decrease in the fuel consumption, saving of emissions, and the reduction of freight vehicles needed to move the same tonnes. On the other hand there are negative effects, such as a greater severity of accidents, and transferred demand from cleaner modes such as rail or barge. The purpose of this paper is to develop a methodological approach to evaluate the impact of allowing LHVs on regional macroeconomic aggregates and on freight traffic flow. In order to evaluate the potential of our methodology and analyze the results, we apply it to the case study of Spain. We compare the results of the LHV scenario to a base-case scenario without them. The paper is organized into five sections. After this introduction, we conduct a literature review. In section 3, we define the methodological approach to assess the economic and transportation impacts resulting from the introduction of LHVs. In section 4, we apply the methodology to the case study of Spain. In section 5, we analyze the results in terms of macroeconomic impacts and traffic flow changes in the network. Finally, we show the conclusions, and propose future research developments.",9
16.0,4.0,Networks and Spatial Economics,26 August 2015,https://link.springer.com/article/10.1007/s11067-015-9306-6,Impact of Logistics on Technical Efficiency of World Production (2007–2012),December 2016,Pablo Coto-Millán,Xose Luís Fernández,Manuel Agüeros,Male,Unknown,Male,Male,"There is little doubt nowadays that information and communication technologies (hereinafter, ICT) and logistics are the basis of significant global production efficiencies. However, this does not mean that we know the exact impact of ICT and logistics on global economic output and its efficiency in the short and long term. There is even some controversy regarding the extent to which new technologies and logistics affect production and efficiency. Ultimately, there is what Stiglitz (2014) calls a puzzle about the nature of the comparative advantages provided by innovation, new information and communication technologies and logistics. There exist very few studies analysing the contribution of ICT and, to the best of our knowledge, there is just one, Coto-Millán et al. (2013), approximating the contribution of logistics to global economic growth. Even more striking is the lack of studies and research on the impact of logistics given the significant expansion that this sector has undergone in recent years. An important question in macroeconomics is how to accurately describe the aggregate production function. Output growth is typically explained as the accumulation of factor inputs and the growth of total factor productivity. Apart from the basic factors of production, growth accounting regressions look for additional determinants that can explain growth. This leads to a regression treating all determinants of output growth as inputs which is conceptually incorrect since many included determinants may only indirectly affect output (Miller and Upadhyay 2000). However, using Stochastic Frontier Analysis (hereinafter, SFA) we could detect the additional determinants of output growth (beyond the factor inputs) affecting the efficiency of real inputs, physical capital, human capital and labour, and thus directly affecting factor productivity. Given these notable shortcomings in the field of logistics, this paper aims to contribute to economic literature in two different aspects: First, by estimating global technical efficiency through SFA techniques, taking as starting point the growth model proposed by Mankiw et al. (1992). Second, by evaluating the impact of Logistics and ICT as explanatory variables of technical inefficiency of the different countries. We will analyse the impact, at least in part, of logistics and ICT on global production efficiency. This question has been considered of great interest, given that progress in logistics is one of the most significant social and economic issues in recent decades, to the point that it has created what some call a new revolution in production, storage, distribution and transport. With this aim, and given the data limitations, a sample of data from the years 2007–2010–2012 corresponding to 34 countries has been studied.Footnote 1 Also, this sample has been further divided into several subsamples in order to analyse whether, as seems logical, there are significant differences in the effect of logistics on efficiency depending on the group of countries considered. The rest of the paper is organised as follows: the second section reviews the existing literature on the impact of ICT and Logistics in the literature closest to this analysis, which is that pertaining to economic growth. The third section proposes the theoretical model which will be subsequently estimated. In the fourth section, the statistical information used in this work is reviewed. Next, the proposed model is estimated and its main results are shown in the fifth section. Finally, the sixth section presents the main conclusions of this research.",15
16.0,4.0,Networks and Spatial Economics,03 December 2015,https://link.springer.com/article/10.1007/s11067-015-9308-4,Population-driven Urban Road Evolution Dynamic Model,December 2016,Fangxia Zhao,Jianjun Wu,Ronghui Liu,Unknown,Unknown,Unknown,Unknown,,
16.0,4.0,Networks and Spatial Economics,27 October 2015,https://link.springer.com/article/10.1007/s11067-015-9309-3,Evaluating Various Road Ownership Structures and Potential Competition on an Urban Road Network,December 2016,Omid M. Rouhani,H. Oliver Gao,,,Unknown,Unknown,Mix,,
16.0,4.0,Networks and Spatial Economics,03 December 2015,https://link.springer.com/article/10.1007/s11067-015-9310-x,Competition and Cooperation in a Bidding Model of Electrical Energy Trade,December 2016,Dávid Csercsik,,,Male,Unknown,Unknown,Male,"Because of its extreme importance, power system economics (Kirschen and Strbac 2004) has been an intensively researched interdisciplinary area. The trends of electricity market liberalization, together with occasionally rapidly extending consumption in the long term and consumption peaks in the short term, put increasing load on system operators and authorities responsible for network operation and expansion. If one wishes to analyze the electrical-energy market as interactions of market participants, one has to take into account that the possible interactions are constrained by laws of physics as well as by market regulations. Studies approaching the topic from the engineering discipline dominantly consider optimal power flow (Wood and Wollenberg 2012; Conejo and Aguado 1998) and direct current optimal power flow (DCOPF) problems. In these approaches the aim is to minimize the total cost of system operation under various constraints. These papers, among others, study how the topology of the network affects transmission efficiency and thus how the usage of flexible ac transmission systems (FACTS) (Hingorani 1993; Hingorani et al. 2000; Song and Johns 1999) may be optimized. Unit commitment (Sheble and Fahd 1994) addresses the problem of economically optimizing generator schedules over a short-term horizon subject to demand and other constraints. The unit-commitment problem has been approached by various optimization methods (see e.g. Cheng et al. (2000); Zhuang and Galiana (1990)), and reformulated to include transmission constraints (Tseng et al. 1999). Regarding safety issues of power system operation, optimal transmission-switching models (Fisher et al. 2008; Hedman et al. 2008; RP et al. 2010), which formulate the optimization as a mixed-integer problem, usually assume n − 1 contingency reliability (Hedman et al. 2009), which means that the effects of single line and generator failures on the network are included in the analysis. The latest models of Hedman et al. (2010) even include generator startup and shutdown costs as well. While the above papers provide valuable insight in the problems of optimal network design and operation, they do not assume profit-oriented generators and neglect several economical incentives of the market participants. When studying the economic aspects of electric-power transmission networks, most of the research has focussed on the topics of competition, market power and regulation (Gilbert et al. 2004; Neuhoff et al. 2005; Chen et al. 2006). A large-scale spatial optimization model of the European market considering nodal or zonal pricing is presented in Leuthold et al. (2012). The proposed model is used for the prediction of optimal congestion management, expansion planning and generator investments under network constraints. A generalized Nash equilibrium model of market coupling in the European market is presented in Oggioni et al. (2012). The Cournot assumption has been used in several papers (Cardell et al. 1997; Yao et al. 2004; Sauma and Oren 2007) to analyze the market power in electrical energy trade models. The paper (Metzler et al. 2003) focuses on the effects of arbitrage in a Nash-Cournot equilibrium model. A further example of Nash-Cournot equilibrium models applied to power markets may be found in the paper (Gabriel et al. 2013) The paper of Ruiz et al. (2012) assumes profit maximizing providers, network constraints and uses an equilibrium framework to predict prices and profits in a pool-based electricity market. Pool-based markets are detailed furthermore in De la Torre et al. (2003, 2004) These and similar models (Bakirtzis et al. 2007) usually assume elastic demand and piecewise constant price-demand curves and market clearing, and lead to MPECs (mathematical program with equilibrium constraints) or EPECs (equilibrium problems with equilibrium constraints) (Ehrenmann 2004). The articles (Hobbs and Kelly 1992; Bai et al. 1997) already use game theory for transmission analysis. Hobbs and Kelly (1992) use static cooperative models to calculate the possible outcomes of short-run transmission games and noncooperative Stackelberg games to model long-run games in which the amount of transmission capacity is a decision variable. The paper by Bai et al. (1997) describes an open-access transmission method for maximizing profits in a power system, where transmission losses are considered. The proposed method is based on the Nash bargaining game for power-flow analysis in which each transaction and its optimal price is determined to optimize the interests of individual parties. The paper of Orths et al. (2001) describes a game-theoretic approach of a multi-criteria optimization problem related to transmission planning and operation. A strategic-gaming approach is described in Kleindorfer et al. (2001). The paper of Harrington et al. (2005) describes a collusive framework motivated by power-generation auctions in which players coordinate in order for each to gain higher payoffs than those determined by the Nash equilibrium solution. This model has been extended by Liu and Hobbs (2013) to take transmission constraints into account as well. Gately (1974) was probably the first to apply cooperative game theory to planning investments of electrical-power systems. In this paper the concepts of the core and the Shapley value are used to determine the mutually acceptable set of final payments. The paper (Evans et al. 2003) describes a cost assignment model for electrical transmission system expansion is using kernel theory. The study by Sauma and Oren (2007) addresses network expansion as well and shows that in the presence of imperfect markets the elimination of network congestion may not necessarily improve social welfare. The methods of cooperative game theory have been applied for the analysis of the transmission expansion problem both in the case of centralized and decentralized environment (Contreras 1997; Contreras and Wu 1999, 2000; Contreras et al. 2009). Corresponding to the pricing problem of electricity markets, Bolle (1992) compares constant and spot prices in oligopolistic models, using supply functions. Fuller (2005) analyzes the relationship between the hourly spot price at a node, and the prices at all nodes adjacent to it. Considering market regulations, Singh et al. (1998) compare a nodal pricing framework with cost-allocation procedures in the case of competitive electricity markets, and analyze some game-theoretic aspects of the proposed model. The paper of Ding and Fuller (2005) considers nodal, zonal and uniform marginal prices and emphasizes that the nodal marginal price correctly accounts for transmission constraints and losses in some cases. As pointed out by Liu and Hobbs (2013) transmission constraints can be exploited by strategic firms to enhance their market power, and collusive generators can strategically exploit transmission congestion and reap additional profits compared to the situation without congestion. Our aim in this paper is to introduce a transferable-utility game-theoretic model (Arnold and Schwalbe 2002), which is able to describe the various levels of generator firms’ cooperation, while taking into account profit-motivated generators, various market regulations and engineering type modelling assumptions of the transmission system as well (n − 1 line contingency reliability). It is natural to assume that under certain market regulations, coalition formation of generators may be illegal; however coalitions may be interpreted also as generators belonging to the same energy-providing company. The proposed market model is liberalized in the sense that consumers, who are characterized by zero elasticity demand, may choose their preference over power providers who compete for them based on their price; but each consumer is be assigned to only one provider at a time. We assume that the formation of coalitions who could raise their prices as high as they wish is prohibited. To analyze how the prices and profits evolve in time, we study an iterative process in which the generators publish their price offers simultaneously in each step, based on which the consumers preferences are determined. Each coalition tries to optimize its expected profit in each step, based on the price offers of the previous step. Our aim in this paper is not to study the equilibrium properties of the proposed framework, but to get an impression of how the dynamics of prices and profits are affected by coalition formation in various cases. A further aim is to analyze the effect of market regulations (allowance or prohibition of zonal pricing) and asymmetric information on the resulting profits of the generators and on the total social cost (the total amount of money the consumers pay to the generators in order to supply their needs). The approach of cooperative game theory has the capability to describe various levels of cooperation between oligopolistic and, in this case, two coalition-competitive scenarios. In transferable-utility cooperative games the concept of superadditivity is used for the description of whether a coalitional merger brings benefits to the merging coalitions, and whether this benefit depends on the behavior of other players (not involved in the merger) or not. A such analysis may provide a valuable tool for the identification of incentives for cooperation - in other words it may predict which generators may be motivated for cartel formation.",10
16.0,4.0,Networks and Spatial Economics,04 November 2015,https://link.springer.com/article/10.1007/s11067-015-9311-9,Spatial Analysis of Single Allocation Hub Location Problems,December 2016,Meltem Peker,Bahar Y. Kara,Sibel A. Alumur,Female,Female,Female,Female,"Hubs are special facilities that act as switching, transshipment and sorting nodes in many large transportation and telecommunication networks. Rather than having direct links for each origin–destination (o-d) pair, hub networks use fewer links to connect the origins and destinations, and thereby concentrate flows to allow economies of scale. Hub networks are designed to serve demand for movement (e.g., transportation of freight or passengers) from specified origins to specified destinations. Transportation hub location problems are concerned with locating the hub facilities, generally with the aim of minimizing the total costs for movement of all flows from origins to destinations. Solutions also require allocating demand nodes to hubs in order to route traffic between the o-d pairs. The first goal of our research is to better understand the characteristics of good transportation hub locations in order to predict likely optimal hub locations based only on the fundamental problem data (demand for travel and spatial locations). Thus, rather than analyzing existing hub networks and traffic flows to identify or classify hubs, we seek to identify locations (e.g., cities) likely to be optimal hubs prior to designing the network. Our second goal is then to exploit this knowledge of promising hub locations in a heuristic solution methodology to better solve hub location problems. Much research in the past 25 years has focused on solving fundamental hub location problems, where the hub network is complete, traffic for each o-d pair is routed through at least one hub, and the cost between two hubs is discounted due to the consolidation of flows (see O’Kelly and Miller 1994; Campbell et al. 2002; Alumur and Kara 2008, and Campbell and O’Kelly 2012). We focus on the original hub location problem introduced by O’Kelly (1986a, 1986b, 1987): the uncapacitated single allocation p-hub median problem (USApHMP). In the USApHMP, p hub nodes must be located, each non-hub node must be assigned to one of the p hubs, and the objective is to minimize the total transportation cost to serve given o-d flows, where the cost rate for inter-hub flows is discounted by the economies of scale factor α (0 ≤ α ≤ 1). With single allocation, the incoming and outgoing flow of each node is routed through a single hub. (In contrast, with multiple allocation problems, demand nodes can be assigned to more than one hub.) The remainder of the paper is organized as follows: Section 2 describes the motivation and goals for the research. Section 3 describes the analysis to identify key characteristics of optimal hub locations and Section 4 explains the conversion of these into methodologies to identify subsets of nodes likely to contain hubs. Section 5 presents computational results of the methodologies over a wide range of data sets. Section 6 is a discussion and conclusion.",15
16.0,4.0,Networks and Spatial Economics,26 November 2015,https://link.springer.com/article/10.1007/s11067-015-9312-8,A Model of Green Acceptance and Intentions to Use Bike-Sharing: YouBike Users in Taiwan,December 2016,Shang-Yu Chen,Chung-Cheng Lu,,Unknown,Unknown,Unknown,Unknown,,
16.0,4.0,Networks and Spatial Economics,04 November 2015,https://link.springer.com/article/10.1007/s11067-015-9313-7,Incorporating Ridesharing in the Static Traffic Assignment Model,December 2016,Oren Bahat,Shlomo Bekhor,,Male,Male,Unknown,Male,"Car commuting trips are a major source of congestion in transportation networks. The economic impact of congestion was estimated to be $ 78 billion in 2005 (Schrank and Lomax 2007), due to delays and wasted fuel. Recent data in the U.S. shows that drive alone continues to be the most popular mode of travel in many metropolitan areas, while carpooling share does not exceed 10 % of commuters (Bureau of Transportation Statistics 2010). As a consequence, car occupancies at peak hours tend to be very low. The motivation for using empty car seats to increase occupancy rate is therefore understood. By exploiting the amount of empty seats that are already travelling daily, one could substantially improve the utilization of the urban transportation network, and potentially reduce congestion, emission and parking problems. Modes comprising sharing of rides or vehicles have been proposed and used with limited success over the years. Ridesharing is inherently a non-profit mode that brings together people with similar travel needs to share expenses. Carpooling, for example, was a relatively popular mode in the 1980’s in the US, but has significantly dropped in the more recent years (Pisarski 2006). Empirical studies on carpools show that it is still considered relatively inconvenient mode of transportation, in comparison with both public transit and private car. Ferguson (1997) analyzes the factors associated with carpooling decline in the US. The most important factors found were increased availability of vehicles, reduced gasoline prices, and higher educational level of commuters. Traditional carpooling, however, is too limiting to accommodate the unconventional schedules of today’s ridesharing demand, where flexible commuting times are an important need of many travelers (Levofsky and Greenberg 2001). The main drawback that limits the popularity is the relative difficulty of forming a carpool, and the lack of flexibility. Dynamic Ridesharing (DRS) tries to solve some of these drawbacks by leveraging technological advances like mobile devices, social networks, etc. (Agatz et al. 2010). Dynamic Ridesharing is an automated service system that finds matches between users that share similar travel needs, in terms of time, origin and destination. A central management of the system replaces the user’s need to form a carpool, and in principle enables drivers and riders to find a match on a short notice, or even en-route. Chan and Shaheen (2012) performed a classification of ridesharing modes. In their classification, dynamic ridesharing refers to carpools formed using internet based computerized ride matching (whether or not using smartphones and GPS). This mechanism can prevent information barriers for users who seek a ride share. In the last decade, many start-up companies initiated DRS services (e.g., - Carticipate, SimRide or Avego). Ghoseiri et al. (2011) performed a thorough review of ridesharing experiments and online services. Clearly, the quality of service given by a dynamic ride-share provider depends on the characteristics of the environment in terms of participant geographic density, traffic patterns, and the available roadway and transit infrastructure. Some of these factors were analyzed by Hall and Qureshi (1997). To create a ridesharing service, the main task of the service is to perform matching between drivers and passengers in an effective way. The methods and implementation of matching algorithms are investigated for example in Kleiner et al. (2011), and is outside the scope of this paper. For simplicity, we assume a greedy strategy of first-come-first-served, which may be easily implemented by a central service management system, although it may be sub-optimal. An estimation of the ridesharing market share is given in Tsao and Lin (1999), using a synthetic entropy model. Using statistical data from Los Angeles area, they found that ridesharing has very little potential to reduce the number of commuter trips. On the other hand, a more recent study (Agatz et al. 2011) simulates travel data based on Atlanta travel survey, and finds that when modern optimization methods are used, ridesharing performance is very good. It is also shown that urban sprawl is not limiting the creation of a sustainable population of ridesharing users. The incorporation of ridesharing mode with other commute modes was investigated by Habib et al. (2011) for example. They showed how to model carpool in a mode choice model, not only as a distinct choice but also in combination with other modes. Another example is Huang et al. (2000), which explicitly investigate the mode choice between car drivers and passengers. Their suggestion for linear utility functions is used in this paper when considering ridesharing mode choice. The utility for ridesharing passengers is uniquely affected by the need to wait for a ride. The modelling of waiting time was already treated in transit mode assignments. Wu et al. (1994) developed a traffic assignment solution to the problem of transit route-choice. Waiting time is an arc on the hyper-graph, and depends on the transit line frequency. Yang et al. (2000) used MM1 queue to model waiting time for taxi, which is more suitable for ridesharing, where there is no scheduled frequency, but rather random appearance of vehicles. The taxi model by Yang et al. (2000) presents the cyclic relationship between level of service parameters like waiting time and demand, which is also applicable in this paper. This paper will use traffic assignment methods to analyze the behavior of a ridesharing service in a given environment. Mathematical programming has become a standard tool to analyze traffic assignment problems. Extensions of these methods to incorporate elastic (variable) demand were developed by Florian (1977) and others. The variable demand traffic assignment program has a unique solution, as long as the demand function is monotonic. This can be demonstrated for simple cases of modal split, where the properties of the different modes are fixed. But showing these conditions for the general case, where mode utility functions can depend on the network flow quantities, is not easily accomplished. In Dafermos (1980), variational inequality is used to analyze these cases, in which there may be interactions between links in the network, and between modes on the same link. These methods were successfully implemented in many traffic assignment examples, for example in Nguyen and Pallottino (1988). In Yang and Bell (1997), the variable demand traffic equilibrium program was extended to include links with limited capacity. This was used to show how toll levels could restrain demand in a congested network, but the same behavior is expected for ridesharing performance which is strictly limited by the availability of ridesharing drivers. A more suitable description of this capacity limitation for passengers is given in Cepeda et al. (2006), where congested transit networks are considered, and a limited capacity is described by a zero frequency. They also investigate the existence of equilibrium in cases in which waiting time functions present infinite asymptotes. When considering traffic assignment of ridesharing modes, supporting asymmetric utility function is a critical property. As an example, the number of ridesharing passengers in a given path will form a queue, therefore affecting their own utility via their waiting time, and also the utility function for drivers in this path. In order to ensure a solution to corresponding traffic assignment problem, the mode utility functions should form a contraction mapping. The exact conditions for existence and uniqueness of solution in the general asymmetric case were investigated by e.g., Magnanti and Perakis (2004) and may be used in the case of ridesharing modes as well. Recent studies emphasize the potential benefit of ridesharing in various areas (Amey 2010; Deakin et al. 2010). In reality, though, the bootstrapping problem of creating a critical mass of users that will enable an efficient service still exists. In that sense, the factors that affect the mode choice behavior of users are of high importance. Therefore, this paper will investigate the behavior of ridesharing service by means of user equilibrium optimization, combining both the mode choice behavior of commuters, and the resulting traffic assignment patterns. The purpose of this paper is to develop a mathematical model which will enable the investigation of the necessary conditions that lead to a successful adoption of ridesharing. A relatively simple discrete choice model for ridesharing vs. other modes will be proposed and incorporated into the user equilibrium framework using traffic assignment methods. As will be explained in detail in subsequent sections, the main contribution of the paper is the development of a combined mode choice and assignment model that accounts for flow-dependent waiting times. The rest of this paper is organized as follows. The next section describes the model assumptions and develops a mathematical formulation of the problem. The subsequent section illustrates model application for simple networks, and also presents results for a real size network in various conditions. The last section discusses the results and outlines further research directions that will enhance model’s capabilities.",27
16.0,4.0,Networks and Spatial Economics,11 November 2015,https://link.springer.com/article/10.1007/s11067-015-9314-6,Endogenous Effects of Hubbing on Flow Intensities,December 2016,Mehmet R. Taner,Bahar Y. Kara,,Male,Female,Unknown,Mix,,
16.0,4.0,Networks and Spatial Economics,14 December 2015,https://link.springer.com/article/10.1007/s11067-015-9315-5,Reference Policies for Non-myopic Sequential Network Design and Timing Problems,December 2016,Joseph Y. J. Chow,Hamid R. Sayarshad,,Male,Male,Unknown,Male,"While dynamic/real-time/online/sequential network models have existed for many decades, the availability of real time “Big” data in recent years has driven an increasing interest in those models in “smart cities” context. A range of different models and algorithms have been developed, in particular, using such data to improve network design and timing decisions under uncertainty with look-ahead policies and rolling horizons, including dynamic vehicle routing problems (e.g., Spivey and Powell 2004; Mitrović-Minić et al. 2004; Thomas and White 2004; Ichoua et al. 2006), dynamic pricing and routing (Figliozzi et al. 2007; Sayarshad and Chow 2015), adaptive network design (Chow and Regan 2011a), among others. These advances come at a time when “smart cities” solutions are needed more than ever before—both urbanization and climate change threaten to increase uncertainty and its effects on society. Contrary to static network design problems (which, for the purpose of this study, encompass the broad class of network models surveyed by Magnanti and Wong 1984: road equilibrium network design problems, facility location problems, vehicle routing problems, capacitated multicommodity flow problems, etc.), sequential network design models under uncertainty feature four notable differences (among others). The problem is conceptually illustrated in Fig. 1. Illustration of sequential network design and timing problem (for a single link) First, decisions can be made over two or more stages, or even continuously over time (e.g., optimal control problems). Earlier decisions are made without yet knowing the decisions to be made in future stages. This distinction itself sets sequential network design apart from many other stochastic network design models in the literature because those typically assume only a two-stage (e.g., stochastic plus recourse, see Zhang et al. 2013) problem (see examples included in a recent survey by Chen et al. 2011, or in Szeto et al. 2013). Second, uncertainty is characterized by adapted stochastic processes that define the state of a system, and such information is revealed over time. Time-independent random variables are special cases of these processes. As a consequence, sequential network design models can make use of historical information, and some can be designed to “look ahead” and anticipate future states that may occur. The “look ahead” variety is referred to as a non-myopic model. Two-stage stochastic models are near-myopic, and do not adequately exploit the value of historical information to anticipate future uncertainty. Third, the presence of states means that a model output is not a deterministic set of actions or decision variables, but is instead a rule or function that selects decision variables based on a state. We call this function a “policy”. For example, one sequential design policy may be “to invest in link A if stochastic volume on that link exceeds 1500 vph”, while another may be “to invest if link volume exceeds 1400 vph”. The optimal policy is to find the optimal rule under the dynamic setting—in the example, the solution is to find the set of states in which it is best to invest in link A (versus other links), and the other states in which it would be better to defer or reject (if at final stage of a finite decision horizon) the investment. Fourth, because uncertainty and decision-making are dynamic, the full set of actions available to a decision-maker is not simply a question of {yes, no} (e.g., build this link or not) but is instead a question of {now, later/never}. The timing component assumes that relevant information for the decision has yet to be revealed, and it may be better to defer the decision until a later time (until a finite horizon, if such exists, at which point further deferral implies rejection). For detailed discussion and unified treatment of sequential network design policies, see Powell (2011) and Powell et al. (2012), and for such policies with more explicit consideration of the both design and timing decisions in a network, see Chow and Regan (2011a) and Chow et al. (2011). This research is applicable to a diverse set of data-driven network design problems in a wide variety of sectors including transportation, energy, telecommunications, spatial economics, and media. Despite a growing number of studies in stochastic dynamic network optimization, the field remains less well defined and unified than other areas of stochastic network optimization (Powell et al. 2012). Unlike clearly defined mathematical programming methods, sequential decision-making features additional dimensions of complexity as discussed above that result in an intractable exact problem (Powell 2011; Chow and Regan 2011a). As a result, algorithms for policies are all based on approximations of some form. Due to the need for approximation methods like approximate dynamic programming (ADP), one of the most significant problems yet to be solved is the lack of adequate benchmarks. Proposed policies are typically compared to another policy that serves as a benchmark, which we call a reference policy. Three reference policies are most often used: a static or a priori policy that does not change decisions during the stages; a myopic policy that is dynamic but does not use the information to anticipate future states; and a “perfect information” or a posteriori policy where the stochastic dynamic problem is solved in hindsight as a problem. The last policy is used in both analytical and computational evaluations; Berbeglia et al. (2010) described the use of this bound as part of an analytical competitive ratios (Karp 1992), but it is restricted to finding worst case bounds for highly idealized cases. Reliance of these benchmarks is evident from Table 1, which illustrates the types of reference policies used in studies of non-myopic dial-a-ride problems (DARP). Each new study requires some form of approximation, and furthermore, the benchmarks used to evaluate their proposed policies and algorithms are rarely with another algorithm or state-of-the-art policy. The sole reliance on these benchmarks persists in other types of network design problems as well; Chow et al. (2011) use static and myopic policies to compare against their non-myopic adaptive network design and timing policy. The three common reference policies are inadequate on their own. Let us illustrate this point. Consider the distribution made by a random walk that starts at x
0 and either moves left or right by Δx with 50 % probability. After t steps, a distribution for x(t) can be determined. However, it is also possible to achieve the exact same distribution with a different x
0 and different left/right probability. A static policy only sees the single distribution at a point in time, so it would not be able to distinguish whether the distribution came from the former or the latter information process. At the other extreme, the perfect information policy sees a single realization, which for three time steps may look like Left-Left-Right. Either of the information processes could have led to this exact same outcome. As such, the values of the static policy or perfect information policy are not dependent on information propagation (see Definition 1), so two networks with different stochastic processes but the same realized outcome would look identical to these benchmarks. A sequential network design reference policy is information sensitive if the value of the policy, Φ, is dependent on adapted stochastic process(es) in which information at time t include realizations of the process(es) from all time s, where s ≤ t. The myopic policies capture the randomness, i.e., they are sensitive to the historical and current state of the network, but they do not look ahead. Chow and Regan (2011b) showed that the value of flexibility in timing a network design when looking ahead can be separated into two parts: the value from timing, and the value from network effects. Since myopic policies by definition do not look ahead, the future value due to network effects cannot be distinguished. As a result, when comparing two myopic policies on two different networks it is not possible to distinguish the portions of the value that are due to network effects or due to timing and information propagation. For example, consider comparing two different instances with different myopic policy values P and Q. Suppose P > Q (where a higher value is desired). Is it because the network with P is more conducive to the policy? Or is it because of the stochasticity of the parameters? It is important to distinguish this for network design purposes because we do not wish to falsely conclude that a certain design is good and transferable when it is due primarily to a particular information state for that instance. Because we cannot break out the value due to network effects, only comparisons on the same instance would make sense for myopic policies. In Definition 2, this condition for network effect measurability in this context is clarified. A sequential network design reference policy is network effect measurable if the value of the benchmark, Φ, exhibits value from flexibility in which network effects can be distinguished. Benchmarks that are not sensitive to information propagation or network structure lead to findings that may be too localized to a specific instance, even if the comparison were made using the same test case. For example, two policies A and B that are compared on a network using the same simulated sample paths may show that policy A operates better than a static reference policy by 20 % while policy B compared to the same static reference policy may improve by only 10 %. However, if the uncertainty parameters were changed, the policy A may only be better by 0.5 % while policy B is better by 0.3 %. One might conclude that A is better than B, but by how much? If policy A and policy B were instead compared to a third reference policy C that was also sensitive to network structure and information, then even as parameters altered, there would be more consistency to the comparison for generalizing the findings. A well-designed reference policy is one that can be used to benchmark existing algorithms on network instances such that conclusions can be generalized to variations of those instances or to other networks. For example, we can show what the effect of changing the demand rate or available fleet size has on algorithm performance with respect to a reference policy. We propose a reference policy that is both information-sensitive and network effect-measurable in this study. The reference policy is derived from an algorithm by Chow and Regan (2011a) that approximates a policy that is consistently defined relative to network effect value, but requires explicit enumeration of sequences. The following contributions are made: A method is proposed to obtain an extreme value distribution of the policy value from Chow and Regan (2011a) without explicit sequence enumeration; The fitness of the extreme value distributions are evaluated to find that the Weibull distribution is a much better fit than the Gumbel distribution for the Sioux Falls example from Chow and Regan (2011a) and the dynamic DARP example from Hyytiä et al. (2012); Sample sizes are evaluated for the Sioux Falls example, and the Weibull-based estimator of the policy value was found to exhibit consistency; A non-myopic adaptive facility location problem is proposed and the reference policy is demonstrated as an upper bound for a policy drawn from sampled sequences; An experiment is conducted in a dynamic DARP environment to replicate the algorithm proposed by Hyytiä et al. (2012) and to evaluate the sensitivity of the proposed reference policy across different parameter instances of a base network. Their policy is shown to be less significant (even negligible) to a myopic algorithm and more sensitive to network parameters when compared to the reference policy.",17
16.0,4.0,Networks and Spatial Economics,15 December 2016,https://link.springer.com/article/10.1007/s11067-016-9337-7,Electric Power Network Oligopoly as a Dynamic Stackelberg Game,December 2016,Pedro A. Neto,Terry L. Friesz,Ke Han,Male,,,Mix,,
17.0,1.0,Networks and Spatial Economics,19 December 2015,https://link.springer.com/article/10.1007/s11067-015-9316-4,Branch-and-Bound-Based Local Search Heuristics for Train Timetabling on Single-Track Railway Network,March 2017,Ampol Karoonsoontawong,Apisak Taptana,,Unknown,Unknown,Unknown,Unknown,,
17.0,1.0,Networks and Spatial Economics,28 December 2015,https://link.springer.com/article/10.1007/s11067-015-9317-3,A Multi-Trip Split-Delivery Vehicle Routing Problem with Time Windows for Inventory Replenishment Under Stochastic Travel Times,March 2017,James C. Chu,Shangyao Yan,Han-Jheng Huang,Male,Unknown,Unknown,Male,"Vehicle routing problems (VRP) have been studied extensively in the literature; see Solomon (1987); Fisher (1995); Golden et al. (2008); and Laporte (2009) for complete reviews. Over the past decades, various important extensions for VRP have been developed and three of them are considered in this study: time windows, split delivery, and multiple trip. The first major extension to VRP in the literature is to require that the customers must be served within specific time windows, which is called VRP with time windows (VRPTW). If delivery within time windows is strictly required, the problems are called VRP with “hard time windows” (Zhong and Cole 2005; Escuín et al. 2012; Norouzi et al. 2012; Low et al. 2013; Mirmohammadsadeghi and Ahmed 2015). On the other hand, if delivery is allowed after the given time windows with a penalty, the problems can be categorized as VRP with “soft time windows” (Taillard et al. 1997). The second major extension to VRP is to allow the demand of a customer to be satisfied with more than one vehicle for more flexibility, which is called split-delivery VRP (SDVRP). The examples of SDVRP include Frizzell and Giffin (1995); Ho and Haugland (2004); Gendreau et al. (2006); Desaulniers (2010), and Salani and Vacca (2011). The third major extension is the multi-trip VRP (MTVRP), where a vehicle can be dispatched more than once in the planning horizon for more efficient utilization of vehicles. Alonso et al. (2008); Mingozzi et al. (2013); Olivera and Viera (2007); Petch and Salhi (2003); Salhi and Petch (2007), and Taillard et al. (1996); Azi et al. (2007, 2010); Brandão and Mercer (1997, 1998), and Tang et al. (2015) are the representative studies of MTVRP. Among the numerous studies in the literature of VRP, Yan et al. (2015a) proposed a multi-trip split delivery vehicle routing problem with soft time windows (MTSDVRPTW), which is the only study that we know of combining te three major extensions in the same model. In this study, we further extend Yan et al. (2015a) to consider a MTSDVRPTW under stochastic travel times. The most noteworthy assumption in this problem is that the travel time between two locations is stochastic rather than an average value. Since traffic conditions are constantly disturbed by various factors in reality, this assumption is realistic and more likely to generate accurate vehicle routing schedules. However, we note that uncertainty in model parameters has rarely been addressed in the problem under discussion mainly because the degree of complexity of the problem is considerable for the commonly used methodologies such as stochastic programming and robust programming. Stochastic programming usually assumes that model parameters follow probability distributions and finds a solution that is feasible in all the scenarios. The objective of such a problem is to optimize the expected value of the performance measure for all scenarios, e.g., Stancu-Minasian (1985); Kenyon and Morton (2003); Ando and Taniguchi (2006); Li et al. (2010a); Tas et al. (2013), and Zhang et al. (2013). One of the drawbacks of stochastic programming is that the optimal solution might perform extremely poorly in some of the scenarios, although the expected value of the scenarios is optimized. Therefore, robust programming adopts the objective that optimizes the expected value of the performance measure for the scenarios plus some indicator of robustness (e.g., variance of the performance measure for the scenarios), e.g., Mulvery and Ruszczynski (1995); Mulvery et al. (1995); List et al. (2003); Han et al. (2014), and Tarhini and Bish (2015). These approaches are highly sophisticated but often too complicated for real-world large-scale problems (Mulvery et al. 1995). First of all, the parameters of the probability distributions that random variables follow must be estimated accurately before they can be incorporated in the problem. Furthermore, all scenarios might be required to be fully identified in order to guarantee the feasibility of the solutions. Both requirements are impractical in a daily planning model for multi-trip capacitated vehicle routing. Secondly, the number of scenarios and the complexity for estimating probability distributions increase significantly as the scale of the problems increases. As a result, the solution time using these approaches is often too large to be applicable for large-scale problems. To create more realistic vehicle routing plans for daily inventory replenishment in supply chains, a practical approach is developed for planning multi-trip capacitated vehicle routing and scheduling with soft time windows under the consideration of uncertain travel times. There are two major features to the planning model. First, instead of estimating complicated joint probability distributions for travel times in a vehicle route, the possible outcomes of vehicle arrivals and product delivery at retailers under stochastic travel times are systematically categorized and their associated penalty and reward are estimated. It follows that the unanticipated costs associated with every scheduling decision can be introduced into the optimization. As a result, more robust solutions for the vehicle routing plan can be obtained when uncertainty exists. This approach has been adopted for airline and intercity bus operations under stochastic demands or travel times in Yan et al. (2006); Yan and Tang (2008), and Yan et al. (2008). The approach has been proven effective in the above fields but has not yet been applied to the vehicle routing problems for inventory replenishment. We also notice that although the simulation may be one of the most natural tools when uncertainty is involved, the technique has rarely been adopted to test those vehicle routing plans that emphasize stochastic travel times. In this study, a simulation-based evaluation method is developed to rigorously validate the proposed approach and the generated plans. Another commonly made assumption in the literature is that the only way to satisfy demand of a retailer is to have the retailer visited once by exactly one vehicle. The second feature of the proposed approach is to relax the assumption by using the time-space network technique, which is capable of generating highly flexible routing plans. Specifically, there is no restriction imposed on the number of vehicle visits and the quantity of each visit for inventory replenishment at retailers. Therefore, more effective vehicle routing plans can be explored in the planning procedure. Another benefit of time-space networks is that the temporal dimension of the routing plan can be described in high detail, which greatly helps the vehicle scheduling with time windows. The remainder of the paper is organized as follows. Section 2 formulates the stochastic model for vehicle routing and proposes a simulation method for evaluating the model. Section 3 describes the two-stage heuristic solution algorithm. Section 4 discusses a case study. Finally, Section 5 concludes the research and provides directions for future research.",16
17.0,1.0,Networks and Spatial Economics,21 December 2015,https://link.springer.com/article/10.1007/s11067-015-9318-2,Structure Indicators for Transportation Graph Analysis I: Planar Connected Simple Graphs,March 2017,Lorenzo Mussone,Roberto Notari,,Male,Male,Unknown,Male,"Every transportation network (transit, road, maritime, aerial) can be represented with a graph, where nodes become vertices and links become (undirected or directed, accordingly) edges. Obviously, what can be subjective in the conversion is the amount of details taken into consideration. Only in transit networks and especially in underground ones, this problem is much less crucial because no at level crossings are allowed and possible paths are perfectly defined. In general, a transportation network can have one-way links or can require the two ways of a link to be separated. In those cases, the network must be translated into a directed graph in order to reflect that property. In many other cases (like for underground networks) this is not generally needed and an undirected graph is adequate. Other network properties concern the performance or property of a link or a node (e.g., length, travel time and speed curves, fares, and so on) and this can be inserted into a graph through weights. Since 1950s, many attempts to analyse networks through graphs are made, without really discerning between the graph and the network; many papers dealing with network properties (e.g., Kansky 1963) are often cited together with papers based on graph properties. The studies on graph properties aim at defining some numerical indicators to pick and synthesize the considered properties. This approach has a twofold justification for what concerns transportation studies: to a short term, to understand a possible different behaviour of networks, for example when they are subjected to dynamic changes in demand, to a medium-long term, how, why and where a network should be modified to better satisfy transport needs. However, it must be stressed that network loading depends much on demand and its composition; usually, interaction between supply (what is represented by a graph) and demand is faced by assignment approaches and therefore we do not take into consideration that feature when we analyse networks through graphs. A comprehensive analysis of the quite large literature on the study of transportation networks through graphs would require a dedicated review paper; therefore, we recall only a few papers among the many relevant ones, to outline briefly past and present researches with a special regard to the aims of the paper. Seminal papers are those by Garrison and Marble (1962, 1964) who really pioneered the field by introducing three parameters, computed from the graph, directly linked to network design: circuits α, degree of connectivity γ, and complexity β, that we recall and comment in Section 2.2. Actually, the number of circuits is the cyclomatic number proposed by Berge (1962) to count the number of independent circuits; it also shows the maximum number of edges that can be removed without disconnecting the (planar) graph. Variants of these indices can be found; e.g., Prihar (1956), working in telecommunication networks, proposes the use of the degree of connectivity calculated as 1/α. Garrison and Marble (1962) prove also that the indices α and γ are quite redundant and, for a constant number of vertices, the correlations between α, β and γ are equal to one. Black (2003) states that, since the strong redundancy between these parameters, only one of them is sufficient to contain graph information. He prefers to use γ since it is the easiest to interpret and it ranges in [0, 1]. The paper by Gattuso and Miriello (2005) faces the problem of a topological and geographical evaluation of metro networks through the definition of a methodological approach based on a set of indicators, at graph and geographical levels. At graph level they use, other than the ones by Garrison and Marble (1962), the local degree, i.e., the number of concurrent links on the generic node; the number of destinations directly connected by i nodes; the number of nodes; generally it does not correspond to the number of network stations (two or three different line stations, placed at the same point, are represented by a unique network node); the number of links (a link covered by several lines is counted only once); the number of cycles. A list of parameters at geographical level is also proposed. Actually, this study is very notable for the development of two indicators at geographical level. Derrible and Kennedy (2010, 2011, 2012) aim at characterizing the network features of metro systems by adapting various concepts of graph theory to describe their characteristics clustered into three classes: State, Form and Structure. In their study, 33 metro mode networks (meaning urban rapid rail transit whether it is underground, at grade or elevated), real working over six major world areas (North and Latin America, Eastern and Western Europe, Africa and Asia) and with different sizes, are taken into consideration. The authors transform the networks into graphs by coupling a node to each station only if the station is a transfer station (where transfer station means a station connected to at least three other stations, both transfer and termini). Obviously, this choice can affect the analysis related to land and service accessibility but not the functional properties of connection between nodes. Variables taken into account are the number of vertices, the number of edges, and the network diameter, i.e., the total number of edges linking the two furthest vertices, using the shortest path. The state indicators are two: the first one is the complexity β and the second one is the degree of connectivity γ both by Garrison and Marble (1962). Other indicators are calculated by using physical characteristics of the network (such as the lengths of edges). The long-standing interest in measuring the spatial structure of road networks has produced many other papers with different aims and tools. Newman (2003), Gastner and Newman (2006) and Nadakuditi and Newman (2012) find some clear patterns that imply a connection between network structure and geography by analysing empirical data and overcoming past limitations on computer resources. Blumenfeld-Lieberthal (2009) analyse the topology of transportation networks within different systems of cities using their characteristics as indicator of economic activity between them. Xie and Levinson (2007, 2008) propose to calculate three parameters, heterogeneity, connection pattern and continuity, to measure structural characteristics of road networks. Heterogeneity is derived from the probability, p

k
, that a vertex is connected to other k vertices and it is proportional to 1/k
a, where k is the degree of a vertex and a is a coefficient to be calibrated. The distribution of p

k
 is called power law degree and networks with this property are called scale-free. On this topic a particular interest assumes the paper by Barabasi and Albert (1999) who study the ways networks can grow but without the constraint of planarity. Connection pattern uses Garrison and Marble’s (1962) definition but it mainly focuses on comparing the total length of roads belonging to a ring or circuit to the total length of roads. However, it appears of great interest the effort of defining a methodology capable of singling out circuit blocks, subsets of a graph containing no bridge or articulation point and remaining connected after deleting a vertex or an edge. Continuity is based on a definition of edge hierarchy which again implies a certain quantification of a network property. A thorough discussion about structure and dynamics in transportation networks is proposed by Ducruet and Lugo (2013); static dimension of those networks is analysed with regard to topology, geometry, morphology, and spatial structure from a global and local point of view, according to the required application (e.g., looking for interconnectivity or vulnerability, respectively). Kurant and Thiran (2006) propose to extract topology of mass transportation networks from their timetables. Many other papers focus on dynamic features (mainly the growth) of transportation networks [see, for example, Xie and Levinson (2009a, b), Levinson (2009)], so many and so specialized that this topic deserves a dissertation apart. Given a graph, with its size we mean the number of vertices and of edges, while with complexity we mean not only the size, but the topology of the graph, too, that is to say, the ways edges connect vertices. With this premise in mind, our research focuses on the following two questions: given a transportation graph, are there indicators that measure its complexity? once the indicators are found, how can we compare the complexity of transportation graphs, different both for size and complexity? It is well known that graphs have shown very useful in many application areas, other than transportation, but it is remarkable that the study of graph theory is still an active research area in pure mathematics, despite the fact that graph were first considered by L. Euler in 18th century. In the last 15 years, the development of both scientific calculus software and computer algebra systems has allowed researchers to study graphs from the point of view of linear and commutative algebra. Even if the properties under consideration in pure mathematics may not be of immediate interest for transport scholars, the selected indicators, e.g., eigenvalues, codimension, Betti numbers, among the others, could be used for measuring the complexity of graphs. E.g., Sáenz-de-Cabezón and Wynn (2009) and Maruri-Aguilar et al. (2012) use Betti numbers to study the reliability of networks and systems. So, the above first question can be rewritten as: can the indicators coming from algebra measure the complexity of graphs? By applying the proposed indices to three different datasets, we give good evidences that the question has a positive answer. So, we tackle the second problem by introducing suitable normalization factors, main novelty for what at our knowledge. Certainly, future studies on different datasets could give new insights on the interpretation of the proposed indicators. In this paper, we consider only planar, connected and undirected graphs. Extensions to non-planar, weighted and/or directed graphs will be the topic of future research. Our hope is that the linear algebraic and algebraic indicators (defined and discussed in Section 2) could support planners to integrate traditional planning factors such as demand, demography, geography, demand assignment, and costs. They could also be used to compare existing systems especially for what concerns their complexity. The paper has four other sections and three appendices: Section 2 presents the methodology and the parameters proposed for the analysis; Section 3 describes the datasets used to answering our two research questions; Section 4 shows and discusses the results giving evidence that the proposed indicators really represent the complexity of transportation graphs. Finally, in Section 5 we summarize the conclusions. Appendix 1 presents a short review on polynomial ideals on which the proposed methodology is based, Appendix 2 contains a mathematical description of the algebraic parameters, and finally, Appendix 3 is a sample of computer session in which we compute the algebraic parameters for a given graph. We stress that the session can be easily adapted to other graphs, and that Singular, the used software, is a computer algebra system for polynomial computations and is free and open-source under the GNU General Public License.",4
17.0,1.0,Networks and Spatial Economics,24 February 2016,https://link.springer.com/article/10.1007/s11067-016-9319-9,A Heuristic for the Doubly Constrained Entropy Distribution/Assignment Problem,March 2017,Huey-Kuo Chen,,,Unknown,Unknown,Unknown,Unknown,,
17.0,1.0,Networks and Spatial Economics,03 March 2016,https://link.springer.com/article/10.1007/s11067-016-9320-3,Capacitated Refueling Station Location Problem with Traffic Deviations Over Multiple Time Periods,March 2017,Mohammad Miralinaghi,Burcu B. Keskin,Arash M. Roshandeh,Male,Female,Male,Mix,,
17.0,1.0,Networks and Spatial Economics,17 March 2016,https://link.springer.com/article/10.1007/s11067-016-9321-2,User-specific and Dynamic Internalization of Road Traffic Noise Exposures,March 2017,Ihab Kaddoura,Lars Kröger,Kai Nagel,Male,Male,Male,Male,"Many studies prove that environmental noise causes cardiovascular diseases, tinnitus, cognitive impairment and sleep disturbances (see, e.g., Ising et al. 1996; Stassen et al. 2008; WHO Europe 2009, 2011; Babisch et al. 2013). This negative impact on public health is addressed by a vast number of noise control measures. Encouraging the use of quieter vehicles (e.g. improved aerodynamics, tires or motor engines), the building of noise barriers, and improved road surfaces, aim to reduce noise exposures. They do, however, not affect the origin of the sound, namely the travel behavior. Traffic control measures allow for a reduction in noise exposures by changing the travel behavior, e.g. the transport route, the mode of transportation, the departure time. Possible means to rearrange traffic flows towards a higher system efficiency are, for example, reduced speed levels, turn restrictions or pricing schemes. The economic principle of optimal price setting by means of internalizing external effects has been widely studied in the literature (Vickrey 1969; Arnott et al. 1994; de Palma and Lindsey 2004; Friesz et al. 2004). In order to prioritize various noise control measures and to quantify external noise cost which may be internalized, the number of individuals that are exposed to certain noise levels is of major importance. Traffic management strategies should ideally consider both, the reduction in noise exposures and the avoidance costs such as increased travel times from driving detours (see,e.g. Lin et al. 2014). Most noise mapping and action planning approaches focus on residential noise exposures based on estimates for the number of residents per building (see, e.g. SenStadt 2012; DEFRA 2015; Gulliver et al. 2015). This is a reasonable approach for nightly exposures (see, e.g., BVU et al. 2003, pp. 187–189). However, static resident numbers are difficult to be used for the computation of noise exposures during daytime as residents leave the house to perform other activities located in other areas. A review of several noise regulations reveals that estimates for the number of exposed individuals should not be limited to residents at their home location. Table 1 depicts limit values of the A-weighted and time-averaged traffic sound level for different land-use types such as hospitals or commercial areas based on the German 16 BImSchV (1990). In context of noise protection at the workplace, noise limit values include noise sources from inside the workplace and therefore refer to the indoor sound level. As an international standard adopted at European and German level, the DIN EN ISO 11690-1 (1997) recommends noise limit values depending on the indoor work type, depicted in Table 2. To translate indoor noise levels to the outside facade, the insulation effect of buildings needs to be considered. The insulation effect of a building depends on several factors such as the wall material and thickness, the window number and sizes, and the glazing technology. Furthermore, indoor noise levels depend on whether windows are opened or closed, which is found to be particularly relevant for bedrooms during the night (WHO Europe 2009). Depending on the type of window, the noise reduction of a closed window ranges from approximately 25 dB to 48 dB (see, e.g. DIN 4109 Beiblatt 1 1989, p. 55–56). Whereas opened windows have an effect of 5 dB sound reduction, tilted-open windows reduce the sound level by approximately 15 dB (see, e.g. RPS 2011, Appendix 8). Overall, the above described regulations and limit values indicate the importance to go beyond residential noise exposures and to explicitly account for individuals affected by noise at work or educational activities, i.e. at school or university.
 In the same context, the Environmental Noise Directive of the European Union 2002/49/EC (2002) explicitly mentions certain building types, i.e. schools and hospitals, indicating that noise exposure analysis should not be limited to residents at their home location. However, the data to be delivered to the European Commission specified in 2002/49/EC (2002), App. 6, and the ’Good Practice Guide for Strategic Noise mapping and the Production of Associated Data on Noise Exposures’ only refer to residential noise exposures (WG-AEN 2006). Several studies address the absence of a standardized methodology to calculate noise exposures and set priorities for action planning in the European Union (see, e.g., Murphy and King 2010; Ruiz-Padillo et al. 2014). Lam and Chung (2012) show that a differentiated analysis of residential noise exposures provides interesting insights. The authors analyzed the population exposures with regard to socio-economic characteristics and find older and less educated residents in Hong Kong to be worst affected by traffic noise. Murphy and King (2010) address the importance to account for the day-to-day dynamics of varying population densities, i.e. weekend commuters. In contrast, the authors do not mention the within day dynamics of varying population densities (e.g. daily commuters). Ruiz-Padillo et al. (2014) propose an approach to compute a road stretch-specific priority index that can be used for noise control action planning. The index sorts road stretches by their noise problems, i.e. taking into consideration the noise level as well as the number of exposed residents. Furthermore, the priority index considers the “occurrence of noise sensitive centers” such as educational, cultural or health facilities. In Tenaileau et al. (2015), the authors address the size of the local living neighborhood to calculate residential noise exposures. The authors describe this exposure area to be usually limited to the home location, and in case outdoor exposures are accounted for, to be enlarged to the relevant neighborhood. The authors’ conclusion is that their current approach should be revised to account for the population’s variability in the daily activity and travel behavior. The authors suggest that population exposures should ideally be computed for each individual separately. Gühnemann et al. (2014) discuss optimal pricing strategies to protect sensitive areas. The authors find that prices should be regulated globally and account for all sensitive areas. Furthermore, the authors address the importance to consider the impact of noise on recreational activities. Lin et al. (2014) address traffic management strategies designed to meet hard environmental constraints. The authors present a criterion which can be used to assess traffic control measures regarding their impact on the network performance. In context of air pollution, Hatzopoulou and Miller (2010) and Kickhöfer and Kern (2015) have pointed out the importance to account for the temporal and spatial variability in air quality and population density. Similar to the latter study, Kaddoura et al. (2015) propose an approach to compute noise exposures which explicitly considers the within day dynamics of varying population densities in different areas of the city and incorporates individuals that may be affected at work, university or school, which is both found to have a substantial effect on the quantification of noise exposures. In this paper, a user-specific and dynamic pricing approach is proposed to internalize road traffic noise damages. The computation of noise exposures follows the methodology described in Kaddoura et al. (2015). The proposed pricing approach uses an activity-based transport simulation to compute noise levels and population densities as well as to assign noise damages back to road segments and transport users. Iteratively, road segment and time dependent noise exposure tolls are computed to which transport users can react. Since tolls correspond to the transport user’s contribution to the overall noise exposures, the incentives are given to change users’ travel behavior towards a higher system efficiency. The presented approach can be used for noise control action planning, i.e. how to manage traffic to reduce noise exposures while keeping the avoidance costs low. Thereby, the proposed approach explicitly accounts for the temporal and spatial variation of the noise level and population density. Furthermore, noise exposures are quantified taking into consideration people that are exposed to traffic noise at work or educational activities. The innovative pricing approach is applied to the case study of the Greater Berlin area. The remainder of the paper is organized as follows: Section 2 describes the applied transport simulation framework and the noise internalization approach. Section 3 provides the setup of the Berlin case study which is used for two pricing experiments. The simulation outcome is analyzed and discussed in Sections 4 and 5. Finally, Section 6 provides the conclusions for policy makers and an outlook on future research.",12
17.0,1.0,Networks and Spatial Economics,02 April 2016,https://link.springer.com/article/10.1007/s11067-016-9322-1,Path Flow and Trip Matrix Estimation Using Link Flow Density,March 2017,Louis de Grange,Felipe González,Shlomo Bekhor,Male,Male,Male,Male,"This article proposes a macroscopic-level static equilibrium model of a road network that simultaneously generates route flow and trip matrix estimates under conditions of heavy congestion. In addition to link cost data, the model uses data on each network link’s vehicle density. This latter information is obtained directly for a given instant from a static image of the network (e.g., a single photograph from which vehicle counts can be made for each link) on the assumption the fundamental traffic equation relating speed, flow and density is known for each link. The main advantage of this use of density data is that it more accurately captures the relationship between flow and cost (the latter represented by trip time or speed) for each link. Classical models for estimating trip matrices from traffic flow counts under congestion (see references, Section 2) normally assume this relationship is increasing and convex. In reality, however, it behaves quite differently in highly congested situations, as is apparent from the fundamental traffic equation. This implies that using density directly instead of flow may enable us to generate better estimates of link costs, a conjecture we will attempt to demonstrate. Another advantage of our proposed approach is that it obviates the need for counting vehicle flows (veh/h) on each network link. All we need to know is the number of vehicles on each link at a given instant—in other words, the density (veh/km)—and the relationship between density, speed and flow for each link. The information for the latter is easily obtained with “loop detectors” installed in various links in the road network. Our model is based on the specification of a maximum entropy optimization problem subject to linear constraints defined by the link flow data for a situation in which each link’s density and cost are in equilibrium. The model is validated using microsimulations of traffic on a small network and obtains good results, the proposed formulation proving capable of reproducing the simulation data for the model variables. For comparison purposes, we also estimate the model using link flow data, the traditional approach in the specialized literature. As will be seen, our proposed approach using link densities instead of link flows delivers better results. The remainder of this paper is divided into six sections. Section 2 briefly surveys the literature dealing with the principal aspects of our model; Section 3 introduces our approach for estimating route flows using link densities; Section 4 develops an extension for the simultaneous of route flows and trip matrices; Section 5 presents a microsimulation for validating our proposed methodology; Section 6 compares the results of the simulation with those obtained using the classic link-flow approach; Section 7 describes an application of the methodology to a real road network in the Chilean city of Santiago; and finally, Section 8 sets out our main conclusions.",16
17.0,1.0,Networks and Spatial Economics,27 June 2016,https://link.springer.com/article/10.1007/s11067-016-9323-0,A-RESCUE: An Agent based Regional Evacuation Simulator Coupled with User Enriched Behavior,March 2017,Satish V. Ukkusuri,Samiul Hasan,Weihao Yin,,Unknown,Unknown,Mix,,
17.0,1.0,Networks and Spatial Economics,16 May 2016,https://link.springer.com/article/10.1007/s11067-016-9324-z,The Impact of Service Refusal to the Supply–Demand Equilibrium in the Taxicab Market,March 2017,Dali Wei,Changwei Yuan,Wesley Kumfer,Female,Unknown,Male,Mix,,
17.0,1.0,Networks and Spatial Economics,20 April 2016,https://link.springer.com/article/10.1007/s11067-016-9325-y,Patent Protection in a Model of Economic Growth in Multiple Regions,March 2017,Amitrajeet A. Batabyal,Hamid Beladi,,Unknown,Male,Unknown,Male,"Knowledge is the key force that is responsible for growth in most modern regional economies and highly skilled workers are the key providers of this knowledge. Therefore, Faggian and McCann (2009) and Batabyal and Beladi (2014) have rightly noted that the available human capital in a region directly affects the production of final consumption goods. In addition, this human capital also indirectly influences the production of these final goods by engaging in R&D which then leads to the invention and the subsequent production of new inputs or machines. There is now a sizeable literature in regional economics and science on the connections between human capital and regional economic growth on the one hand and innovation and patent protection on the other. Contemporary research on human capital has shed light on the many nexuses between human capital use and regional economic growth. Florida et al. (2008) comment on educational and occupational measures of human capital and show that the occupational measure outperforms the educational measure when one attempts to account for regional productivity measured in wages. In contrast, the educational measure is the superior measure if one’s objective is to account for regional income. Using German data, Brunow and Hirte (2009) show that there are age specific human capital effects and that a temporary increase in regional productivity can occur during what these authors call the demographic transition. Hammond and Thompson (2010) analyze patterns in human capital accumulation in the United States and find scant evidence of convergence in college attainment across metropolitan and non-metropolitan areas. To handle the problem of widening regional disparities in Slovakia, Banerjee and Jarmuzek (2010) suggest that policy focus on the ways in which human capital accumulation might be increased. The primarily empirical and case study based literature has studied several aspects of innovative activities and patent protection in regional economies. For instance, Bottazzi and Peri (2003) use European patent data and claim that although a doubling of R&D spending in a region will greatly increase innovation in this region, this doubling will only have a minuscule effect on innovation in neighboring regions. In their study of differential patenting in Spanish regions, Gumbau-Albert and Maudos (2009) show that as expected, a region’s own R&D activities have a significant positive impact on innovation output measured by the number of patents. However, unlike the paper by Bottazzi and Peri (2003), these two authors note that R&D spillovers are important and that such spillovers result in positive effects on a region’s patents. Finally, Chapple et al. (2011) use surveys to make the point that although firms engaging in “green innovation” are likely to respond positively to local and regional markets, this kind of innovation does not always foster regional growth. The studies discussed in the preceding two paragraphs have certainly advanced our understanding of the nexuses between human capital use, innovative activity, patent protection, and the growth process in regional economies. Even so, with the exception of a recent paper by Batabyal and Nijkamp (2013), there are very few theoretical studies that are dynamic and that analyze the effects of human capital use, innovative activity, and patent protection, on endogenous economic growth in an aggregate economy composed of N heterogeneous regions. Batabyal and Nijkamp (2013) first delineate the balanced growth path (BGP) equilibrium of an arbitrary region in their model and then they ascertain the value of the patent expiry rate that maximizes the equilibrium growth rate of this region. In contrast, we use variants of the models in Romer (1990)Footnote 1 and in Batabyal and Nijkamp (2013) to study the effects of patent protection on economic growth in the ith region when this ith region is part of an aggregate economy of i = 1,…,N regions.Footnote 2 Note that the individual regions we have in mind are heterogeneous and spatially separated or geographically non-contiguous.Footnote 3 The regulatory authority in the ith region attempts to control the monopoly power of patent holding input producers by requiring them to charge a price that is parametrically related to the unconstrained monopoly price. Our analysis leads to three results. First, we show the way in which the equilibrium growth rate of the ith region is related to the above mentioned parameter. Second, we demonstrate the impact that changes in the stringency of patent protection have on the ith region’s equilibrium growth rate. Finally, we explain why eliminating the monopoly distortion does not maximize social welfare in the ith region. Before proceeding to the specifics of the paper, we would like to first discuss three modeling issues that are directly related to the particular modeling strategies that we employ in this paper. We begin by emphasizing two points. First, the fundamental objective of our paper is to study the effects of patent protection in the ith region on economic growth in the ith region and not on other N regions in the aggregate economy of N regions. Second, the individual regions in our paper are heterogeneous and spatially separated. As an example, if N = 3 then the individual regions could be the state of California in the United States, the county of Cambridgeshire in the United Kingdom, and the state of Karnataka in India.Footnote 4
 Given these two points, in order to meaningfully study the question that is implicit in the “fundamental objective” mentioned in the preceding paragraph, our position is that two interrelated modeling strategies are needed. As such, our first modeling strategy involves conceptualizing the ith region as an autarkic or closed region. This means that the ith region is an economically self-contained unit and hence it does not trade with any of the other heterogeneous regions in the aggregate economy of N regions. Practically and to continue with the example in the previous paragraph, this means that when thinking of the impacts of patent protection on economic growth in California it is not necessary to account for economic activities in either Cambridgeshire or in Karnataka. Because the patent protection regime one can expect to see in California is typically different from the corresponding regimes in either Cambridgeshire or in Karnataka, from the standpoint of a study focusing on the effects of patent protection specifically on regional economic growth, we contend that little is lost by modeling the individual regions under study as being autarkic or closed. In addition, it is worth noting that we are not the first to use this strategy to study the impacts of alternate regulatory policies—and patents are one kind of regulatory policy—on economic growth in a particular region. For instance, Batabyal and Beladi (2014) analyze the effects of alternate forms of taxation on economic growth in individual autarkic regions. As in our paper, the individual regions together comprise a larger, aggregate economy. Similarly, although there is no overlap in the research question we study in our paper and the research question analyzed in Batabyal and Nijkamp (2013), these authors also study the effects of human capital use, innovative activity, and patent protection, on economic growth in a model with many autarkic regions that together comprise a larger, aggregate economy.Footnote 5
 Having made the case for focusing on autarkic regions, let us now explain why we do not explicitly account for spatial externalities or spillovers between the different regions in our aggregate economy of N regions. In this regard, the most common types of externalities across regions are the ones that stem from knowledge spillovers, the migration of human capital, and the potential relocation of firms. Now, given that we are studying the impacts of patent protection on economic growth in the ith region and not on other regions, we contend that it is necessary to place first order importance on consumption, production, and R&D activities in the ith region specifically. This is, in fact, what we do in considerable detail in our subsequent analysis in this paper. Now, recall that the individual regions that we are studying are geographically non-contiguous or spatially separated. The existing literature on spatial spillovers Footnote 6 clearly tells us that spatial spillovers matter most for regions that are geographically close to each other. Second, spatial spillovers are frequently subject to a “distance-decay effect.” This means that the salience of spatial spillovers tends to decline as the distance between regions increases. Put differently, spatial spillovers from region A are most likely to impact region B when these two regions are close together in space. In contrast, when the distance between these two regions increases, the salience of spatial spillovers also decreases. Finally, the work of Fischer et al. (2009, p. 839) clearly tells us that localized knowledge spillovers are important and that “border effects are more important than geographical distance effects.” From a European standpoint, this last finding means that knowledge flows within European nations more easily than across nations. Putting the three points in the previous paragraph together, it ought to be clear to the reader that if one is interested in analyzing the effects of patent protection in California on economic growth in California, then one loses little by not accounting for spatial spillovers between California and the geographically dispersed Cambridgeshire or Karnataka. This is because to the extent that such spillovers exist, on account of distance, they are likely to be attenuated. We now conclude this discussion of spatial spillovers with two points. First, our assumption that the individual regions under study are autarkic and geographically non-contiguous goes hand-in-hand with the exclusion of spatial spillovers from our subsequent analysis. Second, consistent with the emphasis placed on local knowledge spillovers by Fischer et al. (2009), we point out in section 2 below that we do account for temporal, intra-regional knowledge spillovers and that it is straightforward to account for a second kind of intra-regional knowledge spillover in our analysis.Footnote 7
 The time horizon over which we conduct our analysis in this paper is infinite. Therefore, one can ask whether it is inconsistent to have, on the one hand, an infinite horizon analysis and, on the other, no spatial spillovers and no growth in the stock of human capital. Taking up the matter of no spatial spillovers first, we have already explained in detail in sections 1.2.1 and 1.2.2 above why we are not explicitly accounting for spatial spillovers in our theoretical analysis in this paper. To this we add that our rationale for not modeling spatial spillovers explicitly is not contingent on the length of time over which we conduct our following analysis. Therefore, we contend that it is not inconsistent to have both an infinite horizon analysis and no accounting of interregional spatial spillovers. Moving on to the matter of no growth in the stock of human capital, we emphasize that it is common for endogenous growth models to have an infinite time horizon and a fixed stock of human capital. A recent example of a paper that conducts an infinite horizon analysis of economic growth with a fixed stock of human capital is Batabyal and Beladi (2014). In addition, standard textbooks on economic growth and macroeconomics such as Acemoglu (2009, p. 399) and Romer (2012, p. 126) now routinely discuss models in which the time horizon is infinite and either the stock of labor or human capital is fixed and not growing over time. Therefore, we contend that it is, once again, not inconsistent to have both an infinite horizon analysis and no intertemporal growth in the stock of human capital.Footnote 8
 We conclude this section 1.2 discussion of modeling issues by noting that the modeling strategies we employ in this paper make sense in the context of the basic question that we are analyzing and our attendant assumptions. Having said this, we stress that it is certainly possible for there to be alternate points of view on how, for instance, spatial spillovers and growth in the stock of human capital ought to be modeled and analyzed.Footnote 9
 The rest of this paper is organized as follows. Section 2 describes our theoretical model of an aggregate economy of N heterogeneous regions. Section 3 computes the equilibrium growth rate of the ith region and then illustrates the dependence of this growth rate on the regional authority’s parameter mentioned in the last paragraph of section 1.1. Section 4 conducts a comparative statics exercise to show the effect that changes in the stringency of patent protection have on the ith region’s equilibrium growth rate. Section 5 shows why forcing the patent holding monopolist input producers to price at marginal cost is counterproductive. Finally, section 6 concludes and then discusses three extensions of the research delineated in this paper.",4
17.0,1.0,Networks and Spatial Economics,27 April 2016,https://link.springer.com/article/10.1007/s11067-016-9326-x,Impact of Traffic Management on Black Carbon Emissions: a Microsimulation Study,March 2017,Margherita Mascia,Simon Hu,Martin Litzenberger,Female,Male,Male,Mix,,
17.0,1.0,Networks and Spatial Economics,18 May 2016,https://link.springer.com/article/10.1007/s11067-016-9327-9,Path Flow Estimator in an Entropy Model Using a Nonlinear L-Shaped Algorithm,March 2017,Maryam Abareshi,Mehdi Zaferanieh,Bagher Keramati,Female,Male,Unknown,Mix,,
17.0,2.0,Networks and Spatial Economics,31 May 2016,https://link.springer.com/article/10.1007/s11067-016-9328-8,An Approach for Integrating Valuable Flexibility During Conceptual Design of Networks,June 2017,Y. G. Melese,P. W. Heijnen,P. M. Herder,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Networks and Spatial Economics,27 May 2016,https://link.springer.com/article/10.1007/s11067-016-9329-7,A Parametric Description of Cities for the Normative Analysis of Transport Systems,June 2017,Andrés Fielbaum,Sergio Jara-Diaz,Antonio Gschwender,Male,Male,Male,Male,"Understanding modern cities is a highly relevant task because it is the space where the great majority of the individuals interact socially, economically and politically. From a normative viewpoint intervening that space by means of models is a very challenging problem because of its complexity and because activities and their connectivity evolve dynamically and interactively. There are several ways to describe cities and their respective urban forms, which depend on the objective pursued by the intended research question. It makes a difference if the focus is on the spatial distribution of activities in order to analyze land use patterns, or if the issue under study is the connectivity network and the spatial structure of displacements when the motivation is to understand and solve transport problems. This latter is the subject of this paper. Transport systems analysis deal with the movements of persons and goods between different points in space at different moments. In the urban case this analysis requires some explicit or implicit representation of the city. The network and the activity pattern are crucial on these descriptions because of their relevance for transport modelling. Typical implicit models follow some type of regularity, as in the case of concentric cities (e.g. Byrne 1975; Tirachini et al. 2010) and cities shaped as a grid (e.g. Newell 1979; Daganzo 2010). These and other models are chosen to introduce simplicity and workability simultaneously, without questioning their properties as a reasonable representation of actual urban forms and properties. We consider this as a limitation for relevant transport modelling from a normative viewpoint, as there is a co-evolution of urban settlements and road systems (Yamins et al. 2003; Sun et al. 2007). The objective of this paper is to study different ways to describe a city’s urban form analytically and to propose a flexible description based on parameters such that the public transport system can be studied normatively. We begin by reviewing previous studies on urban forms emphasizing those papers whose scope is useful for the analysis of the transport system. This means focusing on those elements that give information about the generation and attraction of trips, and on the elements that explain the network structure, i.e. the physical space where the buses, cars and different transport modes move. We will examine how good available representations are through different metrics in order to propose a new model which is coherent with the state of the art in this area and that is useful for modeling transport problems. We will search for simplicity, noting that there exist many regularities that can be taken into account to create a model, but keeping those characteristics of the cities that are relevant to analyze the optimal shape of transport services. In the next section we review different ways to represent urban forms explicitly or implicitly, looking at cities’ descriptions and classifications, with emphasis on those that are useful for transport modeling; the role played by the dominant zones within a city will receive particular attention, because they explain much of the generation and attraction of trips. Then we describe the graph representations of cities that underlie the formulation of transit design problems, in order to understand their connectivity features and their street structure. In section 3 we compare the behavior of some indicators calculated over these graph representations for real cities and for some ideal graphs. Section 4 contains the description of a parametric graph model that incorporates the discussion of the previous sections. The new model is applied in section 5 finding the parameters that represent three cities. In section 6 we summarize and conclude.",27
17.0,2.0,Networks and Spatial Economics,22 August 2016,https://link.springer.com/article/10.1007/s11067-016-9330-1,Dynamic Spatial Auction Market Models with General Cost Mappings,June 2017,Elisabetta Allevi,Adriana Gnudi,Giorgia Oggioni,Female,Female,Female,Female,"Spatial equilibrium models describe complex spatially distributed systems where many players interact all together (see e.g. Harker 1985; Samuelson 1952; Takayama and Judge 1971). Variational inequality theory facilitates the formulation of equilibrium problems that describe the interactions of several agents whose choices are subject to technical and economic constraints. The spatial control of such systems requires to consider not only the reactions of separate markets (or agents), but also technical constraints such as production capacity and network limits. Variational inequality theory has been developed and adopted to study the equilibrium behavior of decision makers in many research fields such as spatial equilibrium models (Nagurney 1999; Konnov 2007,  2009; Miller et al. 2007), supply chain management (see, e.g. Nagurney et al. 2002; Nagurney and Toyasaki 2005; Hammond and Beullens 2007), financial networks (see, e.g., Nagurney 2003; Daniele 2006; Barbagallo et al. 2014), transportation networks (see, e.g., Daniele et al. 1999; Scrimali 2004; Nagurney 2000), and energy markets (Abada et al. 2013; Smeers 2003a, (Smeers  2003b)). Auction type mechanisms, based on variational inequality theory, represent an alternative approach to the classical perfect or imperfect competition models developed in literature. In particular, dynamic auction can be a valid mathematical instrument for the analysis of deregulated electricity markets. The restructuring of electricity markets in Europe, started in the 1990s, has deeply changed the organization and the architecture of this sector. Directive 96/92/EC (first legislative package) was issued by European Commission to liberalize electricity markets and to create the Internal Electricity Market (see, e.g., Meeus et al. 2005). This Directive imposed the unbundling of generation, transmission and distribution that, since then, were vertically integrated and controlled by a sole entity operating in a monopolistic regime. In particular, this Directive aimed at promoting the competition in the activities of electricity generation and wholesale through the creation of a “marketplace” and the maximization of transparency and efficiency in the naturally monopolistic activity of dispatching. This Directive was then replaced in 2003 by Directive 2003/54/EC (second legislative package) that enabled new electricity suppliers to enter Member States’ markets and gave the possibility to consumers to choose their own electricity suppliers. In April 2009, Directive 2009/72/EC (third legislative package) further liberalized the market with the aim of creating a single and coordinated EU electricity market. As a direct consequence of this liberalization process, new kinds of spatial models, also based on variational inequalities, have been developed to design these changes in the electricity market organization (see, e.g., Boucher and Smeers 2001; Daxhelet and Smeers 2001; Hobbs and Pang 2004; Chen et al. 2006; Metzler et al. 2003; Wei and Smeers 1999). In most of the restructured European electricity markets, the equilibrium between power demand and supply is defined in the day-ahead market that is based on an auction model. In the day-ahead market, electricity consumers and producers respectively submit their bids and offers for each of the 24 hours of the next day. Both consumers’ bids and producers’ offers consist of a pair of volume and unit price of electricity. Consumers’ bids express the willingness to buy a volume of electricity not higher than the one specified in the bid at a price not higher than the one indicated. On the other side, producers’ offers indicate the willingness to sell a volume of electricity not higher than the one specified in the offer at a price not lower than the one proposed. After the bids/offers submission process has been closed, a Power Exchange (PX)Footnote 1 activates the market resolution process for each hour of the following day, taking into account a simplified version of the transmission network provided by the Transmission System Operator (TSO).Footnote 2 PX ranks all received supply offers in an increasing price order to form an aggregate supply curve and all received demand bids in a decreasing price order to form an aggregate demand curve. The PX will accept bids/offers so as to maximize social welfare, while guaranteeing zonal balance constraints and satisfying maximum transmission limits of the lines connecting the zones in the simplified representation of the network. Therefore, the equilibrium of this market not only defines hourly prices and volumes, but also injection and withdrawal schedules for the next day. Note that the offer acceptance commits power producers to inject the volumes of electricity specified in the offer into the grid in a specified period or, in case of partial acceptance of the offer, the corresponding quota. Auctions can be classified in explicit or implicit depending on the mechanism used to regulate congestion management. In the explicit auction, the transmission capacity on an interconnector between two different countries (or zones) is auctioned separately and independently from the marketplaces where electricity is auctioned. Since the two commodities, transmission capacity and electricity are traded separately, there is a lack of information about the prices of the other commodity that may lead to an inefficient use of interconnectors and thus to a reduced social welfare. In the implicit auction, instead, transmission capacity is used to integrate the spot markets in the different bidding areas and is auctioned together with energy in order to maximize the overall social welfare. The flow on an interconnector is thus found based on market data from the market place in the connected markets. As a result, zonal electricity prices reflect both the cost of energy in each internal bidding area and the possible congestion costs. Implicit auctions also ensure that electricity flows from energy surplus areas with a lower power price towards energy deficit areas with higher power price thus also leading to price convergence. Notice that the implicit auction mechanism is at the basis of both “market coupling” and “market splitting” organizations of electricity markets.Footnote 3
 Auction design for the day-ahead electricity markets has been studied using different mathematical approaches (see, e.g., Contreras et al. 2001; Triki et al. 2005). In this work, we aim at modeling the implicit auction mechanism of day-ahead electricity markets through variational inequality theory. More precisely, the contribution of this paper is twofold: first, it shows how dynamic auction, based on a variational inequality approach, can be used to find the equilibrium of the day-ahead electricity market where zonal market prices reflect the participants’ bidding strategies and the marginal cost of producing energy, increased by transmission costs when network is congested. Second, it allows one to develop numerical methods of finding solution to spatial equilibrium problems based on efficient solution methods for variational inequalities. Moreover, in the literature, the papers devoted to the analysis of electricity markets through variational inequality theory are focused on the description of the power supply chain (see, e.g. Matsypura et al. 2007; Nagurney et al. 2007). Instead, in this paper, taking as reference the approach already developed by Allevi et al. ( 2012), we propose a model of a dynamic system of auction markets linked by transmission lines, subject to zonal energy balance and transmission constraints, to the aim of providing a realistic and therefore sophisticated representation of day-ahead electricity markets. Differently from the model developed in Allevi et al. ( 2012), we here consider the case where the mappings do not need to be integrable, i.e. the problem does not reduce to a saddle point problem. An additional contribution of the paper consists in the fact that, the proposed model allows for commodity storage within a given time period that, in the modeling of electricity market, can correspond to a simplified representation of the pumped-storage hydro plant systems. As a result, we construct a system of variational inequalities whose solutions yield an equilibrium trajectory of this system. Nevertheless, an inexact splitting type method is proposed to find its solution. The remainder of the paper is organized as follows. Section 2 illustrates a general formulation of dynamic spatial equilibrium problem for auction markets, based on variational inequalities, where players exchange a generic homogeneous good. Section 3 shows how this spatial equilibrium model can be adopted to describe the implicit auction mechanism used for clearing the day-ahead electricity market. Section 4 delineates the inexact forward-backward splitting method that we propose to solve such a dynamic spatial equilibrium problem. Section 5 is devoted to the description of the case study based on an application to the Italian electricity market. The results of our numerical experiments are reported in Section 6, while Section 7 closes the paper with some final remarks.",5
17.0,2.0,Networks and Spatial Economics,14 September 2016,https://link.springer.com/article/10.1007/s11067-016-9331-0,Reliable Intermodal Freight Network Expansion with Demand Uncertainties and Network Disruptions,June 2017,Fateme Fotuhi,Nathan Huynh,,Unknown,Male,Unknown,Male,"A robust freight transport system is a key contributor to the success of the U.S. economy (Ortiz et al. 2007). However, the existing freight transport system is running at its capacity due to increase of trades and an aging infrastructure. Thus, it has a limited buffer to handle the severe disruptions (Ortiz et al. 2007). Disruptions can range from frequent events with short term impacts, such as adverse weather, accidents and loading/unloading delays at intermodal terminals, to catastrophic natural and man-made disasters with long term impacts which can drastically degrade the transport capacity (Ortiz et al. 2007; Miller-Hooks et al. 2012). Examples of catastrophic disasters with long term impacts are Hurricane Katrina, which damaged the transportation infrastructure of Gulf Coast area (Godoy 2007) and the West Coast port labor strike which disrupted the U.S. freight supply chain (D’Amico 2002). Thus, in order for the freight transport system to be able to handle such disruptions, there is a need to increase its capacity by incorporating redundant resources and backup facilities, and retrofitting existing infrastructure (Peeta et al. 2010; Liu et al. 2009). Intermodal network design is a strategic level planning problem which finds the location of intermodal terminals and assignment of freight flow to the network. Strategic planning decisions deal with network infrastructure creation or expansion (SteadieSeifi et al. 2014). SteadieSeifi et al. (2014) recently provided a comprehensive overview of models and algorithms developed for strategic, tactical and operational multi-modal freight transport planning problems. According to this review, most strategic studies neglected two important practical aspects. The first is that the design does not consider existing intermodal infrastructure, which is not realistic in practice. It is more realistic to expand an existing freight transport network rather than designing a system from scratch (Gelareh et al. 2010). The second is that prior strategic studies neglected the possible changes in demand and supply that may occur over the planning period. Demand changes are to be expected as a result of pricing and supply changes over time, as well as the influx of new users in the system (Melese et al. 2016). Also, as new terminals come online, demands and freight flow patterns may be very different from what was envisioned at the network design phase (Taner and Kara 2015). This research seeks to fill these gaps by developing a mathematical model to address expansion of an existing intermodal freight network and making it reliable; reliability means that the network can continue delivering service when faced with shocks, such as influx in freight demands or disasters that reduce capacity of network links and intermodal terminals. This study deals with a rail-road intermodal network expansion problem for a private rail carrier that is responsible for its own maintenance and improvement projects, typical of U.S. Class 1 railroads. The strategic decision involves identifying locations for the new intermodal terminals, expanding the existing terminals and retrofitting the existing rail links in the network to enhance its survivability, given a finite budget. The strategic decision will subsequently affect the operational decision concerning freight flow assignment through the network. Thus, the routing decision is factored in the investment decision in the proposed model. A robust optimization approach is utilized to account for forecast demand errors and potential supply disruptions. The optimal decision seeks to minimize the total expansion, transport and lost-sale costs of the railroad company; lost-sale cost is incurred to the railroad company when it fails to deliver shipments on time. The rest of the paper is organized as follows. Section 2 provides a summary of related studies. Section 3 presents the developed mathematical model. Section 4 discusses the proposed solution algorithm to solve the model. Section 5 discusses the results of the numerical experiments. Lastly, Section 6 provides concluding remarks and future research.",19
17.0,2.0,Networks and Spatial Economics,09 September 2016,https://link.springer.com/article/10.1007/s11067-016-9332-z,Traffic Equilibrium and Charging Facility Locations for Electric Vehicles,June 2017,Hong Zheng,Xiaozheng He,Srinivas Peeta,,Unknown,Male,Mix,,
17.0,2.0,Networks and Spatial Economics,21 September 2016,https://link.springer.com/article/10.1007/s11067-016-9333-y,A Closed-Loop Supply Chain Equilibrium Model with Random and Price-Sensitive Demand and Return,June 2017,Younes Hamdouch,Qiang Patrick Qiang,Kilani Ghoudi,Male,,Unknown,Mix,,
17.0,2.0,Networks and Spatial Economics,17 October 2016,https://link.springer.com/article/10.1007/s11067-016-9334-x,Determining the Impact of Personal Mobility Carbon Allowance Schemes in Transportation Networks,June 2017,H. M. Abdul Aziz,Satish V. Ukkusuri,Xianyuan Zhan,Unknown,,Unknown,Mix,,
17.0,2.0,Networks and Spatial Economics,16 December 2016,https://link.springer.com/article/10.1007/s11067-016-9335-9,Mixed Equilibria with Common Constraints on Transportation Networks,June 2017,Xia Yang,Xuegang Jeff Ban,Rui Ma,,Unknown,Male,Mix,,
17.0,2.0,Networks and Spatial Economics,06 December 2016,https://link.springer.com/article/10.1007/s11067-016-9336-8,A Stochastic Multi-agent Optimization Model for Energy Infrastructure Planning under Uncertainty in An Oligopolistic Market,June 2017,Zhaomiao Guo,Yueyue Fan,,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Networks and Spatial Economics,27 February 2017,https://link.springer.com/article/10.1007/s11067-017-9338-1,Network Expansion to Mitigate Market Power,June 2017,Alexander Zerrahn,Daniel Huppmann,,Male,Male,Unknown,Male,"Game theory in constrained networks is a challenging field of study; it is of particular relevance in the electricity system due to power flow characteristics in a meshed network. Strategic firms may be in a position to create congestion at specific lines and thereby appropriate rents beyond the efficient market outcome. In particular instances, they may even be able to partition the market, preventing rivals from competing, and then earn monopoly rents. However, modeling such behavior is not trivial. In their seminal paper, Borenstein et al. (2000) demonstrate the impact of strategic behavior in a stylized two-node model. They motivate their analysis based on the Californian situation, and they illustrate how one generator may be able to prevent a rival from entering the market by congesting the connecting power line. They refer to this as a “passive-aggressive equilibrium”. Their most interesting contribution, however, was made with respect to the line capacity that connects the two nodes. The impact of available transmission is not monotone: contingent on the capacity, results change from Nash equilibria with high profits by one or both firms, to a situation where no Nash equilibrium exists, to the Cournot equilibrium. An interesting result from this work is that even a line with relatively low capacity may be enough to foster competition and evoke substantial welfare gains. In this context, it may be the case that the line is not used at all in equilibrium – however, its existence (at a certain capacity) is sufficient to prevent strategic generators from deliberately congesting the grid and deviating towards a passive-aggressive equilibrium. It is for this reason that their contribution is sometimes referred to as the “thin-line paper”. In our contribution, we build on these insights and add an additional consideration: a network planner expands transmission lines with the aim to maximize total welfare, while anticipating its effect on the strategic generators. To this end, we propose a three-stage model that endogenously captures the trade-off between costs and benefits of network expansion in one integrated model, specifically accounting for the gaming opportunities of generators in the short run. In the first (top) stage, a benevolent system planner aims to maximize overall welfare by deciding on upgrades for existing transmission lines, anticipating the optimal reactions by players at the lower levels and the resulting equilibria. The second stage is formed by strategic generators: they are aware of their impact on grid congestion and may thereby be able to appropriate excessive rents, in some instances partitioning the market and preventing their rival(s) to compete. This may yield outcomes in the “passive-aggressive” sense, as postulated by Borenstein et al. (2000). The third stage is the clearing of the spot market, consisting of demand, competitive (fringe) generators, and network operation. Mathematically, the spot market game in the second and third stage of the model constitutes an Equilibrium Problem under Equilibrium Constraints (EPEC). Standard solution techniques are mostly based on diagonalization algorithms, which are subject to a range of caveats. In contrast, we apply a reformulation based on strong duality, recently used by Ruiz et al. (2012), and generalize it to a quadratic setting. By this methodological contribution, we can effectively integrate the spot market clearing without requiring diagonalization, other auxiliary constructions or overly rigid assumptions. Our work is relevant both in the European and in the US context: in Europe, the push towards market integration across national borders was not yet matched by sufficient investment in transmission lines. As a consequence, power markets remain fragmented for many hours over the course of a year, with large incumbents in effect being the pivotal supplier in a country or region and possibly in a position to exert market power (Bigerna et al. 2016; Gebhardt and Höffler 2013). Furthermore, the power sector is largely organized as price zones, with the TSO in charge of redispatch and implementation of other measures to guarantee feasibility of operations after markets cleared. Hence, there are insufficient price signals to identify bottlenecks and little incentive for firms to run their power plant portfolio in such a way as to reduce congestion, both within a zone or across borders. It is therefore virtually impossible to determine whether a price offer at one node was made based on pure merit-order consideration or with the intent to create congestion and limit the available transfer capacity, thereby counteracting market integration. In the US, in contrast, many power markets are based on nodal pricing and the Independent System Operators (ISO) have tools to identify (and sanction) strategic behavior. Nevertheless, generators have the possibility to “self-schedule” (i.e., submit a fixed output schedule rather than price offer curves), effectively forcing the ISO to clear the market with their output taken as given. Furthermore, in spite of the existence of rules against strategic behavior, it may be difficult to decide in many situations whether a specific decision was undertaken strategically with the aim to congest the network, or whether it was due to valid technical-engineering reasons or based on expectations that turned out to be wrong. Applying our model to a sample network, we identify three distinct effects: firstly, grid investment can increase welfare by mitigating market power potential – leading to a second-best equilibrium close to the first-best outcome of perfectly competitive markets. In some cases, a relatively “thin” line can be sufficient to enforce a higher degree of competition. Moreover, expansion of lines may be required to prohibit gaming – even if these lines are not congested in equilibrium. In this vein, only focusing on existing bottlenecks may yield suboptimal outcomes. Secondly, beyond an increase in aggregate welfare, optimal network expansion entails a relative shift of rents from producers to consumers. Due to the prevention of strategically exploiting bottlenecks, excessive rent extraction by generators can be mitigated such that consumers benefit. Thirdly, ignoring strategic behavior by firms may yield suboptimal decisions for network upgrades – and may possibly create situations in which no stable equilibria can be attained. The remainder of this paper is structured as follows: Section 2 gives an overview of the existing literature and relates it to our research question, Section 3 describes our modeling approach. The mathematical formulation is explained in detail in Section 4, and Section 5 discusses numerical results. Section 6 concludes and outlines avenues for further research.",10
17.0,2.0,Networks and Spatial Economics,02 February 2017,https://link.springer.com/article/10.1007/s11067-017-9339-0,Heterogeneity Within and Across Households in Hurricane Evacuation Response,June 2017,David S. Dixon,Pallab Mozumder,Hugh Gladwin,Male,Unknown,Male,Male,"Natural disasters present civil authorities with complex challenges for both preparation and for response in the aftermath. Hurricanes have the added complexity of a few days advanced warning during which there is considerable uncertainty in the path of the storm and in its consequences. This meteorological uncertainty is met with miscomprehension, misinformation, and indecision among those in the hurricane’s potential path. Under-assessment of the risk results in tragic loss of life. Overestimation of the risk incurs the costs of unnecessary evacuation and dangerously increases risk tolerance in future hurricane events. The human cost of Hurricane Ike was, in part, due to less than fifty-percent compliance to ordered evacuation. For all the uncertainties intrinsic in hurricanes and in responding to them, the most vexing are residents who don’t evacuate when told to. Hurricane Ike struck the Houston area at 2:10 AM on Saturday, September 13, 2008 (Berg 2014). This hurricane, a Category 2 storm, was forecast to be especially dangerous not because of its winds, but because of high anticipated storm surge and subsequent flooding. A hurricane watch was issued on the afternoon of 10 September and a hurricane warning mid-morning on 11 September. A previously voluntary evacuation order for the west end of Galveston Island was made mandatory on Wednesday night, 10 September (TranStar 2008). Mandatory evacuation orders were issued for the remainder of Galveston Island on 11 September, with the low-lying areas of Harris and adjacent counties following suit. The highest storm surge at Galveston Island was measured at 11.48 feet (Berg 2014). The storm left 36 dead, 134 missing, and 3055 properties destroyed in the Houston-Galveston area (Berg, 2014; FEMA, 2008). Hurricanes are low-frequency events with a great deal of variation in meteorological conditions. Hurricane Ike was unusual in that it had low wind speed - Category 2 on Texas landfall (Berg 2014) - but had a much higher storm surge than is normally associated with a Category 2 storm. Nothing like that had happened in a very long time but then four years later in 2012 two major hurricanes of the same type made landfall in the U.S., Isaac and Sandy. This makes Hurricane Ike the first of this case type and important to study since these three storms have led to efforts by the NWS to decouple surge and wind in forecast messaging.",14
17.0,3.0,Networks and Spatial Economics,03 July 2017,https://link.springer.com/article/10.1007/s11067-017-9340-7,A Credit-Based Congestion Management Scheme in General Two-Mode Networks with Multiclass Users,September 2017,Yang Liu,Yu (Marco) Nie,,,,Unknown,Mix,,
17.0,3.0,Networks and Spatial Economics,27 February 2017,https://link.springer.com/article/10.1007/s11067-017-9341-6,Modeling Vehicle Miles Traveled on Local Roads Using Classification Roadway Spatial Structure,September 2017,Xiubin B. Wang,Xiaowei Cao,Teresa M. Adams,Unknown,Unknown,Female,Female,"Vehicle miles traveled (VMT) refers to the total miles traveled by vehicles on the roadway. The Federal Department of Transportation requires that each state report the VMT according to the roadway functional classification, which includes local, collector, minor arterial, and principal arterial roads (FHWA 2013). Local streets are generally the community roads that provide access to properties, and are formally defined as those not included in any other functional classifications in rural or urban areas. According to the 2000 U.S. data, the principal arterial plus minor arterial street system, the collector street system and the local street system accounted respectively for 15–25, 5–10 and 65–80 % of the total road miles and 65–80, 5–10 and 10–30 % VMT nationwide, respectively (FHWA, 2013). The primary function of road classes varies from providing accessibility to mobility. Figure 1a illustrates a transition from accessibility to mobility as travelers move from local streets onto arterial roads, Fig. 1b shows an example neighborhood roadway network with hierarchical roads designed to serve a combination of functions between accessibility and mobility. Interested readers may refer to Levinson and Zhu (2012), which provides detailed description of the origin, purposes, governance and VMT shares of the hierarchical road system, an excellent background for this work. The hierarchical roadway system, whose road density sharply decreases as the road class goes from local, collector to arterial roads, is often featured in scientific studies such as in Chapter 4 of Newell (1980). (a) Accessibility vs. mobility of the U.S. road classification system (Source: FHWA, 1992); (b) Hierarchical structure of the U.S. road system (Source: Hausman, 2011) The VMT for each roadway classification is an indicator of roadway usage. Therefore, the Federal Department of Transportation by law generally allocates apportionment fund to each state under different systems such as the National Highway System (NHS) and the Interstate Maintenance System (IMS) based on the ratio of the total VMT traveled on a state’s public roads to the national total on the same functional class of roads (Fricker and Kumapley 2002). The apportionment fund is a multi-billion dollar budget each year. Accurate report to the federal Department of Transportation about the VMT estimates means accurate implementation of federal law. In addition, accurate VMT numbers provide a sound basis for analysis of gas consumption and environmental impact. A major challenge in the VMT report system has been with the local road VMT estimation for which no established method is available yet to our best knowledge. State departments of transportation (DOTs) and local transportation agencies traditionally resort to traffic count programs to get the traffic count data for VMT estimation. These traffic count programs well cover higher classes of roads, primarily arterial roads by using permanent count stations. FHWA’s Highway Performance Monitoring System (HPMS) (FHWA, 2013) provides detailed guidelines as to data collection and calculation of VMT on higher classifications of roads. In particular, freeways have traffic data from continuous monitoring so that the VMT calculations are generally considered accurate and reliable. However, traffic counts on local roads are not practical and are usually not available. HPMS does not specify any specific sampling method for local road traffic counts. Determining the methods for estimating local road VMT is generally left to each state DOT. The primary difficulty of estimating local road VMT is the lack of sufficient traffic data. Data collection on local roads proves challenging and costly, and is especially so when no proper and effective sampling method is developed. The low traffic density and pervasiveness of local roads make it impossible to count traffic in a similar way as for the arterial or collector roads. Among the better methods for estimating local road VMT is using traffic counts conducted on sampled sections of local roads such as Frawley (2002), which requires traffic count samples every year on local roads but is not able to provide error analysis. In this paper, we make an effort to develop an alternative method to avoid the costly undertaking of sampling traffic count every year. We believe that local road VMT must have a close relationship to higher classification road VMT. Therefore, we choose to use classification roadway spacing and higher class road VMT estimates for the estimation. Under this method, only calibration of the regression equations once every few years through survey would generally be needed. In particular, by following the general continuous approximation concept for vehicle routing, we first analytically develop a structural relationship of VMT between local and collector roads in terms of classification road densities under restrictive assumptions. The structural relationship reveals key explanatory variables and allows to propose sound regression equations using these explanatory variables, calibration of which by using field data would account for noises due to defiance of the assumptions. The regression equations can be relatively easily calibrated. As will be seen, the ease of application of our proposed method presents a compelling advantage. An application is found to fit well in Minneapolis area of Hennepin County, Minnesota, one of the largest urban areas in the U.S.",1
17.0,3.0,Networks and Spatial Economics,26 April 2017,https://link.springer.com/article/10.1007/s11067-017-9342-5,On Node Criticality in Air Transportation Networks,September 2017,Xiaoqian Sun,Sebastian Wandelt,Xianbin Cao,Unknown,Male,Unknown,Male,"Air transportation is a complex socio-technical system, since there are large numbers of different components with different characteristics. Ambitious goals have been set for European air transportation, in terms of quality and affordability, environment, efficiency, safety, and security (European Commission 2001). With rapid development of economic globalization and increased demand of air traffic, congestion frequently occurs and leads to huge economic losses. It is critical to understand the complexity and emergent behavior of air transportation. Network science provides powerful tools for understanding the structures and dynamics of air transportation (Barabasi 2013; Newman 2010; Zanin and Lillo 2013). In particular, complex network theory has been widely applied to air transportation (Vitali and Et al 2012; Sun and Wandelt 2014; Wandelt and Sun 2015), where nodes are individual airports or air navigation route points. A significant number of studies have focused on delay propagation (Fleurquin et al. 2013), epidemic spreading (Gomes et al. 2014), passenger flows (Lehner et al. 2014; Lehner and Gollnick 2014), and temporal evolution (Kotegawa et al. 2011; Sun et al. 2015). An overview of how complex network theory is applied in geography and regional science is provided in (Ducruet and Beauguitte 2014). The impact of large-scale disruptions on air transportation networks has gained interest recently (Caschili 2015a, b), and it is one of the most important problems, besides emission reduction (Yang et al. 2016; Aziz et al. 2016; Mascia et al. 2016). Furthermore, Zhang et al. applied stochastic actor-based modeling to European airport network between 2003 and 2009, in order to understand the effects of exogenous and endogenous factors on the network dynamics (Zhang et al. 2015). Lijesen et al. developed an empirical model to explain the phenomenon of relatively high fares for flights leaving from carriers’ hubs (Lijesen et al. 2004). Radicchi et al. introduced a set of heuristic equations for the full characterization of percolation phase diagrams in finite-size interdependent networks, where the percolation transitions can be understood by the decomposition of these systems into a) the intersection among the layers and b) the remainders of the layers (Radicchi 2015). The smoothness percolation transition depends on which decomposed parts dominate the other. Colizza et al. studied the role of airline transportation network in global epidemic spreading and evaluated the reliability of forecasts with respect to the stochastic attributes of disease transmission and traffic flows (Colizza et al. 2006). It is shown that the complex features of airline transportation network are the origin of heterogeneous and erratic spreading of epidemics. Barrat et al. presented statistical analysis of complex networks whose edges are assigned with certain weights (flow or intensity), worldwide air transportation network and scientific collaboration network were used as two typical examples (Barrat et al. 2004). The correlations among weighted quantities and topological structures were exploited. Several research on international air transport has dealt with gravity models and trade, among which Arvis and Shepard proposed air connectivity index to measure the importance of a country as a node in global air transport system; the value for a country is higher if it has stronger pull on the rest of the network and the cost of moving to other countries in the network is relatively low (Arvis and Shepherd 2011). In this study, we analyze the criticality of nodes in air transportation using techniques from three different domains, and thus, three essentially different perspectives of criticality. First, we examine the unweighted structure of air transportation networks, using recent methods from control theory (maximum matching and minimum dominating set). Second, complex network metrics (betweenness and closeness) are used with passenger traffic as weights. Third, ticket data-level analysis (origin-destination betweenness and outbound traffic with transit threshold) is performed. As a case study, we chose the international air transportation country network. Moreover, we perform multi-criteria assessment of node criticality and we investigate in how far our results are evolutionary stable, by analyzing data for multiple years. The three main contributions of our paper are: 1) We investigate the criticality of nodes in air transportation networks by using techniques from three different domains: Recent methods from control theory, standard weighted network metrics, and ticket data-level analysis. 2) Our experiments on the air transportation country network reveal that all techniques identify a different set of critical countries; while most techniques give preference to selecting high-degree nodes as critical. We also show that the criticality is rather stable over time. 3) Multi-criteria assessment of the node criticality shows that four countries are non-dominated among all 223 countries and the outbound traffic is the most sensitive criterion. This paper is organized as follows. In Section 2, we review the state-of-the-art literature on network criticality. Section 3 presents how to construct the country network based on real ticket data. In Section 4, we present the results of network criticality for international air transportation. Section 5 discusses our findings from a multi-criteria assessment point of view. The paper is concluded in Section 6.",31
17.0,3.0,Networks and Spatial Economics,06 March 2017,https://link.springer.com/article/10.1007/s11067-017-9343-4,On the Uniqueness of User Equilibrium Flow with Speed Limit,September 2017,Zhiyuan Liu,Wen Yi,Jun Chen,Unknown,,,Mix,,
17.0,3.0,Networks and Spatial Economics,06 April 2017,https://link.springer.com/article/10.1007/s11067-017-9344-3,Non-expected Route Choice Model under Risk on Stochastic Traffic Networks,September 2017,Xiangfeng Ji,Xuegang (Jeff) Ban,Bin Ran,Unknown,Unknown,,Mix,,
17.0,3.0,Networks and Spatial Economics,26 April 2017,https://link.springer.com/article/10.1007/s11067-017-9345-2,Finding the Most Reliable Strategy on Stochastic and Time-Dependent Transportation Networks: A Hypergraph Based Formulation,September 2017,A. Arun Prakash,Karthik K. Srinivasan,,Unknown,Male,Unknown,Male,"Optimization of travel time reliability is gaining attention in the literature. This is due to the high costs associated with travel time variability to both travelers and traffic systems. Though there are studies that focus on optimizing reliability on static networks, extensions to time-dependent networks have been scant. In this light, this study address the the most reliable strategy problem on stochastic time-dependent networks, where the link travel times are modeled as time-dependent discrete random variables. A (time-adaptive) strategy is an extension of a path in the context of time-adaptive route choice. Informally, a time-adaptive strategy can be defined as a mapping from nodes to their successive links as a function of the departure-time. In the context of urban transportation networks, the total variability in travel times can be attributed to both dynamics and uncertainty. Dynamics refers to the systemic variability in mean travel times as a function of time-of-day; it could be caused by structural factors such as periodic peaking characteristics and planned disruptions due to special events and work zones. The uncertainty component refers to the inherently random fluctuations due to unpredictable factors including incidents, accidents, and weather. It can be viewed as a residual component of variability after accounting for the travel time dynamics. Static stochastic networks do not differentiate between dynamics and uncertainty and capture them together as total variability. In contrast, Stochastic Time-Dependent (STD) networks allow distinct representation of both dynamics and uncertainty. This study adopts robust-cost as a measure of reliability, which is defined as a conic combination of the mean and standard deviation of travel time. To minimize the robust-cost objective, the conventional approaches —that adopt subpath optimality and subpath non-dominance— are not applicable, complicating the solution efforts. Note that, to determine optimal paths and strategies on the STD networks, the data requirements for the robust-cost objective function and the expected travel time objective function are the same. This implies that no additional data needs to be collected because of adopting the robust-cost objective function. The present study is motivated by following considerations. First, empirical studies show that users value travel time reliability (Bates et al. 2001; Carrion and Levinson 2012). This behavior is not captured by the expected travel time objective which is predominantly used in the literature. Second, there are no studies —to the authors’ knowledge— that address the minimum robust-cost strategy problem. Third, the proposed solution procedures can have applications in broader routing and modeling frameworks that incorporate reliability as a performance measure. In light of the above motivating factors, following objectives were identified: 1) To propose an algorithm to determine the minimum and K-best strategies in the second moment of travel time on STD networks. 2) To determine the minimum robust-cost strategy on STD networks using the procedure developed in the first objective. 3) To perform computational experiments to test and benchmark the solution quality against the existing procedures in the literature. This study contributes to the literature in the following aspects. First, an efficient procedure is proposed to solve for the strategy with the least second moment of travel time on STD networks. The procedure is polynomial and exact as it is based on the shortest hyperpath algorithm. Second, using the earlier procedure, a novel iterative bounds-based algorithm is proposed to determine the minimum robust-cost strategy on STD networks, for which algorithms are yet to be reported in the literature. It iteratively determines the candidate paths for the optimal and terminates when a sufficiency condition is met. The algorithm is proved to be exact and its complexity is shown to be parametrically polynomial. Third, the proposed algorithm can and was also applied to determine the minimum robust-cost path on the STD networks. Fourth, computational experiments were conducted to demonstrate the reasonable performance of the proposed procedure. Further, the sub-optimalities induced because of using expected travel time as objective and because of ignoring time-dependence are also empirically quantified.",9
17.0,3.0,Networks and Spatial Economics,22 June 2017,https://link.springer.com/article/10.1007/s11067-017-9346-1,A Path-Based Solution Algorithm for Dynamic Traffic Assignment,September 2017,Caixia Li,Sreenatha Gopalarao Anavatti,Tapabrata Ray,Unknown,Unknown,Unknown,Unknown,,
17.0,3.0,Networks and Spatial Economics,25 May 2017,https://link.springer.com/article/10.1007/s11067-017-9350-5,Managing the Ship Movements in the Port of Venice,September 2017,Elio Canestrelli,Marco Corazza,Raffaele Pesenti,Male,Male,Male,Male,"We introduce and investigate the Port Scheduling Problem (PSP), that is the problem of scheduling both ships’ and tugs’ movements in a congested canal harbor. Then, we provide a heuristic algorithm for its solution and we present some practical applications. Throughout the paper, we make reference to the Port of Venice in Italy since the recent reorganization of its terminals has motivated our study and our results were used to improve the Port services. The Port of Venice (Fig. 1), situated in the Venice lagoon, is constituted of two areas called Marghera and Marittima. The commercial terminals are in Marghera, whereas the passenger terminal is in Marittima, close to the city historical center.
 The Venice lagoon and the Port of Venice. Actually, the Port of Venice is a canal harbor. The lagoon has two mouths and, due to limited water depth, ships have to navigate inside it only along few waterways which are constantly dredged. Ships cannot be anchored within the lagoon to give way to other vessels. Currently, one mouth, Malamocco, is designated for merchant ships whereas the other mouth, St. Nicolò, is used for passenger ships. At these inlets, the mobile gates known as MoSE will be operative to protect Venice from high tides (Nosengo 2003). Most of the passenger ships transiting in the Port of Venice are mega cruise ships. Their movements cause controversy as they reach Marittima sailing the Giudecca canal in front of the St. Mark’s Basilica. Advocacy groups oppose these ships on air pollution claim. In addition, a new law, issued in response to wreckage of Costa Concordia in 2012 (Schröder-Hinrichs et al. 2012), forbids ships from sailing too close to heritage sites. As a consequence, since 2014, the access St. Nicolò mouth has been precluded to the largest cruise ships. Since then, cruise lines have either served Venice with smaller ships or simply eschew it resulting in a significant reduction of income for both the Port and the cruise industry. In fact, this ban on mega cruise ships has been fought in court. Furthermore, the recent economic downturn has also lead the navigation companies to be more selective in their port choices (Tang et al. 2011; Palacio et al. 2016) and container companies to cooperate with each others (Caschili et al. 2014) in order to reduce costs. In the above framework, the Venice Port Authority (PA) is reorganizing of the harbor terminals also in view of the possible disruption of the maritime traffic that the future operation of MoSE will cause. The PA is even considering the consequences of imposing new routes within the lagoon to passenger ships. Some of these new routes requires the excavation of new waterways or the widening of existing ones. This work introduces a model that the Ca’ Foscari University of Venice provided to support the decisions of PA. There is a vast literature on routing and scheduling problems in maritime transportation (Christiansen et al. 2013; Meng et al. 2013). In this context, routing refers to the sequence of ports to be called during a ship voyage, scheduling for the timing of each visit (Lohatepanont and Kongsermsup 2012; Gatica and Miranda 2011). Schedules are usually designed to be hedged against the uncertainties such as those caused by port congestion (Wang and Meng 2012). Differently, in the PSP, scheduling refers to the timing of ship movements within a harbor. From this viewpoint, the Berth Allocation Problem (BAP) (Lim 1998; Gharehgozli et al. 2015) presents more similarities with the PSP than the previous problems. The BAP aims at defining the allocation of berths to incoming ships. It takes into account spatial constraints such as ship size, and the distance between the berths and container yards. Different models and solution algorithms have been proposed for the BAP (Cordeau et al. 2005). Some of these tackle the impact of disturbances (Imai et al. 2007), scheduling constraints (Golias et al. 2009), tides (Du et al. 2015), service priorities (Imai et al. 2003). Other problems that present some similarities with the PSP are the ones that aim at scheduling vessels to berthing areas (Golias et al. 2010) or planning the vessel arrival times (Golias et al. 2009; Lang and Veenstra 2010). This literature considers scheduling problems in a maritime environment and congested harbors but, differently from the PSP, usually assumes that vessels can move almost freely from the roadstead to the berths. From this last viewpoint, PSP presents some similarities with the scheduling problems of ships transiting waterways (Ulusçu and Altıok 2009) or locks (Nauss 2008; Verstichel et al. 2011), e.g., in the Panama Canal (Franzese et al. 2011), in the Istanbul Strait (Ulusçu et al. 2009; Mavrakis and Kontinakis 2008), in the Welland Canal (Petersen and Taylor 1988). However, this literature considers linear waterways, whereas the Port of Venice has a more articulated structure with very specific constraints. Literature has also focused its attention on gas emissions due to ships’ maneuvering, e.g. (Chang et al. 2013; Chang and Wang 2012; Corbett et al. 2009; Liao et al. 2010; Qu et al. 2016). In this context, the Venice lagoon structure imposes that operations are carried on at reduced speed as suggested in Chang and Wang (2012) and Corbett et al. (2009). In addition, the PA and the Harbormaster’s (HM) are considering the possibility of providing cold ironing and of requiring the use of low sulfur fuel (Chang and Wang 2012). On the other hand, when MoSE gates are closed ships will reach the harbor passing through a lock at the Malamocco inlet. This maneuver may increase the gas emissions (Chang et al. 2013). However, their impact should be minimal given that the Malamocco inlet is about 10 km far from the city and the MoSE gates will close only in the days when the tide is particularly high. Thus, although the MoSE may disrupt the maritime traffic flow, hopefully it will not significantly increase the city pollution. The PSP may be seen as a generalization of train timetabling/scheduling problems when dealing with single track networks or terminal stations. Under this perspective ships correspond more or less to trains and harbor canals to tracks. However, constraints on ship movements are usually more complex than the ones on trains. For example, ships may require assistance from tugs and then must coordinate their movements with them. Despite these limitations, the literature on timetabling/scheduling problems is a possible starting point for the solution of the PSP. In Section 4, we introduce an algorithm inspired by the approaches considered in this literature, but adapted to the maritime environment. The papers by Lusby et al. (2011), Törnquist (2006), and Cordeau et al. (1998) survey the train timetabling/scheduling literature whereas, e.g., Ahuja et al. (2005), and Acuna-Agost et al. (2011) deal with stochastic disturbances, D’Ariano and Pranzo (2009), Carey and Lockwood (1995), and Cai et al. (1998) introduce some heuristic solution approaches, and Zhou and Zhong (2007) and Caprara et al. (2002) propose exact solution methodologies. It should be finally noted that there is not a clear line of demarcation between train timetabling problems and train scheduling problems. However, generally speaking, we can say that the former kind of problems aims at creating a timetable for a rail network, while the latter one is more operations-centric and aims at fixing the schedules of the movements of different trains when conflicts arise over resources. The PSP is more similar to the second kind of problems thus, in this paper, we will use the term scheduling when referring to the PSP. The rest of this work is organized as follows. In Section 2, we define the PSP problem. In Section 3 we present a model for the PSP. In Section 4, we introduce a solution algorithm for the PSP. In Section 5, we apply our solution algorithm to the PSP case for the Port of Venice. Finally, we draw some conclusions and propose some future developments in Section 6.",7
17.0,3.0,Networks and Spatial Economics,05 June 2017,https://link.springer.com/article/10.1007/s11067-017-9351-4,Self-Fulfilling Signal of an Endogenous State in Network Congestion Games,September 2017,Tatsuya Iwase,Yukihiro Tadokoro,Daisuke Fukuda,Male,Male,Male,Male,"Network congestion games (also called selfish routing games) model route choice behaviors of self-interested agents on a graph. One prominent application of network congestion games is in the studies on routing in transportation or telecommunication networks. These studies conclude that selfish behaviors by agents cause congestion and inefficiency in a network (Roughgarden 2005). Therefore, the coordination problem emerges among selfish agents to mitigate the inefficient use of the network and improve overall social welfare. One way to realize such coordination is to charge tolls on the congested edges (Yang and Huang 1998). However, most previous studies are based on the assumption that agents have complete information on other agent types and can therefore predict the flow of all edges in equilibrium. In contrast, agents sometimes make incorrect decisions in real situations because they do not have information regarding other agents in the network and regarding which routes are congested at any given moment. In such a case, incomplete information causes inefficiency (Angeletos and Pavan 2005). Signaling is a possible solution to eliminate the inefficiency arising from incomplete information by sending signals to agents to provide traffic flow information for all edges. When applied to transportation networks, car navigation systems or smartphones can be used to implement signals. However, this solution is not comprehensive because difficulties also arise due to congestion externalities. Since traffic flow is an endogenous variable of the game, a signal about a flow can change the outcome of the flow itself. For example, if a car navigation system reports that one of two routes is congested, agents will then choose the other route, which in turn is likely to become congested due to higher traffic volumes. This results in congestion oscillating between routes, which is called oscillation of traffic flow or hunting phenomenon (Klein et al. 2005; Fukuda et al. 2002; Wahlea et al. 2002). This oscillation generates outcome flows that differ from those informed by the signal, thus turning the system into an inadvertent liar. Designing a traffic flow signal that results in the same flow outcome as the pre-committed signal is necessary to avoid the oscillation. This signal is referred to as a self-fulfilling signal as it has the capacity to realize an outcome equal to the signal itself. Our goal is to design a self-fulfilling signal that mitigates the inefficiency caused by incomplete traffic flow information. We examine a self-fulfilling signal in a network congestion game to mitigate inefficiency caused by incomplete information. Our contribution is threefold. First, we formulate the problem of designing a self-fulfilling signal for an endogenous game variable. This problem formulation is general and includes many applications that have endogenous variables with incomplete information. Second, we show an instance of an asymmetric Gaussian signal that has self-fulfilling capability in multicommodity, atomic, and unweighted network congestion games with affine cost functions and Gaussian priors. This result provides a new alternative of endogenous information to the signaling scheme. Third, we employ examples of congestion games to verify the performance of a self-fulfilling signal in mitigating inefficiency caused by incomplete information. The results indicate the possibilities of coordination by signaling with endogenous information. Section 2 presents our model. Section 3 illustrates our main results regarding self-fulfilling signals in network congestion games. Section 4 describes numerical examples and their results. The idea of Wardrop’s User Equilibrium (UE) and System Optimum (SO) (Wardrop 1952) has been studied in the field of transportation, telecommunication networks and game theory (Altman and Wynter 2004). Since Rosenthal first introduced congestion games (Rosenthal 1973), many varieties of derivative games have been proposed, and significant results have been obtained regarding inefficiencies in equilibrium (see e.g., Roughgarden 2005; Yang et al. 2008, 2016) as well as regarding coordination of agents to optimize social welfare (see e.g., Christodoulou et al. 2009). However, most such studies assume complete information about traffic flow to calculate equilibria. A review of previous related studies reveals that the most popular coordination mechanism is toll collection (Yang and Huang 1998; Friesz et al. 2004). However, this approach introduces costs associated with establishing the collection infrastructure as well as requires charging responsibilities. Coordination between agents and emergence of consensus are also topics that have been extensively researched. Consensus algorithms (Olfati-Saber et al. 2007), El Farol bar problems (Arthur 1994), minority games (Challet and Zhang 1997), and multiagent Markov decision process (Littman 2001) are all examples of coordination problems among autonomous agents without any coordinator. These studies assume the involvement of a constant set of agents and that these agents iteratively play the same game, updating their strategies to avoid conflicts with other agents through reinforcement learning. However, scenarios also exist wherein agents fail to learn coordinated strategies. Former studies showed that reinforcement learning becomes computationally intractable and algorithm may not converge when agents do not have full observation of whole states, such as traffic of all edges (Goldman and Zilberstein 2004; Kaelbling et al. 1996; Zhang and Lesser 2012). In an extreme case wherein agents experience the game only once, such as when they drive to popular leisure venue on vacation, they do not have the opportunity to learn road traffic iteratively. Coordination problems with a mediator have also been examined in leader-follower games (Tharakunnel 2008) and Stackelberg games (Staňková et al. 2006). In these studies, a leader makes a decision that affects followers’ rewards such that followers are motivated to coordinate. However, most of these studies assume complete information on the state of the environment and that both the leader and followers can observe the state. This is not always true in real road networks. Recently, a new coordinating approach has been proposed where a mediator utilizes private information as a signal to persuade agents and realize coordination (Gentzkow and Kamenica 2011). Since this new signaling approach does not require infrastructure such as toll collections and thus is free from the associated initial costs, it has the potential to be quickly implemented in a range of applications, especially on the basis of widespread smartphone technologies or upcoming intelligent vehicles connected to the Internet. For example, Kremer et al. (2014) demonstrated an application of signaling to road networks. A sender collects traffic flow information for each road from individual drivers and subsequently sends all drivers collated information that pertains to their route choices. However, these studies assume that traffic flow is an exogenous variable and neglect congestion externalities that cause the oscillation of traffic flow. Bayes correlated equilibrium (Bergemann and Morris 2013) is a signaling technique that is able to manage externalities. However, it assumes that a signal is exogenous information and not endogenous information such as traffic flow. The oscillations of the game outcome have been examined in the context of stability analysis of repeated games with the theory of Nash dynamics (Fabrikant and Papadimitriou 2008) or evolutionary game theory (Hauert and Szabó 2005; Zhang et al. 2015). However, these studies are based on games among autonomous agents without any coordinators. Nakayama (2009) concluded that drivers’ observation errors accumulate through iterations in selfish routing under incomplete information and the traffic flow disperses over a network and becomes stable autonomously. However, this result seems optimistic when compared with reality. Klein et al. (2005) employs noisy signaling to manage the oscillation. However, their approach is heuristic and the performance is not guaranteed by a theory. Bottom (2000) formulated the oscillation problem as the anticipatory route guidance problem. They use variable message signs (VMS) to send drivers full traffic information. The information is limited only to drivers who pass through the nodes with VMS. This information control splits traffic into several paths and suppresses the oscillation. However, the information control is not flexible compared with the signaling approach that allows partial information disclosure with arbitrary signal distributions. Engelson (1997) proposed a deterministic version of our self-fulfilling signal approach. However, such a deterministic signal sometimes causes unstable traffic and fails to suppress oscillation as shown in the later section of this paper. There are other studies on advanced traveler information systems (ATIS) that suppress the oscillation problem (Ben-Akiva et al. 2001; Paz and Peeta 2009). However, their simulation-based approach are not analytical and do not guarantee the convergence of oscillations. Studies on traffic engineering reveal that the oscillation can actually be observed in real road networks (Fukuda et al. 2002; Kanamori et al. 2012). Their approach against the oscillation is also heuristic and evaluated by a simulation in a specific case. Despite the inroads made by the studies above, the oscillation remains an open problem. We therefore propose a novel theoretical approach for managing this open real-world issue in this study. We consider situations such that agents do not have full observability, and then have biased beliefs on traffic as a result of imprecise adaptive learning.",1
17.0,3.0,Networks and Spatial Economics,04 June 2017,https://link.springer.com/article/10.1007/s11067-017-9352-3,Air and HST Multimodal Products. A Segmentation Analysis for Policy Makers,September 2017,Juan Gabriel Brida,Juan Carlos Martín,Raffaele Scuderi,Male,Male,Male,Male,"Since the early steps of High Speed Rail (HSR), the relationship between this new transport mode and air transport has extensively been studied in the literature. The appearance of HSR has been considered as an authentic transport innovation or as an incremental innovation, especially in the context of the interurban transport mode. However its global diffusion has not been universally accepted, if compared to other interurban transport technological innovations as private cars or air transport (Bonnafous 1987; Loukaitou-Sideris et al. 2012; Vranich and Cox 2013; Liu et al. 2013). Most of previous literature has analyzed HSR and air transport as competitors, thus neglecting the potential cooperation between these two transport modes (González-Savignat 2004; de Rus and Román 2006; Román and Martín 2010; Román et al. 2010; Yang and Zhang 2012a, b; Qu et al. 2016). Such competition is directly affected by different factors. A not exhaustive list of them includes whether trip’s origin and destination is placed near the city centers, location of airports, access to transport terminals, airport security controls vs. less restrictive accessibility to train stations, and finally public subsidies to railway infrastructure vs. full cost-recovery policy of airlines and airports. Despite this, a recent trend of cooperative behavior is challenging the fierce competition between HSR companies and airlines. Examples are given by some SNCF TGV trains, which complement the Air France network for the flight between Brussels and Paris Charles De Gaulle airport and other French national flights, or several German ICE trains, which carry a fixed number of seats from Lufthansa passengers under code-share agreements. Unfortunately, the empirical or theoretical research on this topic is still very scarce (Yang and Zhang 2012a, b), and some of the exceptions are very recent (Albalate et al. 2014; Clewlow et al. 2012; Chiambaretto and Decker 2012; Dobruszkes et al. 2014; Grimme 2007; Jiang and Zhang 2014; Román and Martín 2014). Albalate et al. (2014) find that direct competition between HSR and airlines is still dominant, but they also provide evidence about HSR to feed services to long haul air markets in hub airports, particularly in those with HSR stations. Similarly, Clewlow et al. (2012) suggest that HSR might serve as complementary mode to relieve congestion at airports (see Sun et al. 2017) by providing short-haul services in support of longer-haul airline services. They conclude that HSR lines are likely to serve as successful feeders for international air traffic at Frankfurt Airport and at Paris-Charles de Gaulle (Paris-CDG).Footnote 1 Chiambaretto and Decker (2012) find that intermodal agreements between airlines and rail operators are increasingly prominent features of the transport landscape, as they offer a number of potential advantages for airlines, rail operators, intermodal airports and consumers of transport services. They also take advantage of strong political support, particularly in Europe, in part because of the perceived contribution they can give to the achievement of environmental policy targets (Aziz et al. 2016). However, this assessment is not exempt from controversy. In fact, even when the intermodal integration is successful its environmental benefit appears to be marginal, if not negative, if airport capacity is reused for longer flights. In the current context, such integration sounds more like a business opportunity for airlines, airports and train operators than a sustainable option (Dobruszkes and Givoni 2013). Dobruszkes et al. (2014) find that shorter HSR travel time involves less air services, with similar impact on both airline seats and flights. This impact quickly drops between 2.0- and 2.5-h HSR travel times. The impact of HSR frequencies is much more limited. They do not find any significant impact by the integration of airlines and HSR at the airports. They conclude that metropolitan and national spatial patterns may help to better understand intermodal effects. Grimme (2007) also illustrates cooperation by analysing the case of AIRail, an integrated ticketing and baggage handling service offered by Deutsche Bahn (rail operator), Lufthansa (air carrier) and Fraport (airport). Jiang and Zhang (2014), extend a theoretical model for the analysis of the effects of code-share agreements of airlines or airlines alliances. They find that social welfare of air-HSR integration is not affected by whether the hub capacity is constrained or not, as long as the cross-elasticities of both transport modes are not significant in the overlapping markets. If this is not the case, then welfare is very dependent on the constraints of hub capacity to obtain welfare gains or losses. Román and Martín (2014) find disutility associated to the change of transport modes in the intermodal station. Thus, attractive intermodal alternatives need to be based on some sort of compensation in terms of in-vehicle, connecting, and access time, rather than baggage integration. Meanwhile, fare integration is also very valued. Dargay and Clark (2012), Valeri (2013) and Valeri et al. (2015) conduct empirical analyses that depict relevant implications of transport on tourism. The aim of this paper is to detect different market segments for the Air-HSR intermodal transport in Madrid Barajas airport, and specifically to analyze the characteristics of the group of costumers showing higher propensity to use it. We focus on the estimation of passengers’ preferences for different clusters of travellers. The main interest goes beyond the understanding of individual preferences. Our scope is to propose a more general market segmentation that could be used by transport planners to expand the penetration of this new intermodal alternative. The success of Air-HSR integration will depend on ambitious programs that need to be implemented from a transport planning perspective (Page et al. 2010). These programs require coordination, funding, and above all, adequate marketing campaigns that show the benefits of this new alternative to existing specific market niches. In a previous work concerning the same market, Román and Martín (2014) detected a high degree of random heterogeneity in passengers’ preferences. This result suggests investigating about possible groups of passengers with different attitude towards integrated multimodal services. To this end, in this research we apply cluster analysis techniques in order to detect such groups. We base our analysis on information about travellers’ preferences for this new transport alternative with respect to some basic attributes, like ticket integration, ground-handling integration, price, transfer, and travel time. Considered variables are key drivers for developing a competitive intermodal alternative. Market segments from cluster analysis are then studied with a twofold scope. First, we try to detect differences in preferences as expressed by the willingness-to-pay for a set of attributes characterizing this transport alternative. Second, we analyze the influence of a set of attributes on groups discrimination, with the scope of detecting significant variables that may promote intermodality at Madrid Barajas airport. Our findings are targeted to help planners, policy makers, and air transport and HSR operators to implement the ambitious coordinated programs for this new intermodal alternative. The next section briefly discusses to what extent HSR and air transport have been competing and are nowadays cooperating more than in the past. Section 3 presents the data, and explains how the experiment was conducted. Section 4 presents cluster analysis. We used a robust classification method, namely k-medoids, and subsequent analysis of the determinants of the two-cluster solution. As the results of a cluster analysis may be sensitive to the set of considered variables included, we performed several classifications of subsets from an initial set of both qualitative and quantitative variables. We then selected the classification that better explained the preferences for the Air-HSR alternative, according to Anova classification. Then the paper proceeds with Section 5, which shows the results from discrete choice models where obtained clusters are explicitly put into account, as well as an assessment of the willingness to pay, the elasticities and a discussion on policy implications. Finally, Section 6 depict some suggestions about what has to be done in order to promote the competitiveness of this new transport alternative.",10
17.0,3.0,Networks and Spatial Economics,30 May 2017,https://link.springer.com/article/10.1007/s11067-017-9353-2,On the Equivalence Between SUE and Fixed-Point States of Day-to-Day Assignment Processes with Serially-Correlated Route Choice,September 2017,Paolo Delle Site,,,Male,Unknown,Unknown,Male,"Day-to-day dynamics, whereby the evolution over days of travel choice and traffic congestion is linked through a learning model based on driver’s past experience, has attracted considerable attention in the last decades. Day-to-day dynamics can be formulated within a continuous and a discrete time framework (Cantarella and Watling, 2016a, have recently provided a unifying framework), the latter only is considered in this paper. Deterministic processes were the first to be tackled. A distinctive feature is that route flows are regarded as deterministic variables. After the initial contribution by Horowitz (1984), research has dealt, in particular, with the relationship with Stochastic User Equilibrium (SUE) and stability analysis (Cantarella 1993; Watling 1999; Watling and Hazelton 2003; Bie and Lo 2010; Zhao and Orosz 2014; Xiao and Lo 2015; Cantarella and Watling 2016b; Guo and Huang 2016). The solutions of the fixed point problem termed SUE, including logit and probit SUE, are the fixed point states of the assignment process. A slightly different approach has been developed by Guo et al. (2013) who consider the link flow dynamics in lieu of the route flow dynamics. Other authors have considered route flows as stochastic variables. This alternative assumption can be justified on different grounds as discussed in Watling (2002a). One of the approaches followed is that of stochastic processes, which considers the day-to-day evolution of the probability distribution of route flows regarded as integer variables (Cascetta 1989; Davis and Nihan 1993; Cantarella and Cascetta 1995; Watling 1996; Watling and Hazelton 2003; Hazelton and Watling 2004; Watling and Cantarella 2015; Parry et al. 2016). The other approach is the generalisation of SUE proposed by Watling 2002a, 2002b), in which stochasticity in both flows and costs is endogenous to the equilibration process. The present paper deals with deterministic processes. The paper aims to formulate a theory of deterministic assignment processes able to take explicitly into account the day-to-day correlation of the random terms of the route choice model, and to derive a characterisation of SUE within this theory. Since Daganzo and Sheffi (1977), SUE has been used for its potential to overcome the perfect knowledge assumption of classical Deterministic User Equilibrium (DUE) derived from Wardrop’s first principle (Wardrop 1952). Imperfect knowledge of the network, as it occurs in the absence of Advanced Traveller Information Systems (ATIS), and the associated heterogeneity in the perception of route travel times justify SUE providing strictly positive flows for all routes. A number of contributions have tackled formulation and algorithmic aspects for SUE, many others have used SUE for applications without and with the presence of ATIS (having assumed the perception variance to be related to the information quality), or proposed extensions to SUE. SUE is directly formulated as fixed-point problem, equivalence has been proved with minimisation and variational-inequality problems (see, respectively, Fisk, 1980, and Lo and Szeto, 2001). It is beyond the scope of the paper to provide a full account of literature related to algorithmic aspects, applications and extensions of SUE. To name a few, Powell and Sheffi (1982) and Liu et al. (2009) on the Method of Successive Averages (MSA). Nielsen et al. (2002) for a wide-scale application, Huang and Li (2007) and Huang et al. (2011) for applications in the presence of ATIS, Fan and Liu (2010) on network protection. Uchida et al. (2007) on extension to multimodal route choice, Unnikrishnan and Waller (2009) on extension to en-route choices, Karoonsoontawong and Lin (2015) on combined destination and route choices. The research here is motivated by the desire to address a few limitations of the classical framework of assignment processes (a comprehensive presentation is in Cantarella and Cascetta, 1995; see Appendix 1 of the present paper for a formal review of the relevant assumptions). This framework builds on the formulation of a choice updating process with an implicit Markovian assumption. Probabilities of choosing a path at a given day conditional on the choice of the path in the previous day are at the cornerstone of the framework (these probabilities are called transition probabilities and the associated matrix transition matrix). The Markov assumption makes it possible to derive the probability of the sequence of choices. Based on Markov chain theory (Norris 1997), sequence probabilities are given by a product of conditional probabilities (factoring property). In addition, in the classical framework of day-to-day dynamics, conditional probabilities are usually assumed to be independent of the path chosen the previous day. This eventually implies day-to-day independence of the random terms and no state dependence of the systematic utilities. The random term independence assumption is discussed in Watling and Hazelton (2003) and Watling and Cantarella (2013). They observe that the assumption is somewhat unrealistic as one may expect that traveller’s personal preferences for a particular route would persist from day to day. They remark that different approaches have been proposed to deal with this problem. Watling and Cantarella (2013) discuss, in particular, proportional re-routing, whereby, based on an exponential filter, only a fraction of travellers re-route each day according to the choice model. This approach is found in several papers (Cantarella 1993; Cantarella and Cascetta 1995; Hazelton and Polak 1997; Polak and Hazelton 1998; Cantarella and Watling 2016b). Another approach, suggested by Polak and Hazelton (1998), considers that re-routing occurs only if the traveller perceives an alternative that has utility at least s units higher than her present route. This approach has similarities with state-dependence, whereby the systematic utility of the present route is assigned an extra-utility to represent the additional psychological cost of switching, as in the model by Cascetta and Cantarella (1991). The framework that is proposed in the present paper moves a step backwards: instead of starting from a choice updating model with an implicit Markovian assumption, it models the random term day-to-day updating process that occurs together with the adjustment of systematic utilities. It then derives the consequent choice process which, in general, is not Markov.Footnote 1
 The random term updating process is able to deal with habit persistence, because it assumes day-to-day statistical dependence. The prevailing interpretation of random utility models regards the random terms as individual specific, i.e. random terms account for inter-personal heterogeneity (since McFadden, 1981). Each individual, when faced with a sequence of choices, may change her random term vector. Independence across choices is an un-necessarily restrictive assumption, since some degree of serial correlation is likely to exist because of temporally persistent unobservables and tastes. At the other extreme, we have the perfect correlation assumption, whereby random terms are unchanged across choices. Realistically, day-to-day correlation will lie somewhere between independence and perfect correlation. Day-to-day dependence of the random terms has impacts on the joint probability, i.e. the probability of choosing alternative i the day before and alternative j the day after. We will refer to this probability as transition, or switching, probability, consistently with the convention adopted in discrete choice literature, in particular by De Palma and Kilani (2005, 2011). Transition probabilities in discrete choice models are investigated in a stream of research that is ultimately concerned with the implications on welfare measures. De Palma and Kilani (2005 and 2011) provide analytic functional forms of transition probabilities, for logit and general random utility models, under the assumption that random terms remain unchanged. Delle Site and Salucci (2013) extend the framework provided in De Palma and Kilani (2011) to imperfect before-after correlation and review numerical methods for computation. In a subsequent paper, Delle Site and Salucci (2015) provide analytic functional forms of transition probabilities for general day-to-day correlation patterns in the case of logit. In the conventional day-to-day dynamics framework, when conditional probabilities are assumed independent of the choice made in the previous day, re-routing is quantified by transition probabilities given, in the light of the independence assumption, simply by the product of the probability of choosing alternative i the day before times the probability of choosing alternative j the day after. Transition probabilities have a different value if the independence-across-days assumption is removed. The paper recasts the theoretical framework of deterministic processes. At the lower level, we have the sequence of systematic utilities, based on the learning filter, and the stationary stochastic process of the random terms. The paper will show different options that can be used to define this stochastic process. These will allow to derive logit and probit SUE from the new framework. At the upper level, as a consequence of the lower levels, we have the stochastic process of route choice which gives rise to route flows. The paper presents this new framework and shows the noteworthy implications in terms of interpretation of classical logit and probit SUE. Transition probabilities play a key role in this respect. The organisation is as follows. Section 2 presents notation and assumptions. Section 3 presents the consequent properties of assignment processes and the derivation of SUE. Section 4 provides two illustrative numerical examples.",7
17.0,3.0,Networks and Spatial Economics,13 July 2017,https://link.springer.com/article/10.1007/s11067-017-9355-0,An Alternative Approach for Solving the Environmentally-Oriented Discrete Network Design Problem,September 2017,Inbal Haas,Shlomo Bekhor,,Female,Male,Unknown,Mix,,
17.0,3.0,Networks and Spatial Economics,05 July 2017,https://link.springer.com/article/10.1007/s11067-017-9356-z,A General Equilibrium Framework for Integrated Assessment of Transport and Economic Impacts,September 2017,Edward N. Robson,Vinayak V. Dixit,,Male,Unknown,Unknown,Male,"In the strategic transport planning process, working with separate transport and economic models can be costly and time-consuming. Design options must be iterated through models, limiting the number that can be tested, and each model makes assumptions about the other. Integrated models, where the transport and economic models are formulated together or are in direct communication, are increasingly viable with modern computing. These have the potential to capture a wider range of effects, provide faster results and generate a richer set of outputs. Under a cost benefit analysis (CBA) framework (see Transport for NSW (2013) for example), improvements to transport networks are calculated through user generalised cost savings, agency cost savings and ad-hoc measurements of environmental externalities. This approach works well if there are no technological externalities and the distribution of impacts is not important (Bröcker et al. 2010). In particular, the typical CBA methodology assumes a partial economic equilibrium where markets external to transport, such as labour and land-use, remain static. When a project is sufficiently large that technological externalities become significant, wider economic impacts, as described in Lakshmanan (2011), require assessment with other models. Computable general equilibrium (CGE) models simulate the function of entire economies with microeconomic behavioural functions. These represent the actions of households, firms and others, solved simultaneously to trace non-linear relationships throughout an economy. A key advantage is that nearly any measure of economic impact can be derived, including consumption, production, prices and welfare, as the model is built from the utility and production functions of its agents. Naturally, building a CGE model requires assumptions about the aggregate behaviour of households and firms, large datasets and, depending on the size of the model, substantial computational resources. Existing CGE models for transport have been applied in policy analysis (Kishimoto et al. 2014), infrastructure evaluation (Siegesmund et al. 2008) and spatial equity analysis (Koike et al. 2009). In these studies, the transport network and its performance were taken as exogenous; that is, transport costs were generated from external models and did not incorporate feedback. Transport network modelling requires its own economic assumptions. In the traditional four-step process, origin–destination matrices are determined from fixed land-use patterns derived externally. When network models account for economic effects, for example in combined distribution and assignment models (Boyce 2007), it is again on a partial equilibrium basis as the utility functions do not account for changes outside the transport network. Land-use transport interaction (LUTI) models (Wegener 2004) and other urban simulation models (Waddell et al. (2003) for example), however, simulate transport networks and land-use together. These models accommodate a wide range of behaviours and are spatially detailed, but are driven by empirical relationships. Recent examples have been mathematically integrated and formulated together as the one problem (Briceño et al. 2008), but without a microeconomic basis, it is difficult to determine welfare and replicate price mechanisms (Bröcker and Mercenier 2011). This paper develops an integrated spatial CGE model and transport network model as an introductory step towards a CGE economic appraisal model for transport networks. The model extends the literature by simulating the impacts of transport network changes on both households and production, solved together within the one model. Household trips, including leisure trips, are generated from economic activity and assigned to the real transport network as calibrated from existing strategic transport models, returning travel costs to the economic model. From an economics perspective, this provides a rapid appraisal of transport network changes by endogenously calculating transport costs. From a transport modelling perspective, the CGE framework generates and distributes trips for traffic assignment. A CGE model is used since welfare is measured at the household level, rather than the transport level, providing a natural structure to include externalities. The model will help planners and decision makers evaluate the wider economic impacts of transport projects in combination with conventional impacts. The following sections provide a literature review, model description and demonstration of how the model can be calibrated and applied to real data for the Sydney transport network.",17
17.0,3.0,Networks and Spatial Economics,03 July 2017,https://link.springer.com/article/10.1007/s11067-017-9357-y,The q-Ad Hoc Hub Location Problem for Multi-modal Networks,September 2017,Hyun Kim,Megan S. Ryerson,,,Female,Unknown,Mix,,
17.0,3.0,Networks and Spatial Economics,29 June 2017,https://link.springer.com/article/10.1007/s11067-017-9358-x,Multicriteria Stochastic Shortest Path Problem for Electric Vehicles,September 2017,Ehsan Jafari,Stephen D. Boyles,,Male,Male,Unknown,Male,"Electric vehicles (EVs) are promising solutions to pollution issues and dependency of transportation network on fossil fuel. These vehicles, however, face two major issues: the scarcity of charging stations and long charging time (Huang et al. 2015). The driving range of EVs is less than a conventional gasoline fuel vehicle, and may need to be recharged at public charging stations. The number of public charging stations is growing but still they are scarce and not accessible easily. In addition, depending on type of the charger, it may take up to 10 hours to fully recharge the battery from an empty state. Another important concern of the EV users is variation in energy consumption which may result in vehicle running out of charge before reaching destination or the next charging station. This issue is called range anxiety (Adler et al. 2016). Gasoline vehicles, however, have easy access to the fuel stations through the network and the refueling time has a very marginal impact on travel cost. As a result, the routing problem for an EV is different from gasoline vehicles, and the EV owner needs to consider different factors such as state of charge (SoC), probability of running out of charge, location of charging stations, recharging time, and electricity price when planning their trip. In this study, we model the a priori routing of a single EV in networks with stochastic travel times. There are charging stations with different electricity prices and charging rates located in the network. We assume that charging stations have enough capacity so that the waiting time is negligible (extending the model to incorporate the waiting times at charging stations is straightforward.) The charging cost includes the charging time and electricity price. To represent the variations in energy consumption and address the issue of range anxiety, we assume stochastic arc energy consumptions, formulated as a linear function of arc length and arc travel time. The length variable defines the energy consumed for traversing the arc, and the time component captures variations in the energy consumption because of the time spent to travel the arc. Specifically, as noted by Lave et al. (1995), adjusting the cabin temperature, through either heater or air-conditioning, has an enormous impact on battery range and as the travel time increases this issue gets more prominent. In addition, the performance of battery highly depends on the temperature (Pesaran et al. 2003), and the variability in travel time, especially in summer or winter times, can significantly influence the driving range. The dependency of arc energy consumption on travel time (which is stochastic) in addition to distance traveled implies that arc energy consumption is not pre-determined and depending on the realized value, the vehicle may run out of charge if initial level of charge is not sufficient. A path is defined as a sequence of arcs between the origin and destination nodes with information about which charging stations to be visited and how much charge to be taken at each of them. The traveler aims to minimize the generalized cost of travel, formulated as a weighted sum of arc travel time and charging cost, subject to a minimum reliability threshold (MRT). The MRT is user-dependent and specifies the risk attitude of the traveler: a risk-prone individual may be willing to reduce the cost by selecting less reliable paths (paths with higher chance of running out of charge), while a risk-averse user may choose a more reliable path with a potentially higher cost. The travel time, charging cost, and reliability values of a path are formulated as functions of arc travel time distributions. We refer to travel time, charging cost, and reliability values of a path as path labels. To avoid excess calculations and to use previous information, we re-write the path labels as a function of the labels of their immediate subpaths. We show that starting from a transfer node (node with no charging capacity), just one path can be constructed as extension of a specific subpath, while from a charging node and depending on the set of recharging choices, multiple paths can be constructed. The inconvenience and delay caused due to running out of charge is modeled via a high time penalty factor M. This penalty term represents the additional time it takes to tow the vehicle to a charging station. We show that the resultant problem violates the monotonicity assumption (Bellman’s principle of optimality), which implies that subpaths of an optimal path may not necessarily be optimal. This makes the problem more involved, since solution algorithms based on dynamic programming (DP) may fail to find the optimal solution. Here we propose a solution algorithm based on the idea of generalized DP (GDP) which uses the weak principle of optimality (Carraway et al. 1990). The weak principal of optimality suggests that an optimal path must be composed of subpaths that can be part of an optimal path. These paths are called Pareto optimal paths. The properties of the Pareto optimal paths are then investigated. Specifically, it is shown that an optimal path can not traverse a cycle with zero recharging amount, and that the number of times that a cycle with positive recharging amount can be visited is finite. Also we prove that algorithm terminates after a finite number of iterations. The main contribution of the paper is proposing a mathematical model that takes into consideration the stochasticity of energy consumption. As discussed before, the uncertainty and variation in energy consumption results in range anxiety which is one of the major barriers facing the market penetration of these vehicles. In this paper, we formulate the time, cost, and reliability of a path (path labels) by modeling the energy consumption stochasticity, and later propose simple formulas to compute path labels using information from immediate subpath instead of evaluating complex integrals. The properties of the proposed routing algorithm are mathematically proved and then investigated on randomly-generated networks. For the rest of the paper, we summarize related studies in Section 2. Section 3 discusses the notation used in the paper and presents the problem formally. In Section 4, Pareto optimal paths are introduced and their properties are discussed. The proposed solution algorithm is discussed in Section 5. Section 6 presents the numerical analysis, and Section 7 concludes the paper with some remarks on future research.",18
17.0,4.0,Networks and Spatial Economics,18 December 2017,https://link.springer.com/article/10.1007/s11067-017-9380-z,Lagrangian-based Hydrodynamic Model for Traffic Data Fusion on Freeways,December 2017,Ke Han,Tao Yao,Terry L. Friesz,,,,Mix,,
17.0,4.0,Networks and Spatial Economics,14 December 2017,https://link.springer.com/article/10.1007/s11067-017-9379-5,Continuity of the Effective Delay Operator for Networks Based on the Link Delay Model,December 2017,Ke Han,Terry L. Friesz,,,,Unknown,Mix,,
17.0,4.0,Networks and Spatial Economics,19 August 2017,https://link.springer.com/article/10.1007/s11067-017-9359-9,Component Importance Measures for Multi-Industry Vulnerability of a Freight Transportation Network,December 2017,Mohamad Darayi,Kash Barker,Joost R. Santos,Male,Unknown,Male,Male,"In response to the growing vulnerability of critical infrastructure given their exposure to natural hazards, malevolent attacks, and the challenges of aging, the Presidential Policy Directive on Critical Infrastructure Security and Resilience (PPD-21) (White House 2013) was established to focus national efforts to enhance the critical infrastructure network resilience. 
The Nation’s critical infrastructure provides the essential services that underpin American society. Proactive and coordinated efforts are necessary to strengthen and maintain secure, functioning, and resilient critical infrastructure – including assets, networks, and systems – that are vital to public confidence and the Nation’s safety, prosperity, and well-being.
 Presidential Policy Directive/PPD-21: Critical Infrastructure Security and Resilience (The White House 2013) Among the critical infrastructures defined by the US government are transportation networks, which are vital to a society and support many economic activities including commerce and tourism. Disruptions triggered by natural hazards, human-made events, or common failures can severely compromise a region’s ability to move people and commodities, consequently leading to irrecoverable economic losses as well as public safety concerns. Many recent large-scale examples highlight the growing need to deal with disruptions: Hurricane Sandy that affected multiple infrastructure networks, including downed power lines and massive flooding on New York and New Jersey roadways and one million cubic yards of debris that impeded transportation networks (Lipton 2013); the August 2003 US electric power blackout that caused transportation network disruptions (Minkel 2008); and Hurricane Isabel that adversely impacted the transportation system of the Hampton Roads, VA region in 2003 and overwhelmed emergency response (Smith and Graffeo 2005). The current state of disrepair of the US transportation network (e.g., roads given an American Society of Civil Engineers Infrastructure Report Card grade of D, bridges a C+, inland waterways a D- (ASCE 2013a)) could make the network especially vulnerable to a disruptive event. The situation is no better for the state of Oklahoma, where bridges in particular received a lower letter grade of D+, which by definition is interpreted as a “poorly performing” infrastructure (ASCE 2013b). Recent US planning documents focus on transportation network preparedness (The House Committee on Transportation and Infrastructure 2013; US Department of Transportation 2014; Yusta et al. 2011), emphasizing “securing and managing flows of people and goods” along transportation networks (DHS 2014). The physical freight transportation network of the US, the largest in the world, consists of four million miles of public roads, 140,000 miles of railroad tracks, 11,000 miles of navigable waterways, and a network of airports with the combined ability of shipping almost 68,000 tons of cargo per year (U.S. Department of Transportation 2013). Furthermore, the same document highlights the importance of the US transportation network in facilitating the convenient movement of resources among suppliers, manufacturers, wholesalers, and customers, with more than 300 million people and 7.5 million organizations across 3.8 million square miles being served. The vital role the freight network plays in transporting raw materials and final products between manufacturers and consumers highlights its position in commerce. The functionality of this network is threatened by disruptive events that can disable the capacity of the network to enable flows of commodities and cause an interruption of economic productivity across multiple industries. That is, the ultimate usefulness of understanding transportation network disruptions is not just a descriptor of physical damage, but of economic interruption due to infrastructure inoperability (Tierney 1997, Webb et al. 2000). As such, discussions of transportation network vulnerability should account for multi-industry impacts. This work focuses on the freight transportation network, particularly on its role of enabling the flow of commodities and facilitating economic productivity, and thus a methodological approach to measure network vulnerability in the context of multi-industry impacts is sought. That is, this work seeks to answer: if a transportation node or link is disrupted, what is the effect on local industries? This research addresses (i) measuring the vulnerability of a multimodal freight transportation network with multi-industry impacts in mind, and (ii) using this vulnerability analysis to develop a measure of importance for each network component. This paper is arranged as follows. Section 2 offers some background literature regarding the vulnerability and economic impacts of transportation networks. Section 3 describes the proposed methodology for freight transportation network vulnerability analysis integrating a multi-commodity network flow formulation with a risk-based economic interdependency model. Also, a new network component importance measure based on multi-industry vulnerability analysis is developed. Section 4 presents an illustrative example based on a partial freight transportation network within the state of Oklahoma consisting of three important business economic areas and the multi-modal freight network infrastructure that facilitates trade with centers out of the state. Section 5 provides concluding remarks and future research avenues of this work.",30
17.0,4.0,Networks and Spatial Economics,08 August 2017,https://link.springer.com/article/10.1007/s11067-017-9361-2,Identifying Critical Components of a Public Transit System for Outbreak Control,December 2017,András Bóta,Lauren M. Gardner,Alireza Khani,Male,,Unknown,Mix,,
17.0,4.0,Networks and Spatial Economics,25 September 2017,https://link.springer.com/article/10.1007/s11067-017-9363-0,Efficiency and Stability in Electrical Power Transmission Networks: a Partition Function Form Approach,December 2017,Dávid Csercsik,László Á. Kóczy,,Male,Male,Unknown,Male,"In liberalized markets for electric power the transmission of energy involves complex interactions between consumers, producers and network operators. During the operation of the transmission system, the objectives of efficiency and social welfare maximization compete with the aims of profit maximization of individual players and investors (Kirschen and Strbac 2004). Besides this rich strategic interaction one must not overlook the laws of physics that govern the flow of electric current in such a power grid: The formation of a coalition will generally change the load on the network, changing the congestion level and will therefore affect other players: generators and consumers as well. We study the interaction between the physical and economic aspects of the power transmission system operation focussing on the incentives for group formation. We define a cooperative game to describe the interactions in a simplified model of the electrical energy market, considering both physical and economical aspects of the participants and the network itself. The game is defined in partition function form (Thrall and Lucas 1963) to allow for the explicit modeling of widespread externalities: the effects on third parties not directly involved in the formation of coalitions. Despite the flexibility of this game form, few studies have used it in applications. Funaki and Yamato (1999) model a pooled resource problem, such as a fishing pond with competing fisheries. Chander and Tulkens (1997) study the problem of international environmental agreements, Bando (2012) analyzes matching with externalities. Unfortunately, the complexity of the model does not allow for general analytical results. Also here, when studying the formation of balancing groups in electric power networks our contribution is the description of a model that accounts for the rich interaction of generators and consumers, of economics and physics as well as the demonstration that such games may have somewhat unexpected properties. We are not the first to study power transmission networks from an economic point of view, but most of the research has insofar focussed on the topics of competition, market power and regulation (Gilbert et al. 2004; Neuhoff et al. 2005) and market structures (Smeers 2003a, b). There are furthermore results using large-scale optimization approach to capture the complexity of the problem (Leuthold et al. 2012). Only a few authors study the market and the transmission issues in their whole complexity (Kirschen and Strbac 2004) and only a couple of papers use game theory for studying transmission or energy pricing (Hobbs 1992; Bai et al. 1997; Metzler et al. 2003; Neto et al. 2016; Csercsik 2016), expansion analysis related to market power (Zerrahn and Huppmann 2017) or market coupling (Oggioni et al. 2012) in similar models. A Nash-Cournot approach including discrete variables as well is described by (Gabriel et al. 2013). There are in addition results on multi-agent stochastic approaches of the problem as well (Guo and Fan 2017). A multi-period equilibrium approach is described by (García-Bertrand et al. 2005). Hobbs (1992) uses static cooperative models to calculate the possible outcomes of short-run transmission games and noncooperative Stackelberg games to model long-run games with transmission capacity as the decision variable. Bai et al. (1997) describe an open access transmission method for maximizing profits in a power system with transmission losses. They use a Nash bargaining game for power flow analysis in which transactions and optimal prices are driven by the interests of individual parties. Gately (1974) was probably the first to apply cooperative game theory to planning investments of electrical power systems using the core and the Shapley value to determine the mutually acceptable set of final payments. Evans et al. (2003) describe a cost assignment model for electrical transmission system expansion is using kernel theory. Lately cooperative game theory has been applied for the analysis of the transmission expansion problem both in the case of centralized and decentralized environment (Contreras 1997; Contreras and Wu 1999; Contreras et al. 2009). Our approach differs fundamentally from these studies as we explicitly model externalities by using the partition function form. The structure of the paper will, accordingly, be as follows: First we describe the physical properties of the network and explain the simplified model to derive the partition function form game. We also recall the recursive core that is used to find stable, that is, undominated payoff configurations. The main part of the paper is Section 3, where, by means of a series of examples we demonstrate some unexpected properties that might provide incentives for network participants to go against the usual trends of network development such as increased levels of integration. We close with a brief summary and a set of open questions.",6
17.0,4.0,Networks and Spatial Economics,21 September 2017,https://link.springer.com/article/10.1007/s11067-017-9364-z,A Novel Model for the Time Dependent Competitive Vehicle Routing Problem: Modified Random Topology Particle Swarm Optimization,December 2017,M. Alinaghian,M. Ghazanfari,H. Nouralizadeh,Unknown,Unknown,Unknown,Unknown,,
17.0,4.0,Networks and Spatial Economics,09 September 2017,https://link.springer.com/article/10.1007/s11067-017-9365-y,Complex Interactions in Large Government Networks,December 2017,Shilei Wang,,,Unknown,Unknown,Unknown,Unknown,,
17.0,4.0,Networks and Spatial Economics,11 September 2017,https://link.springer.com/article/10.1007/s11067-017-9366-x,Exploring Bikesharing Travel Patterns and Trip Purposes Using Smart Card Data and Online Point of Interests,December 2017,Jie Bao,Chengcheng Xu,Wei Wang,,Unknown,,Mix,,
18.0,1.0,Networks and Spatial Economics,07 August 2017,https://link.springer.com/article/10.1007/s11067-017-9354-1,The Complexity in the Study of Spatial Networks: an Epistemological Approach,March 2018,Dimitrios Tsiotas,Serafeim Polyzos,,Male,Male,Unknown,Male,"Modeling communication systems is an important and complex process. Its importance is related to the immanent human need of communication, whereas its complexity regards the variety of factors contributing to their configuration and evolution. Provided that geography sets inevitable constraints in the conduct of communication, many of such systems can be considered as spatial. In terms of the Network Science (NS) (Barthelemy 2011; Barabasi 2013; Brandes et al. 2013; Ducruet and Beauguitte 2014), which is an emerging discipline using the network paradigm to model complex communication systems as pair-sets of interconnected nodes and their linkages (edges), the communication systems embedded in the physical (geographical 2d or 3d) space are defined as spatial networks. For a network, the spatial property is accompanied with a set of characteristics affecting either its structural configuration, or its functionality, or its attributes, or even its evolution (Barthelemy 2011; Ducruet and Beauguitte 2014). For example, according to Watts and Strogatz (1998), space constraints the process of preferential attachment that often occurs over shortest distances and is responsible for the development of hierarchies in networks’ structure. Barthelemy (2011) notes in his work that spatial constraints affect the organization of a network, its centrality, the typology of its degree distribution, and the correlations captured between network topology and traffic. Additionally, Ducruet and Beauguitte (2014) observe some disciplinary driving forces on scholars studying spatial networks, which diversify the way that such networks are interpreted and thus they complicate the integration in this research field. Moreover, node functionality in spatial networks usually appears quite different compared to this of networks not embedded in space (Barthelemy 2011; Ducruet 2013; Ducruet and Beauguitte 2014). For example, in a social network the nodes (representing either individuals or social configurations) operate directly as actors, which are ruled by behavioral and cognitive driving forces. On the contrary, in a spatial network, the functionality of nodes (such as are transport terminals, for instance) is the resultant of the actions performed by many different actors and not by “one actor” per se, and thus it is indirectly related to the social behavior and human cognition. Within this framework, space obviously matters in networks and towards this direction numerous researches have already provided rich theoretical and empirical documentation (Barrat et al. 2005; Boccaletti et al. 2006; Bagler 2008; Barthelemy 2011; Wang et al. 2011; Jia and Jiang 2012; Ducruet 2013; Ducruet and Beauguitte 2014). Among them, the works of Barthelemy (2011) and Ducruet and Beauguitte (2014) suggest two fundamental reviews addressing this issue. The first focuses on the theoretical and empirical framework of spatial network analysis, described mainly under the physicists’ perspective, whereas the second on how this complex research is integrated into geography and regional science. These two articles appear to have introduced the epistemological dialogue in the study of spatial networks, which was so far absent or scattered in the relevant research (Ducruet and Beauguitte 2014). This paper takes its inspiration from both these review articles (Barthelemy 2011; Ducruet and Beauguitte 2014) and attempts to provide an integrated epistemological consideration in the complex study of spatial networks. The basic research question is whether the evolution in the study of spatial networks converges or diverges, in conceptual, methodological, and disciplinary (through the way that scholars approach them) terms. The further purpose of this paper is to shape a complete methodological framework for the study of spatial networks, incorporating the existing structural, functional and socioeconomic approaches. This framework can be used as a procedures’ manual describing the successive steps necessitating to study networks embedded in space. It generally aspires to contribute to the comprehension and the standardization of the procedures in the analysis of spatial networks, providing utility to real world applications, to promote the interdisciplinary research of complex networks, and to introduce the dialogue about how Complex Network Analysis (CNA) and NS can move methodically from the research field into the academic didactics. The remainder of this paper is structured as follows; section two distinguishes the aspects composing so far the epistemological framework in the study of spatial networks, whereas section three proposes a methodological approach integrating these considerations. Finally, the conclusion section discusses potential utility and addresses of further research of the proposed consideration.",30
18.0,1.0,Networks and Spatial Economics,12 August 2017,https://link.springer.com/article/10.1007/s11067-017-9349-y,Decongestion of Urban Areas with Hotspot Pricing,March 2018,Albert Solé-Ribalta,Sergio Gómez,Alex Arenas,Male,Male,Male,Male,"Urban life is characterized by a huge mobility, mainly motorized. Amidst the complex urban management problems there is a prevalent one: traffic congestion. INRIX Traffic Scorecard (http://www.inrix.com/) reports the rankings of the most congested countries worldwide in 2014. US, Canada and most of the European countries are in the top 15, with averages that range from 14 to 50 hours per year wasted in congestion, with their corresponding economic and environmental negative consequences. Several approaches exist to efficiently design road networks (Yang et al. 1998; Szeto et al. 2015) and routing strategies (Bast et al. 2007; Qian and Zhang 2013), however, the establishment of collective actions to prevent or ameliorate urban traffic congestion require further improvements, given the complex behavior of drivers. An striking, as well as controversial, strategy to address the problem is congestion pricing (Boarnet et al. 2014; De Palma and Lindsey 2004; Friesz et al. 2004). It consists in taxing vehicles for accessing a road/area, at certain times, based on the supply-demand model (Samuelson et al. 1995). Since the supply quantity is fixed (no more lanes or roads are usually added to the transportation network) the access to demanded areas is taxed. Two main types of congestion pricing (de Palma and Lindsey 2011) exist: i) road pricing, where vehicles are charged for using a particular road section —such as freeways, ring roads, tunnels or bridges—, and ii) cordon pricing, where vehicles are charged to access a particular zone susceptible to traffic congestion —such as historical towns, business districts or simply crowded areas—. A similar variant is area pricing, where the tax applies per day. While road pricing is usually understood as a Pigovian tax to compensate for the externalities caused by drivers (Arnott and Small 1994), cordon pricing can be understood solely as an incentive for reducing the traffic congestion and improving the air quality of the city (Parrish and Zhu 2009), but eventually also becomes a tax income for urban areas. Generally speaking, cordon/area pricing is, in general, effective in reducing the overall amount of cars accessing restricted areas and reducing pollution (Moroni et al. 2013; Beser Hugosson and Eliasson 2006) but it is still insufficient to reduce congestion hotspots within the taxed zone. These hotspots usually correspond to junctions and are problematic for the efficiency of the network as well as for the health of pedestrians and drivers. It has been shown (Petersson 1987) that drivers in-queue are the most affected collective to car exhaust pollution inhalation. In addition, these hotspots are usually located in the city center, magnifying the problem (Raducan and Stefan 2009). Assuming that congestion is an inevitable consequence of urban motorized areas, the challenge is to develop strategies towards a sustainable congestion regime at which delays and pollution are under control. Since ten years ago the scientific community has proposed models to analyze the problem of traffic congestion (Tadić et al. 2004; Zhao et al. 2005; Gawron 1998; Nagel et al. 2008) and decongestion (Singh and Gupte 2005; Yan and Lam 1996; Arnott et al. 1993), pollution generated by traffic (Kickhöfer and Nagel 2016; Grote et al. 2016; Misra et al. 2013; Panis et al. 2006), transitions between traffic states (Guimera et al. 2002; Echenique et al. 2005; Kim et al. 2009), and the design of optimal topologies (Donetti et al. 2005; Danila et al. 2006; Barthélemy and Flammini 2006; Li et al. 2010) and algorithms (Ramasco et al. 2010; Scellato et al. 2010) to avoid it. The focus of attention of most of the previous works was the onset of congestion, which corresponds to a critical point in a phase transition, and how it depends on the topology of the network and the routing strategies used. However, the proper analysis of the system after congestion has remained analytically slippery. It is known that when a transportation network reaches congestion, the travel time and the amount of vehicles queued in a junction diverge (Dorogovtsev et al. 2008). Here, we rely on our Microscopic Congestion Model (MCM) to identify urban traffic hotspots in real scenarios and devise a mechanism to palliate its congestion (Solé-Ribalta et al. 2016). The mechanism is a taxing scheme that charges directly vehicles crossing congested spots (junctions) considering the overall topological structure and traffic functionality of the network. The aim is to eliminate the congestion hotspots using a network topology pay-per-use scheme. Specifically, we build up a flow model based on two steps: (1) detection of the hotspots using MCM, and (2) prediction of the required tax to be applied to every congested junction to encourage drivers to divert the excess flow to neighboring and less congested regions. Our approach follows a similar idea to the one proposed by Vickrey back in 1963 (Vickrey 1963), with the main difference that we now can analytically predict the model behaviour considering real data.",25
18.0,1.0,Networks and Spatial Economics,02 December 2017,https://link.springer.com/article/10.1007/s11067-017-9368-8,Tabu Search Heuristic for Joint Location-Inventory Problem with Stochastic Inventory Capacity and Practicality Constraints,March 2018,Puntipa Punyim,Ampol Karoonsoontawong,Chi Xie,Unknown,Unknown,,Mix,,
18.0,1.0,Networks and Spatial Economics,09 November 2017,https://link.springer.com/article/10.1007/s11067-017-9370-1,The Road most Travelled: The Impact of Urban Road Infrastructure on Supply Chain Network Vulnerability,March 2018,Nadia M. Viljoen,Johan W. Joubert,,Female,Male,Unknown,Mix,,
18.0,1.0,Networks and Spatial Economics,16 November 2017,https://link.springer.com/article/10.1007/s11067-017-9372-z,Maximizing Network Throughput under Stochastic User Equilibrium with Elastic Demand,March 2018,Jian Wang,Muqing Du,Xiaozheng He,,Unknown,Unknown,Mix,,
18.0,1.0,Networks and Spatial Economics,06 March 2018,https://link.springer.com/article/10.1007/s11067-018-9384-3,Retail Equilibrium with Switching Consumers in Electricity Markets,March 2018,C. Ruiz,F. J. Nogales,F. J. Prieto,Unknown,Unknown,Unknown,Unknown,,
18.0,1.0,Networks and Spatial Economics,16 March 2018,https://link.springer.com/article/10.1007/s11067-018-9392-3,A Multi-Scenario Probabilistic Simulation Approach for Critical Transportation Network Risk Assessment,March 2018,Nima Haghighi,S. Kiavash Fayyaz,Ran Wei,,Unknown,,Mix,,
18.0,1.0,Networks and Spatial Economics,15 March 2018,https://link.springer.com/article/10.1007/s11067-018-9394-1,"Trade Openness, Transport Networks and the Spatial Location of Economic Activity",March 2018,Nuria Gallego,José L. Zofío,,Female,Male,Unknown,Mix,,
18.0,2.0,Networks and Spatial Economics,26 June 2018,https://link.springer.com/article/10.1007/s11067-018-9404-3,New Data and Methods in Accessibility Analysis,June 2018,Ana Condeço-Melhorado,Aura Reggiani,Javier Gutiérrez,Female,Female,,Mix,,
18.0,2.0,Networks and Spatial Economics,11 August 2017,https://link.springer.com/article/10.1007/s11067-017-9360-3,Accessibility in a Post-Apartheid City: Comparison of Two Approaches for Accessibility Computations,June 2018,Dominik Ziemke,Johan W. Joubert,Kai Nagel,Male,Male,Male,Male,"Accessibility describes the ease with which activities may be reached from a given location using a particular transport system (Morris et al. 1979; Chen et al. 2007; Knowles 2009; Litman 2010; Bocarejo and Oviedo 2012; Venter and Cross 2014; Ziemke 2016). It is, therefore, also referred to as the potential for interaction (Hansen 1959; Reggiani and Martín 2011). As such, accessibility is what ultimately makes a location more attractive than others (El-Geneidy and Levinson 2011). Accordingly, the improvement of accessibility is often stated as a main goal of proposed land use and transport policies (Handy and Niemeier 1997; Vandenbulcke et al. 2009; El-Geneidy and Levinson 2011; Geurs et al. 2012). Typical infrastructure assessment instruments are mostly based on travel alone (like measuring and monetizing changes in travel times, highway levels of service, or delays) (Handy and Niemeier 1997; El-Geneidy and Levinson 2011). As El-Geneidy and Levinson (2011) point out, such measures have limited utility as they only consider the ability of residents to transport themselves under certain conditions, while they neglect the actual purpose of travel, i.e. the ease of reaching valued destinations. This is what the concept of accessibility, which focuses more strongly on the needs of individuals and households (Litman 2010), is intended to capture. As such, quantitative computations of accessibilities can be used as a comprehensive and efficient planning instrument and are seen as a potential alternative or supplement to traditional planning tools (Koenig 1980; Handy and Niemeier 1997; Vandenbulcke et al. 2009; Venter and Cross 2014). Accessibility is determined both by the patterns of land use and by the characteristics of the transport system (Reggiani et al. 2011). Hence, measures of accessibility usually consist of two components, a land-use (or activity) component and a transport component (Koenig 1980): The land-use component reflects the spatial distribution of opportunities and is characterized by both the amount and the location of different types of activities: The more opportunities and the greater the variety, the greater the accessibility. The transport component reflects the ease of travel between locations, determined by the quality of service provided by the transport system: The less time and money spent in travel, and thus the more places that can be reached within a certain travel budget, the greater the accessibility (Handy and Niemeier 1997). Handy and Niemeier (1997) distinguish three types of accessibility measures: 
 
Isochrone-based (or cummulative opportunities) measures count the number of opportunities reachable within a given travel time or distance cutoff (isochrone). Because all reachable destinations within that isochrone are weighted equally, this measure emphasizes the number of potential destinations. 
Gravity-based measures, related to the well-kwown gravity model for trip distribution, weight opportunities by travel impedance, which is usually a function of travel time or travel cost. The negative exponential function is most commonly used. The closer an opportunity is, the more it contributes to accessibility. Next to many others, the approach by Hansen (1959), which is generally regarded as the start of quantitative accessibility computations (Reggiani and Martín 2011), uses a gravity-based measure (El-Geneidy and Levinson 2011). Note that the isochrone-based measure is a special case of the gravity-based measure, with the impedance function equal to one if within the isochrone, and zero otherwise. Finally, utility-based measures, based on random utility theory, assume that individuals assign a utility to each possible destination and, based on this, select the alternative which maximizes their utility. In Section 3.1, it is pointed out in more detail why the expected maximum utility, the logsum term, can be interpreted as a measure of accessibility. The utility function reflects both the attractiveness of the potential destination and the travel impedance that must be overcome to reach that destination. Handy and Niemeier (1997) highlight that this measure, while similar in form to gravity-based measures, has theoretical advantages. In contrast to isochrone-based measures, which only count the number of opportunities that are located within a defined boundary, gravity-based and utility-based measures, in principle, sum all available opportunities. Therefore, the analyst does not have to specify a cutoff distance or travel time (Venter and Cross 2014). Östh (2011, p.585) describes and illustrates issues, which can arise from using measures that apply a predefined catchment area. Next to questions of interpretability und usability (Handy and Niemeier 1997; Naudé et al. 1999; Venter and Cross 2014), a major obstacle to a more widespread use of accessibility measures in policy analysis consists in lack of data availability (Pozzi et al. 2010; Venter and Cross 2014). Freely available volunteered geographic information (VGI) like information from OSM (OpenStreetMap 2016), which is increasingly becoming a worldwide standard for geospatial data, offer a solution. While many transport and accessibility studies have been using data from OSM to create representations of the transport network and to perform network-based computations, this study uses OSM data on a broader basis. Two approaches for accessibility assessment for Nelson Mandela Bay in South Africa are presented. The approaches possess different levels of utilization of OSM data, which both exceed the use of OSM data for network creation. In the first approach, the transport network as well as locations and types of activity facilities are taken from OSM. Additionally, a synthetic population is created based on a census (Statistics South Africa 2001). The corresponding travel demand is generated based on a travel survey (Nelson Mandela Bay Municipality 2006). Based on local expert knowledge, a household-based accessibility indicator is designed, which takes into account various characteristics of land use and travel, such as (1) travel time to work and/or education and travel time to the nearest healthcare/shopping facility, (2) availability of different transport options, (3) walking time to transport options, and (4) presence of facilities within walking distance. Weights are used to combine the respective values of these characteristics into a composite, household-based accessibility score. The household-based indicator can be regarded a person-based accessibility measure. Some authors have argued that these types of measures are more sensitive to individual activity patterns and accessibility in space and time than its counterpart, place-based accessibility measures (Reggiani and Martín 2011). Accordingly, such an approach appears particularly suitable in South Africa with its high diversity in society and residential patterns. The second approach relies exclusively on OSM data, which are used to create the network and activity facilities. The approach applies an econometric accessibility indicator, which calculates accessibilities on a set of given measuring points (e.g. a regular grid) as the weighted sum over the utilities of all opportunities including the costs of reaching them. It can, therefore, be regarded a place-based accessibility measure (Reggiani and Martín 2011). In contrast to the first approach, no synthetic population is used for the calculation. No assumptions about levels of acceptability have to be made. The approach is highly portable since no input data other than those from OSM are used. Furthermore, the approach can be easily applied for policy analysis. Besides data utilization, the two accessibility computation approaches also differ in terms of modeling philosophy: The first approach is based on a representation of households and individuals of the study region and their travel behavior (person-based measure), enriched with local expert knowledge, to perform accessibility computations. In particular because of its reliance on a travel survey, the measure is largely based on stated behavior. The second approach, by contrast, operates on a regular spatial grid of measuring points (place-based measure) and applies an econometric accessibility measure without relying on a population representation. In contrast to the household-based measure, it focuses on the potential to participate in activities that are available to individuals. In this paper, the insights obtainable from both approaches are presented and the particular strengths and weaknesses of the two approaches are discussed. As study area, Nelson Mandela Bay, a smaller of the eight metropolitan municipalities of South Africa, located in the Southern part of the Eastern Cape province, is chosen. It is located relatively distant from other conurbations, which facilitates the delineation of the scenario. As depicted in Fig. 1, it consists of the city of Port Elizabeth, the nearby towns of Uitenhage and Despatch, several townships, and surrounding rural areas.
 Southern portion of Nelson Mandela Bay Metropolitan Municipality (Source: OSM) There is a broad general consensus that accessibility is one of the major determinants of growth and development, both regarding economic and social dimensions. Naudé et al. (1999) assert that a lack of accessibility to market centers is one of the major constraints of economic development in South Africa. Pozzi et al. (2010) find that remote or inaccessible areas are likely to have high concentrations of chronic poverty. Venter and Cross (2014) point out that accessibility is central to promote sustainable livelihood and that deficits thereof may lead to social exclusion and the denial of basic human rights. In particular in the context of development economics, such relationships have been studied with a focus on African countries (e.g., Naudé and Krugell 2004; Christiaensen et al. 2003), where different explanatory variables like descriptions of geographic location, market connectedness, remoteness, and distances to certain points of interest are applied. While many studies do not consider accessibilities explicitly or perform related computations, it is obvious that aforementioned variables are related to the concept of accessibility. Despite the amount of literature on regional development in Africa and the widely acknowledged relationship between accessibility and economic growth, there are few studies that directly focus on accessibilities and its quantitative analysis (Pozzi et al. 2010; Cheruiyot and Harrison 2014). This is noteworthy since the relationship of growth and accessibility is of particular interest in post-Apartheid South Africa. Apartheid laws like the ‘Group Areas Act’ of 1950 relocated the majority of black people to places where levels of accessibility were low, including ethnic homelands in the spatial peripheries of the country and townships in the spatial peripheries of cities (Cheruiyot and Harrison 2014). As such, the natural growth and development of South Africa’s cities and towns was artificially curtailed (Naudé and Krugell 2004). The resulting spatial structures continue to exist today and are highly inert towards change. Once a low-income household resides in an inaccessible location, options to change that location are limited. As such, the general tendency that low-income households reside in areas that are less accessible, but more affordable in terms of housing costs, which can be observed in most countries, is reinforced in South Africa as townships are often located particularly remote from conurbations. Because of remote housing locations and low incomes, which exclude many transport options, a significant share of people residing in these locations travels very long distances by walking (Venter and Behrens 2005). Consequently, South Africa has been characterized by significant inequality in spatial economic activity (Naudé and Krugell 2004; Cheruiyot and Harrison 2014). The Gini index, which measures the extent to which the distribution of income within an economy deviates from a perfectly equal distribution, has been higher in South Africa than in any other country of the world over the last decade (The World Bank 2015). The following studies with explicit computations of accessibilities in the South African context could be identified: Naudé et al. (1999) measure the accessibility of rural centers in a remote region of the Wild Coast in South Africa, where residents from 188 out of 900 villages have to travel more than one hour to reach the nearest market center. Analysis is done with a GIS-based tool that applies an isochrone-based procedure (cf. Section 1.1). The authors identify inaccessible villages (with travel times to the nearest market center exceeding one hour) and analyze the effects of both a land-use and a transport policy. They conclude that their procedure provides a useful indication of the scale of possible accessibility improvements. Although the authors highlight the importance of such approaches to be easy to use and, especially in the context of developing countries, to be affordable for users, they mention that their tool is only partly open and not fully free of charge. Tanser et al. (2006) model the accessibility of primary health care (PHC) facilities in the rural district of Hlabisa in KwaZulu-Natal, South Africa, intending to identify vulnerable populations with limited access. They compute both walk times and public-transport-based travel times to the nearest PHC facility. Based on log-likelihood maximization, they fit their model against data obtained from interviews with 23,000 homesteads and obtain a model that predicts the PHC attended by a given homestead (cf. person-based measures in Section 1.1) with an accuracy of 91%. They find that a homestead, which can reach a PHC facility within 30 minutes, is ten times as likely to use this facility as a homestead that has to travel 90 to 120 minutes. While providing very insightful results for the specific analysis under consideration, the model’s reliance on a high number of interviews effects that the model is not transferable to other thematic and spatial contexts. In a study conducted in Uganda, Pozzi et al. (2010) explore the relationships between poverty and market accessibility. Next to beeline distances to roads and markets, travel times to roads and markets, and population density, they use a ‘travel-time-weighted population’, which belongs to the class of gravity-based approaches (cf. Section 1.1), with the number of people in other locations as a proxy for opportunities. Only a weak correlation is found between poverty and distance and travel time to roads, while poverty is more strongly correlated with distance and travel time to markets and population density. Also, the authors find a strong correlation of the poverty indicators with their accessibility index, which reaffirms the necessity to take into account reachable opportunities apart from pure transport-system-related characteristics (cf. Section 1.1). The authors do, however, report difficulties related incompleteness of the required input data, in particular those required to determine the existence and location of markets, which are taken from a geospatial research database. Cheruiyot and Harrison (2014) examine the relationship between economic activity, measured as gross value added (GVA), and accessibility, measured as travel times to the four major metropolitian areas in South Africa (Gauteng (Johannesburg and Pretoria), Cape Town, eThekwini (Durban), and Nelson Mandela Bay), for South African municipalities. The authors report that they cannot prove the proposed relationship between accessibility measure and GVA on a nationwide scale and speculate that their model may be unable to deal with a multi-polar spatial landscape as it is the case when considering South Africa as a whole. This seems plausible, since in their model a higher distance only translates into a worse accessibility score, while it does not take into account the fact that a remote destination is likely of less importance and should, therefore, only have a reduced impact on the score. As will be shown in Sections 2 and 3, the two models of the present paper do account for this effect. On a regional level, with less complex spatial structures (e.g. the metropolitan areas of Cape Town and the Eastern Cape (Nelson Mandela Bay)), they find the hypothesized correlation between GVA and accessibility. Venter and Cross (2014) describe ’access envelopes’ as a new accessibility measure, which they regard suited to assessing the impact of transport and spatial development strategies. The computation result is referred to as net wage after commute (NWAC) and represents the potential income obtainable at a specific job location minus the time and money required to reach that location. The authors apply the method to a number of case studies, e.g. the extension of the bus rapid transit (BRT) system in the City of Tshwane (Pretoria) and conclude that their method is capable of presenting complex spatial relationships in an intuitive manner suited for land use and transport analyses. As input data, their method requires information on jobs, potential wage levels, public transport coverage, walking times, and public transport costs. The authors see this as a limitation to their approach, in particular as accurate spatial data on public transport routes and fares are seldom available. In their computation for Tshwane, potential incomes in potential destinations were obtained from the Gauteng transport model’s updated job location data, which will likely limit the transferability of the approach to other regions. In summary, out of the five discussed studies, four are focused on South Africa, one on another African country (cf. Table 1). In terms of methodology (cf. Section 1.1), two studies (Cheruiyot and Harrison 2014; Pozzi et al. 2010) apply gravity-based measures, two isochrone-based measures (Naudé et al. 1999; Tanser et al. 2006), and one a novel measure (’access envelope’ Venter and Cross 2014). At least three of these studies report rather high input data requirements and/or difficulties with inconsistency or incompleteness of required input data (Venter and Cross 2014; Pozzi et al. 2010; Tanser et al. 2006). Cheruiyot and Harrison (2014) also report conceptual difficulties in that their measure may not be suitbale for a spatially complex study area. With its focus on transport (and potentially land use) policies at a metropolitan level, the study by Venter and Cross (2014) is most similar to the studies presented in this paper. In particular when compared to the econometric accessibility indicator (cf. Section 3), however, the input data requirements of the study by Venter and Cross are significantly higher. Also, similar to the other presented studies, it must be asserted that their approach, mainly due to context-specific input data, is not transferable to other spatial contexts. Such limitations are often regarded as hindering the more widespread application of otherwise favorable accessibility approaches. The present study is intended to address such limitations.
",19
18.0,2.0,Networks and Spatial Economics,05 May 2017,https://link.springer.com/article/10.1007/s11067-017-9348-z,Dynamic Accessibility using Big Data: The Role of the Changing Conditions of Network Congestion and Destination Attractiveness,June 2018,Borja Moya-Gómez,María Henar Salas-Olmedo,Javier Gutiérrez,Male,,,Mix,,
18.0,2.0,Networks and Spatial Economics,21 December 2017,https://link.springer.com/article/10.1007/s11067-017-9376-8,The Role of Transport and Population Components in Change in Accessibility: the Influence of the Distance Decay Parameter,June 2018,Marcin Stępniak,Piotr Rosik,,Male,Male,Unknown,Male,"One of the most visible outcomes of transport infrastructure development is an increase in accessibility, facilitating people’s ability to reach desired destinations. Nevertheless, transport infrastructure constitutes only one of the two main components of accessibility that may influence its level and spatial pattern. The land use component, which includes the amount and spatial distribution of opportunities and the demand for those opportunities (Geurs and van Wee 2004), is responsible for shaping the spatial pattern of travel demand, and in so doing, it may have a decisive impact on the measurement of accessibility. The role of the land use component is usually under-investigated in the case of those accessibility studies that focus on the evaluation of transport infrastructure development. These studies, which aim to evaluate a net effect of transport infrastructure development (e.g. Stelder 2013; Salas-Olmedo et al. 2014; Rosik et al. 2015), usually apply fixed values in order to describe land use pattern. This approach is straightforward for an assessment of change in accessibility resulting from a particular investment or set of investments (e.g. road network development programmes). Nevertheless, the mid or long term monitoring of accessibility should consider that infrastructure development does not act in a vacuum but in specific external conditions. Any significant change of these external conditions, like e.g. the depopulation of peripheral regions, the rise or decline of particular urban areas etc., creates new settings and affects both the results of accessibility measurement and the way in which we interpret these results. This is especially the case when an accessibility study focuses on the impact of developments of the transport network on territorial cohesion. Unequal accessibility is not necessarily problematic (van Wee and Geurs 2011), as space, “by its very nature is divided into centre and periphery” (Martens et al. 2012: 687). Moreover, a delineation of peripheral, less accessible areas, such as their level of peripherality (Spiekermann and Neubauer 2002), might evolve over a time. In consequence, the level of territorial cohesion also changes, regardless of the actual development of the transport infrastructure. Thus, our intention is to overcome the existing obstacles in contemporary accessibility studies, by separating out accessibility changes that result from the development of transport infrastructure from those caused by the changes in land use. The relatively long period of time investigated (20 years) consists of a period of stagnation in transport investment (1995–2005) followed by a significant acceleration in the development of the road network (2005–2015). It facilitates the provision of meaningful conclusions regarding the role of a given component, irrespective of external conditions. These include, apart from infrastructure-related, demographic change, the changes in population distribution in Poland (e.g. depopulation, shrinking small and medium-size towns, suburbanisation etc.). The aim of the paper is to determine the role of given components, namely population and infrastructure, in change in accessibility and territorial cohesion in Poland during the last two decades (1995–2015). During the investigated period of time, and in the second decade in particular, Poland has experienced a significant growth of road development, comparable to what one could observed in France (Fayard et al. 2012) or Switzerland (Erath et al. 2009) in the mid of the twentieth century, or in Spain since 1980s (Condeço-Melhorado et al. 2017). In order to do this we use the potential accessibility model, which is widely used to evaluate effectiveness and the equity impact of transport infrastructure developments in a mid- or long-term period of time i.e. a decade or more (Vickerman et al. 1999; Holl 2007; Condeço-Melhorado et al. 2011b; Levinson et al. 2012; Rosik and Stępniak 2015; Rosik et al. 2015; Salas-Olmedo et al. 2015). As the potential accessibility model is sensitive to the parameters of the distance decay function (Haynes et al. 2003), the selection of particular parameters for the model potentially influences the balance between the infrastructure and population components of accessibility change and its impact on the level of territorial cohesion. Thus, this study contributes to the existing body of accessibility literature by taking into consideration different gradients of the distance decay function that enable the researcher to obtain results independent of impedance parameters. The paper is organised as follows. The next section starts by presenting some key concepts relating to the evaluation of accessibility change with special attention given to the role of land use change and infrastructure development. A short description of the study area, focusing on changes in the population distribution (the land use component) and road network development (the transport component) is then presented. The main results are presented in Section 4, followed by general conclusions and final remarks.",23
18.0,2.0,Networks and Spatial Economics,01 February 2018,https://link.springer.com/article/10.1007/s11067-017-9375-9,"Social Capital, Resilience and Accessibility in Urban Systems: a Study on Sweden",June 2018,John Östh,Martina Dolciotti,Peter Nijkamp,Male,Female,Male,Mix,,
18.0,2.0,Networks and Spatial Economics,08 April 2017,https://link.springer.com/article/10.1007/s11067-017-9347-0,The Geography of Logistics Firm Location: The Role of Accessibility,June 2018,Adelheid Holl,Ilaria Mariotti,,Female,Female,Unknown,Female,"With the increasing need to transport quickly and efficiently, transport and logisticsFootnote 1 play a key role in overcoming the constraints of time and distance in modern supply chains. This is even more true in a context where firms are facing competition that is ever more global. Firms are now concentrating more on specific consumers’ requests, on delivering goods with greater speed, seeking ways to reduce costs, and improving quality (Bonacich and Wilson 2008). To achieve these ends, firms formulate intelligent strategies, including the use of international logistics techniques to gain competitive advantage in the management of supply chains (Wood et al. 2002). Indeed, logistics is becoming increasingly popular as a competitive device for companies to reduce delivery times, increase reliability and flexibility in deliveries, heighten customer responsiveness, and facilitate the successful implementation of Just-In-Time (JIT) manufacturing and distribution systems (Lai and Cheng 2009; Brouwer et al. 2013). Since the 1950s, the logistics industry has experience the so-called “logistics revolution” that can be explained with five interrelated phenomena: (i) the consumer-oriented economy demanding a level of service customization and delivery speed which is only possible if more frequent shipments of goods are made; (ii) Internet-based information systems; (iii) the substantial reductions of trade barriers, tariffs and transportation costs; (iv) the European Traffic Policy; and (v) the processes of vertical disintegration and value-chain decomposition in most industries associated with the ongoing globalization of the economy that has increased the amount of goods flows to be moved around the globe (Maggi and Mariotti 2012; Mariotti 2015). The consumer-oriented economy, which is confronted nowadays with the increasing trend of automatization and customization in the last mile delivery to clients, has heightened the complexity of logistics processes. The management of such complexity has been made feasible by the Internet-based information systems developed in the 1990s. These systems have made the exchange of information drastically simpler and cheaper, while Internet-based mail order businesses have boosted parcel services. Moreover, a key role has been played by various technology innovations in freight moving and handling: for instance, the rapid growth in roll-on, roll-off trucking technology, gains in containerization technology and capacity, rapid-turnaround shipping and the increased speed and efficiency of air transport technologies (McCann 2008). All of these technological developments have contributed to a huge reduction in transport costs (Hummels 1999; Van Veen-Groot and Nijkamp 1999; Glaeser and Kohlhase 2004; Levinson 2006; Notteboom 2007; McCann 2008; Notteboom and Rodrigue 2009; Coto-Millán et al. 2016). In particular, during the twentieth century the costs of moving goods declined by over 90% in real terms, and this reduction is continuing. Indeed, the average cost of transporting a ton a mile decreased from 18.5 cents in 1890 (in 2001 dollars) to 2.3 cents in 2004 (Glaeser and Kohlhase 2004). At the same time, the EU Traffic Policy intended to favour the liberalization of truck traffic, has greatly increased the truck fleet and traffic flows in Europe. In around only ten years, from 1990 to 1999, road traffic within European member states increased by 76% (Vahrenkamp 2010). Moreover, the globalization of the economy has strongly affected transport and logistics. Vertical disintegrationFootnote 2 is linked to the Post-Fordist paradigm, which promoted the switch from the mass production of standardized goods to the market-oriented production favouring so-called flexible specialization.Footnote 3 Finally, in an increasingly globalized environment, logistics has also become one of the main engines of competitiveness and economic development (see also Jiang et al. 2016). As stressed by the literature, there is a bidirectional link between economic development and logistics performance (see Arvis et al. 2007; Ferrari et al. 2011).Footnote 4
 These trends have significantly changed the geography of freight distribution and logistics and the way goods move through the economy. Yet, research into this topic is generally underrepresented in regional science and economic geography (Hesse and Rodrigue 2004). Besides, while the literature on manufacturing firm location is extensive, highlighting the role of accessibility as one of the key variables in firm location decisions (see, for example, Reggiani 1998; Head and Mayer 2004; Holl 2004; Rietveld and Bruinsma 1998), surprisingly little is still known about the location patterns of logistics firms and how accessibility considerations shape this pattern. The aim of this paper is to contribute to filling this gap in the literature by studying the geography of logistics firm location taking into account the role of accessibility. We ask how accessibility to different transportation networks and accessibility determined by the urban structure influence the location pattern of logistic firms. It cannot be denied that understanding the location decisions of transport and logistics firms is important for the society because the demand for “logistics floor space” is expected to grow substantially in advanced economies, while the demand for “industrial floor space” is expected to decline (McKinnon 2009).Footnote 5 The expected growth in logistics floor space is correlated with the predicted growth of freight transport volumes, which is estimated in the EU to reach about 82% in 2050 (European Commission 2011). It is, therefore, crucial for policy makers to investigate the location decisions of transport and logistics firms since it is strongly related to the demand for freight transport, and the choice of freight transport modes (Bowen 2008). The issue is also of direct relevance for urban planning, but it is still often neglected in the planning process (Heitz and Beziat 2016). The location of logistics activities raises land consumption, contributes to urban sprawl, and can also reduce the well-being of individuals in local communities because of noise, air pollution, congestion and safety (for a review see Aljohani and Thompson 2016). The scant literature on the location choice of transport and logistics concerns mainly logistics sprawl,Footnote 6 and the location choice of transport and logistics foreign direct investments (FDIs) at national level, mainly referring to the case of China and Italy. Only the works by Bowen (2008) and Verhetsel et al. (2015) analyse explicitly the role played by accessibility measures, related to several kinds of transport networks, in fostering warehousing location in US in 1998–2005, and in influencing location choice of logistics companies in Flanders (Belgium), respectively. Our paper contributes to this literature by investigating the role of accessibility for logistics firm location at a spatially disaggregated level for Spain. While Bowen used US county level data, and Verhetsel et al. (2015) focused on a specific region (i.e. Flanders), our study is at a spatially finer level of analysis using municipality data and for the whole of mainland Spain. Specifically, we use geo-coded firm level data from the SABI database (Sistema de Análisis de Balances Ibéricos), generated by INFORMA and Bureau Van Dyck, along with detailed information on transport infrastructure, accessibility measures, and municipality characteristics that we describe in more detail in Section 3. By combining our geo-coded data for the whole of mainland Spain with Geographic Information System (GIS) techniques, this paper contributes to the literature by providing a comprehensive spatially detailed quantitative analysis of the geography of logistics firms at the national level. We organize our empirical analysis in two parts. First, we carry out a spatial analysis to explore the geography of logistics firms compared to other sectors (manufacturing, business services, and transportation) regarding access to transportation infrastructure and urban structure. Second, we study the location behaviour of new logistics firms born in 2002–2007 and the role played by accessibility using an econometric analysis. Specifically, we apply a Poisson model and examine the robustness of our results to the use of a negative binomial model as well as a zero-inflated negative binomial model. We provide evidence that logistics firms are located closer to highways and other transport infrastructure compared to other firms (manufacturing, business services, and even transport operators). Besides, they are strongly attracted to urban areas (mainly Madrid and Barcelona) for their market size and market potential, but also increasingly to suburban locations and, to some extent, extra-urban locations that have good accessibility. In contrast, central cities of urban areas have experienced a declining share of logistics firms. Recent research on other regions (for the US: Bowen 2008; Dablanc et al. 2014; the Netherlands: Van Den Heuvel et al. 2013; UK: Allen and Browne 2010; Germany: Hesse 2004; Belgium: Verhetsel et al. 2015; Paris region: Heitz and Dablanc 2015; Heitz and Beziat 2016; and the Tokyo Metropolitan area: Sakai et al. 2015) has highlighted this phenomenon of sprawl in relation to warehousing and logistics. We provide empirical evidence based on the logistics sector and in a European context. Our results of the econometric analysis furthermore confirm that logistics firms are strongly attracted by access to transportation infrastructure (especially highways) and large markets. The paper is structured into five sections. The introduction is followed by the literature review on the location patterns of logistics firms. Section three is dedicated to data description while the empirical analysis and its results are described and discussed in section four. Section five presents conclusions and policy recommendations.",59
18.0,2.0,Networks and Spatial Economics,11 September 2017,https://link.springer.com/article/10.1007/s11067-017-9362-1,Road Accessibility in Border Regions: a Joint Approach,June 2018,Ana Condeço-Melhorado,Panayotis Christidis,,Female,Unknown,Unknown,Female,"Accessibility is considered an essential condition for regional development and competitiveness in global markets. Over the 50-years period in question, improvements in accessibility through investment in transport infrastructure reduced economic costs (Robson and Dixit 2017) and increased safety levels, both for passenger and freight transport. This is in some ways related to economic development but there is no consensus about the direction, strength and magnitude of this relationship (Holguin-Veras et al. 2005; OECD 2009; Fernald 1999; Moreno et al. 1997). In the Portuguese and Spanish context, many studies point towards the importance of accessibility for company location (Holl and Mariotti 2017; Holl 2012; Jiménez and Perdiguero 2011; Holl 2004b; Holl 2004a), although other factors, such as education policies, seem to outweigh the positive effects of the reduction in travel time (Ribeiro et al. 2010). With regard to accessibility, however, many differences remain between the northern and southern or eastern and western countries of Europe, as well as between central and peripheral regions within the EU. At national level, peripheral areas are generally located in border regions, which are associated with low levels of accessibility, poor infrastructure provision and greater distances between economic centres. At the same time, border regions are generally poorly endowed with natural resources and have low agricultural productivity and less developed social and business environments, together with large differences in legal, administrative and social welfare systems and in language and cultural traditions (Nijkamp 1993). Brakman et al. (2010) concluded that despite the positive effect of EU integration on population growth in border cities, location close to a national border remains a burden. Proximity to national borders may also be a drawback for the economic performance of border regions. Some studies point towards the significance of national borders in reducing trade (Salas-Olmedo et al. 2015; Salas-Olmedo et al. 2014; Chen 2004; Head and Mayer 2000; Nitsch 2000; Wei 1996), which gives rise to discontinuities in economic relations and increases the marginal costs of interaction. Difficult physical access, related to the lack of transport infrastructure and integrated public transport systems, is perceived as one of five main obstacles for border regions (European Union 2016). In fact, many border regions are more poorly endowed in terms of transport infrastructure because of the lack of a critical mass to justify investment in transport. In addition, efforts are uncoordinated, since each country chooses its level of investment according to the welfare of its own citizens. This lack of coordination of national transport policies lowers the effectiveness of transport connections between countries and consequently a series of missing links and bottlenecks are found in cross-border areas. In this respect, the EU devotes considerable effort to increasing regional accessibility by planning and financing transport infrastructure, paying special attention to those projects with higher European added value. The Trans-European Transport Network (TEN-T) and Transport Infrastructure Needs Assessment (TINA) are good examples of such programmes. This cross-border transport infrastructure favours economic interactions between locations on both sides of national borders, thereby reducing barriers to trade while benefiting specialization and generating gains in trade and productivity. Complementarities occur on both sides of the border, with an increase in production and the exchange of products between cross-border regions. Some studies show that cross-border corridors not only generate important benefits in accessibility to the countries where they are built but also to other countries not directly involved in financing those projects (Lopez et al. 2009; Gutiérrez et al. 2011). The role of TEN-T and TINA corridors in improving accessibility and territorial cohesion has been studied by many authors (Rosik et al. 2015; Ribeiro et al. 2010; Spiekermanm and Wegener 2006). However, very few studies look specifically at the condition of accessibility in border regions. Some exceptions in the European context are Fontes et al. (2014), who use travel time to the closest national and international centres as an independent variable to explain regional economic growth in the Portuguese-Spanish border region. Topaloglou et al. (2005) use a gravity formulation of accessibility, together with other indicators, to establish a typology of border regions in the enlarged European Union. Given the importance of border regions in light of current EU Cohesion Policy, the purpose of this study is to analyse accessibility within these particular areas, examining the changes in accessibility between 1960 and 2010. The border areas analysed are located in Portugal, Spain and the French Pyrenees, where many municipalities are below the EU average in terms of GDP, have a low population density and face accessibility problems related to their remoteness. The accessibility of selected border regions will be analysed using two different perspectives: first, we will compare the accessibility of border regions with the national average; and then we will compare their accessibility to national and international centres. A joint approach that uses different accessibility indicators will offer complementary information on the complexities related to the geographical and structural conditions of border areas. Previous accessibility studies have analysed accessibility levels at regional level (Moya-Gómez et al. 2017) within and between countries, but less work has been done to compare the accessibility of border regions with the national average or to understand the impact of national and international connections in border regions. Furthermore, as far as we know, this is the first time that the accessibility of border regions has been analysed using such a long time span. In this study, and by combining several accessibility indicators, we will examine the evolution of accessibility in border regions and seek to shed some light on the effectiveness of transport policies in increasing the access of these regions to markets and providing greater opportunities for interaction. This article is structured as follows: after this introduction, the second section will review the suitability of certain indicators for measuring the accessibility of border regions; sections 3 and 4 will present data and methods; while section 5 and the conclusions will present the results of the accessibility analysis.",15
18.0,2.0,Networks and Spatial Economics,04 January 2018,https://link.springer.com/article/10.1007/s11067-017-9378-6,The TUM Accessibility Atlas: Visualizing Spatial and Socioeconomic Disparities in Accessibility to Support Regional Land-Use and Transport Planning,June 2018,Benjamin Büttner,Julia Kinigadner,Gebhard Wulfhorst,Male,Female,Male,Mix,,
18.0,2.0,Networks and Spatial Economics,07 March 2018,https://link.springer.com/article/10.1007/s11067-018-9391-4,Strategic Assessment of Lisbon’s Accessibility and Mobility Problems from an Equity Perspective,June 2018,Camila Soares Henrique Fontenele Garcia,Rosário Macário,Carlos Felipe Grangeiro Loureiro,Female,,Male,Mix,,
18.0,3.0,Networks and Spatial Economics,20 February 2019,https://link.springer.com/article/10.1007/s11067-019-09453-w,Uncovering Links Between Urban Studies and Network Science,September 2018,Ben Derudder,Zachary Neal,,Male,Male,Unknown,Male,,11
18.0,3.0,Networks and Spatial Economics,01 June 2018,https://link.springer.com/article/10.1007/s11067-018-9403-4,Sea-Land Interdependence in the Global Maritime Network: the Case of Australian Port Cities,September 2018,Justin Berli,Mattia Bunel,César Ducruet,Male,Male,Male,Male,"Measuring and analyzing intersections between networks is a major challenge for both researchers and practitioners. In transport geography and economics, there is a consensus that the efficient combination of sea and land accessibility is necessary - if not vital – to port activity and to the expansion of wider supply chain systems. Yet, this perspective still lacks rigorous empirical validation today, taking into account the topological structure of both land and maritime networks. Early geographers long focused on ports and port cities at the local level of the port-city interface from a morphological perspective (Rimmer 2015; Robinson 2015a). During many decades, numerous port studies (see Ng et al. 2014 for a synthesis) left behind continental and maritime connectivity issues despite the existence of conceptual frameworks such as the “port triptych” back in the late 1970s (Slack 2017), in which the port is a node connecting hinterland (inland market area) and foreland (overseas market area). Considering these two objects as one single, intermodal and interconnected, network, however, is subject to a number of issues deserving discussion across various research fields. One of them is the urban perspective on global networks (Taylor and Derudder 2016), which mainly focuses on multiple immaterial linkages created by multinational firms, with less importance given to transport networks and especially to ports (Jacobs et al. 2010). In geography and elsewhere, mainly local/national planar and technical networks have long been at center stage (Ducruet and Beauguitte 2014) until the extension of such analyses to non-planar transport and global networks. Yet, transdisciplinary urban studies keep on insisting about the need to understand the metabolism (Kennedy et al. 2015) and the “pace of life” of cities (Bettencourt et al. 2007), understanding that efficient connectivity is favorable to urban growth (see also Neal 2017). The analysis of combined networks witnessed a growing emphasis from researchers during the last decade or so. Changes in topological structure inferred by the interconnection of different networks (and/or different layers within a single network) occupy the bulk of related research on both nodes and links. Although analyses of single networks remain dominant in the literature, it is possible to find several examples of works looking at least at two types of linkages. Throughout the latter perspective, most empirical studies looked at networks or layers of the same nature: planar networks, such as electrical transmission network and Internet network backbone (Rosato et al. 2008) or non-planar networks, such as airline networks and corporate networks (Liu et al. 2013), to name but a few (see a recent review by Ducruet 2017). The present research wishes to investigate the interconnection between two networks of different structure, namely planar and non-planar networks. Planar networks can be defined as two-dimensional graphs where links cannot cross without creating a node at the place of the intersection. This is the case of more land-based transport networks but also of so-called “technical networks” in general from electric power grids to the Internet. In geography, one of the earliest examples of the kind was provided by Cattan (1995) in her study of barrier effects affecting air and rail flows between European cities (see also van Geenhuizen 2000). Later on, Berroir et al. (2012) revealed the emergence of subnetworks among French urban areas based on multiple interactions such as air flights, commuting flows, scientific collaborations, high-speed rail linkages, and firms’ networks. This particular approach of the “French school” (P.A.R.I.S. team) takes its roots, at least in part, in earlier works of the so-called “New Geography” on the Swedish side (see a review by Peris 2016), with the pioneer works of the geographer G. Törnqvist, for instance, who proposed already to study both material and immaterial flows linking cities. More recently, generative network models were applied to a composite transport network defined as a “surrogate measure of three individual transport networks: road, rail, and air transport” among Southeast Asian cities (Dai et al. 2017). Recent attention has been paid, however, to the importance of road transport for supply chain performance using a multiplex network framework (Viljoen and Joubert 2017). Such studies have in common, however, to neglect the importance of shipping linkages, depending on the context of the study area, and to focus on a composite network. The latter perspective does not allow grasping the respective role of the individual networks or layers in the overall centrality of nodes or cities. The main purpose of our paper is thus to untangle the effects of each network or layer and to better understand the specific role of shipping linkages in the connectivity of cities. The fact that ports and port cities benefit from being well connected to both sea and land networks is common thought in transport studies and port geography, referring back to the port triptych model proposed by the French geographer André Vigarié (1979) in which ports connect foreland and hinterland. Recurrent arguments include the fact that sea-land transshipment may generate economies of scale while increasing the number of destinations and shipping frequency, thereby opening new markets to ports beyond their traditional hinterlands (van Klink and van den Berg 1998). While the influence of physical infrastructure interconnection cannot be ignored (Bottasso et al. 2018), most studies focus more on the role of vertically integrated actors in establishing intermodal services (Franc and Van der Horst 2010; Notteboom et al. 2017) and so-called dry ports (inland) to relieve seaports from congestion and enhance distribution systems, among other goals (Roso et al. 2009). Hinterland delineation, a longstanding issue in transport studies, recently improved but tended to ignore shipping linkages per se (Halim et al. 2016; Tiller and Thill 2017; Zanon Moura et al. 2017; Jung et al. 2018). Most related empirical investigation of sea-land interaction remains qualitative to date (see Torbianelli 2010). Such an approach is more developed in airline network studies such as Choi et al. (2006) crossing airline networks and the Internet backbine, Matisziw and Grubesic (2010) looking at air transport flows in relation to land access, and Derudder et al. (2014) combining airline flows with planar networks such as bus schedules, railways and roads in Southeast Asia. In fact, across a sample of 728 scientific articles published about ports in geography journals and other disciplines between 1950 and 2012, Ng et al. (2014) found that those dealing with supply chain linkages, intermodal transportation, and networks rapidly increased in number, although very few of them actually offered an empirical, quantitative analysis of combined sea-land networks connecting ports. An early breakthrough in that direction by Chapelon (2006) looked at the inland accessibility of European seaports, simulating their respective population and Gross Domestic Product potentials based on trucking time, road quality, driving regulations, tolls, etc. One originality of his study was to include a handful of ferry linkages to complement the sole road network, but the European maritime network remained too simplified to account for a realistic account of shipping connectivity. A radically different approach came out with the work of Nelson (2008) providing a composite index of accessibility for the world’s cities. No less than twelve layers compose this index using raster methods, including AIS data for shipping flows (Automated Identification System), road and railway networks, elevation, land use, etc. Halim et al. (2015) used Spatial Computable General Equilibrium (SCGE) modelling to explore the impact of opening new shipping routes on port activity, including a simplified road network connecting ports with hinterlands. More recently, Guerrero et al. (2017) provided a new analysis of sea-land connectivity using subnational regions as units of analysis, AIS data, and Euclidian distances between ports and inland markets, while Shen (2017) combined trade and customs data with road transport networks to analyze door-to-door shipping flows between U.S. and Chinese cities through GIS methods. In a similar vein than Chapelon (2006), Wang et al. (2017) explored the effects of opening a new coastal shipping artery across the Bohai Rim on local economic development and sea-land accessibility. Other interconnection studies combined shipping linkages with other networks such as airlines (Parshani et al. 2010) and trade flows (Catalayud et al. 2017). What remains lacking is the modeling of the shipping network and the analysis of its interconnection with landside networks, especially in an urban context. Most of the time, network nodes are transport terminal and/or ports thereby ignoring their urban context and dimension (see Qu et al. 2016 on an intermodal study focusing on greenhouse gas emissions). Although the contemporary literature dominantly insisted on the dereliction of port-city linkages, several studies argued in favor of maintained positive externalities offered by cities to port activities (Hall and Jacobs 2012), demonstrating that the global maritime network had remained tied to cities and, in particular, extended city-regions crossing vessel traffic and demographic size as the main indicators over the last 120 years (Ducruet et al. 2018). This is the goal of the present research to tackle such lacunae and to go one step further by combining the road network with the maritime network to study urban functions and hierarchies. Despite all the major advances made by scholarly research on sea-land connectivity, a number of issues remain unresolved and/or unexplored. Firstly, it is rather common that the urban dimension of network nodes is largely ignored, the terminal or port (or the subnational region) being the main unit of analysis. Thus, it remains unclear whether sea-land connectivity influences, or is determined by, urban activity (e.g. population) or port activity (e.g. throughput). This paper thus adopts a new perspective, considering how urban hierarchy and port hierarchy correlate with mono-layered and bi-layered centrality. This has implications for both network studies, transport studies, and urban studies, in terms of specialization and multiplexity. Second, the concrete spatial distribution of the shipping network remains ill defined, i.e. without taking into account its geographical and operational features. This article proposes a new approach through the modeling of a maritime grid capable of mapping and measuring shipping connectivity in a more precise manner, especially in relation to landside transport networks. The choice of Australia is motivated by several aspects. First, it is an island country, which avoids hinterland competition like in Europe or USA among ports serving the same “contestable” markets, as Australian port cities all have their own port and there is limited traffic shifts among them. Population is concentrated along the coastline in those cities mainly, so that the heartland of the country is relatively empty. Second, Australia as a whole underwent steady vessel traffic growth since the late 1970s (Fig. 1), with a rising (albeit relatively minor) world share, thus motivating the study of its maritime connectivity. Currently, Australia is the world’s 23rd largest export economy, although its imports surpass its exports in terms of trade value, exporting mainly solid bulks (iron ore, coal briquettes, gold, wheat, and crude petroleum) and importing liquid bulk (refined and crude petroleum) as well as finished goods (cars, computers, and packaged medicaments (Observatory of Economic Complexity 2018). It is now famous for having become the world’s largest exporter of iron ore and the world’s leading coal exporter, partly fostered by Chinese demand (Wang and Ducruet 2014). Vessel traffic evolution of Australian ports, 1977–2008. Source: own realization based on Lloyd’s List data Australia’s international trade thus occurs mainly by sea up to 99% of its total, and is highly diversified serving a developed economy and serving both specific industries and a population of about 24 million inhabitants concentrated in large coastal (port) cities. Heavy industries need considerable port infrastructures close to the mines located mainly in the Northwest of the country for their exports, while main urban centers locate more in the Southern part, from Perth in the West to Sydney in the East and Brisbane more in the Northeast. Those latter locations are mainly responsible for the trade and import of manufactured goods and for passenger traffic as well as oil imports to feed the urban system with energy. The Australian port system, and especially bulk ports, thus necessitates efficiency intermodal facilities to connect production and consumption centers to global markets (see Robinson 2007, 2015a). As such, this port system is marked by a high degree of specialization (Fig. 2) whereby a majority of ports handle solid bulks and the rest is specialized in one main traffic category, such as container ports being also large cities: Melbourne, Adelaide, Port Botany (Sydney), Brisbane, and Fremantle (Perth). Except from liquid bulk that is still handled by numerous ports, a minority is specialized in one main traffic type, such as passengers including cruise and ferries (Hayman Island, Broome, Exmouth, Eden, and Burnie), food products including fishing and drinks (Darwin), and general cargo (Port Alma and Port Lincoln). Although the figures might have changed since the study year 2008, there is a high stability in the type of products handled by either large cities or specialized bulk ports. This look at the Australian port system in relation with trade and settlements complements recent works focusing on one particular port city such as Sydney, looking at traffic specialization and fluctuation overtime (Paflioti et al. 2017). Other earlier studies investigated the strong influence of cities (rather than ports) in the attraction of the maritime service industry in Australia (O’Connor 1989) and later at the world scale (Jacobs et al. 2011), long after pioneering studies of Australian ports where urban and hinterland elements where mentioned (Rimmer 1967) to explain at least partly port dynamics in reference to classic models of port system evolution. Earlier works looked, for instance, at the rank-size distribution of coastwise shipping flows among Victoria state ports (Britton 1965). Vessel traffic distribution among Australian ports, 2008 (Unit: % DWT). Source: own realization based on Lloyd’s List data. N.B. maximum height = 100% The remainder of the paper are as follows. Next Section 2 presents the methodology for modeling and combining shipping and road networks, focusing on the particular case of Australia as a testing ground. Section 3 proposes computes and compares classic centrality/accessibility algorithms on individual and combined networks, while Section 4 identifies the degree to which such measures correlate with port and urban hierarchies in Australia. The final section serves as a discussion and conclusion about the contribution of the findings to existing research and opens pathways for further research in the field of urban network studies.",17
18.0,3.0,Networks and Spatial Economics,30 June 2018,https://link.springer.com/article/10.1007/s11067-018-9408-z,Are ‘Sister Cities’ from ‘Sister Provinces’? An Exploratory Study of Sister City Relations (SCRs) in China,September 2018,Xingjian Liu,Xiaohui Hu,,Unknown,Unknown,Unknown,Unknown,,
18.0,3.0,Networks and Spatial Economics,24 August 2018,https://link.springer.com/article/10.1007/s11067-018-9419-9,Correction to: Are ‘Sister Cities’ from ‘Sister Provinces’? An Exploratory Study of Sister City Relations (SCRs) in China,September 2018,Xingjian Liu,Xiaohui Hu,,Unknown,Unknown,Unknown,Unknown,,
18.0,3.0,Networks and Spatial Economics,28 July 2018,https://link.springer.com/article/10.1007/s11067-018-9409-y,Measuring the Accessibility of Railway Stations in the Brussels Regional Express Network: a Node-Place Modeling Approach,September 2018,Freke Caset,David S. Vale,Cláudia M. Viana,,Male,Female,Mix,,
18.0,3.0,Networks and Spatial Economics,20 August 2018,https://link.springer.com/article/10.1007/s11067-018-9420-3,Correction to: Measuring the Accessibility of Railway Stations in the Brussels Regional Express Network: a Node-Place Modeling Approach,September 2018,Freke Caset,David S. Vale,Cláudia M. Viana,,Male,Female,Mix,,
18.0,3.0,Networks and Spatial Economics,31 July 2018,https://link.springer.com/article/10.1007/s11067-018-9410-5,The Evolution of the Systems of Cities Literature Since 1995: Schools of Thought and their Interaction,September 2018,Antoine Peris,Evert Meijers,Maarten van Ham,Male,Male,Male,Male,"Cities do not function in isolation, but are organised in systems of cities characterised by strong interdependencies that develop at the scale of a large region, a nation, a continent or even at the global scale (Pumain 2011). A large literature on interrelated cities has developed since the end of the nineteenth century. Early contributions include work observing the regularities in the size distribution of cities in countries (Auerbach 1913; Gibrat 1931; Zipf 1949) as well as the formulation of central place theory (Christaller 1933). These contributions provided the basis for an upsurge of work on intercity relationships in the 1960s and 1970s, addressing many aspects of a system of cities such as the size, location and specialisation of cities as well as the uneven circulation of people, goods and information among them (Berry 1964; Bourne and Simmons 1978; Pred 1977). The definition of a ‘system of cities’ by Allan Pred (1977, p.13) is still valid today: “a national or regional set of cities that are interdependent in such a way that any significant change in the economic activities, occupational structure, total income or population of one member city will directly or indirectly bring about some modification in the economic activities, occupational structure, total income or population of one or more other set members”. Nowadays, this definition can also be extended to global urban systems because of long-distance interrelationships between cities, particularly those at the top of national urban hierarchies becoming more common. Since the 1990s the literature on systems of cities has developed further and expanded, but the current landscape of research appears rather fragmented. Increasingly the term ‘paradigm change’ is used by researchers willing to position themselves in opposition with ‘classical’ approaches. For example, Friedmann (1995) talked about a ‘world city paradigm’, by which he meant an encompassing approach of different aspects of intercity relations at the global scale, which tended to be studied separately. Also Capello (2000) and Meijers (2007) suggested a ‘paradigm change’, claiming that the classical Central Place model was unable to describe contemporary trends in the pattern of intercity relations. More recently, Batty (2013) wrote about the rise of a ‘new paradigm’ in the conception of cities. Building on previous works that consider cities as emerging from the multitude of interactions between individuals, he underlines that processes of centralised decision-making such as planning and governing have a limited influence on cities. These different theoretical positions have an important impact on the research approach. While some studies focus more on stakeholders (Alderson and Beckfield 2004; Sassen 1991), others look at the emergent properties of a system of cities by considering the basic interactions between urban agents, for instance in a simulation framework (Sanders et al. 1997) or in the methodological individualism of economics (Fujita et al. 1999). There are also different positions regarding the scale at which the most important urban processes take place. For some researchers, in the context of globalisation, the global scale has become most determinant (Taylor and Derudder 2015). For others, the erosion of national borders in this context puts the regional scale at the centre of economic processes (Kloosterman and Musterd 2001; Parr 2014). Somewhere in between these scales is a research stream stressing the importance of the national scale given its determining influence on many structures and parameters that experience strong path dependencies (Pumain 1997; Bretagnolle and Franc 2017). Differences in ontological and epistemological perspectives translate into wildly varying objectives of research, ranging from identifying universal laws of urbanisation (Bettencourt et al. 2007) to much more policy oriented studies (Meijers and Romein 2003). This variety in objectives partly relates to the disciplinary background and sources of influence of the researchers. While theoretical and quantitative geographers and physicians will look for basic mechanisms, planners will aim to give policy recommendations. As the field exploring relationships between cities has received contributions and influences from many different disciplines such as geography, regional science, sociology, economics, physics and the interdisciplinary movement of complexity theories, it seems that increasingly separate approaches or subfields have emerged. This paper aims to answer the following questions: How did the system of cities literature evolve over the last two decades? Which different schools of thought can be distinguished and what are their defining elements? And, to what extent do these schools interact? Assessing interdisciplinarity in this research field is all the more important given the frequent calls for interdisciplinarity in urban systems research (Pflieger and Rozenblat 2010), and because there is clear evidence that innovation in geography – still the main discipline addressing systems of cities - is fostered by collaborations among disciplines (Ducruet and Beauguitte 2013). This study of the evolution of the urban systems literature does not take the form of a classical literature review paper. Rather it adopts a bibliometric approach to analyse a set of 1491 papers on intercity relationships from 1995 onward. The main advantage of this method is that certain bias in reviewing the literature can be avoided, such as a too narrow disciplinary point of departure. Indeed, during our readings, we noticed that certain studies were ignoring entire parts of the field. By limiting human intervention on the collection of the set of papers, we intend to overcome this bias and show the diversity of the field and its complex internal structure. This does not mean that our approach can replace extensive readings, at the contrary, but it allows to get a ‘bird’s eye view’ for further exploration. Our approach is inspired by the hyper-network approach, which combines the analysis of semantic and citation networks. This approach has for instance been applied recently to the papers of a journal (Raimbault 2017), or to the classification of a large set of patents (Bergeaud et al. 2017). The approach entails a two-step approach. First, following Chavalarias and Cointet (2013) who define scientific fields “as sets of ‘keywords’ delineating a research area”, a semantic network based on co-occurrences of words in the titles and abstracts of the set of papers is extracted. This allows then to identify the different subfields or schools of thought on systems of cities. Second, the pattern of citations of the papers developed within these schools are analysed to understand the connections between the different subfields. The following section presents and discusses the bibliometric method as a way to undertake a literature review, as well as the delineation procedure that is needed to select a relevant corpus of texts (section 2). In the subsequent section, we present the content-analysis based on the vocabulary of the papers and how differences in vocabulary allow to identify different schools of thought. In addition, we explore the relations between these different schools of thought by analysing the evolution of citation patterns (section 3). The last section concludes and discusses the implications of our findings (section 4).",25
18.0,3.0,Networks and Spatial Economics,07 July 2018,https://link.springer.com/article/10.1007/s11067-018-9411-4,A Network Approach to Link Visibility and Urban Activity Location,September 2018,Asya Natapov,Daniel Czamanski,Dafna Fisher-Gewirtzman,Female,Male,Female,Mix,,
18.0,3.0,Networks and Spatial Economics,27 July 2018,https://link.springer.com/article/10.1007/s11067-018-9416-z,Are ‘Water Smart Landscapes’ Contagious? An Epidemic Approach on Networks to Study Peer Effects,September 2018,Christa Brelsford,Caterina De Bacco,,Female,Female,Unknown,Female,"In the face of increasing climate variability and growing water demand associated with rising populations, policymakers are faced with the harsh reality of water scarcity. Conservation measures targeting outdoor landscaping have become popular because consumers often are unaware of their outdoor water use, suggesting that substantial savings may be generated with even small incentives and changes in customer awareness. “Cash for grass” style water conservation programs have become an important water conservation strategy in the western US over the last two decades. The Southern Nevada Water Authority’s (SNWA) Water Smart Landscapes program (WSL) pays homeowners to replace their lawns with xeric (desert) landscapes, one of the longest running “cash for grass” programs. While the difference in watering requirements of mesic vs. xeric landscaping are well established, and short-run savings have been demonstrated in a few cases a number of questions are unanswered about turf-removal subsidy programs (Brelsford and Abbott 2017). WSL participation is highly visible to neighboring homes, and may trigger a mechanism of social learning that can impact the neighbors’ adoption behavior. This stimulates a key research question: can we observe neighborhood based peer effects in household participation in Las Vegas’ Water Smart Landscapes program? Measuring the influence that an individual has on their peers is important for estimating the indirect benefits a conservation policy may create, but distinguishing true peer effects from homophily, correlated unobservables, and reflection is econometrically challenging. Modeling peer influence on networks through epidemic modeling techniques may provide a useful tool for identifying peer effects without the bias that these challenges can introduce. We analyze a very high resolution dataset from the WSL program in Las Vegas, Nevada. This dataset includes both the day of application, and the day of completion of all program requirements for each participating household in the Las Vegas Valley Water District Service area. These data thus allow us to differentiate between the decision to participate in WSL and its implementation. The challenges to unbiased identification of peer effects are substantial in the absence of experimental data, and have been well described by Manski (1993, 2000), Brock and Durlauf (2001), Soetevent (2006), Aral et al. (2009), Shalizi and Thomas (2011), Angrist (2014), Ryan (2017) and others. There are three major challenges to the econometric identification of peer effects. First, reflection between two participating homes or individuals with similar adoption times, makes it difficult to determine which participant influenced the other. Second, homophily, where households that are predisposed to participate in the WSL program self-select into the same neighborhoods. Finally, correlated unobservables may also spuriously generate the appearance of peer effects, when in fact, there are spatially mediated unobserved factors which differentially influence households to participate in WSL. For example, perhaps local environmental conditions make grass particularly difficult to keep alive due to locally higher temperatures, excessive wind, or poor soil quality. Several recent and working papers by Bollinger and Gillingham (2012), Graziano and Gillingham (2015), Baranzini et al. (2017) have explored the role of peer effects in solar panel adoption and have made important progress in developing techniques to address the challenges described by previous authors. Towe and Lawley (2013) identified peer effects among homes in a neighborhood in the context of home foreclosures. There are important similarities between WSL program adoption and solar panel adoption, but the existence of a peer effect has not been explored in “cash for grass” style subsidy programs. Bollinger and Gillingham (2012) exploit the lag between the decision to adopt and actual installation of solar panels to address the reflection problem. Graziano and Gillingham (2015) handle the definition of peers in a spatially disaggregated manner, which avoids some kinds of boundary problems that aggregating to a geopolitical boundary induced in Bollinger and Gillingham (2012). Graziano and Gillingham also use a rich set of spatial and temporal fixed effects to address correlated unobservables. The core analytical method used by each of Bollinger and Gillingham; Graziano and Gillingham and in the Baranzini et al. (2017) working paper are variations on linear regression with fixed effects at an appropriate level of spatial aggregation. They estimate the space and time specific fraction or count of eligible places that may adopt solar panels in that time step. This method does not explicitly address the fact that solar panel adoption is a one-time event, and places that have already installed solar panels cannot do so again, no matter how much stronger the influential factors may become. Thus, hazard models such as that employed by Towe and Lawley may be conceptually more appropriate to study peer effects in one-time behaviors. The Cox proportional hazard model Towe and Lawley employ implicitly requires a strictly multiplicative relationship between the various factors influencing the household hazard rate. An additive or additive-multiplicative relationship between the various factors that influence participation (as allowed in the linear models employed by Gillingham and co-authors) is desirable. A purely multiplicative model forces the hazard rate to zero if either one of the epidemic or endemic effects is zero. This does not happen in models where the hazard rate is instead a superposition of epidemic and endemic effects, as in our case or additive-multiplicative survival models as in Höhle et al. (2005), Höhle (2008, 2009). An additive-multiplicative model, which allows for non-zero probabilities of participation when some factors are zero might be more appropriate in cases like ours, where we want to allow a baseline participation probability to contribute to the hazard rate even in the event the peer effect is zero (and vice versa). However, the types of hazards models that have been employed in the economics literature cannot be recast into an additive-multiplicative model, while SEIR style models from mathematical epidemiology can be. In order to combine and extend the novel contributions of each of these papers, we follow Bollinger and Gillingham’s approach to address reflection issues by using the gap between the decision to participated in the WSL program and actual adoption. We fully exploit the spatially rich parcel level nature of our dataset to develop an accurate, household specific model of peers, additionally exploring the importance of both Euclidean and on-road travel distance to define the set of peers attributed to each home. Finally, drawing from and extending the ideas used by Towe and Lawley to select a hazard model as the main choice for inference, we model the influence of peers using an epidemic modeling approach on networks at the household level which permits an additive/multiplicative model of influence. This paper contributes to filling the gap in cross field interactions that has been identified by Ducruet and Beauguitte (2014). Identification of peer effects is a core problem in economics and an area of active research, e.g. Towe and Lawley (2013), Graziano and Gillingham (2015), and Baranzini et al. (2017). In this paper, we show that an additional strategy for peer effects identification that relies primarily on methods from computational network science, rather than economics can also be used. We believe this interdisciplinary perspective can enrich both economics and network science, by bringing a rich data set and explicitly spatial perspective to the computational network science perspectives, and also inviting more explicit consideration of the underlying network and transmission dynamics that are implicit in typical economic models of peer effects. The language of contagion, infection, and transmission has been widely used in economic research on peer effects but the application of the mathematical tools developed for the purpose of modeling disease transmission in this context is novel. The epidemic modeling approach we use allows the use of an additive/multiplicative model of influence instead of a strictly multiplicative model, even within a hazards context. It also permits complete flexibility in modeling transmission dynamics on networks and makes the implicit network structure used in the aggregate linear models completely explicit. This allows adoption patterns on the actual network to be simulated, so predicted transmission dynamics can be compared to actual dynamics. Because self-sorting into neighborhoods (homophily) and other correlated unobservables change slowly relative to the rate of program adoption, comparison of simulated temporal dynamics with or without a peer effect to actual temporal dynamics should provide evidence about the existence of a peer effect without bias from homophily and correlated unobservables. Finally, networked epidemic models of peer effects open an opportunity to explore a variety of factors that have been considered in epidemiological studies, but not in a conservation program adoption context. For example, this method could be used to study how spatial properties of neighborhoods and cities might influence program adoption; or allow simulation for how targeted campaigns may speed program adoption. In this paper, we strive to use the language of both epidemiology and economics in a way that is intelligible to readers in either field. A home that has had a landscape conversion recorded through WSL can be referred to as active or infected. In this paper, the epidemic effect is synonymous with the economic term peer effect: the role that one home’s WSL adoption behavior plays in influencing their neighbors’ eventual participation probabilities. The endemic effect contains all of the reasons a household might participate in WSL except for the influence of their neighbors’ participation, and is thus a major component of an individual homes activation probability. A home that autoinfects is one that becomes active independent from the influence of their neighbors. We adapt epidemic modeling tools to the inference of peer effects by mathematically modelling the dynamics of participation in the WSL program as a discrete-time epidemic spreading model SEIR with autoinfection. In contrast with standard epidemic models, we allow a house to activate, i.e. adopt the program, because of a combination of endemic causes in addition to the standard epidemic transmission, where infection is ‘transmitted’ along the network by a neighboring house through the process of social influence. A susceptible home in this model is one that has no recorded prior WSL conversions. An exposed home is one that has submitted an application for a WSL conversion to the SNWA, and thus has already made the decision to activate, but has not yet completed the landscape conversion process. Infectious homes have completed the landscape conversion process as required by SNWA, and so their changed landscape is visible to their neighbors. A home that has recovered is one in which the WSL conversion has happened sufficiently long ago that it no longer influences its neighbors activation probability, not that the home has reverted from a xeric landscape back to turf. We find evidence of peer effects in several of the sample neighborhoods. The presence of WSL participating neighbors increases a non-participating house’s probability of adoption. For neighborhoods where we see a peer effect, the inclusion of this effect in addition to the endemic effect means that we both obtain better model likelihoods, and we are also better able to predict the evolution of adoption dynamics for several months after the observation period. These two strategies for identifying the best model will not necessarily provide the same result. In addition, we find that a WSL participating neighbors’ influence on their peers fades in time with finite recovery rates on the order of a few months to a year, after which the influence fades. The remainder of the paper is structured as follows. Section 2 covers the relevant history of the WSL program and other important characteristics of the adoption environment in Las Vegas during the study period. Section 3 describes the dataset we use. Section 4 describes the analytical methods used. Section 5 presents the results of the conditional probability estimates, as well as inference and prediction performance results. Section 6 concludes.",5
18.0,3.0,Networks and Spatial Economics,24 July 2018,https://link.springer.com/article/10.1007/s11067-018-9417-y,Is the Urban World Small? The Evidence for Small World Structure in Urban Networks,September 2018,Zachary Neal,,,Male,Unknown,Unknown,Male,"An interest in networks has been present in urban studies for at least several decades (e.g. Haggett and Chorley 1969), and frequently network analyses within urban studies employ analytic techniques developed in related social science disciplines (e.g. centrality indices in sociology; Freeman 1978). However, a new brand of network analytic techniques have recently emerged, developed primarily in physical science disciplines such as physics, under the heading of network science (e.g. Newman 2010). The techniques in the network science toolkit often focus on the topological structure of the graph as a mathematical object, rather than on the substantive content of the network as a conceptual object. They have found application in urban studies in a number of ways, including the detection of communities and the identification of specific degree distributions (e.g. Expert et al. 2011; Guimera et al. 2005; Neal 2014), however the integration of network science techniques and urban studies concepts has been uneven (Ducruet and Beauguitte 2014). This has been particularly true in the case of one network science focus: small world networks (Barthélemy 2011; Rozenblat and Melançon 2013). In 1998, Watts & Strogatz published what would become a landmark paper in the emerging network science literature, defining a class of networks they called “small world.” This name derived from a much earlier social psychology experiment – the small world experiment – which suggested that everyone in the world is connected to everyone else by just a few intermediary acquaintances (i.e. by “six degrees of separation”; Milgram 1967). The structure of networks belonging to this class helped explain how such a social phenomenon was possible. Watts and Strogatz (1998) initially showed that a small world structure could be observed in a network of Hollywood actor collaborations, the transmission line network of the U.S. power grid, and the neural network of the nematode C. elegans. This triggered a rush among network scientists, working in a variety of fields and with data from many different contexts, to identify and document empirical examples of small world networks (see Schnettler 2009, for a review). For example, Humphries and Gurney (2008) examined 33 different networks ranging from scientific coauthorship to linguistic associations to protein interactions. Researchers studying urban networks – both networks of cities and networks in cities – have also participated in this exercise, but because their work took place in a variety of disciplines, no definitive answer has emerged to the question: Is the urban world small? The purpose of this paper is to seek an answer to this question through a systematic review of the evidence for small world structure in urban networks. To this end, I begin in section 2 by briefly reviewing what a small world network is, and why this class of network structure might be important in the urban context. In section 3, I describe a systematic search for all urban networks described in sufficient detail to evaluate whether they exhibit a small world structure, and review the three most widely used indices of small worldliness, which I then use to evaluate these networks in section 4. Finally, I conclude in section 5 with a discussion of these findings, focusing on recommendations for future research aimed at discerning whether an urban network is small world.",16
18.0,3.0,Networks and Spatial Economics,05 December 2018,https://link.springer.com/article/10.1007/s11067-018-9425-y,Assessing Daily Urban Systems: A Heterogeneous Commuting Network Approach,September 2018,Ann Verhetsel,Joris Beckers,Michiel De Meyere,Female,Male,Male,Mix,,
18.0,3.0,Networks and Spatial Economics,27 October 2018,https://link.springer.com/article/10.1007/s11067-018-9426-x,Measuring the Impact of Street Network Configuration on the Accessibility to People and Walking Attractors,September 2018,M. Bielik,R. König,T. Varoudis,Unknown,Unknown,Unknown,Unknown,,
18.0,3.0,Networks and Spatial Economics,25 January 2019,https://link.springer.com/article/10.1007/s11067-018-9430-1,Disentangling link formation and dissolution in spatial networks: An Application of a Two-Mode STERGM to a Project-Based R&D Network in the German Biotechnology Industry,September 2018,Tom Broekel,Marcel Bednarz,,Male,Male,Unknown,Male,"Network analysis has gained great popularity in many spatial disciplines (Ducruet and Beauguitte 2014). For instance, in urban studies, network analyses are intensively used to study city-networks (Liu et al. 2013), while economic geography focuses on R&D networks’ facilitating of the flow of knowledge between cities and regions (e.g., Murphy 2003; Boschma and Ter Wal 2007). In both fields, studies have sought to explain the evolution of inter-organizational relationships in time and space by relying on longitudinal network data (Broekel et al. 2014). Most of the existing research focuses on the relative importance of factors facilitating link formation. Crucially, network evolution consists of link formation and dissolution processes, though different factors might drive each process. For instance, Balland (2012) noted “[…] that the creation and dissolution of ties are not generally strictly inverse mechanisms […]” (p. 749). Moreover, Krivitsky and Handcock (2014) explained that “social processes and factors that result in ties being formed are not the same as those that result in ties being dissolved” (p. 35). For instance, in order to benefit from scale effects, firms might participate in joint R&D projects with other firms that have a similar technological background (i.e., they are cognitively proximate). Over the course of the project, they realize that their technological similarity stimulates unintended knowledge spillovers, and they end the collaboration to sustain their competitive advantages. Hence, cognitive proximity fostered collaboration in the first place and subsequently increased the likelihood of an early termination of the collaboration. However, while substantial empirical evidence of the first process exists, much less attention has been paid to the second process. The present paper contributes to the spatial network literature in two ways. Firstly, it demonstrates the use of separable temporal exponential random graph models (STERGMs) as a method for investigating formation and dissolution processes in spatial (knowledge) networks (Krivitsky and Handcock 2014). We apply STERGM to a spatial network emerging from subsidized R&D projects in the German biotechnology industry between the years 1998 and 2013. Secondly, we demonstrate STERGM’s ability to handle two-mode network data, which overcomes the (still) common but sometimes questionable one-mode project of network data when constructing spatial (knowledge) networks (Scherngell and Barber 2009, 2011; Balland 2012; Hoekman et al. 2013; Broekel and Hartog 2013b; Buchmann and Pyka 2015). We thereby extend the work of Liu et al. (2015), who applied a cross-sectional two-mode exponential random graph model to analyze global city networks by presenting an application of ERGMs to longitudinal data. While, alternatively, such data can be investigated with stochastic actor-oriented models (SAOMs) (Liu et al. 2013), these models require specific assumptions (e.g., agency) that are often doubtful in the context of spatial networks (Broekel et al. 2014). In addition, STERGMs have been shown to be empirically similar if not preferable to SAOM models (Leifeld and Cranmer 2015). This paper is organized as follows: Section 2 discusses the process of an inter-organizational R&D cooperation network evolution. It addresses the relevance of organizations’ attributes, their relational characteristics, and structural level effects. It also considers why existing empirical analyses on their relative importance might be biased, which motivates the use of STERGMs. The STERGM approach is introduced in Section 3. Section 4 discusses the network data and the empirical model specification. The analyses’ results are presented and discussed in Section 5. Section 6 concludes the paper.",26
18.0,3.0,Networks and Spatial Economics,07 May 2019,https://link.springer.com/article/10.1007/s11067-019-09452-x,Urban Activity Mining Framework for Ride Sharing Systems Based on Vehicular Social Networks,September 2018,Bilong Shen,Weimin Zheng,Kathleen M. Carley,Unknown,Unknown,Female,Female,"Ride sharing systems can solve traffic jams, bring down transportation fees, and make the environment better by utilizing the empty seats of vehicles. There are many implications for ridesharing. Firstly, ride sharing has both direct economic consequence and indirect economic benefits (Lee 1984). One study suggests that sharing cabs can reduce the cumulative trip length by at least 40% and reduce service costs through split fees (Santi et al. 2014). Secondly, it also good for less emissions, less noise, and less fuel consumption. E.g. Decrease in vehicular homicide (Greenwood and Wattal 2015), decrease in emissions (Minett and Pearce 2011). Thirdly, it also can alleviate traffic jams, make integration of idle resources. As there are so many benefits, more and more people are starting to engage in the research of ride sharing. Vehicle allocation, price strategy, route planning, waiting time for passengers, and other factors all determine the experience of ride sharing systems. It is necessary to effectively discover urban activity pattern for improving ride sharing systems. With the popularization of mobile networks and the development of GPS, the large-scale trajectory data and the electronic map containing POI information brought new opportunities for ride sharing systems. These days, it is effortless to collect the trajectories of moving objects, for example from taxi trips to check-in data. GPS equipment generates ample activity data. The data includes not only the route of vehicles or devices but also the activity of humans in cities. This information gives insight into daily life patterns, movement features, and function of cities. The POI data records the characteristics of different regions in the city. The different POI affects people’s activities in the different region. The POI data contains features as identifier, location, content, and category. POI data can also be used to analyze urban activity. The urban activity mining based on trajectories and POI can be used to improve ride sharing systems. For example, it is necessary to formulate a vehicle dispatching strategy based on the activity to rationally allocate the vehicle according to the condition of passenger requests, and to avoid the occurrence of partial capacity over saturation and passenger requests that cannot be served in other areas. However, the state of the art methods have two disadvantages. Firstly, only focusing on matching current requests can lead to unbalanced vehicle allocation. For example, if the system only considers request-vehicle-matching at the current time. After the system runs for a while, there may be too many vehicles in some places, the driver cannot have enough order services, and in other places, the vehicles are insufficient for passengers. Secondly, POI features do not contain information that can reveal the importance of the POI. For example, two restaurants may have the same category and similar names, but one may produce very delicious food and be very popular, while the other may do not have enough customers. If we describe the POI feature of a location by statistics of POI, the POI feature of the location is still imprecise. For example, ten small pharmacies are not more visited than one large airport. We cannot distinguish the impact of them by the name and category features, or by statistical methods. These limitations result in distortions of characteristics of the urban activity. To reveal the relationship between different regions to guide vehicle-demand balance and utilize POI to describe the region feature precisely, we propose a vehicular social network based analytical framework for ride sharing systems. The framework contains two key elements. First is embedding POI feature generation for location description by real activity. Second is the construction of flow network by network theory and special features for ride sharing systems. To constructcial network of cities, we partition a city into different regions. Each region is a node of the network. The edge between two nodes is activity between different regions. The activity is represented by historical location changing by vehicle trajectories. Based on the above, the vehicular social network is constructed and we utilize network theory to analyze the activity patterns. The progress of analysis time is just cost minutes to finish. The contribution of this paper consists of four points: 
 We propose a novel trip mapping method named Trip-Embedding POI Decomposition Method (TEPID) to describe the POI feature of different regions. This method overcomes the shortage of grid-based partition map decoding based on static features such as name, category, and location, which is not able to precisely describe the POI feature. We propose a Vehicular Social Networks Based Analytical Framework (NBAF) to mining the activity patterns for ride sharing systems. In this framework, special features for ride sharing systems are considered. We propose a novel clustering method to reveal activity and feature of different locations for ride sharing systems. The revealed activities can be used to design commute ride sharing systems and guide route planning for ride sharing systems. We evaluated our method using large-scale and real-world datasets, consisting of POI datasets of New York City, containing 30 days, 11,135,471 trips in 2016 from New York City. The rest of the paper is organized as follows: We discuss related work in Section 2. In Section 3, we introduce the concept of spatial metrics, network metrics, and our analysis goal. In Section 4 we introduce our urban activity analysis framework. Vehicular social network generation, feature construction, and clustering methods are also discussed in this section. Further, we introduce experiments and the corresponding results in Section 5. Finally, we conclude the article and discuss the future work in Section 6.",5
18.0,3.0,Networks and Spatial Economics,07 November 2018,https://link.springer.com/article/10.1007/s11067-018-9427-9,Street Network Studies: from Networks to Models and their Representations,September 2018,Stephen Marshall,Jorge Gil,Lucas Figueiredo,Male,Male,Male,Male,"With the increasing urbanisation and the associated relevance of urban studies exploring the environmental, economic, demographic and social dimensions of cities, street networks have become a central object of global scientific interest over the last fifty years. Because street networks support a wide range of urban processes they attract attention from scholars in many disciplines, including transport and urban planners, architects, geographers, environmental psychologists and, recently, physicists. Street network studies include investigations into network structure, connectivity, centrality, circuity, traversal, hierarchy, typology and evolution (e.g., Courtat et al. 2011; Crucitti et al. 2006; Giacomin and Levinson 2015; Jiang and Okabe 2014; Lagesse et al. 2015; Louf and Barthelemy 2014; Masucci et al. 2014; Stavroulaki et al. 2017; Strano et al. 2012; Xie and Levinson 2007; Yerra and Levinson 2005). This cross-disciplinary interest and the plurality of scholarly approaches and purposes is a welcome sign of scientific relevance. Yet the lack of communication between these approaches raises a problem of methodological and terminological fragmentation and entrenchment. There is then the risk of duplicated, contradictory or incommensurable results and a lack of replicability. The various quantitative, computational studies of street networks predominantly apply methods based on graph theory and network science (Newman 2003; Brandes et al. 2013; Ducruet and Beauguitte 2014; Kivelä et al. 2014). However, as Butts (2009: 416) has pointed out, ”To represent an empirical phenomenon as a network is a theoretical act (...) the appropriate choice of representation is key to getting the correct result”. Graphs are, in other words, only a mathematical abstraction, a formal representation of a model removed from the physical reality of street environments through a process of abstracting and modelling. This involves generating a simplified representation of the street network by singling out the main elements of study and identifying their relations. Crucially, it determines what will be represented as nodes (vertices) and links (edges) in a graph and what additional parameters of the street network the graph should capture. We call this step network modelling, which is normally embedded in a larger model of a specific phenomenon. The plurality of approaches is not always evident in the literature. For instance, from Network Analysis in Geography (Haggett and Chorley 1969) to a recent review on spatial networks (Barthelemy 2011), the dominant network model is one that represents the street junctions as vertices in the graph and the linear street segments as its edges. In street network studies, this step is often not commented upon, not necessarily performed as a conscious modelling decision. Yet, it is a selective decision that determines analytical possibilities (Anez et al. 1996; Winter 2002; Meeteren et al. 2016). Thus, in the network modelling step researchers often simply follow disciplinary precedents, unaware of, or unclear about, the diversity of approaches available in other fields of urban research. As a result, they may reproduce past studies inconsistently and under different nomenclature. Some authors acknowledge alternative street network models, for example (Anez et al. 1996; Batty 2004a; Winter 2002; Porta et al. 2006a, b). These different models are, however, often presented from a narrow, application-specific perspective. Here, we call for a broader reflection on the relative suitability of broad families of network models. We open up this debate by analysing the different approaches to street network modelling (Section 2), including the consideration of their graph representations and the differences in nomenclature and the roles of topology, geometry, directionality and weights (Section 3); and the different types of network data used and the manipulations performed to support diverse analyses (Section 4). In highlighting their purpose and characteristics, we finally argue that widening the range of approaches available to each discipline and clarifying the relations between them opens up opportunities for developing street network studies more comprehensively and effectively (Section 5).",59
18.0,3.0,Networks and Spatial Economics,23 March 2019,https://link.springer.com/article/10.1007/s11067-019-09450-z,"World City Networks Shaped by the Global Financing of Chinese Firms: A Study Based on Initial Public Offerings of Chinese Firms on the Hong Kong Stock Exchange, 1999-2017",September 2018,Fenghua Pan,Ziyun He,Jinshe Liang,Unknown,Unknown,Unknown,Unknown,,
18.0,4.0,Networks and Spatial Economics,22 September 2017,https://link.springer.com/article/10.1007/s11067-017-9367-9,Multiperiod Multi Traveling Salesmen Problem Considering Time Window Constraints with an Application to a Real World Case,December 2018,Haluk Yapicioglu,,,Male,Unknown,Unknown,Male,"Anadolu University, founded in 1958, has the third largest open education system in the world (List of largest universities by enrollment 2015). According to the university’s 2014 annual report, the open education system has nearly 1.4 million students throughout Turkey. One of the major activities of this massive open education system is evaluating students’ success. For this, Anadolu University organizes exams in 114 exam centers in Turkey four times a year, using multiple choice tests. Exam centers are located in 81 city centers and 33 densely populated towns throughout Turkey. In each exam center, a local exam organization unit is responsible from managing the activities necessary to execute the exam, such as finding an adequate number of buildings in which exam is going to be held, finding enough manpower to handle various activities related to the exam, and securing the exam material before and after the exam is held. Exam buildings are selected from public schools and public university buildings in the city. In order to ensure that the exam is being held in accordance with the standards set forth by Anadolu University, the university sends representatives from Eskisehir, the city where the University campus is located, to all exam centers. The number of university representatives are determined based on the number of exam buildings in exam centers. Once the university representatives arrives at the designated city, they need to visit all the buildings, where the exam is held, at least once. The two-day exam consists of two sessions in each day, which sums up to four sessions in total. The duration of sessions might be different within sessions and it may also be different across different exam buildings in the same session. The university management requires the university representatives to visit every exam building at least once in one of these sessions during the exam hours. In small cities and towns where there are only few exam buildings, this is a fairly straightforward task. However, in major cities such as Ankara, Istanbul and Izmir ensuring that every exam building is visited by a university representative becomes more and more difficult. There are several contributing factors to this situation: Firstly, the university representatives do not have the information on road network, travel paths and travel times across exam buildings. Therefore; determining a route in the presence of missing information is difficult. Second, since exam duration varies across sessions and exam buildings, this has to be taken into account in determining the route of a university representative during a session. Third, due to missing information, it is possible to assign less than required number of university representatives to major cities, in which case even if all university representatives have perfect information on the city, they would not be able to visit all exam buildings. From one exam organization to the next; number of buildings used, location of these buildings, and the duration of the exam on these buildings in sessions changes as well. Consequently, a solution developed for one exam organization probably cannot be applicable to subsequent exam organizations. These exams are held four times a year and each exam organization takes about two months from its start to the completion. Considering the other activities involved in the exam organization process, a quick and reliable method to determine the number of university representatives and their paths in major cities is required. This study aims at developing a solution approach so as to help decision makers to assign an adequate number of university representatives to the exam centers so that all exam buildings in an exam center is visited at least once. To this end, a mathematical programming formulation of the problem is developed. The model is a Mixed Integer Linear Programming (MILP) model that considers the number of university representatives and assignments of exam buildings to the university representatives considering both sessions and the duration of these sessions. MILP model also considers the minimization of the total distance covered by all university representatives. Solving the model even for moderate size problems proved to be difficult (Yapicioglu 2014), so two heuristic approaches based on evolutionary computation methods are developed, and performance of these approaches discussed in this study. As explained in detail in subsequent sections, the MILP formulation proposed in Yapicioglu (2014) is only solved with an objective function of aiming at the minimization of the number of university representatives, so solving the model with the objective function of minimizing the total travel distance is another contribution to the existing literature. Contribution of this study is as follows: The model presented in this study is the most general definition of the single depot traveling salesman problem (SDTSP). The model considers time window constraints, multiple traveling salesmen and multiple periods at the same time. Unlike the models presented before (Gilbert and Hofstra, 1992) the model allows multiple cities to be visited in a single period. Moreover, the lengths of the periods may also vary. The minimization of the total number of salesmen used in a solution, which is assumed to be constant and known in previous studies, is introduced as a decision variable. Along with the model proposed, a new solution representation and solution decoding and encoding mechanisms are developed, which can be used in other traveling salesman problems with time windows with slight modifications such as school bus routing problems, waste collection vehicle routing problems and scheduling problems. The organization of the manuscript is as follows: In §2, the model is introduced and a performance of the mathematical programming approach is discussed. In §3 the solution representation, encoding and decoding mechanisms and the implementation details of solution approaches, namely the simulated annealing and the robust tabu search is presented. In order to evaluate and compare the two solution approaches, numerical examples are used. Findings of these experiments are reported in §4. In addition to these examples, a real case taken from one exam organization in Izmir discussed in §5. The paper concludes with the future research directions in §6.",3
18.0,4.0,Networks and Spatial Economics,29 November 2017,https://link.springer.com/article/10.1007/s11067-017-9374-x,Author Correction: Multiperiod Multi Traveling Salesmen Problem Considering Time Window Constraints with an Application to a Real World Case,December 2018,Haluk Yapicioglu,,,Male,Unknown,Unknown,Male,,
18.0,4.0,Networks and Spatial Economics,09 May 2018,https://link.springer.com/article/10.1007/s11067-018-9400-7,Network Effects of International Shocks and Spillovers,December 2018,Alexei Kireyev,Andrei Leonidov,,Male,Male,Unknown,Male,"International spillovers reflect the impact of macroeconomic changes, possibly following a policy action, in one country on other countries. The spillovers are possible because of the integrated nature of the international economy, where any country is linked to other countries across the world by multiple flows captured in its balance of payments. Such patterns reflect the multilayer network effects of balance of payments flows generated in the epicenter country and propagating to its economic partners across the world. International spillovers originate from a shock at the epicenter country. Usually, such shocks are driven by an unexpectedly lower GDP growth compared with the baseline projections. The lower than expected growth can reflect domestic developments in the epicenter country, such as a domestic banking crisis, loss of consumer confidence, fiscal contraction, or exogenous developments such as a drop in international prices for the main export commodity, natural disasters, or geopolitical crises. Spillovers may also originate in policy actions taken unilaterally by governments, such as a decision to restrict imports based on political considerations (sanctions) or in retaliation for its trading partners’ actions (dumping, export subsidies). Spillovers operate through several channels. Any balance of payments flow can be a potential channel of shock transmission, with varying implications for partner countries depending on the channels involved. Trade and financial flows are the most important channels of shock spillovers for most countries. Growth slowdown usually has a negative impact on demand for imports of the affected country with substantial spillovers on its trading partners. From the supply side, such shocks can disrupt global supply chains and would negatively affect production in partner countries. Financial spillovers are also important as cross-border claims of banks and equity holdings have grown recently. Other channels that may be important for some countries include remittances, direct and portfolio investment, tourism, and commodity prices. Countries may amplify, absorb or block spillovers depending on the structures of their economies. The purpose of the paper is to develop a method for assessment of the network effects in cross-border shock spillovers. The network effects are defined as second-round effects derived from the network structure of balance of payments flows. Such effects have been largely disregarded in the existing literature on spillovers but can be substantial and at times exceed the initial shock. This paper proposes a method for quantifying the network effects using a nominal demand shock as an example. The method consists of a sequential transformation of the inflow-outflow matrices of bilateral flows, and captures spillovers from the initial shock and the subsequent network effects, including spillin and spillback effects. The method is illustrated by application to spillovers from an import demand shock in a large country (China) and a medium country (Ukraine) through the trade channel. To model international spillovers on a network, the paper proceeds as follows. Section II reviews the exiting literature and proposes a network model of economic spillovers. Section III discusses the empirics of spillovers in a network context. Section IV applies the model to spillovers originating from a large and a medium country. Finally, section V presents conclusions and practical recommendations.",6
18.0,4.0,Networks and Spatial Economics,26 June 2018,https://link.springer.com/article/10.1007/s11067-018-9405-2,Correction to: Network Effects of International Shocks and Spillovers,December 2018,Alexei Kireyev,Andrei Leonidov,,Male,Male,Unknown,Male,,
18.0,4.0,Networks and Spatial Economics,30 May 2018,https://link.springer.com/article/10.1007/s11067-018-9398-x,"Urban Growth, Transport Planning, Air Quality and Health: A Multi-Objective Spatial Analysis Framework for a Linear Monocentric City",December 2018,Judith Y. T. Wang,Richard D. Connors,,Female,Male,Unknown,Mix,,
18.0,4.0,Networks and Spatial Economics,07 August 2018,https://link.springer.com/article/10.1007/s11067-018-9412-3,Agent-Based Modelling of Locating Public Transport Facilities for Conventional and Electric Vehicles,December 2018,Chengxiang Zhuge,Chunfu Shao,,Unknown,Unknown,Unknown,Unknown,,
18.0,4.0,Networks and Spatial Economics,01 August 2018,https://link.springer.com/article/10.1007/s11067-018-9415-0,An Effective Bilevel Programming Approach for the Evasive Flow Capturing Location Problem,December 2018,F. Hooshmand,S. A. MirHassani,,Unknown,Unknown,Unknown,Unknown,,
18.0,4.0,Networks and Spatial Economics,08 January 2019,https://link.springer.com/article/10.1007/s11067-018-9436-8,A Multi-Product Production/Distribution System Design Problem with Direct Shipments and Lateral Transshipments,December 2018,Jianing Zhi,Burcu B. Keskin,,Unknown,Female,Unknown,Female,"The supply chain landscape continues to evolve in response to unprecedented pressures to reduce costs while addressing increased customer expectations for faster delivery, increased variety, and cheaper products and services. A production distribution network serves as an effective tool for modeling the supply chain activities of a firm. In general, a production distribution system design (PDSD) problem involves the determination of the best combination of location and size of suppliers and distribution centers (DCs), their technology, and product offerings, and transportation for achieving a firm’s long-term goals (Keskin and Üster 2007a). The involved parties include suppliers, manufacturers, distributors, and retailers (customers). In this paper, we consider a three-stage network with suppliers \((\mathcal {K})\), capacitated DCs \((\mathcal {J})\), and retailers \((\mathcal {I})\) that supply multiple types of products \((\mathcal {L})\). The products are shipped from capacitated suppliers at the first stage to capacitated DCs in the second stage, and then they are transported from DCs to retailers in the third stage to satisfy the demand for various products. We assume that the locations of suppliers and retailers are known, and the capacities of suppliers and demands of retailers are also given. There exists a set of potential locations for capacitated DCs. We decide on the number and location of the DCs as well as the flows from suppliers to DCs and DCs to retailers to minimize the total cost of the network. In this paper, distinct from other PDSD studies, we allow for direct shipments from suppliers to retailers and lateral transshipments among the DCs. Figure 1 shows the basic structure of this three-stage PDSD configuration.
 A three-stage supply chain network where multiple \(|\mathcal {L}|\) products flow from \(|\mathcal {K}|\) suppliers to \(|\mathcal {J}|\) DCs and later to \(|\mathcal {I}|\) retailers The unique feature of this paper is the consideration of direct shipments from suppliers to retailers and lateral shipments among DCs while designing the overall supply chain network. The advent of online sales has made direct shipping (also known as drop shipping or direct-to-store) a common feature of many supply chains. The convenience and cost savings opportunities of direct shipping attract new firms for inclusive supply chain designs with direct shipment capabilities and force existing supply chains into a redesign. Products that are directly shipped from suppliers are typically fast turning, high velocity, and high-demand items. When a supply chain distributes multiple items, as in our problem, it is possible for some products to follow the direct shipment paths whereas others are handled with more traditional, indirect shipment. On the other hand, lateral transshipments, shipments among the same level of entities, increase the flexibility and robustness of a supply chain (Paterson et al. 2011; Firouz et al. 2017). There are two types of lateral transshipments: i) reactive transshipments that are useful to respond for emergency stock outs; and ii) proactive transshipments that help redistribute inventory throughout the supply chain and create additional dummy stages, especially if the economies of scale in transportation support it. For instance, for Retailer 4 (from the left) in Fig. 1, the supply chain becomes a 4-stage supply chain, whereas for Retailer 1, it is a 2-stage supply chain, and for Retailer 5, it is a 3-stage supply chain. In the same figure, it is possible that the last retailer is offering two products where one of them is directly shipped (2-stage supply chain), and the other one is shipped through a DC (3-stage). Allowing direct shipments and lateral shipments increase the adaptability of the supply chain to various market conditions, especially in the presence of multiple items with differing characteristics. The problem under consideration belongs to the general class of capacitated facility location problems, and it can be reduced to the p-median problem, which is known as NP-hard (Kariv and Hakimi 1979), due to the selection of p DCs. The known complexity of the problem drives us to solve the problem via heuristic approaches to find high-quality solutions to large size problem instances. Existing optimization solvers such as IBM ILOG CPLEX are efficient for solving small problem instances but time-consuming for larger ones. In many of the network design problems, the complete model needs to be re-run with different problem settings and data scenarios. For larger instances of the problem, it is harder to complete a robust analysis. Therefore, we propose heuristic methods to facilitate the decision-making process while still maintaining the solution quality. In this paper, we propose two customized meta-heuristic approaches, which are built on top of a Simulated Annealing (SA) algorithm and a Greedy Randomized Adaptive Search Procedure (GRASP) to solve the three-stage PDSD problem with direct shipments and lateral transshipments. Prior studies (Keskin and Üster 2007a, b; Camacho-Vallejo et al. 2015) have employed Scatter Search (SS) and Tabu Search algorithms to solve the PDSD problem without direct shipments and transshipments among DCs. However, to the best of our knowledge, solution approaches based on SA and GRASP have not been explored in the PDSD context. Therefore, we want to verify the efficiency of SA and GRASP on this problem since these two approaches are widely applied on p-median and covering problems. Moreover, we employ strategies of performance improvement for both SA and GRASP. In particular, we design a construction procedure for GRASP to generate a good initial solution, which can speed up the searching process for locating a high-quality solution. Also, we devise a greedy selection heuristic that accelerates the local search without sacrificing quality. The experimentation results show that both SA and GRASP are competitive with SS regarding both solution quality and algorithm efficiency. Both construction procedure and greedy selection heuristic can be applied to general p-median problems as well. The remainder of this paper is organized as follows. In Section 2, we provide a review of the related literature. In Section 3, we formulate a mixed integer linear programming (MILP) model for PDSD. Note that the MILP formulation is solved using IBM ILOG CPLEX for benchmarking purposes. In Sections 4 and 5, we present the design of the SA-based and GRASP-based solution algorithms, as well as the adjustments and improvements that are integrated into the algorithms. In Section 6, we deliver the results of the computational experiments regarding the performance of the two proposed heuristics and a comparison with the SS which serves as a benchmark. In Section 7, we conclude the paper and discuss future research directions.",6
18.0,4.0,Networks and Spatial Economics,15 March 2019,https://link.springer.com/article/10.1007/s11067-019-09455-8,Short-Term Land use Planning and Optimal Subsidies,December 2018,L. M. Briceño-Arias,F. Martínez,,Unknown,Unknown,Unknown,Unknown,,
18.0,4.0,Networks and Spatial Economics,25 March 2019,https://link.springer.com/article/10.1007/s11067-019-09463-8,Computation of Multi-facility Location Nash Equilibria on a Network Under Quantity Competition,December 2018,Blas Pelegrín,Pascual Fernández,María Dolores García,Male,Male,,Mix,,
18.0,4.0,Networks and Spatial Economics,21 November 2017,https://link.springer.com/article/10.1007/s11067-017-9373-y,RETRACTED ARTICLE: Appraisal of Science and Economic Factors on Total Number of Granted Patents,December 2018,Dušan Marković,,,Male,Unknown,Unknown,Male,"Innovations are the basis of the economic and social development of each country, whether it was a developing country or a developed country. The speed of change and the development of hyper currency, which arise as a result of the process of globalization, have led to a shift from an economy based on an industrial revolution to a new economy based on knowledge and innovation. The importance of statistics and indicators that support, monitors and measures scientific and technological activity in the broadest sense, was recognized already in the sixties of the last century when the (OECD 1963; Freeman 1969) initiated the work on the development of guidelines for measuring research and experimental development and other scientific and technological activities. Therefore, in modern, developed societies, significant material and financial resources are constantly invested in research, innovative activity and human resources as a result of forecasting and planning of sustainable development. The concept of “innovation”, which aims to systematically measure, was launched in the 1990s, due to the need to determine the effects of innovation. In addition, two problems were observed. The first problem is related to the existence of different definitions of innovation, and accordingly, there are many different measures and indicators of innovation. For this reason, many studies with mutual contradictory results appear. The second problem stems from the fact that most of the analysis of innovation measures at both the micro and macro (state and regional) level is performed in the most developed and innovative countries of the world Markovic (2017), and that the underdeveloped countries are poorly present in these surveys. As the development of new technologies is accelerated, it is necessary to develop new methodologies for the analysis of innovative development. Widely known indices that measure innovation are: Global Innovation Index, Europe 2020, Competitiveness Index, Innovation Scoreboard, Comparison of Innovation and Competitiveness of the EU and the US, Global Innovation Technology Index, Global Innovation Policy Index. In the above mentioned indices, the goal is to determine the degree of innovativeness and competitiveness of a state. In this paper, the goal is to identify and rank indicators that constitute innovation potential at macro level. Innovation potential is the basis for achieving sustainable competitiveness and development of a state. Logistics and information and communication technologies (ITC) are important channels for improving efficiency according to the paper (Coto-Millán et al. 2016). Discerning the behavior and the mechanisms underlying network effects remains an elusive but economically important research objective. In research (Westland et al. 2016) was shown that network effects can be perceived as a manifestation of the consumer adoption life cycle behaviors, and that contagion models provide an accurate description of the mechanism underlying network effects. Paper (Andergassen et al. 2015) argued that the routine firms adopt, chosen by tuning between either global or local search protocols, is crucial to determine the system’s aggregate performance and resilience. Hu and Mathews (2008) consider that patents, as a set of exclusive rights guaranteed to protect innovation, play a major role in the innovative development of the China innovation system. Gössling and Rutten (2007) developed a model for describing innovations at the region level is proposed, i.e. factors that positively or negatively influence innovative activities at the macro level (level of the region, level of the state) are identified. The number of patents was used as an output of the innovativeness. As support for the proof of the hypothesis, SPSS Software was used. Innovations and patents could also major role in economic growth which was confirmed in literature (Capello and Lenzi 2013; Batabyal and Nijkamp 2014; Batabyal and Beladi 2017;). In this study the total number of granted European patents was analyzed based on science and economic factors that represents factors of innovation potential on macro level. The factors were identified in the following literatures (Gössling and Rutten 2007; Meek et al. 2009; Bass and Kurgan 2010; OECD 2013). The inputs that have been taken into consideration in this paper refer to the main generators of innovations at macro level: Private sector, Government sector, Universities. The main goal of the study was to determine which factor has the highest impact on the number of the granted patents. The total number of the granted patents belongs to the electrical engineering, instruments, chemistry, mechanical engineering and other fields. The total number of granted patents or patent applications represents an innovation output in many studies that measure innovativeness at macro level (Audretsch and Feldman 1996; Bottazzi and Giovanni 2002; Gössling and Rutten 2007; Zoltan and Audretsch 1989). The goal of any inventor in a business sense is to patent specific product or service in order to strengthen competitive position in the marketplace. Adaptive neuro-fuzzy inference system (ANFIS), developed by Jang (1993), was used as the searching methodology.",1
18.0,4.0,Networks and Spatial Economics,01 February 2018,https://link.springer.com/article/10.1007/s11067-017-9381-y,Network Design Model to Integrate Shelter Assignment with Contraflow Operations in Emergency Evacuation Planning,December 2018,Xiaozheng He,Hong Zheng,Yongfu Li,Unknown,,Unknown,Mix,,
18.0,4.0,Networks and Spatial Economics,11 April 2018,https://link.springer.com/article/10.1007/s11067-018-9397-y,A Complex Network Methodology for Travel Demand Model Evaluation and Validation,December 2018,Meead Saberi,Taha H. Rashidi,Kenneth Ewe,Unknown,Male,Male,Male,"A system consisting of several non-identical elements connected by diverse interactions can be viewed as a complex network where the nodes are the elements of the system, links represent the interactions between the elements, and there are flow movements on the links. This paper is motivated by the recognition that travel demand, consisting of thousands or millions of origin–destination trips, can be viewed as a complex network. However, the full potential application of network science in analyzing interaction among places, people and between people and places in the context of travel demand modeling still requires further exploration (Chen et al. 2016). A recent study by Saberi et al. (2016) demonstrated that urban travel demand patterns can be characterized by a set of statistical network measures and could potentially be used to complement existing travel demand modeling approaches. In this paper, we propose a complex network methodology to evaluate and validate travel demand models and demonstrate its applicability in evaluating different disaggregate travel demand models from Melbourne, Australia. Travel demand modeling approaches, currently used in practice, include trip-based, tour-based and activity-based models. The trip-based models encompass the classical four step models having the four of trip generation, trip distribution, mode choice and route assignment steps. Trip-based models are typically developed at the aggregate level of zones. They lack time dimension, and overestimate the mode shift. Generally, the main criticism around aggregate travel demand modeling pertains to them being policy insensitive and not being behavioral (Axhausen and Garling 1992). To address these shortcomings of aggregate models, tour-based, agent-based, and activity-based models have been introduced (Axhausen 2000; Bowman and Ben-Akiva 2001; Lam and Huang 2003; Raney et al. 2003; Vovsha et al. 2004;). Disaggregate agent-based travel demand estimation paradigms, mainly represented by tour-based and activity-based models, propose a structure that captures spatial and temporal distribution of activities generated by transportation network users based on the expected utility from activities. Activity-based models are based on the premise that travel demand derives from people’s needs and desires to participate in activities. The behavioral basis of such agent-based models and the fact that they can incorporate a wide range of explanatory variables in their structures resulted in their growing deployment around the world. Although there is yet no standard structure for tour-based/activity-based models (Castiglione et al. 2015), most frameworks include the representation of the number, purposes (activities), timing, location, party composition, and travel mode of traveler’s tours and stops. Surveys on daily travel behavior, which includes information on travel durations and reasoning on traveling behavior, allows for an extensive view of common trends and travel patterns that occur based on what activities each individual prioritizes on a day-to-day basis. Given the extensive data required for development of activity/tour-based models and computational cost of developing and calibrating such highly complicated models, four-step models still dominate alternative travel demand models with regard to their spatial distribution of operating platforms around the world. Borrowing concepts from both aggregate and disaggregate modeling structures, transferability models have been recently introduced which are less computationally cumbersome compared to activity-based models, and with higher precision level and better realization of individual behavior compared to aggregate models (Ghasri et al. 2017). Recent advancement in this area has yet to be examined in terms of model precision, which is one of the major contributions of this paper by coupling a number of recently developed transferability models with an emerging network-theoretic approach. Evaluating and validating generated travel-activity sequences in activity-based models is a challenging task. A number of studies in the past (Roorda et al. 2008; Cherchi and Cirillo 2010; Sammour et al. 2012; Flötteröd et al. 2012) examined various calibration and validation measures including average travel distance, average travel duration, and sequence alignment mostly using survey-based data. However, many of the existing validation measures often fail capturing the structure and connectivity in the complex network of travel demand. Other studies (Jiang et al. 2013; Do and Gatica-Pereza 2014; Schneider et al. 2013) explored the possibility of using non-traditional (e.g. mobile phone use) data to overcome some of the limitations of the traditional methods. A recent study by Liu et al. (2014) proposed an evaluation and validation framework using travel information derived from mobile phone data. Their framework extends the application of non-traditional data to travel behavior modeling by comparing the activity-travel profiles derived from the mobile phone call data with statistics obtained from traditional activity-travel surveys. Despite the importance of model evaluation and validation, there are very few studies in the travel demand modeling literature that focus on application of emerging network science techniques in improving, evaluating, and validating existing demand modeling methodologies. In this paper, we propose a complex network-driven methodology for travel demand model evaluation and validation. The proposed methodology is not a replacement for the traditional model evaluation approaches; rather, it is a supplement that is capable of reflecting new aspects of goodness-of-fit of models from a network perspective. The new methodology views travel demand as a complex network consisting of vertices and edges representing origins-destinations and trips, respectively. It applies network-theoretic statistical properties to compare model outcomes and real world observations. The proposed approach can be applied using both traditional (e.g. survey-based) and non-traditional (e.g. mobile phone use) data. It is also not limited to any travel demand modeling method. The method can be applied to any aggregate or disaggregate demand model. The objective of the paper is not to promote any demand modeling method; rather, we only focus on the network methodology for evaluation and validation of travel demand models using complex network measures. The remainder of the paper is organized as follows. First, a brief background on growing literature on network science is provided. We then explain the travel demand modeling approach and data used in this study. Next, existing approaches for model evaluation and validation will be discussed. The main idea behind complex network theory and its application to evaluate and validate travel demand models will then be explained. The probability distributions of various network parameters from the observed and selected demand models will be discussed. Finally, selected goodness of fit measures for the models will be explained, applied and discussed followed by concluding remarks and directions for future research.",10
19.0,1.0,Networks and Spatial Economics,30 June 2018,https://link.springer.com/article/10.1007/s11067-018-9407-0,Optimal Detection of Critical Nodes: Improvements to Model Structure and Performance,March 2019,Gokhan Karakose,Ronald G. McGarvey,,Unknown,Male,Unknown,Male,"The cost-effective protection of vulnerable networks is a topic of considerable importance, in applications as varied as transportation (Murray-Tuite and Mahmassani 2004), infectious disease control (Nerlich and Koteyko 2012), and internet infrastructure (Yan et al. 2010). Many researchers have addressed this problem by examining the “criticality” of network components (arcs and/or nodes). Although there is no universally-accepted definition for “criticality” in this arena, the term generally refers to the magnitude of the change in the network’s performance in the event that an arc or node becomes disabled. Some authors have proposed measuring network performance in terms of the increase in travel distance (Cappanera and Scaparra 2011) or time (Murray-Tuite and Mahmassani 2004) that system users endure in the event of a disruption. Alternatively, one could determine the most critical arcs or nodes of a network as those whose blockage has the greatest impact on the network connectivity (if, for example, one’s objective were to maximize the decrease in interactions among individuals in a terrorist network). The identification of critical nodes is of interest to both interdictors wishing to degrade the network’s performance, and to defenders aiming to preserve network performance in the face of disruption. In either case, because it is assumed that a limited number of nodes can be disabled (or protected), an efficient strategy is needed to identify how a limited set of attacking (or defending) resources should be allocated across network nodes. Veremyev et al. (2014b) and Veremyev et al. (2014a) have presented the best-performing models, to date, for the defender’s problem when nodes can be disabled (node deletion). The key contribution of this research is the development of exact optimization models for the defender’s critical node problems that perform better than these benchmarks in terms of computational time. Accordingly, Section 2 contains a literature review of research on the identification of critical nodes. Section 3 presents new mathematical formulations, incorporating novel valid inequalities for the node deletion problems. Section 4 presents extensive computational testing comparing the performance of these new models to the existing benchmarks on various networks. Finally, Section 5 contains conclusions and suggestions for future research directions.",3
19.0,1.0,Networks and Spatial Economics,31 October 2018,https://link.springer.com/article/10.1007/s11067-018-9422-1,A Combinatorial Dynamic Network Trajectory Reservation Algorithm for Connected Autonomous Vehicles,March 2019,Michael W. Levin,,,Male,Unknown,Unknown,Male,"Connected autonomous vehicle (CAV) technologies admit a variety of new marketplace mechanisms for daily traffic operations. Using connected vehicle communications for negotiations, and vehicle automation to guarantee safe and reliable compliance with marketplace outcomes, proposed mechanisms allow travelers to pay for advantages in priority. For instance, vehicles might pay other vehicles to switch ordering on a congested corridor (Le Vine and Polak 2016). Autonomous intersection management (AIM) (Dresner and Stone 2004; 2006), in which vehicles reserve space-time paths through individual intersections, also admits auctions for vehicle priority (Schepperle and Böhm 2007; Vasirani and Ossowski 2012). A major limitation of these previous studies is that auctions were held at every intersection or link. Travelers wishing to ensure an on-time arrival at their destination might find themselves paying for priority at each intersection without any guarantee on their arrival time. This is clearly undesirable from a travelers’ perspective. Ideally, travelers would be able to bid on their arrival time at their destination, rather than small changes in priority along their route. To that end, this paper develops a method of auctioning and reserving network space-time trajectories. A network space-time trajectory is a path through the network with fixed arrival times at every node along the path. Previous work on link permit systems explored reserving entire routes (Wada and Akamatsu 2013; Akamatsu and Wada 2017), but required that links remain at free flow. We relax this requirement and use the kinematic wave model for traffic flow (Lighthill and Whitham 1955; Richards 1956). Any restricted access system must ensure that vehicles do not enter links without the necessary permit or reservation. Although such restrictions may be difficult to enforce for human-driven vehicles, the AIM protocol for CAVs provides a solution by controlling intersection access for individual vehicles. This trajectory reservation system also addresses an open problem for AIM — the question of how to prioritize vehicle access to intersections. The contributions of this paper is the development of a combinatorial trajectory reservation algorithm using the cell transmission model (CTM) (Daganzo 1994; 1995), a Godunov (1959) approximation to the kinematic wave model. Trajectory reservations are made in arbitrary order, which is open to auctions or credit-trading mechanisms. The main challenge in reserving trajectories through a congestible network is maintaining the validity of existing reservations. We solve this problem through careful modification of cell connectivity labels in the time-expanded cell network. Our algorithm is tractable for large numbers of vehicles and large city networks. We analyze the results from our trajectory algorithm on Sioux Falls and the downtown Austin network, and compare them with dynamic traffic assignment (DTA). We also compare them with previous link reservation models requiring that links remain at free flow. The remainder of this paper is organized as follow. Section 2 reviews previous work on trajectory reservations. Section 3 develops the trajectory reservation algorithm and connectivity labels to ensure that reserved trajectories remain feasible. Experimental results are presented in Section 4, and we conclude in Section 5.",4
19.0,1.0,Networks and Spatial Economics,06 November 2018,https://link.springer.com/article/10.1007/s11067-018-9423-0,Bifurcation Theory of a Racetrack Economy in a Spatial Economy Model,March 2019,Kiyohiro Ikeda,Mikihisa Onda,Yuki Takayama,Unknown,Unknown,,Mix,,
19.0,1.0,Networks and Spatial Economics,09 October 2018,https://link.springer.com/article/10.1007/s11067-018-9424-z,Benders Decomposition Algorithms for Two Variants of the Single Allocation Hub Location Problem,March 2019,Nader Ghaffarinasab,Bahar Y. Kara,,Male,Female,Unknown,Mix,,
19.0,1.0,Networks and Spatial Economics,07 January 2019,https://link.springer.com/article/10.1007/s11067-018-9435-9,Redistributive Effects of Gasoline Prices,March 2019,Demet Yilmazkuday,Hakan Yilmazkuday,,Female,Male,Unknown,Mix,,
19.0,1.0,Networks and Spatial Economics,12 February 2019,https://link.springer.com/article/10.1007/s11067-019-09445-w,Socio-Economic Determinants of Student Mobility and Inequality of Access to Higher Education in Italy,March 2019,Umut Türk,,,Male,Unknown,Unknown,Male,"An intuitive way to increase spatial accessibility is to decentralise the service in question. This was the strategy implemented by the Italian authorities in the period 1990-1998. With one of the lowest participation and graduation rates in Europe, the supply of higher education (HE) was expanded drastically. The reforms required larger universities to set up new types of faculties and nine new higher education institutions were established as a result of the decentralisation process (MIUR 1997). However, these reforms took place without any field examination of accessibility or demand (Bratti et al. 2008). More than a decade later, there is no explicit measure of spatial accessibility of the universities in the country. In a system granting free access to HE for every potential candidate, the foremost aim of policymakers is to guarantee full access to the service irrespective of the location of residence. Previous research has focused on measuring accessibility through an examination of the match between the locational distribution of facilities or services and the locational distribution of residents (Talen and Anselin 1998). In this framework, the spatial distance between the residence of origin and the location where opportunities are located is regarded as an important factor determining the spatial accessibility. The underlying idea is that people from more isolated locations face considerable costs to access to opportunities with costs growing with spatial distance. Since parents determine the residential location of students prior to HE enrolment, inequalities in access to HE across students’ locations of origin should be regarded as unfair. A modern theory of inequality building on equality of opportunity (EOp) arguments suggests that the differences in outcomes due to the factors that are beyond individual responsibility are unfair and should be compensated by the society (Dardanoni et al. 2006). Reducing geographical disparities in access can be seen in fact as a way of levelling the playing field (Roemer 1998) and providing equal opportunities to benefit from HE irrespective of the place of origin. The geographical location can be an unfair source of inequality in access on a large scale: a student living in an area where HE services are well-supplied faces smaller costs of transportation, lower opportunity costs in commuting, and no housing costs compared to those who need to commute or move to benefit from HE services. However, focusing only on geography may leave the influence of socio-economic factors in relation to gender, experiences at home, and parental background unexplored. The paper argues the existence of a gradient of economic circumstances of origin on the degree of spatial access to HE. Although distance matters in explaining accessibility, other variables determine differences in costs of movement correlated with distance, which at the same time, influence the distribution of accessibility. This paper tries to single out the contribution of spatial distance and economic circumstances on inequality in access to HE. The paper redefines the problem of disparities in spatial access on the basis of both the physical distance from universities and the socio-economic distance between student groups that generates an additional inequality in access within the same location. The paper studies the variability in access both when focusing on comparisons of people located at different origin points from HE but all sharing the same family background (highlighting the share of inequality due to spatial distribution of HE institutions in the country), and when comparing people located at the same origin points but with differing backgrounds of family origin. The latter indicates the ability of families to cover the costs of displacement and compensate for distance from the location of origin and shows the share of inequality in access due to the socio-economic background of students. In order to undertake the analysis, the paper sequentially employs a model and an index to measure overall inequality in access, which is then decomposed into its geographical and socio-economic components. First, a spatial interaction model (SIM) is used to disentangle the mobility dynamics of different student groups. Being flexible and straightforward enough these models enable to investigate flows between origins and destinations (Sen and Smith 2012). SIMs likewise model actual flows of commodities, information, emails, phone calls, money, and of people along with any other sort of movements (see Haynes and Fotheringham 1984; Sen and Smith 2012, for reviews). In the present application, student flows between parental residents and universities are defined as interactions between localities. To account for socio-economic factors influencing the access, the observed flows of students are partitioned into subgroups each representing a different type. It is a common practice for EOp studies to partition the population according to exogenous factors, which are assumed to be beyond people’s control (Checchi and Peragine 2010, see for instance; Ferreira and Gignoux 2011) and resulting subgroups are represented by types (Roemer 1998). For the second step, the parameters that are distinctively calibrated for each type by the SIMs are imported to a Hansen (1959)-like index to measure potential accessibility for 110 Italian provinces (NUTS3 level regions). Finally, the inequality in accessibility among provinces is decomposed as follows: the access score in each province is replaced with its average access score across socio-economic groups hence the only variation is allowed to be due to the geographical distribution of universities. Then the access scores computed for each socio-economic group is replaced with its average access score across provinces hence remaining variation is allowed to be due to socio-economic backgrounds. This operation enables investigating the relative contributions of spatial and aspatial factors in the total inequality in access to HE. The paper contributes to the literature by extending the classical spatial accessibility analysis to incorporate the socio-economic circumstances of students in a spatial accessibility measure for Italian HE institutions. This practice goes beyond the mere concern of inequalities in outcomes. For the spatial accessibility analysis, this means that the inquiry may shift from “spatial accessibility where?” to “spatial accessibility where and for whom?”. It also contributes to the EOp literature by showing how the spatial dimensions of the theory can be incorporated into models that rely solely on geography. Finally, the findings in this paper provide detailed information for policymakers regarding which groups of students to target and specifically in which locations the assistance is needed most for a fair access to HE. The remainder of the paper is organized as follows: the second section introduces the model, and the accessibility index adopted, the third section sets out the data and variables, the fourth section shows the empirical method for calibration and findings where inequality in access is decomposed into within and between components for different populations of students. The fourth section demonstrates the extent to which the spatial inequality has decreased following the HE expansion efforts initiated in the 90s. Finally, the conclusions and a set of policy implications are given in the fifth section.",11
19.0,1.0,Networks and Spatial Economics,29 March 2018,https://link.springer.com/article/10.1007/s11067-018-9393-2,Relationship Between Shipping Accessibility and Maritime Transport Demand: the Case of Mainland China,March 2019,Liquan Guo,Zhongzhen Yang,,Unknown,Unknown,Unknown,Unknown,,
19.0,1.0,Networks and Spatial Economics,18 August 2018,https://link.springer.com/article/10.1007/s11067-018-9418-x,A Methodology for Revenue Analysis of Parking Lots,March 2019,Igor Lazov,,,Male,Unknown,Unknown,Male,"The parking lots are demanding environments for planning and design, since e.g. a lack of local parking spaces can contribute to traffic congestion. From an environmental standpoint, that translates to incalculable amounts of wasted fuel and carbon emissions. Congestion and parking are also interrelated since looking for a parking space (called “cruising”) creates additional delays and impairs local circulation, especially in central areas of large cities. A lack of parking is not the sole reason for traffic congestion; it is, however, a major contributing factor. Off-street parking located too far away from shopping or retail businesses may also cause drivers to circle in search of parking that is more convenient. Thus, we can agree that minimizing unnecessary circling or driving is good for everyone and aids sustainability. The distributed parking, i.e., placing smaller facilities closer to more popular destinations, is one potential method for that. For example, Automated Parking Systems (APS) are ideal for putting parking spaces in small or otherwise unusable areas closer to retail or high-volume destinations. The parking lot size is fixed, but the vehicle arrival/service mode is random. That is, the times at which vehicles arrive are unpredictable, and, similarly, the lengths of time that the vehicles will stay parked are unpredictable; yet, the park designers must decide how many resources (i.e. places, ramps, etc.) to provide to accommodate this random demand. If the resources are provided too sparingly, then the quality of service will be low (e.g. too many vehicles will find all places occupied on arrival, because the required resources are not available when needed); but, if resources are provided too generously, then the costs will be too high. Given these challenges and many more, there is a need for sophisticated planning tools to address operational and economic issues associated with existing and proposed parking facilities, simultaneously taking into account the distinctiveness of the parking environment. Good parking management policies are required to efficiently assign solicited parking places to drivers especially in highly solicited urban environments. The basic concepts of the parking reservation system and parking revenue management system are discussed in Teodorovic and Lucic (2006). In this context, in Mejri et al. (2013) an efficient semi-centralized parking slot assignment approach is proposed and studied. Different smart applications have been developed in order to assist drivers in their parking search. Thus, the parking solutions presented in Lu et al. (2009) and Ayala et al. (2011) are examples of solutions, where infrastructure assistance does not exist and the status of the parking places is either predicted or estimated. In Zou et al. (2015) a mechanism design based approach for public parking slot assignment is proposed, in an environment empowered by recent advances in parking sensing. Further, denser downtown areas have garage as well as curbside parking. The friction of space confers market power on parking garages. However, Arnott and Rowse (2009) show that raising the curbside parking fee appears to be a very attractive policy since it generates efficiency gains that may be several times as large as the increased revenue raised. Many theoretical and empirical papers analyze the quantity and pricing of parking by concentrating on particular aspects of the issue. For example, Inci (2015) reviews the literature on parking with an emphasis on economic issues. The study in Migliorea et al. (2014) develops a model for designing an efficient parking pricing policy, which aim is an intelligent control and management system of parking pricing integrated with a redefinition of the circulation scheme for a limited traffic zone. Also, in Qian and Rajagopal (2013), the relations between dynamic parking prices and provision of parking in a general parking network are investigated. Here, travelers are provided with the real-time occupancy and pricing to make their parking choices. The parking choices are formulated under the user equilibrium conditions using the variational inequality approach. Further, in Qian and Rajagopal (2014), a parking pricing strategy dependent on real-time sensing is proposed to manage the parking demand; here, the optimal parking policies are based on stochastic control models. In He et al. (2015), a system of nonlinear equations to describe the equilibrium assignment of parking spaces to vehicles is presented, and then optimal pricing schemes that steer such parking competition to a system optimum assignment of parking spaces are discussed. The optimal parking pricing was also discussed in D’Acierno et al. (2011) between private cars and transit systems. In Bifulco (1993) parking pricing is modeled for general transportation networks. Several parking types and fees are incorporated into the static traffic assignment for general networks, and the efficacy of parking policies is evaluated for long-term planning purposes. The parking policy is one of the key links between transport and land-use policy. The transport and land-use components in the accessibility studies in urban planning and transport geography might be characterized by development of congestion (e.g. Moya-Gómez et al. 2018) or road pricing (e.g. Solé-Ribalta et al. 2018). Thus, Marsden (2006) presents a review of the evidence base upon which commuter, leisure and shopping, and residential parking policies are based. Due to the proliferation in the number of vehicles on the road, the problem of traffic congestion and insufficient parking places inescapably crops up. To mitigate this problem, the smart parking systems (e.g. parking guidance and information (PGI) systems) have been developed (see Idris et al. 2009), which rely on the park occupancy and, besides on the road, are used in the management within the parking lot as well. They direct the vehicles and optimize the use of parking spaces. But, they do not take into account the uncertainty (i.e., risk) of a parking lot, although the data gathered through them can be used to predict future traffic patterns and some pricing strategies can also be manipulated. However, there is a lack of a general underlying methodology to connect real-time parking prices with the occupancy. Anyway, in general, while many aspects of the parking and traffic management sector have been evaluated (e.g. Cascetta 2009; Washington et al. 2010; Tumlin 2012) and of modeling and handling uncertainty in representing and analyzing transportation systems have also been reviewed (e.g. Dell’Orco et al. 2005; Ottomanelli and Wong 2011), analytical models based on parking lot information and parking lot entropy analysis are largely absent. Thus, there exists only the widely used and exploited information-theoretic method, based on maximization of entropy given certain constraints, e.g. Ferdinand (1970); Thomas (1979); Guiasu (1986); Karmeshu (2003); Agrawal et al. (2005); Caggiani et al. (2014). Entropy is a useful concept that has been used to describe the structure and behavior of different systems, and entropy models have emerged as valuable tools in the study of various social and engineering problems of spatial interaction. Their use in the study of transportation is pioneered in Wilson (1970) and Webber (1979). These models proved to be of great importance in urban planning, from public transport networks to retail locations. Also, in Nie et al. (2007), an entropy measuring model is put forward based on information theory, after analyzing the dissipative structure property of a regional transportation system, and the process and direction of system evolution is analyzed based on entropy change theory. Additionally, in Huynh and Simon (2016), it has been found that individuals who focus on the uncertainty involved in a decision are more likely to choose a sure thing, whereas individuals who focus on consequences are more likely to choose a gamble. On the other front, in Ducruet and Beauguitte (2014), a number of classic and novel approaches to network analysis by both natural and social sciences around the concept of space has been reviewed. In this work, relying on the fundamental concepts of informationi and entropyS of a given parking lot (e.g. (un)covered lots, multi-storey modular lots or robotic parking garage), with N – current and M – maximum number of simultaneously present vehicles (i.e., vehicles resident at the parking lot), 0 ≤ N ≤ M, modeled by a Birth-Death Process (BDP) with size M + 1, we promote a methodology for revenue analysis of parking lots; the BDP, modeling (the behavior of) the parking lot, is in the state n, n = 0, 1, …, M, if N = n. Thereby, by definition, the park entropy S = E(i) is the uncertainty, associated with the park. Formally, we apply Gibbs (1878) formula for entropy, and its adaptation to random variables, firstly used by Shannon (1948) (but, solely its discrete version). Two metrics are essential in this methodology. The first metric, utilization parameter ρ of the parking lot, ratio of the primary birth and death rates in the BDP, modeling the park, ρ = λ/μ, physically, defines (equilibrium) macrostates of the park. The vehicle mean arrival rate is λ and mean parking time is T = 1/μ (i.e., mean departure rate is μ). The park information i has possible values in, in ≝  − ln pn, n = 0, 1, …, M; while pn = Prob{N = n} is the weight associated with the state n of the park (i.e., of the BDP), then inis quantity of information the park possesses in the state n. In such a way, this methodology introduces a new paradigm for describing the behavior of a given parking lot. In this work, we concern parking lots with finite size – the size M of the park is the second metric in this methodology. Namely, if we suppose in advance a park with infinite size (i.e., the number N of vehicles is not limited), the most of the key quantities, emerged here, are lost. An arriving vehicle is accepted in the park if at least one place in the park is unoccupied. Thus, each newly arriving vehicle is given its private place. Vehicles, which upon arrival find all places occupied will be refused entry to the park. So, a parking lot represents M-server loss system. When the number N of present vehicles follows truncated geometrical distribution, we have an information linear parking lot with size M. Also, when the park revenue gained from any vehicle is constant, we have a revenue linear parking lot with size M. In this work, the information of a parking lot is the foremost performance consideration, while the revenue of a parking lot (here expressed in, say, EUR/h) is also regarded as a key one; here, an important relation between these measures is established. Further, we offer a view on how the park information (and, entropy) and park revenue (and, average revenue) change as functions of park utilization ρ. Only in the revenue liner park, the park revenue does not depend on park information. The exposed procedure for evaluation of the above performance measures of a parking lot will offer the flexibility to parking operators and designers for efficient dimensioning of the parks. So, the entropy enables a high fidelity quantification of the uncertainty, associated with the given parking lot, and the optimal values of ρ and M allow a suitable management of that uncertainty, being a function of these two metrics. Thereby, given ρ and M, the difference among the park average revenue, full park revenue (revenue attained from all M vehicles, i.e., parking places), revenue of the corresponding revenue linear full park, and average revenue of the corresponding revenue linear park is determined. The rest of this paper is organized as follows. In Section 2, we present a review of the information analysis of a park (taken from Lazov and Lazov 2014, where an arbitrary population of entities is a concern), introducing the park model, and park information & entropy. In Section 3, the revenue and average revenue of a park, together with the mean normalized square deviation of park revenue from its linear part, are analyzed. Also, in these two sections, nine important values of the utilization parameter ρ (i.e., utilization points), emerged here, are studied. In Section 4, the benefits for a park design are systemized. In Section 5, the illustration of the methodology to public parking lots is shown. Section 6 is a conclusion of the paper.",4
19.0,1.0,Networks and Spatial Economics,15 December 2018,https://link.springer.com/article/10.1007/s11067-018-9431-0,Maximizing Expected Coverage of Flow and Opportunity for Diversion in Networked Systems,March 2019,Timothy C. Matisziw,,,Male,Unknown,Unknown,Male,"In networked systems, such as those supporting vehicular, energy, data and social movements, a range of services are provided to help ensure efficient and effective operation. Facilities providing these services can take many forms, such as sensors that collect information about network activities, locations at which a service can be obtained (i.e., communications, assistance), staging locations for mounting response to problems (i.e., law enforcement, emergency response), as well as those that relay information to users of a system (i.e., navigation assistance, future/expected conditions, location of disruptive events). The purposes of these types of services though can be very different. Some types of services, such as monitoring network conditions and relaying information about future conditions, are typically provided in hopes that users are exposed to a facility at some point along a path of movement. Other types of services, such as those conveying information on the location of disruptive events, are provided to assist users in making decisions about how to best proceed within the network, once a service is received. For example, facilities that provide such services to users of a transportation system include, kiosks containing analog information, signs/message boards, audio devices, Bluetooth emitters, dynamic message sign (DMS), and highway advisory radio. In such cases, the information is most relevant when it is provided prior to locations offering an opportunity to make effective use of the service. For instance, if the information is provided to alert users to disruptive events so as to reduce travel delays, the service has little value if no opportunity for altering a route exists after receiving the information. Regardless of the intended purpose of a service, there is no guarantee that it will be available and/or effectively received. For instance, in many networked systems, users often do not have the same access to services and/or may not detect, understand, or receive services in a uniform manner given a range of technical, physical, and behavioral differences. Complicating the situation is that networks can support a diversity of flow given the relationships among origins, destination and how users utilize the system. As such, a service provided at one location could vary in relevance to exposed flows. Finally, as with many types of services, siting facilities can be costly in cases where expensive supporting infrastructure, equipment, technology, and operation are involved. For example, siting facilities such as DMS along roadways, can cost hundreds of thousands of dollars and involve significant annual maintenance costs as well (ITS 2017). Therefore, planning new (or extending) service systems often requires imposing limitations on the cost of and/or number of facilities to be sited. To address these planning considerations, several new mathematical models for optimizing the location of facilities for providing service to network flows are developed. The proposed models build primarily upon three location models - the maximal coverage location problem, the maximal expected coverage problem, and the flow capturing location model, which are described in the next section.",4
19.0,1.0,Networks and Spatial Economics,24 January 2019,https://link.springer.com/article/10.1007/s11067-019-9444-3,Calibrating and Applying Random-Utility-Based Multiregional Input–Output Models for Real-World Applications,March 2019,Chris Bachmann,,,,Unknown,Unknown,Mix,,
19.0,1.0,Networks and Spatial Economics,17 January 2018,https://link.springer.com/article/10.1007/s11067-017-9382-x,Assessing Vulnerability of Transportation Networks for Disaster Response Operations,March 2019,Victor Cantillo,Luis F. Macea,Miguel Jaller,Male,Male,Male,Male,"Disasters can have a major impact on people’s quality of life and wellbeing. Moreover, their cumulative effects can hamper human and social development, especially in developing countries where their impacts are even greater. Most people in developing countries live in areas that are at high risk of natural disasters and extreme events, or live in poorly constructed buildings and other vulnerable conditions that can result in massive human losses. According to the United Nations (2014), disaster affected 4.4 billion people between 1994 and 2013, claiming 1.3 million lives and generating more than US$2 trillion in economic losses. These statistics illustrate the vulnerability of modern societies and the challenges for disaster risk management and responders. When disasters or catastrophic events occur, in addition to impacting populations, the physical infrastructure, and supporting systems, they also create uncertainty affecting the response itself. Demand for critical supplies such as water and food may increase due to the partial or total destruction of local inventories of goods. Moreover, the flow of basic supplies is limited due to the collapse or temporal disruption of distribution and transportation systems. Affected populations have to cope and experience deprivation and suffering. The absence of functioning markets that prevents people from buying, selling or trading goods or services is one critical feature in this context (Holguín-Veras et al. 2013). As a result, the demand for critical supplies increases as well as the population’s suffering, forcing an almost immediate action from relief agencies in a race against time (Stauffer et al. 2016). The more the response is delayed, the lower the beneficiaries welfare and their ability to cope with the disaster impacts (Cohen 2008). Therefore, relief agencies must design a prioritized plan to provide life-saving emergency assistance to beneficiaries in the short term. According to Holguín-Veras et al. (2012); Holguín-Veras et al. (2013); Pérez and Holguín-Veras (2015); Holguín-Veras et al. (2016), social cost minimization must drive such plans, thus allowing a socially optimal distribution of scarce resources. Social costs are the summation of the impacts of those logistical decisions over all sectors of the society affected by the relief operation. In general terms, social costs include the logistics costs (e.g., inventory, transportation, delivering and distribution) internalized by the relief groups, and the direct impacts (the economic value of the deprivation costs) on the affected population (Holguín-Veras et al. 2013). It is also important to consider the impacts on the individuals who do not receive aid because their deprivation costs will increase as they wait for supplies. These additional costs are the opportunity costs of the delivery strategy, and are very important because, typically, the amount of distributed supplies do not meet all demands. In such context, transportation networks play a major role in determining the experienced deprivation costs because they facilitate the movement and access to goods and people in different areas. Therefore, network disruptions have significant impacts on society because they can hinder evacuation procedures, emergency response, development of humanitarian supply chains, and the subsequent recovery of the affected areas. Delivery and response vehicles may have to travel longer distances, increasing travel times (or may not be able to access at all), and there is considerable uncertainty about the state of the network (Holguín-Veras et al. 2012). It is clear that disaster response times influence the welfare of those affected. The impacted population may be trapped, injured, at risk of death, or emotionally and psychologically affected; and access to the impacted areas by the disaster and humanitarian response operations is a critical issue that has not been considered in current network vulnerability formulations and analyses. The current approaches to assess vulnerability of transportation networks only consider some technical features related to transportation costs such as travel time (Scott et al. 2006; Jenelius 2009; Nagurney and Qiang 2009; Sullivan et al. 2010; Chen et al. 2012; Jenelius and Mattsson 2012; Lu et al. 2014; Rodríguez-Núñez and García-Palomares 2014; Jaller et al. 2015; Wang et al. 2016); generalized costs (Jenelius et al. 2006; Taylor et al. 2006; Chen et al. 2007; Gómez et al. 2011; Qiang and Nagurney 2012; Luathep et al. 2013); network topological features (Taylor and D’Este 2005; Sohn 2006; Qiang and Nagurney 2008; Zhong et al. 2010); and traffic flow and congestion effects (Sohn 2006; Jenelius 2010; Balijepalli and Oppong 2014; Rupi et al. 2015a; Wang et al. 2016). Other works propose multi-objective optimization approaches for network restoration during disaster recovery, assessing for instance, the tradeoffs between minimization of system (private) costs and maximization of system flows (Matisziw et al. 2010). To fully account for the socio-technical impacts of response and humanitarian operations, a comprehensive analysis of transportation network vulnerabilities should include social costs (Holguín-Veras et al. 2013). Lack of consideration will result in disaster response and humanitarian assistance strategies not reaching a socially optimum level, and will also hamper the development of resilient humanitarian supply chains, especially in regions with high levels of risk. To fill this gap, this paper proposes a valuation model of transportation network vulnerability that explicitly considers social costs and is particularly useful for the design and planning of disaster response and humanitarian resilient supply chains, and to prioritize the rehabilitation (access restoration) of the disrupted network. The organization of the paper is as follows: Section 2 reviews the relevant literature and discusses related concepts such as accessibility and resilience. Section 3 puts forward the proposed vulnerability model. Section 4 evaluates the model implementation through two numerical experiments with different complexities. Section 5 applies the model to a real case study using the Colombian Coffee Zone road network. Finally, Section 6 discusses findings and conclusions.",31
19.0,1.0,Networks and Spatial Economics,17 March 2018,https://link.springer.com/article/10.1007/s11067-018-9395-0,Investigating Lock Delay on the Upper Mississippi River: a Spatial Panel Analysis,March 2019,T. Edward Yu,Bijay P. Sharma,Burton C. English,Unknown,Unknown,Male,Male,"The multimodal freight transportation network, such as road, rail and waterway, plays a crucial role to regional economy and industries development (Darayi et al. 2017). For U.S. agriculture, inland waterways are of great importance in the multimodal transportation network because of their comparatively low transport cost for low-valued agricultural commodities such as grain/oilseeds and feedstuff as compared to overland modes. An estimate by the Iowa Department of Transportation (IDOT 2013) shows that barge cost on the upper Mississippi River (UMR), on average, is $11/ton less than the cost of truck and rail. Transport costs linking two trading regions (e.g., north central US and ports at the Mississippi Gulf) can have a direct influence on commodity prices in both origin and destination markets. Thus, navigation efficiency on the UMR and resulting cost advantage would enhance the US’s competitiveness in world grain and oilseeds markets. A total of 28 lock sites managed and maintained by the US Army Corps of Engineers (USACE) are the key components for the navigation on the UMR (See Fig. 1).Footnote 1 Most locks have a 600-ft (ft) long and 110-ft wide chamber, except for lock 19, Melvin Price lock, and lock 27 with a 1200-ft long chamber. A 600-ft lock can process at most eight jumbo-barges (plus the towboat) in a single lockage, while a 1200-ft lock can accommodate up to 17 jumbo-barges in addition to the towboat. Currently, an average of 15 barges are in a tow on the UMR. Therefore, the tows will need to be re-coupled in order to pass a 600-ft long lock chamber (so called double lockage) and reassembled after exiting the chamber. The average duration of double lockage plus related operations at 600-ft locks takes almost 2 h, while a single lockage of towboat and barges at a 1200-ft lock typically requires about 30 to 45 min. Locks and dams on the upper Mississippi River and Illinois waterway. Source: Transportation Research Board (2001) This lock and dam system on the UMR, primarily built during World War II, has long passed the originally designed lifespan (50 years) and been given a grade “D” by the American Society of Civil Engineering (ASCE) (ASCE 2013). In addition, the constrained capacity on the 600-ft locks has generated concern about the navigational efficiency of the system. The delay at the locks in the lower reach on the UMR has increased considerably over the past decade. For instance, at Lock 25, the annual average delay time for delayed vessels has increased from around 122 h in 2005 to more than 267 h in 2015 (USACE 2006, 2016). The ASCE projects a loss of $3.6 billion in agricultural exports in the next decade if waterway infrastructure continues to deteriorate (ASCE 2013). Thus, the USACE, barge industry, and agricultural commodities groups have advocated the construction of a new lock and dam system for decades. The US Congress authorized the Navigation and Ecosystem Sustainability Program in 2007 to address the capacity constraints on the most congested segment on these waterways. However, the expensive capital investment and potential environmental damage associated with the construction of a new lock and dam system have drawn considerable debates. Instead, less costly, non-structural methods, e.g. scheduling, maintenance, etc., were suggested as means to control congestion and delay on the River (Transportation Research Board 2015). A number of studies have explored the strategies or means of lockage management to mitigate the congestion issue at locks using simulation techniques (e.g. Ting and Schonfeld 1998, Meyer and Kruse 2007; Smith et al. 2009). In contrast, little attention has been given to identifying the statistical relationship between potential factors and lock delay until recent studies (Reynaerts 2014; Zhang et al. 2015). However, these studies have neglected spatial interdependency of river locks where the departure from a given lock could affect the arrival distribution in the adjacent locks in a waterway network. (Chien and Schonfeld 1993; Ting and Schonfeld 1998). Thus, delay at a given lock could result in a build-up of departure traffic and presumably affect the traffic flow and arriving frequency at nearby locks, hence generating delay at those nearby locks. Similarly, the spatial autocorrelation issue could also be observed in traffic or lockage processing time. The lock interaction effects and the resulting estimation error of neglecting the lock interdependence could be considerable if the locks are located relatively close and when congestion levels are high (Chien and Schonfeld 1993). As a result, the traffic and delay on the river should be evaluated systematically based on the spatial relationship between locks. To help complete the literature of statistical analysis of inland waterway congestion, the objective of this study is to examine the impact of selected factors on delay occurring at the UMR lock and dam system via incorporating spatial interaction. More specifically, the focus will be on the impact of lockage processing time (related to lock capacity, i.e. hardware issue) and unscheduled lock outages/stalls (related to system maintenance, i.e. management issue) on lock delay (Transportation Research Board 2015). The analysis can help decision makers better understand the lock delay issues and make related adjustments or modernizations of the inland waterway system that will ultimately benefit US economy.",3
19.0,1.0,Networks and Spatial Economics,19 March 2018,https://link.springer.com/article/10.1007/s11067-018-9396-z,Congestion Control for a System with Parallel Stations and Homogeneous Customers Using Priority Passes,March 2019,Yasushi Masuda,Akira Tsuji,,Male,,Unknown,Mix,,
19.0,1.0,Networks and Spatial Economics,29 March 2018,https://link.springer.com/article/10.1007/s11067-018-9399-9,Identifying Networks in Social Media: The case of #Grexit,March 2019,Georgios Magkonis,Karen Jackson,,Male,Female,Unknown,Mix,,
19.0,2.0,Networks and Spatial Economics,30 October 2017,https://link.springer.com/article/10.1007/s11067-017-9369-7,"Efficient Insertion Heuristic Algorithms for Multi-Trip Inventory Routing Problem with Time Windows, Shift Time Limits and Variable Delivery Time",June 2019,Ampol Karoonsoontawong,Onwasa Kobkiattawin,Chi Xie,Unknown,Unknown,,Mix,,
19.0,2.0,Networks and Spatial Economics,15 December 2017,https://link.springer.com/article/10.1007/s11067-017-9377-7,The Impact of International Crises on Maritime Transportation Based Global Value Chains,June 2019,Rodrigo Mesa-Arango,Badri Narayanan,Satish V. Ukkusuri,Male,Male,,Mix,,
19.0,2.0,Networks and Spatial Economics,17 January 2018,https://link.springer.com/article/10.1007/s11067-018-9385-2,An On-Demand Same-Day Delivery Service Using Direct Peer-to-Peer Transshipment Strategies,June 2019,Wei Zhou,Jane Lin,,,Female,Unknown,Mix,,
19.0,2.0,Networks and Spatial Economics,23 January 2018,https://link.springer.com/article/10.1007/s11067-018-9388-z,"Labor Migrant Networks: Growth, Saturation, and Deflection to New Labor Markets",June 2019,Fernanda Herrera,Gabriel González-König,,Female,Male,Unknown,Mix,,
19.0,2.0,Networks and Spatial Economics,01 February 2018,https://link.springer.com/article/10.1007/s11067-018-9386-1,Day-to-Day Assignment Models and Traffic Dynamics Under Information Provision,June 2019,Xiaomei Zhao,Chunhua Wan,Jun Bi,Unknown,Unknown,,Mix,,
19.0,2.0,Networks and Spatial Economics,02 February 2018,https://link.springer.com/article/10.1007/s11067-018-9383-4,"Wheat Self-Sufficiency, Water Restriction and Virtual Water Trade in Iran",June 2019,Hmaed Najafi Alamdarlo,Fariba Riyahi,Mohamad Hasan Vakilpoor,Unknown,Female,Male,Mix,,
19.0,2.0,Networks and Spatial Economics,06 February 2018,https://link.springer.com/article/10.1007/s11067-018-9387-0,Multimodal Transportation Flows in Energy Networks with an Application to Crude Oil Markets,June 2019,Olufolajimi Oke,Daniel Huppmann,Sauleh Siddiqui,Unknown,Male,Unknown,Male,"Multimodal flow analyses are critical to characterizing and solving problems within today’s energy networks. The movement of fuels between nodes and across various levels within these networks involves multiple decisions under uncertainty and constraints of capacity, regulation and environmental impact limitation (Melese et al. 2016). As urgent steps are being taken to tackle climate change and related effects, greater efforts must be made to capture salient properties of various modes of energy transfer in order to provide reliable frameworks for best policy implementations (Heijnen et al. 2014). This is especially important for existing energy networks that negatively impact the environment, such as fossil fuel markets, as they must be properly managed while they are yet relevant. Networks are primarily defined by their topology, which constrains internodal movements or connections. Within certain networks, arcs may be differentiated by mode of travel or conveyance. Network models have been successfully used to describe complex systems in order to solve a variety of problems, including those of allocation, equilibrium, flow optimization, prediction, scheduling, among others. With appropriate calibration, these types of models can also allow for intervention experiments and scenario analyses. For energy applications in particular, we are often interested in optimizing player objectives, finding a market equilibrium and making decisions while limiting harmful effects, such as greenhouse gas emissions. Implementing scenario analyses provides multiple layers for subsequent policy intersections. Various modeling approaches have been employed for energy markets including multiobjective optimization and linear programming, but complementarity modeling has grown in importance given its ability to capture complex network interactions (Ruiz et al. 2014; Gabriel et al. 2013a). Mixed complementarity problems (MCPs) generalize equilibria and nonlinear programs, and they can be solved by a variety of Newton-based methods (Ferris and Pang 1997). In a competitive marketplace, each player’s optimization problem can be expressed as a set of Karush-Kuhn-Tucker (KKT) equations (Kuhn and Tucker 1951). The concatenation of the KKT conditions yields an MCP, and the solution to this system of equations is a market equilibrium of the underlying non-cooperative game (Nash 1951). The scope of energy applications in complementarity modeling is highlighted through the following examples: Abrell and Weigt (2012) developed an MCP model with a focus on investigating interactions between energy networks. Similarly, Huppmann and Egging (2014) formulated a complementarity model that accommodates multiple fuels and markets and features fuel substitution, which they applied on a global scale. Showing that discretely-constrained Nash games could be implemented as MCPs, Gabriel et al. (2013b) modeled energy networks with new approaches to finding solutions. Metzler et al. (2003) also used an MCP to model the Nash-Cournot equilibrium between market players, exhibiting its capability to characterize arbitrage in a power network. In an application to gas markets, Abada et al. (2013) employed a variational inequality formulation, which is a special case of an MCP. Christensen and Siddiqui (2015) captured the complex interactions in a biofuel market also using a complementarity model. We want to note that there exist a wide variety of modeling techniques for energy systems that are not optimization- and equilibrium-based (Lin and Magnago 2017; Jebaraj and Iniyan 2006; Suganthi and Samuel 2012). These techniques provide detail in different parts of the energy system, such as extraction, physical processes, and generation. As such, these models can be useful in answering specific questions pertaining to these processes. Given that our goal here is to consider a “what-if” policy analysis under different market scenarios, we chose to use equilibrium problems expressed as complementarity problems. This allows us to explicitly represent infrastructure under different market interactions. In this paper, we present a dynamic multimodal partial-equilibrium model built as an MCP and applicable to multifuel energy networks. Notably, the model features modal differentiation in order to accurately account for the distinct effects of each mode of conveyance in the energy network of interest. We consider this a major contribution as prior energy market models did not distinguish between modes of transportation for internodal transfer. Thus, they were not able to account for variations in cost and technology based on mode choice. Given the increasing concerns relating to environmental and climate impacts, modal disaggregation provides for detailed analyses of emissions contributions and safety risks of each flow variable. Consequently, various intersections can be examined and scenarios explored either to minimize or mitigate these risks and hazards. As an illustration of its capabilities, we apply this model to the North American crude oil market with transportation considered via railway, pipeline and waterway [river and sea] modes, as well as distinguishing between light and heavy crude oil qualities. We then perform scenario analyses to explore avenues for reducing the public-safety and environmental impact of crude-by-rail transport. This application and level of node disaggregation at the US state level in the North American market is also a first in the academic literature, and we also note that no model for North American crude with transfer mode specificity exists. Our model can be potentially coupled with climate assessment models for further impact-based decision and policy analyses. Furthermore, its multimodal features can be incorporated into existing energy-optimization-complementarity models. The remainder of the paper is organized as follows. Section 2 gives a detailed description of the model, its mathematical formulation and implementation. In Section 3, we motivate and discuss the application of this model to the North American oil market. The results of this modeling effort and analyses are explained in Section 4. We discuss their impact and implications in Section 5, while Section 6 provides avenues for future work. Section 7 summarizes and concludes the paper.",4
19.0,2.0,Networks and Spatial Economics,15 February 2018,https://link.springer.com/article/10.1007/s11067-018-9390-5,Integrators’ Air Transport Networks in Europe,June 2019,Paolo Malighetti,Gianmaria Martini,Davide Scotti,Male,Male,Male,Male,"Each year, air freight carries more than one-third of all trade by value (Airbus 2015), and the industry is expected to grow approximately 4.7% annually through 2033 (Boeing 2014). World air cargo incorporates three main service categories: (1) scheduled freight, which accounts for about 88% of all world air cargo traffic, (2) charter freight, and (3) mail (Boeing 2014). Among the operators providing freight services are the so-called integrated carriers (or integrators): shipping carriers controlling complete air and road delivery networks and offering a wide range of package delivery services. Different from other dedicated air cargo (or passenger cargo) offerings such as airport-to airport freight services for third parties, integrators provide door-to-door freight services on the basis of their own schedules (Park et al. 2009; Lakew 2014). This ability to meet the needs of speed and reliability coming from supply chain management (Onghena 2011; Lakew 2014) is making integrated carriers increasingly important. Europe is particularly attractive for integrated carriers because e-commerce, which relies heavily on small-package delivery services, has dramatically increased (Stevens 2015) and is expected to grow even more in the future (CRR 2015). However, this is not the only reason why integrated carriers have drawn attention on themselves. The European oligopolistic market structure, where only four operators—DHL, FedEx, TNT, and UPS—are completely integrated across all transport modes, has recently evolved towards a higher degree of concentration due to the recent approval by the European Commission (EC) of FedEx’s acquisition of TNT.Footnote 1 In addition, the industry has been further spiked by the fact that Amazon.com has recently started to develop its own air freight network and begun the construction of a new hub at the Cincinnati/Northern Kentucky Airport (Stevens 2017).Footnote 2 Despite such increasing relevance, attention given to integrators and their air transport networks (and this is especially true for Europe) is limited in contrast to passenger airlines (Bowen 2012; Onghena et al. 2014), whose network connectivity, both at the carrier (Gillen 2006) and airport levels (Redondi et al. 2011), has been largely investigated. This paper tries to fill this gap in the literature. More specifically, after a review of the existing literature on integrators, we use a novel data source to analyse the European air transport networks of integrated carriers (Section 4). The network structures are analysed in detail, looking at (i) hub choices, (ii) main connections, (iii) airport hierarchy, (iv) network centralization, and (v) airport centrality scores. Then, in Section 5, we also relate some of our findings to the current antitrust issues in the industry. More specifically, starting from the analysis provided in Section 4, we see what happens to some of the analysed network measures when FedEx and TNT are considered as a unique integrated carrier. Finally, given the relationship between network and market characteristics (Gillen 2006), we move toward a more traditional analysis (i.e., based on market shares) of the competitive dynamics and see whether the changes at the market structure level reflect the observed changes at the network level.",11
19.0,2.0,Networks and Spatial Economics,22 February 2018,https://link.springer.com/article/10.1007/s11067-018-9389-y,Analysing the Effect of Partner Characteristics on the Performance of Horizontal Carrier Collaborations,June 2019,Lotte Verdonck,Katrien Ramaekers,Gerrit K. Janssens,Female,Female,,Mix,,
19.0,2.0,Networks and Spatial Economics,15 May 2018,https://link.springer.com/article/10.1007/s11067-018-9401-6,An Integrated Supply-Demand Approach to Solving Optimal Relocations in Station-Based Carsharing Systems,June 2019,Sisi Jian,David Rey,Vinayak Dixit,Unknown,Male,Unknown,Male,"The increase of automobile ownership has brought significant issues to the sustainable development of urban transport, such as traffic congestion, air pollution, and low road resource utilization. To mitigate these negative externalities, urban carsharing systems represent effective alternatives (Millard-Ball 2005; Katzev 2003). In carsharing systems, the operator locates a fleet of vehicles to designated vehicle pods. Users are free to pick up vehicles at any station and return them for a relatively shorter time period compared to traditional car rental, usually by the hour or by the minute. From the perspective of individual travelers, carsharing systems offer travelers higher flexibility than public transport, reduce their traveling costs compared to private vehicles, and provide them comparable travel experience simultaneously. Moreover, studies have shown that carsharing has a positive impact on reducing vehicle ownership (Cervero et al. 2007; Prettenthaler and Steininger 1999). As reported by Ter Schure et al. (2012), the average vehicle ownership among carsharing member households was 0.47 as compared to 1.22 among non-carsharing member households in San Francisco. The decrease in vehicle ownership implies more efficient use of road resources, less congested in urban transport networks, lower transport costs for urban society, and lower pollutions to the environment. These attractive features of carsharing systems boost their development around the world. By 2013, carsharing has spread to over 27 countries around the world with an estimated 1,788,000 carsharing members sharing over 43,550 vehicles (Shaheen and Cohen 2013). Carsharing systems are categorized into different forms based on vehicle return policies. The most common and traditional form is round-trip carsharing that requires users to return the vehicles to the same places where they pick them up. This form of carsharing systems is simple for operators to maintain but not always convenient for travelers. A more flexible system type is the one-way station-based carsharing which allows users to return their vehicles to any carsharing stations close to their destinations. The flexibility of such systems is better suited to users who only need to make one-way trips, such as leisure, shopping, and sporadic trips. Therefore, one-way carsharing systems have the potential to capture a substantially larger market share than round-trip carsharing systems. The market attractiveness of one-way carsharing systems and the fierce competition in carsharing industry have encouraged a number of companies to provide one-way carsharing services to users, such as Autolib in France (Autolib.eu 2016), Zipcar and Car2go in U.S. (Zipcar.com 2016), and Communauto in Canada (Communauto.com 2016). As for the city of Sydney, Australia, over 15,000 travelers have joined the carsharing schemes since 2003 (Cityofsydney.nsw.gov.au 2016). None of the operators provide one-way trip services to the users presently. However, the potential for attracting a substantially larger market demand of one-way carsharing has encouraged Sydney carsharing operators to consider one-way trip services to the existing round-trip carsharing systems. Along with its potential benefits, the flexibility of one-way carsharing also brings more complex problems to the operators. One dominant challenge is to ensure the supply of carsharing vehicles can meet the demand of carsharing users. Since travelers’ origins and destinations are not necessarily uniformly distributed across urban networks, carsharing vehicle stocks can become spatially and temporally imbalanced. This may influence vehicle availability. A possible situation can be users’ pick-up reservations cannot be fulfilled due to a lack of vehicles, and users’ parking needs cannot be satisfied due to surplus vehicles. Such poor accessibility may considerably impair the profitability of one-way carsharing systems. Hence, it is critical to dynamically relocate vehicles to rectify and anticipate vehicle imbalance. Relocation operations represent an increase in operational costs, which could potentially be compensated with the revenues from the increased demand for one-way trips or, in turn, could reduce benefits. Jorge et al. (2015a) proposed a model to evaluate the profitability of introducing one-way carsharing services to the round-trip carsharing system in Logan Airport. They concluded that the supplementary one-way services could be profitable, but the operators needed to develop efficient relocation strategies. This result highlights the importance of conducting cost-benefit analysis before adding one-way carsharing services, and the necessity of developing efficient relocation models. Under such circumstances, several research efforts have been focused on building effective relocation strategies for one-way carsharing systems. The proposed strategies are mainly categorized into operator-based relocation methods (Fan et al. 2008b; Nair and Miller-Hooks 2011; De Almeida Correia and Antunes 2012; Jorge et al. 2012; Barth and Todd 1999; Kek et al. 2006; Kek et al. 2009; Weikl and Bogenberger 2015; Nourinejad and Roorda 2014) and user-based relocation approaches (Barth et al. 2004; Uesugi et al. 2007; DI Febbraro et al. 2012). Operator-based relocation requires extra staff to redistribute vehicles between stations, while user-based relocation encourages carsharing users to relocate vehicles by offering them price incentives. A detailed literature review with respect to one-way carsharing relocation methods can be found in Jian et al. (2016), where we found most relocation models did not consider the impact of demand variation on the availability of carsharing vehicles as well as vehicle stock distribution. Fan et al. (2008a), Nair and Miller-Hooks (2011), Nair et al. (2013), Jorge et al. (2012) and Jorge et al. (2015b) considered stochastic travel demand in the relocation models. They found that demand variation would impair the performance of relocation models. Jorge and Correia (2013) also highlighted in their literature review paper that it is difficult to accommodate demand variation when developing relocation optimization models because the demand and the supply in carsharing systems are interdependent. Users’ travel demand for carsharing services is influenced by the available supply of carsharing vehicles. Demand, on the one hand, brings about the revenues to carsharing operators; Relocation, on the other hand, generates extra costs to them. These two factors contribute in determining the profitability of carsharing systems. Hence, the interdependence between demand and supply in one-way carsharing systems makes the profitability analysis more complex. An attractive solution to this logistical challenge is to undertake an integrated supply and demand cost-benefit analysis before introducing one-way carsharing services. Jian et al. (2016) proposed an approach that incorporates an Integer Linear Programming (ILP) model with a discrete choice model. In their study, users’ travel demand is determined by the discrete choice model which considers trip cost and travel time to be the main factors influencing users’ demand. Vehicle availability together with users’ travel demand are the inputs of the ILP model. The ILP model solves the optimal relocation decisions and determines which trips to accept. The outputs further change vehicle availability across carsharing vehicle pods. The ILP model and the discrete choice model work together to account for the interaction between users’ travel demand and the supply of vehicles in carsharing systems. However, it should be noted that the choice model cited by the study itself does not consider vehicle availability as a factor that could directly influence the elasticity in demand. The integrated approach treats vehicle availability as a constraint to incorporate the impact of vehicle availability on trip demand. But if we aim to understand the interaction between demand and supply in carsharing system, the best solution is to incorporate the ILP model with a choice model that considers users’ demand to be directly elastic to vehicle availability. In this paper, we build on the model proposed by Jian et al. (2016) and present a refined integrated formulation for one-way carsharing optimization. In the new integrated supply-demand model, vehicle availability is the output of the ILP model and serves as the input of the discrete choice model simultaneously. In this way, user demand and vehicle supply are linked by vehicle availability. However, another difficulty arises in the integrated supply-demand model owing to the nonlinearities induced by the integration of the discrete choice model, because vehicle availability is a decision variable in both the ILP model and the discrete choice model. A similar issue has been found in the automated vehicle assignment problem studied by de Almeida Correia and van Arem (2016). In their study, travelers’ mode choice between public transport and automated vehicles was supposed to be determined by a choice model with a Logit or Probit structure. But in order to avoid the nonlinearity issue, they ignored the probabilistic part of the choice model and only considered the deterministic part. In another carsharing study conducted by Jorge et al. (2015b), the authors assumed carsharing demand to be elastic to trip price. Based on this assumption, they proposed a pricing model to balance vehicle stocks. The price elasticity of demand caused the model to be nonlinear. They proposed an iterated local search (ILS) metaheuristic to solve the nonlinear model. These two studies either simplify the choice model to avoid the nonlinearity, or proposed heuristic to solve the nonlinearity. Apart from these two approaches, nonlinear formulations can be linearized and converted into linear models in some cases, at the expense of auxiliary decision variables. The linearized model can be conveniently solved using robust methods such as LP-based branch and bound method, and implemented in codes such as CPLEX and XPRESS (Grossmann 2002). We apply this technique to the present integrated logistical problem. To the authors’ best knowledge, this paper is the first to use linearization method to solve a Mixed Integer Nonlinear Programming (MINLP) model embedding a discrete choice model. The paper is organized as follows. In the next section we present the extended integrated supply-demand model derived from the ILP model developed by Jian et al. (2016). Then, the linearization method of the model is described. The Sydney carsharing network case study is presented in the fourth section, followed by the results of the sensitivity analysis. The major findings obtained in this study and the recommendations for further studies are highlighted in the last section.",18
19.0,2.0,Networks and Spatial Economics,16 April 2019,https://link.springer.com/article/10.1007/s11067-019-09454-9,Range-Constrained Traffic Assignment with Multi-Modal Recharge for Electric Vehicles,June 2019,Xiang Zhang,David Rey,Nathan Chen,,Male,Male,Mix,,
19.0,3.0,Networks and Spatial Economics,20 June 2018,https://link.springer.com/article/10.1007/s11067-018-9402-5,Airport Taxi Situation Awareness with a Macroscopic Distribution Network Analysis,September 2019,Jianan Yin,Minghua Hu,Dan Chen,Unknown,Unknown,Male,Male,"With the tremendous growth of air transport industry over the past few decades, airport network structure, aircraft ground movement, and airport operational environment have become increasingly more complex. This is accompanied by a drastic increase in aircraft conflicts, airport congestion and flight delays. With the significant efforts undertaken to improve en-route operation, there has been a major shift of congestion from en-route airspaces to airport surface (Smeltink et al. 2003). This change has urged air navigation service providers, airports and airlines to improve, individually or collaboratively, the efficiency of their services or processes including taxi planning (Marin 2006; Clare et al. 2009; Mori 2013), arrival and departure scheduling (Bohme et al. 2007; Hesselink and Basjes 1999) and turn-around management (Norin 2008). Integration of all these processes is explored by EUROCONTROL (2005) by issuing the implementation manual of airport collaborative decision making, which, since then, has become a mature guide and effectively enhanced the performance of hub airports. During the entire operational period of aircraft, airport ground movement plays a critical role and contributes to airport congestion and delay. The taxiway network is the most significant component of airport capacity, and is central to the mitigation of congestion. Due to the significant complexity and uncertainties associated with aircraft movements in taxi network, the accurate awareness of airport taxi situation is, and continue to be, a critical issue for air transport decision makers to ensure the safe and efficient operation of air traffic management (ATM) systems. Most literature on taxiway network management focuses on the optimization. The optimization of the taxi process encompasses both spatial and temporal dimensions. The spatial planning focuses primarily on taxi routing between the gates and the runways (Balakrishnan and Jung 2007; Keith et al. 2008; Gerdes and Temme 2012; Guépet et al. 2016). The temporal planning focuses on scheduling of taxi activities, which is used to assign time stamps to aircraft concerning when to reach certain point on the airport surface along its taxi route (Smeltink et al. 2004; Rathinam et al. 2008; Montoya et al. 2010). Regarding the objective of these optimization problems, many of the previous studies focus on minimizing the total taxi time between the runway and the gate (Pesic et al. 2001; Deau et al. 2009; Ravizza et al. 2013a, 2013b), while others consider multi-objective optimization. For example, Marín and Codina (2008) solve the problem of taxi network design by adopting a weighted linear objective function to balance a list of conflicting performance measures, including airport throughput, aircraft taxi time, flight delays and operational costs. On the constraint side of airport surface operation, minimum aircraft separation constraints, taxiing speed constraints, arrival/departure time constraints and route priorities are synthetically considered based on the theory of conflict detection and resolution (Atkin et al. 2010; Smeltink et al. 2003). Finally, most of these optimization problems are solved with heuristic or metaheuristic methods due to the complexity of the dynamics and constraints. For example, the genetic algorithm (Gotteland et al. 2001), A-star algorithm (Brinton et al. 2002), particle swarm algorithm (Liu et al. 2011) and ant colony algorithm (Nogueira et al. 2014) are adopted to solve the taxi planning problems. Most of the literature reviewed above focuses on the spatial-temporal information of single aircraft, rather than the entire aircraft fleet in the taxiway system. Given that the efficient taxiway network management requires precise and reliable assessment of the entire traffic situation, we propose a macroscopic distribution network model to assess the airport taxi situation, aiming at providing precise information of airport taxi situation for airport managers and air navigation service providers. In the airport system, taxi time is one of the key performance indicators to analyze the airport taxi situation. Extended taxi-out time and taxi-in time, including large queuing times before entering the runway, are direct consequences of inefficient air traffic management, and are often associated with excessive operational and maintenance costs, increased risks, as well as negative environmental impacts. Regarding taxi time, many studies employ statistical models that rely on probability distributions of flight delays and aircraft operation times, in order to predict aircraft taxi time (Shumsky 1995; Signor and Levy 2006). Idris et al. (2001) identify some factors that affect aircraft taxi time and establish a prediction model taking into account the most significant factors such as takeoff queue size. Clewlow et al. (2010) analyze the impact of arrivals on departure taxi operations at airports and find that the impact increases as interaction between departures and arrivals increases. Balakrishna et al. (2008, 2010), George and Khan (2015) define the number of arrivals that are taxiing on the surface as one of the elements of system state, and adopt reinforcement learning algorithm to estimate aircraft taxi time, followed by assessment of the accuracy of these models. Most of the aforementioned taxi situation prediction models focus on either the arrival taxi process or departure taxi process separately, where in reality these two processes are clearly coupled and interdependent on each other. Moreover, they exclusively focus on the aircraft taxi time without considering other relevant factors or performance indicators pertaining to airport taxi situation, such as taxi delay, pushback rate, runway queue length, traffic volume and the interactions between arrivals and departures. Although taxi time is a key performance indicator of airport ground movements, it alone cannot sufficiently represent airport taxi situation in its entirety.Footnote 1 In additional, much attention of existing studies was focused on the situation of single aircraft while ignoring the analysis of taxi situation on the network level. For these and other reasons that will become clear when we present the results, it is necessary to identify all relevant performance indicators at the levels of aircraft and network, independent or correlated, in order to distinguish and identify the accurate  taxi situation, which is the focus of this paper. From the literature review, we conclude that there is a lack of systematic taxi situation awareness methods that rely on indicators beyond taxi time; nor is there a study on the complexity of airport taxi situation. Aiming at modelling, analyzing and assessing the taxi situation in complex airport systems with full consideration of the influencing factors of aircraft taxi process, this paper proposes a novel method for characterizing airport taxi situation based on a macroscopic distribution network (MDN) and a full list of TSIs. Specific contributions and findings are as follows. This paper focuses on airport taxi situation awareness at the aggregate level, by establishing a MDN to analyze the spatial-temporal characteristics of aircraft taxi process. With a given reference aircraft, we divide all the departure and arrival aircraft into 8 categories with the consideration of airport traffic in its entirety. Two sets of taxi situation indices (TSIs) are formulated, from the perspectives of single aircraft (hereafter referred to as Level-1 indices) and network (hereafter referred to as Level-2 indices). The TSIs at Level-1 and Level-2 include 5 categories and 19 indices based on the proposed MDN model. Then, we investigate the coverage of the TSIs system. A three-step hierarchical framework is proposed to assess the airport taxi situation at both aircraft and network levels. This consists of data analysis (TS-1), situation indices refinement (TS-2) and multiple situation awareness (TS-3). In TS-3, we conduct a comprehensive correlation study for all the TSIs and identify the most key influencing factors of aircraft taxi time indices. The proposed framework can be used for taxi situation awareness at strategic, pre-tactical, tactical, and post operations in a complex airport system. We propose two new metrics CTSa, CTSn to assess the taxi situation at Level-1 and Level-2 respectively, instead of using two systems of multiple TSIs. A significant relationship is revealed between the taxi delay and CTSa at Level-1, and the taxi time and CTSn at Level-2, which provides strong reference to airport ground movements for control and management purposes. The rest of this paper is organized as follows. Section 2 presents the macroscopic distribution network (MDN) of aircraft taxi process to describe the relationship between spatial and temporal resources. In Section 3 we define two systems of TSIs from the perspective of aircraft and network, and propose ways to compute them. Section 4 analyzes the coverage of the proposed TSIs system. In Section 5 we conduct a real-world case study of airport taxi situation awareness, and provide findings and insights by analyzing the correlation between different TSIs and complexity assessment results of airport taxi situation. Finally, some conclusion remarks are presented in Section 6.",9
19.0,3.0,Networks and Spatial Economics,04 July 2018,https://link.springer.com/article/10.1007/s11067-018-9414-1,Market Coordination Under Non-Equilibrium Dynamics,September 2019,Arnaud Z. Dragicevic,,,Male,Unknown,Unknown,Male,"Approaching the well-known topic of coordination by market mechanisms can be done without necessarily going through the panoply of tools used in standard microeconomic theory. While recalling some of its fundamental principles in the following paragraphs, the work presented hereafter builds on three streams of academic literature, which are catallactics, agent-based modeling and risk management. To start with, let us first recall that the issue of coordination is of major importance in economics, the question being addressed is how a multitude of actors interacting with each other manage to successfully coordinate their actions (Kapás 2006). Indeed, agents evolve in a setting where their own decisions depend upon the decisions of others (Ebeling 1987). Klein (1997) and Sautet (2002) refer to two kinds of coordination. The first one implies a concatenation of activities that leads to the production of coordinated results. The second relates to situations where an agent intentionally coordinates its actions with those of others. In this case, coordination is understood as an achievement of concerted action. One of the prevailing research questions in microeconomic theory is about the market equilibrium resulting from a collective coordination, in which quantity demanded and quantity supplied are equal. In time, formalizing this economic state has become a problem of mathematical analysis. Its solution has been mostly found in the use of optimization techniques borrowed from Lagrangian and Hamiltonian mechanics (Mirowski 1991). In game theory, agents are considered to be strategic players in competition on the markets where they exert their market power (Abada et al. 2013). An interesting result from the game-theoretic literature shows that the indeterminacy of equilibrium, as a concept of optimal outcome issued from the players’ interactions, is inherent to the joint hypotheses of rationality and its common knowledge. In this case, the structure of the economy is reflexive, such that the goal of an agent is to forecast the behaviors of others (Taillard 2006). Schelling (1960) observed that focal points, as a mechanism that creates convergent expectations on which equilibrium to choose, play an essential role in coordination problems. In general, the papers related to this topic documented the way agents focus their attention on one prominent or conspicuous equilibrium (Young 1993). The two other equilibrium selection theories are introspection (Harsanyi and Selten 1988) and dynamics of convergence (Crawford and Haller 1990). The absence of equilibria can be the outcome of uncoupled dynamics, where the adjustment of a player’s strategy does not depend on the payoff functions of the other players (Hart and Mas-Colell 2003). The possibility that non-equilibria might be focal too, and thus might facilitate coordination, needs to be recognized and investigated (Bosch-Domènech and Vried 2013). The last authors find that the non-equilibrium focal point acts as an equilibrium selection device from which players coordinate on a small subset of what they term the associated Nash equilibria. A complex system is regarded as efficient if it produces high coordinatedness. The fact that markets provide the broad institutional framework for coordinating is in the public domain. For instance, catallactics or spontaneous order is a theory proposed by Hayek (1978) to describe the order, brought about by the mutual adjustment of many individual economies, in a system that aims at reaching exchange rates and prices. The process has been suggested for the purpose of providing an explanation for the emergence of the free market system despite diverse ends pursued by its actors. It can be understood as a network of firms and households and has no specific purpose of its own (Kapás 2006). As for the kinds previously mentioned, the coordination achieved through catallactics forms part of the first one, because agents are not aware that they participate in a coordination game. In this sense, coordinatedness is a spontaneous outcome of market activities. The theory does not insist on the existence of equilibrium or that of multiple equilibria, for it sees coherence in economies to namely come from exchange. It should also be noted that it considers the unlikelihood of attaining optimality in coordination proceedings. Therefore, the only valid policy implication in this paradigm is to prevent artificial impediments to the exchange processes (Barry 1992). Instead of representing individuals constrained by strong consistency requirements relative to equilibrium and rationality, agent-based modeling describes heterogeneous agents living in complex systems that evolve through time (Windrum et al. 2007; Müller and Pyka 2016). An underlying element of agent-based modeling is its bottom-up perspective, which describes a system from the perspective of its constituent units (Axelrod 1997; Bonabeau 2002). One well-known example of this principle comes from Reynolds (1987), who recreated the complex behavior of a flock of birds by disaggregating the flock into birds using three behavioral rules. It is noteworthy that no central authority governs the components in this framework: as a decentralized activity, each of them individually follows the rules and reacts only to its local neighborhood.Footnote 1 When applied to economics, this consists in modeling macro-entities, such as economic systems, through the actions and interactions of micro-agents, such as firms or households (Müller and Pyka 2016). By analyzing the strategic decision making in large populations of small interacting components, we situate the analysis within the mean-field theory, such that the effects on an agent from its neighborhood are approximated by an averaged effect (Lasry and Lions 2007). A well-functioning market can howbeit be exposed to a multitude of shocks that can imperil the overall coordination organizing the economic system. Market risk is the possibility for an agent to experience profitability losses due to factors that affect the overall performance of the markets in which it is involved. It arises from the fluctuating prices of investments. The sub-components of market risk are currency risk, interest rate risk, commodity risk and equity risk. They can affect, in an undifferentiated way, various economic sectors both in terms of trade and profit. The market risk then spills over into a business risk (Reuvid 2011), the after-effects of which are taken into consideration in the present work. Exogenous economic shocks usually provoke unanticipated and exaggerated market volatility (Bloom 2009), which causes sharp swings in trade intensities. In response, long-term contracts were designed as a risk-sharing measure between producers and traders. They can also be used as a tool to mitigate the market power of the producers (Abada et al. 2013). Timely, risk-sharing through contracts and agreements occurs when two parties identify a market risk and agree to share the potential loss upon the likelihood of loss (Ramamohan Rao 2016). It is meant to be a precautionary measure which prevents the economic system from disintegrating. This scheme can be envisioned in form of flocking behavior facing an external obstacle (Olfati-Saber and Murray 2003), in which each agent applies a risk-sharing protocol so as to cushion the shock. In order to study the market coordination under non-equilibrium dynamics, such as the one outlined in catallactics, we consider a multi-agent system with fixed topology, based upon a Hamiltonian structure, subject to flocking behavior. The clustering of agents through flocking has previously been considered in Raafat et al. (2009) and Terano et al. (2009), but mostly to investigate on biological and mobile-agent systems. To the best of our knowledge, the present research article is the first to envisage market emergence and evolution as such. With respect to the existing literature in economics (Debreu 1959) combined with agent-modeling (Matsuda et al. 2010), the assumptions of market segmentation – as a result of monopolistic competition (Chamberlin 1953; Guttman and Maes 2006) – and of imperfect competition (Burguet and Sákovics 1999; Bunn and Martoccia 2008) are introduced. Indeed, assumptions of monopoly or perfect competition are often made in classical microeconomics, whereas real world settings are mostly located in between (Weiss 1999). In case of perfect competition, all agents try to maximize their expected profit. The absence of monopolistic markets is then justified by the fact that when a monopoly begins to vindicate its market power, the consumers react by decreasing their demand (Csercsik 2016). The paper could also fit into the alternative paradigm of complexity economics, which holds that the economy is a system neither in equilibrium nor optimal, which would otherwise be performed by dint of optimal collective moves. The system is rather in motion and is built on structures that form and reform. Such a complex system is associated with non-equilibrium (Arthur 2014). We show that the evolution of market positions depends on market powers and on the evolution of trade intensities, the latter being contingent on the joint-evolving of partnering agents and of market segmentation. Likewise, we find that the catallactic framework leads to the emergence of a stable market coordination, but is also a dissipative structure of cyclical nature, such that imperfect competition gives rise to a pseudo-competitive market. In the risk-sharing scenario, the evolution of market positions depends, in addition, on the number of partnering agents and on the level of risk transfer they agree on. In this case, the catallactic framework emerges as an unstable trading system which transforms the market risk into a systemic risk. The remained of the paper is organized as follows. Section 2 provides a detailed description of agent flocking behavior in a Hamiltonian energy-based structure. In Section 3, we extend the analysis to the risk-sharing scenario. Section 4 is devoted to illustrating simulation examples. Section 5 concludes.",2
19.0,3.0,Networks and Spatial Economics,07 July 2018,https://link.springer.com/article/10.1007/s11067-018-9413-2,"New Technological Knowledge, Rural and Urban Agriculture, and Steady State Economic Growth",September 2019,Amitrajeet A. Batabyal,Karima Kourtit,Peter Nijkamp,Unknown,Female,Male,Mix,,
19.0,3.0,Networks and Spatial Economics,11 July 2018,https://link.springer.com/article/10.1007/s11067-018-9406-1,Detecting Organization-Targeted Socialbots by Monitoring Social Network Profiles,September 2019,Abigail Paradise,Asaf Shabtai,Rami Puzis,Female,Male,Male,Mix,,
19.0,3.0,Networks and Spatial Economics,01 October 2018,https://link.springer.com/article/10.1007/s11067-018-9421-2,Competition between High Speed Rail and Conventional Transport Modes: Market Entry Game Analysis on Indian Corridors,September 2019,Varun Raturi,Ashish Verma,,Unknown,Male,Unknown,Male,"In India, rapid urbanization, growing economy and increasing per capita income has caused increasing growth in intercity travel. Indian Railways has played an immense role in assimilating various disjointed sectors and thereby acting as a catalyst in invigorating the Indian economy. From the technological frontier perspective of the twentieth century, HSR is viewed as one of the major breakthrough in the passenger transport market (Campos and Rus 2009). China already leads in the race of most extensive HSR network in the world given that its first HSR services started as late as 2007 (Fu et al. 2012). In order to cater to the ever increasing passenger traffic and demand for better services, the Government of India is exploring the option of introducing HSR services as a sustainable mode of transportation. The Indian Railways’ vision 2020 (Vision 2020, 2009) envisages the following on high-speed rail Corridors: “India is unique and alone among the major countries of the world in not having a single high-speed rail corridor capable of running trains at speeds of over 250 kmph. HSR corridors have played a major role in revitalization of Railways in Japan and Europe.” High-speed rail Corporation of India Limited (HSRC) has been formed for the sole purpose of analysing and conducting various feasibility studies of the proposed high-speed rail projects. HSRC mentions in the railway budget speech of 2012–2013 that tackling the issue of capital intensiveness and innovative funding mechanisms is necessary, to make this project a reality. India and Japan have signed a Memorandum of Understanding on December 12, 2015 for the collaboration and assistance in the Mumbai-Ahmedabad High Speed Rail Project (Priyadharshini and Selladurai 2016). The Ministry of Railways (Railway Board) has decided to carry out a prefeasibility study for implementing high-speed rail services on the Diamond Quadrilateral Network connecting four major metro and growth centres of the country - New Delhi, Mumbai, Chennai and Kolkata. Six High-speed rail corridors have already been identified for technical studies as shown in Table 1. Since Indian government perceives HSR as a feasible future mode of intercity transportation, this study is timely as current literature on HSR in Indian context is sparse. Indian Railways provides the most important mode of public transport in India. This is the most commonly used and cost effective long distance transport system of the country. Verma and Raturi (2016) states that in India the cultural diversity, inequitable job distribution, economic disparity leads to a demand for an affordable and reliable mode of transportation to fulfil this need for travel. In the process, the Indian Railways acts as a catalyst for socio-economic development and can serve as a strategic instrument for social welfare. Thus through this reasoning, profit maximization may not be the primary objective for the Railways. Thus this study tries to incorporate maximization of social welfare function also as the primary objective which may be more relevant for the Indian context. Majority of the competition studies in foreign context deals with profit maximization as the primary objective as the railways operators are private sectors. Hence this study aims to provide a scientific tool for policy level decision making.",8
19.0,3.0,Networks and Spatial Economics,15 November 2018,https://link.springer.com/article/10.1007/s11067-018-9428-8,"Analyzing Diversity, Strength and Centrality of Cities Using Networks of Multinational Firms",September 2019,Owais A. Hussain,Faraz Zaidi,Céline Rozenblat,Unknown,Unknown,Female,Female,"The presence of multinational firms plays an important role in the economic development of cities and regions (Rodriguez-Clare 1996; Young et al. 1994). The economic and financial ties of these firms create linkages among cities irrespective of national and continental borders which eventually drive domestic and world economies (Dunning and Lundan 2008; Sassen 2011). The strong ties among the trio of New York-London-Tokyo is a good manifestation of this phenomena (Sassen 2001) as intense economic ties exist despite the geographic distances among them. These linkages among cities often form complex networks (Watts and Strogatz 1998; Barabási and Albert 1999; Ducruet and Beauguitte 2014; Tsiotas and Polyzos 2018). These ties or relations have high significance as they cause high interdependence between cities: a crisis in a city will impact highly the other cities to which it is well connected. This opens up an important dimension in studying these networks, which is to identify important or critical nodes. Identification of these critical nodes has attracted a lot of research activity from various domains such as ranking individuals in social networks (Wasserman and Faust 1994; Burt 2005), identifying Achilles heel in power grid networks (Kinney et al. 2005), immunizing potential spreaders to avoid epidemics (Moore and Newman 1999), identifying brideges in a collection of web pages (Zaidi et al. 2009) and studying criticality in transportation networks (Sun et al. 2017; Bóta et al. 2017; Kelly 2014). Researchers have also used the terms influence (Kempe et al. 2003), power (Bonacich 1987), central (Freeman 1979) to represent the concept of importanceFootnote 1 depending on the context it is being used in. Several methods and techniques have been proposed to identify these critical nodes in large complex networks. These methods generally use network structure to calculate metrics to reveal the importance of a node. Different metrics reveal various aspects of the network structure often revealing critical or influential nodes in certain contexts. For example, the cities which are most critical can be the ones with the highest number of direct connections to other cities, or be strategically located in a network to influence other cities. In the context of networks of cities, we identify three indicators to explore these networks: Diversity, Strength and Centrality. Each of these measures captures a different notion of importance in a network and details are presented in Section 4. In order to find important cities considering all three indicators combined, we introduce a cumulative metric which is a composite of Diversity, Strength and Centrality. The proposed metric reveals the cities which are critical across the two networks as well as the cities which have either increased or decreased their importance during the years from 2010 and 2013. The changes in the importance of a city demonstrate the focus of economic shift that took place in the aftermath of the economic crisis in 2008. As the world economy continues to change with new markets competing with existing economies, this study provides an interesting insight to the change occurring between the 2010 and 2013. Based on our findings, we ascertain the position of several major cities in the world’s economy as well as discover some of the developing cities and regions based on multinational firms across the globe. The paper is organized as follows: Section 2 reviews the literature. We describe the data in Section 3 followed by details of the metrics used and the experimental set-up in Section 4. We discuss the results in Section 5 and we conclude in Section 6 providing future research directions.",5
19.0,3.0,Networks and Spatial Economics,12 December 2018,https://link.springer.com/article/10.1007/s11067-018-9429-7,Cooperative Cover of Uniform Demand,September 2019,Tammy Drezner,Zvi Drezner,,Female,Male,Unknown,Mix,,
19.0,3.0,Networks and Spatial Economics,12 December 2018,https://link.springer.com/article/10.1007/s11067-018-9432-z,Tradable Credit Scheme for Control of Evolutionary Traffic Flows to System Optimum: Model and its Convergence,September 2019,Ren-Yong Guo,Hai-Jun Huang,Hai Yang,Unknown,,,Mix,,
19.0,3.0,Networks and Spatial Economics,11 January 2019,https://link.springer.com/article/10.1007/s11067-018-9433-y,Computing Dynamic User Equilibria on Large-Scale Networks with Software Implementation,September 2019,Ke Han,Gabriel Eve,Terry L. Friesz,,Male,,Mix,,
19.0,3.0,Networks and Spatial Economics,14 January 2019,https://link.springer.com/article/10.1007/s11067-018-9434-x,A Dynamic and Flexible Berth Allocation Model with Stochastic Vessel Arrival Times,September 2019,Shangyao Yan,Chung-Cheng Lu,Han-Chun Lin,Unknown,Unknown,,Mix,,
19.0,3.0,Networks and Spatial Economics,23 January 2019,https://link.springer.com/article/10.1007/s11067-018-9438-6,Reliable Routing of Road-Rail Intermodal Freight under Uncertainty,September 2019,Majbah Uddin,Nathan Huynh,,Unknown,Male,Unknown,Male,"Freight transportation involves various transportation modes, such as road, rail, air and water. The use of different transportation modes provides greater efficiency because it takes advantages of the strength of each transportation mode. Intermodal freight transportation uses two or more modes to transport goods without handling the goods themselves. Intermodal transportation offers an attractive alternative to unimodal transportation by highway in terms of cost for freight transported over long distances, and it reduces the carbon footprint of transport compared to the highway mode (Bureau of Transportation Statistics 2015). In recent years, intermodal freight transport volume has grown significantly due to the aforementioned advantages. Transportation infrastructures, particularly those supporting intermodal freight, are vulnerable to natural disasters (e.g., hurricane, earthquake, flooding) and man-made disasters (e.g., accidents, labor strike). These disruptions can drastically degrade the capacity of a transportation mode and consequently have adverse impacts on intermodal freight transport and freight supply chain (Miller-Hooks et al. 2012; Uddin and Huynh 2016). For examples, Hurricane Katrina significantly damaged the transportation infrastructure in the Gulf Coast area (Godoy 2007), and the West Coast port labor strike severely disrupted the U.S. freight supply chain (D’Amico 2002). Therefore, there is a need to develop a modeling framework that takes into account the reliability of the freight transport network when making strategic routing decisions. Network reliability means that the network can continue to deliver acceptable service when faced with disasters or disruptions that reduce capacity of network links, nodes, and intermodal terminals. The majority of the studies that deal with intermodal freight shipments seek to minimize routing cost. Barnhart and Ratliff (1993) proposed a model for minimizing routing cost in a road-rail intermodal network. They developed procedures involving shortest paths and matching algorithm to help shippers in deciding routing options. Boardman et al. (1997) developed a software-based decision support system to assist shippers to select the best combination of transportation modes considering cost, service level, and the type of commodity. Xiong and Wang (2014) developed a bi-level multi-objective genetic algorithm for the routing of freight with time windows in a multimodal network. Ayar and Yaman (2012) investigated an intermodal multicommodity routing problem where release times and due dates of commodities were pre-scheduled in a planning horizon. Uddin and Huynh (2015) developed a methodology for freight traffic assignment in large-scale road-rail intermodal networks to be used by transportation planners to forecast intermodal freight flows. Rudi et al. (2016) proposed a capacitated multicommodity network flow model for the intermodal freight transportation problem that seeks to minimize transportation costs, carbon emissions, and in-transit holding costs. Their model was validated using industry data from an automotive supplier. Qu et al. (2016) provided a multimode multicommodity service network design model for intermodal freight transportation considering greenhouse gas emissions and intermodal transfer cost. Intermodal freight transportation has also been studied in the context of network equilibrium. Friesz et al. (1986) presented a network equilibrium model for predicting freight flows considering the role of both shippers and carriers. Their model considered the route choice decisions of shippers and carriers sequentially on a multimodal freight network with nonlinear cost and delay functions. Guelat et al. (1990) used a Gauss–Seidel–Linear approximation algorithm to assign multiproduct freight flows on a multimodal network for strategic planning. In addition to link costs, their algorithm considered intermodal transfer costs to determine the shortest paths. Agrawal and Ziliaskopoulos (2006) used variational inequality to develop a dynamic shipper-carrier freight assignment model. In their model, the market equilibrium is reached when no shipper can reduce its cost by changing carrier for any shipment. Li et al. (2014) proposed a receding horizon approach for the intermodal container flow assignment problem. Their approach assigned container flows using the solution from a nonlinear optimization model that is evaluated at each time step for each node in the network. Corman et al. (2017) presented an equilibrium model for multimodal container transportation. Their model minimized the generalized costs, which is a function of mode, travel time and waiting time for freight consolidation. In addition to the use of mathematical programs to solve the intermodal freight transportation problem, other methods that have been utilized include network simulation, econometrics, and geospatial analysis. Mahmassani et al. (2007) developed a dynamic freight network simulation-assignment framework that can be used to analyze multiproduct intermodal freight transportation systems. Their framework modeled individual shipment mode-path choice behavior and terminal transfer. Zhang et al. (2008) applied the Mahmassani et al. framework to a pan-European rail network and validated it by analyzing the convergence patterns and performance measures. Lim and Thill (2008) utilized geographic information system-based mapping to evaluate the performance of the U.S. intermodal freight transportation network. They also utilized geographically weighted regressions to identify the factors affecting the improvement of accessibility due to intermodalism. Winebrake et al. (2008a) presented a geospatial model for analyzing intermodal freight network in terms of cost, time-of-delivery, energy and environmental impact. Their model can also be used to explore tradeoffs among different mode combinations. Meng and Wang (2010) proposed an algorithm based on Monte Carlo simulation to estimate the probability of shippers selecting an intermodal route involving a port. Their algorithm maximized the utility of shippers, which is defined as a summation of transportation cost and transport time multiplied by the perceived value of time. All of the aforementioned studies assume that the freight transport network is always functioning and is never disrupted. Daskin (1983) considered disruptions by taking into account the facility unavailability in a maximum covering location problem. Snyder and Daskin (2005) presented a uncapacitated location problem considering failure of facilities in the network. Their reliability models find facility location by taking into account the expected transportation cost after failure, in addition to the minimum operational cost. Cui et al. (2010) extended this work to consider failures with site-dependent probabilities and re-routing of customers when there are failures. Unnikrishnan et al. (2009) developed a two-stage linear program with recourse for the shipper-carrier network design problem under uncertainty. In their model, the shipper decides the optimal capacity for the transshipment nodes in the first stage, and in the second stage, it chooses a routing strategy based on the realized demand. Peng et al. (2011) considered disruptions at facilities in their work on design of reliable logistics network. In contrast, Cappanera and Scaparra (2011) sought to improve network reliability by optimally allocating protective resources in shortest path networks. Chen and Miller-Hooks (2012) developed a method to quantify resilience of an intermodal freight transport network. Miller-Hooks et al. (2012) extended this work to maximize freight transport network resiliency by implementing preparedness and recovery activities within a given budget. Huang and Pang (2014) evaluated resiliency of biofuel transport networks under possible natural disruptions. They formulated a multi-objective stochastic program to optimize the total system cost and total resilience cost. Marufuzzaman et al. (2014) proposed a reliable multimodal transportation network design model, where intermodal hubs are subject to site-dependent disruptions. This model was solved using the accelerated Benders decomposition algorithm and tested on a large-scale network. Uddin and Huynh (2016) proposed a stochastic mixed-integer model for the routing of multicommodity freight in an intermodal network under disruptions. Their study found that goods are better shipped via road-rail intermodal network during disruptions due to the built-in redundancy of the freight transport network. Li et al. (2016) formulated a freight routing model considering both reliability and sustainability under link travel time uncertainty. Fotuhi and Huynh (2017) developed a robust mixed-integer linear model that can be used by railroad operators to evaluate intermodal network expansion options when there are uncertainties in freight demands and network element capacities. A number of studies have considered network vulnerability in planning decision. Chen et al. (2007) presented network-based accessibility measures for assessing the vulnerability of transportation networks under disruptions. In addition to the increase in travel time due to link failures, their model considered the behavioral responses of users. Peterson and Church (2008) investigated rail network vulnerability by formulating both uncapacitated and capacitated routing-based model. Garg and Smith (2008) presented a methodology for designing a survivable multicommodity flow network, which analyzes failure scenarios involving multiple arcs. Rios et al. (2000) studied a similar problem, but their objective was to find the minimum-cost capacity-expansion options such that shipments can still be delivered to receivers through the network under disruptions. Gedik et al. (2014) proposed a capacitated mixed-integer interdiction programming model for coal transportation. They assessed network vulnerability and re-routing of coal by rail under network disruptions. Viljoen and Joubert (2018) presented a model to quantify the impact of transportation infrastructure on supply chain vulnerability; their methodology used a multilayered network. Another area of research that involves network uncertainty is disaster management, relief routing, response planning, and emergency and humanitarian logistics. Researchers have developed a wide variety of classical optimization programs to address these challenging problems. Haghani and Oh (1996) presented a disaster relief routing model for multicommodity freight in a multimodal network using the concept of time-space network. In the work by Ozdamar et al. (2004), commodity relief routing was studied as a hybrid of classical multicommodity network flow and vehicle routing problem. Given the uncertainty associated with network disruption, their model attempted to deliver commodities such that unsatisfied demand is minimized in a multimodal network. Barbarosoglu and Arda (2004) proposed a stochastic programming model for transporting multicommodity freight through a multimodal network during a natural disaster. Their model considered random arc capacity, where randomness is represented by a finite sample of scenarios. Chang et al. (2007) studied the rescue resources location-routing problem in the event of a flooding disaster. Shen et al. (2009) investigated how to route vehicles in the event of a large-scale bioterrorism emergency. Their solution approach involves adjusting routes generated at the planning level to consider effects of disruptions. Rennemo et al. (2014) proposed a model comprising several stages to optimally locate relief distribution facilities. Cantillo et al. (2018) developed a transportation network vulnerability assessment model to identify critical links for siting distribution centers for disaster response. Table 1 provides a summary of the key features addressed by prior studies related to the routing of freight. All of the prior studies where network uncertainty is considered make an explicit assumption about the probability density function (PDF) of the network link and/or node capacity. However, given that disruptive events are rare, there is often limited or no historical data available to determine the PDF of the network link or node capacity under a particular disruption scenario. A wrong assumption could have serious consequences of over design or under design. For example, assuming that a link capacity will follow the normal distribution in the event of a flash flood when in fact it follows a gamma distribution would lead to over design. This study contributes to the current body of knowledge by relaxing this explicit PDF assumption. A novel distribution-free approach is used to provide probabilistic guarantees on the resulting routes. This approach uses symmetric random variation, which is a popular method for solving robust optimization models (Bertsimas and Sim 2004; Ng and Waller 2012). The remainder of this paper is organized as follows. Section 2 describes the problem considered in this study and presents the mathematical formulation of the problem. Section 3 describes the application of the proposed methodology on a real-world road-rail intermodal transportation network. Lastly, section 4 provides a summary of the study, its contributions, and future work.",23
19.0,3.0,Networks and Spatial Economics,24 January 2019,https://link.springer.com/article/10.1007/s11067-019-9443-4,An Advanced Parking Navigation System for Downtown Parking,September 2019,Zhibin Chen,Stephen Spana,Yuchuan Du,Unknown,Male,Unknown,Male,"Finding a parking space in downtown areas can be a nightmare for many drivers, especially during peak hours. For example, Shoup (2006) pointed out that the average cruising time for a curbside parking space can be as much as 14 min in some major cities. In the district of Schwabing in Munich, Germany, 44% of the traffic is searching for free parking places, leading to 20 million Euros annually in waste (Caliskan et al. 2006). To reduce drivers’ cruising time, researchers have investigated the optimal deployment of parking facilities (e.g., Zhuge and Shao 2018; Song et al. 2017; Khakbaz et al. 2013; Li et al. 2008). Recently, thanks to the development of sensing and wireless communication technologies, the provision of real-time availability and prices of parking spaces becomes feasible (Chen and Chang 2011; Mathur et al. 2009; Polycarpou et al. 2013; Guo et al. 2013; Lazov 2018). Additionally, the widespread adoption of smart phones (Statista 2016) allows drivers to access such parking information easily. Using these technologies, various advanced parking management approaches, such as parking reservations (Chen et al. 2015, 2016; Teodorović and Lučić 2006; Liu et al. 2014a, b; Yang et al. 2013; Rehena et al. 2018), parking pricing (He et al. 2015; Fosgerau and De Palma 2013; Ayala et al. 2012b; Qian and Rajagopal 2014; Mackowski et al. 2015; Liu and Geroliminis 2016; Lei and Ouyang 2017; Liu 2018), parking information provisions (Caicedo 2010; Kokolaki et al. 2013; Du and Gong 2016), and parking guidance (Shin and Jun 2014; Idris et al. 2009), have been explored to help drivers find open parking spaces quickly. This study focuses on developing an advanced parking navigation system that can guide drivers to open parking spaces and substantially reduce their cruising time for parking. Similar systems have been discussed and designed in the literature (see, e.g., Shin and Jun 2014; Ayala et al. 2012a, c; Idris et al. 2009; Leephakpreeda 2007). Shin and Jun (2014) developed a smart parking navigation system in which drivers are guided to their most preferred parking lot. In this system, a parking utility function considering driving duration, walking distance, parking cost, etc. is defined to measure the appropriateness of parking lots. Idris et al. (2009) proposed a navigation system where drivers can be guided to the nearest empty parking space utilizing a wireless sensor network and an ultrasonic sensor. Ayala et al. (2012a) presented a gravitational approach to guide drivers to parking spaces. Ayala et al. (2012c) applied a two-sided matching framework to achieve the equilibrium parking assignment. Leephakpreeda (2007) applied fuzzy knowledge-based decision making to guide drivers to the best parking spaces. However, most of the above systems are not ready for practical implementation, due to expensive computation, strong assumptions or the need for disclosing drivers’ private information. What is worse, these systems may guide multiple drivers to the same space, which will inevitably intensify the parking competition. Consequently, the adoption of a parking navigation system in reality is still limited. We propose to develop an advanced parking navigation system to address the above limitations. The system manages a finite number of parking spaces located in a downtown area. It is envisioned that the system can access the status information of parking facilities via sensing and wireless communication technologies, and drivers can access the system via their smartphones. Guiding drivers to open spaces is essentially a two-sided matching problem: drivers are on one side and spaces are on the other side. Therefore, given drivers’ real-time locations and their parking space preferences, a two-sided matching algorithm will be firstly developed to achieve a stable driver-optimal matching. Under such a stable matching, drivers will be guided to their most appropriate spaces (if any) at all times, and thus no driver can benefit from disobeying the navigation system. Moreover, the proposed matching algorithm will be strategy proof, implying that drivers will have no incentive to misreport their private information (e.g., real-time parking space preferences). Although drivers’ private information is required for applying the proposed matching algorithm, an efficient distributed solution procedure will be applied to achieve the space assignment without disclosing such information. To demonstrate the performance of the proposed navigation system (hereinafter referred to as “matching system”), an agent-based simulation experiment will be conducted to show how the matching system improves the management of curbside parking compared with other navigation systems. The structure of this paper is as follows. Section 2 describes the architecture and the procedure of the parking navigation system. Section 3 introduces the two-sided matching model and the relevant results rooted in the stable driver-optimal matching strategy, and a distributed solution procedure is proposed in Section 4. An agent-based simulation experiment is conducted in Section 5. Section 6 concludes the paper.",19
19.0,3.0,Networks and Spatial Economics,05 February 2019,https://link.springer.com/article/10.1007/s11067-018-9437-7,Multi-depot Two-Echelon Fuel Minimizing Routing Problem with Heterogeneous Fleets: Model and Heuristic,September 2019,Surendra Reddy Kancharla,Gitakrishnan Ramadurai,,Unknown,Unknown,Unknown,Unknown,,
19.0,4.0,Networks and Spatial Economics,08 February 2019,https://link.springer.com/article/10.1007/s11067-019-9442-5,Stochastic Ridesharing User Equilibrium in Transport Networks,December 2019,Chen-Yang Yan,Mao-Bin Hu,Hao-Xiang Liu,,Unknown,Unknown,Mix,,
19.0,4.0,Networks and Spatial Economics,08 February 2019,https://link.springer.com/article/10.1007/s11067-019-9441-6,An Integrated Disaster Preparedness Model for Retrofitting and Relief Item Transportation,December 2019,Alper Döyen,Necati Aras,,Male,Male,Unknown,Male,"The devastating effects of huge earthquakes can be scaled down on one hand by pre-disaster mitigation efforts and on the other hand by efficient post-disaster response decisions. Pre-disaster mitigation efforts help to eliminate the calamitous effects of disasters in terms of monetary and human suffering. The aim of post-disaster response is to supply relief items such as food, medicine, shelter and clothes in an adequate and quick manner. The resilience of the structures (i.e., buildings and transportation infrastructure) is a key factor that has a bearing on the required amount of relief item demand and the condition of the transportation infrastructure in the aftermath of an earthquake. Consequently, an effective planning regarding the pre-disaster mitigation, which considers the uncertainty related to the earthquake magnitude, is essential for having a successful response. Strengthening the buildings and/or transportation infrastructure to a higher resistance level referred to as retrofitting is one of the most significant mitigation activities. There is a rich literature on disaster mitigation studies of buildings, however, few of them use in fact an optimization-based decision making approach to determine retrofitting decisions. Here we review first the studies that consider retrofit decisions involving buildings to reduce post-disaster reconstruction costs and then works that are concerned with pre-disaster transportation link investments to get a better disaster response. Dodo et al. (2007) propose a linear program that aims to determine optimal retrofitting decisions for groups of buildings under a limited budget. A building differs from any other building by its census tract location, structural type, occupancy type and seismic design level attributes. The buildings with the same attributes constitute a “building group”. The authors define a continuous decision variable to denote the amount of square footage of a building group that is retrofitted from one design level to an upper one in a given time period. The area of the building groups belonging to a design level at the end of each period is also a continuous decision variable. Higher design level means lower damage level for buildings, which translates into lower expected post-earthquake reconstruction costs. The objective is to minimize the total of retrofit and expected reconstruction costs. A Dantzig-Wolfe decomposition and a greedy heuristic algorithm are presented to solve the model. Xu et al. (2007) extend the previous model by introducing a multi-objective one by taking into account the risk of high reconstruction costs in addition to optimizing the total costs of mitigation and expected reconstruction. Risk minimization is achieved by penalizing the reconstruction costs that are above a certain level. The authors first show the equivalence of multi and single-period formulations, and then using a real problem instance, they solve a single-period model by Dantzig-Wolfe decomposition method. The models developed by Dodo et al. (2007) and Xu et al. (2007) are altered by Vaziri et al. (2010) in such a way that they can be employed in developing countries where economic resources are insufficient, an earthquake may create a more pervasive damage, and the number of deaths is much higher than those occurring in a developed country. The new model proposed allows for reconstruction of damaged buildings in a later period when more funds are available since the limited availability of economic resources in developing countries leads to the possibility that the reconstruction of some damaged buildings cannot be realized immediately after an earthquake. Furthermore, the objective function is augmented with an additional term to minimize the chance of an extreme number of deaths. Notice that this approach turns out to be very similar to the risk-return trade-off idea implemented by Xu et al. (2007), the only difference being that it focuses on the risk of large life loss rather than large economic loss. Zolfaghari and Peyghaleh (2015) propose another extension of Xu et al. (2007). They additionally consider fairness in the allocation of mitigation budget among different groups of people. Although the retrofitting decisions of transportation infrastructure systems can be given via a prioritization approach (Sohn 2006; Bana e Costa et al. 2008) using certain factors, these factors may not capture the interrelations between the individual components. Therefore, optimization models are introduced to take care of the overall network performance issue. Viswanath and Peeta (2003) develop a multi-objective integer programming model. Their model seeks for critical routes between the origin-destination pairs which are needed to be functional after an earthquake. The routes are determined due to the following objectives: minimizing the total travel time and maximizing the total population of covered centers between the origin-destination pairs. The functionality of routes are increased by retrofitting the bridges on them under a limited budget. Their model is a deterministic one because uncertainty related to an earthquake is not taken into account. A non-linear optimization model is proposed in Sanchez-Silva et al. (2005) with the objective of maximizing the accessibility among the centers existing in the whole network. The accessibility of a center is measured by a decreasing function of the expected cost of traveling from every other center to that center. Here, the expected cost of traveling is calculated by means of a continuous time Markov chain the main parameters of which are the failure and repair rates of the links in the transportation network. Failure rates can be decreased and repair rates can be increased by investing more, and the model decides on the optimal rates under a limited budget. Taylor et al. (2006) put forward a method that can be used to assess the vulnerability in road networks. To do so, they take into account several accessibility indices that are affected by the degradation level of the networks. Chen et al. (2007) also assess the vulnerability of urban transportation networks by defining network-based accessibility measures. These measures reflect the effect of link failures on both the increase in the travel time or cost and the response of the users to the disruption in the network accessibility. The authors develop a travel demand model to estimate the long-term equilibrium condition in the network, which helps to measure the consequences of changes in the supply side (i.e., link availability) and demand side (i.e., travel choices of users). Fan and Liu (2008) consider the protection problem of a transportation network before a disaster occurs. A two-stage stochastic programming (TSSP) model is formulated with equilibrium constraints, where the decisions are related to the selection of network components for protection and the objective is the minimization of the total expected physical and social losses due to potential disasters. Liu et al. (2009) also use the approach of TSSP, and propose a model to determine the optimal retrofit decisions of transportation links with the objective of minimizing the mean-risk of the system-wide cost. The first-stage variables of the model are the retrofit decisions which are made prior to an earthquake. The damage on the links occurs according to the following scenario. A link is not damaged if that link is retrofitted before the earthquake. If it is not retrofitted, the pre-retrofit state of the link determines whether it is damaged or not. The cost of the first-stage is the retrofitting cost, and that of the second stage consists of the repair cost and weighted flow cost which depends on both the first-stage retrofit decisions and particular realizations of link damages. The authors propose a Bender’s decomposition algorithm for solving large problem instances. Peeta et al. (2010) develop a TSSP model to minimize the travelling costs following an earthquake. The probability of a link that continues to be functional after an earthquake is assumed to be an increasing function of the degree of link retrofitting efforts before the earthquake. A large penalty cost is incurred if there does not exist a path connecting an origin and a destination. The authors develop an approximate solution procedure, and the quality of the solution obtained by this procedure is demonstrated on a real transportation network by proving that the generated solution is indeed a local optimum. Later, Du and Peeta (2014) improve this study to have a more realistic model. They evaluate the importance of a link not only in terms of connectivity but also traffic flow and survivability improvement relative to the retrofitting cost. They use continuous link investment decisions rather than binary ones to model partial investments on link retrofitting. Moreover their model includes additional stochasticity related to multiple disaster scenarios rather than a single disaster scenario as in Peeta et al. (2010) model, in which stochasticity arises only from link failures. Günneç and Salman (2011) adopt various performance measures to evaluate the functionality and performance of a transportation network following a disaster. By considering multiple disaster scenarios and varying the vulnerability probability of each link across the scenarios, the authors propose a new dependency model for link failures, and implement a polynomial-time algorithm for its solution provided that the number of paths connecting an origin-destination pair is fixed. A problem instance is generated using data on İstanbul, Turkey. This instance is solved by means of the polynomial-time algorithm and different dependency structures are also compared based on the same instance. Miller-Hooks et al. (2012) propose a two-stage stochastic program to maximize the resilience of a network by optimally allocating the budget between preparedness and recovery, where both factors are assumed to affect the resilience level. They employ Monte Carlo simulation to generate each network state under different scenarios and an integer L-shaped algorithm to solve an abstract network of a real problem. One can examine Faturechi and Miller-Hooks (2014) for a detailed review on transportation system studies related to disaster management optimization. Yücel et al. (2018) address the issue of increasing the resilience of a transportation network against disasters in terms of improving the expected post-disaster accessibility. Adopting a TSSP framework, the authors develop a model to minimize the pre-disaster investment cost by determining which links of the network to strengthen by using a new dependency model for random link failures. The pre-disaster resilience of buildings affect the level of post-disaster damage, which in turn determines the amount of relief item needs. Furthermore, the infrastructure vulnerability due to an earthquake will affect the connectivity of the transportation network, which is a key factor in effective distribution of relief items. Therefore, retrofitting decisions taken before an earthquake to reinforce the structures (buildings and transportation networks) are highly crucial for the efficiency of post-disaster response. As a consequence of this fact, it is necessary to consider the pre- and post-disaster decisions concurrently. Ignoring to do so, namely handling pre- and post-disaster objectives separately results in suboptimal solutions to the overall problem of managing catastrophic events, as mentioned in Tufekci and Wallace (1998). Motivated by this fact, in our study we not only include both the transportation link and building retrofitting decisions in the same model, but also integrate a set of post-disaster decisions. The latter type of decisions are mostly dealt with in the literature by stochastic facility location models in the context of humanitarian relief logistics. Although there exist deterministic models such as the one given in Yushimito et al. (2012) that focus on locating facilities with the goal of providing quick response for disaster relief, the majority of these models (e.g., Günneç and Salman 2007; Mete and Zabinsky 2010; Salmerón and Apte 2010; Rawls and Turnquist 2010; Döyen et al. 2012) are generally defined in terms of a TSSP model with recourse, where the location, size or capacity expansion levels of the relief/rescue facilities to be established are the first-stage decision variables, while the efficient allocation of the available relief items to the demand points and/or transfer of people to rescue centers constitute the second-stage variables (i.e. post disaster decisions). Interested readers may further refer to surveys (e.g., Grass and Fischer 2016; Hoyos et al. 2015) on two-stage stochastic programs in the disaster management context. The survey by Anaya-Arenas et al. (2014) focuses on the logistics aspects of relief distribution networks, while the one in Habib et al. (2016) categorizes the mathematical models developed into two groups as facility location models and relief distribution models. Most countries implement a specific disaster management strategy with a fixed amount of budget for the overall mitigation activities including transportation infrastructure as well as building retrofitting (Phaup and Kirschner 2010). Thus, the central authority (e.g., state government or city municipality) has a limited budget for all mitigation actions and also bears the post-disaster response costs. Therefore, there is a need for a strategic plan that takes into account retrofitting and response actions together under a limited mitigation budget. Thus the model proposed in this study is developed from a public decision making perspective, which is also the case in some previous studies such as (Dodo et al. 2007; Xu et al. 2007; Vaziri et al. 2010; Zolfaghari and Peyghaleh 2015; Liu et al. 2009; Miller-Hooks et al. 2012). In this paper, we propose an optimization model that takes into account the interrelations between pre- and post-disaster decisions. It can be used as a pre-disaster decision making tool that helps a decision-maker to give efficient decisions considering the post-disaster problem. In order to consider the interactions between pre- and post-disaster decisions and deal with the inherent uncertainty in the disaster, the model is developed as a TSSP model. Discrete probability scenarios are utilized in modeling the uncertainty with respect to earthquake intensity. The model aims to minimize the total system cost by providing efficient pre-disaster retrofitting decisions for both buildings and transportation links within a limited mitigation budget as well as post-disaster response related decisions. To the best of our knowledge, this study is the first attempt to combine transportation infrastructure retrofitting, building retrofitting, and response related decisions in the humanitarian logistics model. As opposed to the previous studies (Dodo et al. 2007; Xu et al. 2007; Vaziri et al. 2010; Zolfaghari and Peyghaleh 2015) where the benefit of building retrofit decisions result in reduced reconstruction costs in the aftermath of a disaster, the model of this paper examines the tradeoff between the retrofit expenditures for buildings and the amount of post-disaster relief item demand. The latter is a decision variable whose value depends on both the pre-disaster condition of buildings after retrofitting and the realized earthquake intensity scenario. Moreover, studies on optimization of mitigation decisions account either for building retrofitting (Dodo et al. 2007; Xu et al. 2007; Vaziri et al. 2010; Zolfaghari and Peyghaleh 2015) or transportation link retrofitting (Viswanath and Peeta 2003; Sanchez-Silva et al. 2005; Liu et al. 2009; Peeta et al. 2010; Günneç and Salman 2011; Miller-Hooks et al. 2012; Du and Peeta 2014). In our model, however, we include both types of the retrofitting decisions at the same time. Network disruptions cause a certain level of decrease in the connectivity of a transportation network. In case of a disaster, a non-retrofitted link could be non-functional (if there exists a vulnerable bridge/viaduct/tunnel on the link) restricting the assignment of a rescue center to a demand point located on the link. Since retrofitting has a positive impact on the post-disaster functionality of the network links, it increases the post-disaster response efficiency, and decreases the total system cost as well. Similarly, retrofitting of buildings reduces the damage level sustained by the buildings resulting in lower relief item demand, which implies that transportation and unsatisfied demand costs go down. The model we propose gives retrofitting decisions for each single building rather than retrofitting decisions for the area of specific building types. In the latter modeling approach, which is adopted more frequently in the literature, it is possible to employ continuous retrofitting decision variables. This, however, also has the drawback of not being able to specify which building is mitigated. Since we use binary variables to denote whether each building is retrofitted to a specific seismic code level or not, more accurate decisions can be made at the expense of a more difficult model. This means that the problem size increases rapidly and a general-purpose mixed-integer linear programming solver cannot handle the solution of the monolithic model. Therefore, we propose a Lagrangean relaxation scheme and five different Lagrangean heuristics to solve the model effectively and efficiently. The remainder of the paper is organized as follows. We introduce the model in Section 2 and explain the details of the solution methodologies in Section 3. Experimental results are presented in Section 4. Finally, we conclude in Section 5.",4
19.0,4.0,Networks and Spatial Economics,19 February 2019,https://link.springer.com/article/10.1007/s11067-019-09447-8,Origin-Destination Matrix Estimation Problem in a Markov Chain Approach,December 2019,Maryam Abareshi,Mehdi Zaferanieh,Mohammad Reza Safi,Female,Male,Male,Mix,,
19.0,4.0,Networks and Spatial Economics,20 February 2019,https://link.springer.com/article/10.1007/s11067-019-09451-y,Impacts of Public Transport Policy on City Size and Welfare,December 2019,Walid Chatti,Bassem Ben Soltane,Turki Abalala,Male,Male,Unknown,Male,"Interregional and intraregional transportation costs have direct and indirect effects on industrial location and regional integration. Regions with better access to domestic transport networks are usually the most attractive for economic activity (Stepniak and Rosik 2018); consequently, urbanization processes usually develop around the city center, which leads to higher urban costs, entailing numerous socio-economic and environmental changes over the long term, including dynamic demographic, residential, industrial and commercial zoning (Cavailhès et al. 2007). Public transport policies are essential to accompany the relocation of both firms and workers and guide sustainable, responsible and efficient development, taking into consideration the combination of the endogenous characteristics of cities and transport networks to enhance spatial equity (Bertolini and Spit 1998; Holl and Mariotti 2018). This paper uses two kinds of transportation costs to analyze the impacts of public infrastructure spending on the city size and welfare: the intra and interregional transportation costs. In the literature there are different theoretical models as those proposed by Helpman and Krugman (1985) and Krugman (1991),Footnote 1 which studied the relationship between interregional trade costs and industrial location. These models had two points in common: firstly, they demonstrated that the reduction in the interregional transportation cost induces more agglomeration in fewer regional centers, and therefore increases spatial disparities between regions. Secondly, by assuming the neutrality of space, they considered regions as simple dots without spatial dimensions, and consequently ignored intraregional transportation costs (Behrens et al. 2009). Martin and Rogers (1995) considered domestic and international transportation infrastructure in their analysis of how public spending on transport infrastructure affects the location of economic activity and welfare. They showed that any improvement in local transport networks in the home country attracts firms to this country, if the increase in the demand for manufacturing goods is larger than the decrease in demand due to the associated increase in taxes. On the other hand, improving international transportation infrastructure in a country which has a poor quality of domestic infrastructure will imply a relocation of firms outside this country. In reality, the intraregional transportation costs are significant and positively affected by higher urban costs (poor quality of transportation networks, land rents, commuting cost, delays, accidents, pollution, etc.); these important dimensions form the basis for urban economics. Several research contributions have been made, but most of them are neglected by interregional trade economists. Tabuchi (1998) unified the disparate disciplines that form the basis of urban economic, particularly proposing a synthesis of Alonso (1964) and Krugman (1991) by developing a general equilibrium model, with the presence of congestion costs (land rents and commuting costs). Ottaviano et al. (2002) analyzed variations in transportation cost and travel using a system of two cities (with agriculture sector and fixed housing consumption), where the commuting cost is exogenous. According to a quasi-linear utility function, they succeeded in providing an analytical solution that demonstrated the possibility of asymmetric equilibrium (when the commuting costs are different between cities), and found the inverted U-shape equilibrium when transport costs decrease. Other papers contributed to the linkage of these two growing fields. For instance, Krugman and Elizondo (Krugman and Livas Elizondo 1996) studied the effects of trade liberalization and congestion costs on the spatial sizes of cities in a developing country (Mexico). Land rents and commuting costsFootnote 2 along with numerous other factors can define urban costs. In most developed and developing countries they represent a large and growing share of the household budget. The performance of firms is negatively affected by the increase of urban costs in large cities (Cavailhès et al. 2007). In other words, local firms will bear higher production costs due to higher land rents, and wages that should be paid to workers to compensate urban costs. Consequently, these costs act as barriers to entry and result in higher prices, rendering firms less competitive in both local and foreign markets. Despite the advantages of the economic agglomeration (Duranton and Puga 2004; Duranton and Turner 2012), the presence of high urban costs may negatively affect the location of firms within large cities (Brakman et al. 1996; Krugman and Livas Elizondo 1996; Tabuchi 1998; Brueckner 2000; Duranton and Puga 2001; Cavailhès et al. 2004; Cavailhès et al. 2007; Goryunov and Kokuvin 2014; Jedwab et al. 2017). This incentivizes firms to leave the central urban areas, forming secondary employment centers or clusters (Henderson and Mitra 1996; Lucas Robert and Rossi Hansberg 2002). Workers may profit from a less localization cost by choosing to live in suburban or rural areas (Glaeser and Khan 2004; Holmes and Stevens 2004; Zhang 2016). Firms may be able to pay lower wages than in the city center, while relocating inside the metropolitan area (i.e. proximal to the traditional center), thus benefiting from agglomeration economics. Also, urban costs may slow down (intensify) rural-to-urban (urban-to-rural) migration, thereby decreasing urban expansion in the long run (Jedwab et al. 2017). The formation of small clusters or cities within a wider metropolitan area, often radiating from a traditional center, is known as a polycentric city, which appears to be a natural response to increasing urban costs (Cavailhès et al. 2007). We also propose a mixed general equilibrium model of New Economic Geography (NEG) and New Urban Economics (NUE), including a public sector. In this model, we assume that public expenditure may positively affect the labor supply and thus the productivity of workers when increasing the quality of local transportation infrastructure (Duranton et al. 2014; Mayer and Trevien 2017). For this, we introduce two kinds of transport costs: intraregional and interregional transport costs. Also, we consider the existence of urban cost inside each region in terms of commuting cost and resulting land rent, then we can analyze the different impacts of public spending on transport infrastructure on city size and welfare. Our main contribution is the explicit consideration of impacts the two types of transportation costs (and their associated infrastructures) in a general equilibrium model that features both NEG and NUE attributes. Unlike previous studies (Krugman 1991; Krugman and Livas Elizondo 1996; Tabuchi 1998; Ottaviano et al. 2002), our model integrates the public sector by introducing taxes on regional incomes, with the assumption of the immobility of labor between regions. This paper is in four parts. Section 2 presents the general equilibrium model of economic geography and urban economics and derives the short-run equilibrium conditions. Section 3 shows the impact of public transport policies on the city size. It analyses the stability conditions of the spatial geographic equilibriums. Section 4 derives policy implications on welfare. Section 5 presents conclusions.",12
19.0,4.0,Networks and Spatial Economics,22 February 2019,https://link.springer.com/article/10.1007/s11067-019-09449-6,Subnetwork Origin-Destination Matrix Estimation Under Travel Demand Constraints,December 2019,Chao Sun,Yulin Chang,Jie Ma,,Unknown,,Mix,,
19.0,4.0,Networks and Spatial Economics,22 February 2019,https://link.springer.com/article/10.1007/s11067-019-09448-7,Exclusive Bus Lane Network Design: A Perspective from Intersection Operational Dynamics,December 2019,Jing Zhao,Jie Yu,Yun Yuan,,,,Mix,,
19.0,4.0,Networks and Spatial Economics,23 February 2019,https://link.springer.com/article/10.1007/s11067-019-09446-9,Network Structure and Dynamics of Chinese Regional Incubation,December 2019,Haiyan Li,Yong Tang,,Unknown,,Unknown,Mix,,
19.0,4.0,Networks and Spatial Economics,28 February 2019,https://link.springer.com/article/10.1007/s11067-019-9440-7,A Bicriteria Perspective on L-Penalty Approaches – a Corrigendum to Siddiqui and Gabriel’s L-Penalty Approach for Solving MPECs,December 2019,Kerstin Dächert,Sauleh Siddiqui,Juan Miguel Morales,Female,Unknown,Male,Mix,,
19.0,4.0,Networks and Spatial Economics,13 March 2019,https://link.springer.com/article/10.1007/s11067-018-9439-5,"A comparison of Euclidean Distance, Travel Times, and Network Distances in Location Choice Mixture Models",December 2019,Sabina Buczkowska,Nicolas Coulombel,Matthieu de Lapparent,Female,Male,Male,Mix,,
19.0,4.0,Networks and Spatial Economics,30 April 2019,https://link.springer.com/article/10.1007/s11067-019-09467-4,Dynamic Passenger Assignment for Major Railway Disruptions Considering Information Interventions,December 2019,Yongqiu Zhu,Rob M. P. Goverde,,Unknown,Male,Unknown,Male,"Unexpected events affect railway operations in everyday life, which are either small service perturbations called disturbances or relatively large incidents called disruptions. During disturbances, train services will be delayed, but not cancelled/short- turned which however is necessary during disruptions. Due to the complexity of handling disruptions, contingency plans are designed beforehand for different disruption scenarios. When a disruption happens, the corresponding contingency plan is selected, and possibly modified by traffic controllers in terms of the specific condition (Ghaemi et al. 2017b). However, in either the design or modification procedure, passengers who should have been put first, are as yet not incorporated directly, because traffic controllers are unable to anticipate the passenger flows over the network. As a result, many alternatives for passenger reroutings are not considered, and thus passenger travel experiences during disruptions are usually less than satisfactory. To support passenger-oriented train service adjustments, it is necessary to have a passenger assignment model that can anticipate the distribution of passengers. Based on the model, whether a timetable is passenger-friendly or not can be evaluated, and further how to adjust the timetable in a passenger-friendly way can be guided. Until now, passenger assignment models are mostly proposed for planning purposes or disturbance management (generally regarded as delay management), where services are considered to be reliable or with minor perturbations. When major disruptions like complete track blockages occur, multiple dispatching measures, e.g. retiming, reordering, cancelling and short-tuning trains, are commonly applied, which result in delayed trains, changed train orders, completely cancelled trains and short-turned trains (Ghaemi et al. 2017a). As a result, the train services available during disruptions are rather different from the ones on normal days, thus leading to rather different path options to passengers. For passenger assignment models during disruptions, it is necessary to formulate the major service variations properly and model passenger responses to such major service variations accurately. Therefore, this paper proposes a dynamic passenger assignment model taking major service variations, vehicle capacity, and time-dependent passenger all into account. A preliminary version of the model can be found in Zhu and Goverde (2017), which is improved by introducing a new network formulation and information interventions for altering passenger behaviour in this paper. This paper considers passengers’ en-route travel decisions rather than passengers’ pre-trip travel decisions. This means that passengers are assumed to have planned paths in mind before they actually arrive at the origin stations, however, possibly they have to re-plan their paths due to major service variations, denied boardings or train congestion. Such an assumption is justified, since nowadays passengers can rely on various travel-planner applications or the official websites of operator companies to find their preferred paths. This is particularly true for passengers who have a clear travel purpose (e.g. commuters). Thus, once disruptions occur, passengers would make en-route travel decisions by comparing the alternative paths during disruptions with their planned paths. Passenger attitudes towards path alternatives during disruptions could be different from the ones on normal days. For example, due to reduced operation frequency during disruptions, passengers may be willing to spend more waiting times at origin/transfer stations than usual. Considering this, a new method is proposed to formulate the network with less arcs, which ensures all paths that could be chosen by passengers to be fully covered. The formulated network is a directed acyclic graph (DAG) with passenger perceived times on arcs, based on which the optimal paths perceived by passengers can be searched using efficient shortest path algorithms. Path alternatives can be different if passengers re-plan paths at different locations and times. This paper tracks the location of each passenger who starts travelling before, during, or after the disruption, and decides when and where he/she re-plans the path based on the information received. Information interventions are considered by delivering two kinds of information, service variations and train congestion, separately at different locations. Usually, the congestion effect is considered as the increase in travel times perceived by passengers (Cats et al. 2016; Larrain and Muñoz 2008). Instead, this paper aims to avoid travel time increase due to denied boarding, by using congestion information to affect passenger behavior in the following way. Imagine that a train is highly congested when departing from a stop, and there are still many passengers wishing to board this train at its next stop. It is possible that the train is unable to handle all these passengers. Thus, only some of them can board the train successfully, while the others have to be denied. If there must be some passengers being denied for boarding a train, avoiding them to choose the train may help them find better alternative paths compared to the ones they can find after being denied. Considering this situation, if a train is potentially unable to handle all passenger demand at its next stop, part of these passengers are notified with the congestion information in order to encourage them to choose another train, while the other part of these passengers are kept unaware of such information to ensure they will stay with their choice for this train. The key contributions of this work are summarized as follows:
 Proposing a new schedule-based passenger assignment model during major disruptions. Developing a new network formulation to formulate the timetable as a directed acyclic graph (DAG) with passenger perceived times on arcs. Taking time-dependent passenger demand, service variations, and vehicle capacity constraints all into account. Formulating passenger responses towards major service variations, like short-turned or cancelled trains. Using information interventions to influence passenger behaviour. Dealing with passengers who start travelling before, during and after the disruption. The remainder of this paper is organized as follows. Section 2 gives an overview of the relevant work. Section 3 explains the network modelling approach. In Section 4, the proposed dynamic passenger assignment framework is shown, followed by the explanation of the main parts in the framework. Next, the time complexities of the proposed algorithms are analysed in Section 5. Finally in Section 6, a case study of a complete open track blockage in part of the Dutch railway network is performed.",20
19.0,4.0,Networks and Spatial Economics,01 May 2019,https://link.springer.com/article/10.1007/s11067-019-09466-5,Application of Complex Networks Theory in Urban Traffic Network Researches,December 2019,Rui Ding,Norsidah Ujang,Jianjun Wu,Male,Unknown,Unknown,Male,"The importance of urban traffic network studies to urban economic and social development is self-evident. The most far-reaching impact on the modern urban form has been the development of traffic technologies such as the automobile, highway, metro and subway. The expansion of these urban traffic networks have shaped the morphology of modern cities, while the change of urban forms will in turn affect urban traffic network structures. Here we define an “urban traffic network” as an urban land-based traffic network, with emphasis on road and rail networks, and it both related with the traffic flow and infrastructure. Air transport and waterway transport are not included. In the urban traffic planning process, normally following this basic process: traffic survey, background prediction, traffic forecasting, layout scheme design, and project evaluation. Moreover, urban traffic network design is the most crucial point of layout scheme design (Liu 2001), as it fundamentally determines the basis of the future urban planning, economic development direction, and operational efficiency, while their functional layout conspicuously affects the urban form (Fig. 1). The development of traffic networks provides a strong guarantee for steady growth in the urban economy, and the rational distribution of a traffic network can effectively promote the flow of urban economic activities (Rodrigue et al.2013). Due to the stability of urban development and the difficulty of changing land-use patterns, the research and prove of how the traffic network topology structure to affect urban traffic distribution has earned much attention (Ducruet and Lugo 2013; Lammer et al.2006; Wu et al.2006a, 2006b). Hence, the designation and choice of new networks layout call for particular concern. The linkages between urban traffic networks and urban form Complex network theory is a multidisciplinary part of complexity science, which has seen a surge of interest since Watts and Strogatz (1998) and Barabasi and Albert (1999) described the collective dynamics of small-world networks, and the emergence of scaling in random scale-free networks. A small-world network is structured with a high clustering coefficient and small average shortest distance, while a scale-free network is a type of network in which the degree distribution of nodes obeys the Power-law distribution. Complex networks theory has been widely used and led to astonishing achievements in Empirical Science and Basic Science, being regarded as a paradigm representative of complexity system science. It mainly concentrates on the following aspects: the empirical research of the networks characteristics; network hub nodes detection; dynamical changing and spreading processes; the seeking and detection of communities and groups; robustness and vulnerability; and multilayer network theory and applications. We denote that the terms graph and network, node and vertex, and link and edge are respectively synonymous in this context. Research into complex networks provides an affordable and solvable method and novel insight into analyzing a complex urban system, while urban traffic networks are continually evolving. The paradigms of the small-world and scale-free networks changed the stereotyped thinking of urban traffic networks. Before these two models were proposed, urban traffic networks were normally addressed as either regular networks or ER random networks (Erdos and Renyi 1960), as inherited from the definitions of graph theory. However, recently scholars have recognized the importance of complex network science, including Masud et al. (2008), who treated this theory as an independent and key chapter in the book Operations Research and Management Science Handbook. In urban studies, Neal (2012) described in The Connected City: How Networks are Shaping the Modern Metropolis the application of network science related indices in this specific direction. Additionally, Batty (2013), in The New Science of Cities, related his vast experience in urban models and complexity research. Most recently, Barthelemy (2018) announced his new book Morphogenesis of Spatial Networks, which not only introduces some basic concepts of complex network theory, but also introduces the Operations Research based idea “optimization” to the complex network theory. These works have made significant contributions to the art of the complex network research in modern urban science. Hence, we believe that the complex network theory offers huge potential for urban studies. As a favourable practice, many network analyses based studies have been conducted to help urban and transportation planners to examine the structures and functions of the urban traffic networks (Chan 2007; Domenech 2009; Erath et al.2009; Gao et al.2006; Holme 2003; Scheurer et al.2008; Sen et al.2003). With an understanding of urban traffic networks in the context of network science, we can better understand the reasons for methodical urban form variation and then determine the potential parts of the future development (Ding et al.2015), while determining static and dynamic structural characteristics can provide relevant references to the urban planning, design, optimization, and sustainable development and maintenance (Barthelemy and Flammini 2008, 2009; Batty 2007, 2012, 2013; Boccaletti et al.2014; Boccaletti et al.2006; Morris and Barthelemy 2012). The history and basic indicators and functions of such research have been reviewed by many researchers (Barthelemy 2011; Derrible and Kennedy 2011; Ducruet and Beauguitte 2014; Gao et al.2006; Xie and Levinson 2009a). Also discussed was the origin of graph theory, illustrating that after Euler opened the door of network science, network science inherited and comprehensively combined with the characteristics of the Scientific Management, System Science, Cybernetics, Information Technology, Behavioural Science, Economics, Operations Research, and other disciplines. Based on these accomplishments, researchers have begun work on the ultimate goals of this subject, which are the optimization of the network structure and maximizing network performance. However, certain novel applications and new directions have been generated, and some of these reviews did not consider the network growth and evolution processes, which are critical in urban traffic network planning research; further, the network optimization methods are not considered. So, on the basis of these reviews, we aimed to fill these gaps, complement the above-mentioned contents and complex network knowledge and classifications, and cover their applications in urban traffic network studies in detail. With the perfection of our efforts, this novel concept in urban planning may be more easily and widely accepted and used by urban planners, designers, and other related scholars. In section 2, inheriting and carrying forward these review papers, we reviewed these novel applications of complex network theory in urban traffic network studies in several directions, including network representation methods, topological and geographical indicators and their applications, the mining of the urban traffic network communities, the network robustness and vulnerability and their applications, big-data-based research, the optimization (both structural and flow related optimization), the co-evolution research and the multilayer network theory with their applications. In the conclusion, we illustrate the weak points and potential paths for the further research.",57
19.0,4.0,Networks and Spatial Economics,02 May 2019,https://link.springer.com/article/10.1007/s11067-019-09468-3,Moving Towards a More Accurate Level of Inspection Against Fare Evasion in Proof-of-Payment Transit Systems,December 2019,Benedetto Barabino,Sara Salis,,Male,Female,Unknown,Mix,,
20.0,1.0,Networks and Spatial Economics,15 May 2019,https://link.springer.com/article/10.1007/s11067-019-09465-6,Impact of Weather Conditions and Built Environment on Public Bikesharing Trips in Beijing,March 2020,Pengfei Lin,Jiancheng Weng,Siyong Ma,Unknown,Unknown,Unknown,Unknown,,
20.0,1.0,Networks and Spatial Economics,09 May 2019,https://link.springer.com/article/10.1007/s11067-019-09464-7,Optimal Guidance Algorithms for Parking Search with Reservations,March 2020,Michael W. Levin,Stephen D. Boyles,,Male,Male,Unknown,Male,"Most drivers now have access to smartphones with routing software (such as Google Maps) while driving. Such software tools, while providing congestion-aware guidance to the destination, do not usually provide information about parking. After arriving at the destination, drivers often have to circle nearby roads to find on-street parking or parking garages with open spaces. This searching behavior is more prevalent in urban areas, where parking availability is limited. Previous studies have estimated that searching for parking caused 34% of congestion in central urban areas (Shoup 1997) and up to 40% of travel time (Axhausen et al. 1994) during peak hours. To avoid searching for parking, several researchers have developed prototype Internet-based systems to reserve parking (Inaba et al. 2001; Sun et al. 2003; Yan et al. 2011b). Reserving parking at parking garages is relatively simple, as many garages already deny entrance to vehicles when spaces are unavailable. Previous work on parking reservation systems has developed communication and implementation protocols (Yan et al. 2011a; Wang and He 2011; Qing-Feng and Qing-Gang 2011), with some analyses on optimal system policies (Kaspi et al. 2014; Kaspi et al. 2016). Besides the reliability benefits to drivers, congestion caused by searching for parking would also be reduced. Parking garages also benefit from reservations through more reliable revenue. Although drivers could choose to reserve parking before departing, early reservations are not without disadvantages. Holding a space in reserve either has specific time window constraints or requires a monetary fee. Tsai and Chu (2012) assumed that spaces could only be held reserved while unoccupied for 15 minutes. Inaba et al. (2001) required that drivers arriving earlier or departing later than their reserved parking time window pay a premium fee. Due to the inherent stochasticity in travel and activity times, specifying the exact times that a parking space will be used is difficult for drivers. Therefore, we make the assumption of Qing-Feng and Qing-Gang (2011): drivers pay a fee per unit time a space is held in reserve. This system is favorable to parking garage operators, who will receive revenue whether a parking space is occupied, or unoccupied but reserved. Under this pricing system, drivers have monetary incentives to reserve parking as late as possible (or not at all). For instance, a driver making a two-hour trip may not wish to pay for two hours of parking, and may find it sufficient to reserve parking five minutes before arriving at the destination. However, reserving parking too late in the trip could necessitate time spent cruising for parking if parking availability is low. We seek to augment existing smartphone routing software with guidance to and integration with parking reservation systems. Drivers would input their actual destination and value-of-time, and the routing software would instruct them on the route to take as well as reserve parking if and when it is optimal to do so. We define the optimal behavior as the route choice and parking behavior that minimizes the expected generalized cost of travel time and parking fees. Our results show that the optimal route choice and reservation behavior may be online or adaptive, i.e. it changes based on whether a parking reservation is held, and how far the driver is from the destination. Our objective is to optimally guide drivers through the parking reservation search process. Due to the complexity in selecting between a variety of parking options, as well as managing the probabilities of obtaining parking at different locations, optimal behavior requires use of computer guidance. Nevertheless, we do not neglect the possibility of drivers visually finding open parking either. We consider two options for finding parking:
 A driver who drives by an empty parking space can choose to park there. Routing software can guide drivers to areas most likely to have open spaces. Drivers can use Internet-based parking reservation systems to reserve parking at any location in the network. Routing software can reserve parking for drivers while optimally guiding them towards parking. Existing software is effective for finding the deterministic shortest path between two points. Because parking availability is stochastic, optimizing guidance to parking requires a stochastic optimization approach to finding the parking search and reservation behavior that minimizes the expected cost (in time and monetary fees). We will show that the optimal location to reserve parking depends on the driver’s location in the network. Therefore, parking search guidance must be integrated with route choice for optimality. A new “shortest path” algorithm is required for individual drivers, and we develop one using a Markov decision process (MDP). The primary contribution of this paper is to model and solve the parking reservation search and route choice behavior problem from a MDP perspective. The solution algorithm provides all-to-one optimal parking reservation search policies. The solution algorithm developed here could be integrated into GPS routing software on smartphones to provide parking search guidance in addition to route guidance. Such guidance could be valuable with or without parking reservation systems. Our results show that the optimal parking reservation search behavior cannot be separated from the route choice. We present examples to demonstrate the potential savings from this algorithm on the downtown Austin city network. These examples show that reserving parking during travel is optimal for many locations, and that reserving parking reduces the expected cost compared to not reserving parking. Also, route choice is affected by whether a reservation is held. Therefore, using guidance software for combined route choice optimal parking search behavior could be helpful to reduce travel times for individual drivers as well as network congestion due to searching for parking. The remainder of this paper is organized as follows. Section 2 describes previous work on modeling parking search behavior and parking reservation systems. Section 3 describes our assumptions about parking behavior, and Section 4 formally defines the traffic network. In Section 5, we develop a dynamic programming algorithm to determine the optimal parking reservation behavior. Section 7 demonstrates the utility of this algorithm, and how the optimal parking behavior depends on the driver’s location, on a city network. We discuss conclusions in Section 8.",12
20.0,1.0,Networks and Spatial Economics,30 May 2019,https://link.springer.com/article/10.1007/s11067-019-09470-9,Micro and Macro Resilience Measures of an Economic Crisis,March 2020,Cristina Bernini,Maria Francesca Cracolici,Peter Nijkamp,Female,Female,Male,Mix,,
20.0,1.0,Networks and Spatial Economics,11 July 2019,https://link.springer.com/article/10.1007/s11067-019-09472-7,A Kernel Search Matheuristic to Solve The Discrete Leader-Follower Location Problem,March 2020,Dolores R. Santos-Peñate,Clara M. Campos-Rodríguez,José A. Moreno-Pérez,Female,Female,Male,Mix,,
20.0,1.0,Networks and Spatial Economics,23 July 2019,https://link.springer.com/article/10.1007/s11067-019-09469-2,Optimal Deployment of Electric Bicycle Sharing Stations: Model Formulation and Solution Technique,March 2020,Zhiwei Chen,Yucong Hu,Xing Wu,Unknown,Unknown,,Mix,,
20.0,1.0,Networks and Spatial Economics,03 August 2019,https://link.springer.com/article/10.1007/s11067-019-09471-8,Optimizing Traffic System Performance with Environmental Constraints: Tolls and/or Additional Delays,March 2020,Xin Lin,Chris M. J. Tampère,Stef Proost,,,Male,Mix,,
20.0,1.0,Networks and Spatial Economics,08 August 2019,https://link.springer.com/article/10.1007/s11067-019-09477-2,Evolution of Regional Innovation with Spatial Knowledge Spillovers: Convergence or Divergence?,March 2020,Jinwen Qiu,Wenjian Liu,Ning Ning,Unknown,Unknown,,Mix,,
20.0,1.0,Networks and Spatial Economics,20 August 2019,https://link.springer.com/article/10.1007/s11067-019-09476-3,Discovering the Hidden Community Structure of Public Transportation Networks,March 2020,László Hajdu,András Bóta,Lauren M. Gardner,Male,Male,,Mix,,
20.0,1.0,Networks and Spatial Economics,03 September 2019,https://link.springer.com/article/10.1007/s11067-019-09474-5,Comparison of Bus Network Structures in Face of Urban Dispersion for a Ring-Radial City,March 2020,Hugo Badia,,,Male,Unknown,Unknown,Male,"A transit system is the key transport mode for urban mobility when faced with congestion, pollution, space degradation, or inefficient energy consumption derived from the excessive use of automobiles. However, the development of efficient transit systems—in order to compete with private vehicles—has to face the process of increasing urban sprawl, which generates complex mobility patterns due to changes in urban land use (Sun et al. 2007). This aspect reduces the successful implementation of this transport mode and opens a discussion about how transit networks should be designed to overcome this handicap (Dodson et al. 2011). Focusing on that discussion, this research connects the transit network design problem with the evolution of the urban form. Some authors such as Anas et al. (1998) and Rodrigue et al. (2006) have summarized the evolution of urban forms in three basic phases. At the beginning, a city is highly centralized, with most of activities being concentrated in the central city core. This core is surrounded by residential areas, shaping a centripetal vector of displacements. In a second phase, activities are progressively relocated to areas adjacent to the initial center. The city core is expanded and new trips that do not depend on the initial center appear. Finally, the last stage is a dispersed urban form where new activities are located in external areas far from the city center. Although cities tend toward dispersion, each of them is in a particular stage of this process. Some of them retain a strong center, while at the other extreme, others are completely dispersed with no centrality. Different studies such as Bontje and Burdack (2005), Riguelle et al. (2007), and Lee (2007) show this evolution and disparity of the degree of urban sprawl in European and American cities. There is a connection between urban form and mobility patterns (Aguilera 2005). Therefore, the previous urban sprawl process changes the initial centripetal pattern, where destinations are in the city center, to complex scenarios, where peripheral trips out of the traditional center account for a higher percentage of journeys. This evolution of the mobility pattern evinces that an initial radial scheme for transit networks cannot be a competitive alternative to satisfy the increasing number of peripheral trips. As a consequence, an alternative network design approach is needed if transit systems are to be an attractive choice among transport modes. In the literature, authors like Mees (2000) and Nielsen et al. (2005) compare two main design approaches called, in this paper, the direct-trip-based structure and the transfer-based structure, which is also named hybrid network. In the former, the original radial network remains and additional lines are introduced to connect the new required displacements through direct services. This approach is based on the fact that planners assume that transfers are perceived negatively and therefore seek to limit their number. In this line, some design models of transit networks seek to minimize the number of transfers (Zhao 2006) or restrict the number to a certain maximum (Baaj and Mahmassami 1995). The resultant networks following this approach are diffuse systems consisting of the sum of many lines that operate independently from each other without working as a real network and have a low understandability that makes their usage difficult. For that reason, the previous authors (Mees 2000; Nielsen et al. 2005) question whether this is the best alternative, especially with the increasing mobility disparity, which emphasizes the weaknesses of this kind of structure. Mees (2000) and Nielsen et al. (2005) defend the transfer-based design approach in a qualitative way. The main characteristic of this structure is a simple scheme composed of a small number of lines adapted to the street pattern. This design makes the operation more efficient and users can take advantage of its high understandability. The main change with regard to the direct-trip-based design approach is the role of transfers. In the transfer-based structure, transfers are an essential step in completing most trips. The direct-trip-based structure has been the most widely used in cities, but some of these cities (Vancouver, Copenhagen, Stockholm, Barcelona, etc.) have completely or partially redesigned their bus networks using a transfer-based structure in the last decades. Reviewing the literature on transit network design, we find recent analytical models that are suitable tools for the design of transfer-based structures. Daganzo (2010) proposed a hybrid structure composed of a central grid in the city core and a hub-and-spoke scheme in the periphery. By means of this model, we can define the general layout of a simple bus network that is adjusted to a grid street pattern. One example of this is Estrada et al. (2011), where the model was used for the design of a new bus network in Barcelona. On the other hand, Badia et al. (2014) extended the application of the hybrid scheme on a ring-radial street pattern, and Chen et al. (2015) compared the design of the hybrid structure for both street layouts. However, these contributions do not discuss the advantages and weaknesses of the design approach behind the hybrid network, that is, a transfer-based structure, in comparison to the direct-trip-based strategy. Therefore, a quantitative comparison is lacking. Authors like Thompson (1977) and Newell (1979) compared alternative design approaches in extreme scenarios of complete dispersion or high concentration. The former defended transfer-based structures due to the increasing urban sprawl, while the latter considered that the most suitable solution was still a network focused on the traditional center since that center remained as the main focus of demand. Recently, more complete formulations have been developed in Badia et al. (2016) and Fielbaum et al. (2016) to answer the same question. Although they work with different methodologies, their final objective is the same: to create a tool to help make strategic decisions about the most efficient network structure depending on the evolution of the urban form. These last two papers (Badia et al. 2016; Fielbaum et al. 2016) work with general outlines of the urban form where different degrees of demand dispersion are represented. For their simplified cities, different network structures are compared to determine the most suitable solution in a city with a specific degree of dispersion. On the one hand, Fielbaum et al. (2016) use a hierarchical description of the city by means of a graph. This description is introduced in Fielbaum et al. (2017). How the demand is distributed among the different nodes defines the level of decentralization. On the other hand, Badia et al. (2016) work with a city as a continuous area where the size of a central attractant zone, which is the zone where all the destinations are located, determines the mobility pattern for a specific degree of dispersion. With this city representation, the model includes the access cost in the comparison of network structures, which is omitted in Fielbaum et al. (2016). Focusing on Badia et al. (2016), the design of network structures is adapted to a specific street pattern, namely a grid. This factor opens the question of what happens if that street layout changes. To answer this question, in the current paper, a comparison among network structures is made considering a circular city characterized by a ring-radial street pattern. This is the other regular pattern widely found in urban areas (Dickinson 1961; Lynch 1962) and studied in transit networks (Vaughan 1986; Badia et al. 2014; Chen et al. 2015; Chen and Nie 2018). In order to make this alternative comparison, we adapt the pre-existing models to describe the operation of the different structures on that ring-radial street pattern: radial, direct-trip-based and transfer-based structures. For this last network, the starting model is the ring-radial hybrid model presented in Badia et al. (2014), although the original model is modified to represent different scenarios of demand dispersion. Following the same approach, new formulations are derived for the other two structures. In this way, the formulations presented in this paper extend the applicability of this tool at the strategic level of bus system planning to most regular cities. At the same time, to determine the effects of the street layout, the new results are matched with the previous results obtained on a grid. Finally, we consider the effects on the applicability of these networks if we include a constraint on the service headway, a factor that is not examined in Badia et al. (2016). The exposition of this paper is as follows. Section 2 presents the analytical model used for the transit network design. The results of the comparison among network structures are compiled in Section 3, where the area of applicability of each structure is identified. Then, Section 4 compares the behaviors in the two street patterns: ring-radial and grid. Finally, Section 5 summarizes the most important conclusions and Section 6 proposes future research lines.",11
20.0,1.0,Networks and Spatial Economics,04 September 2019,https://link.springer.com/article/10.1007/s11067-019-09480-7,Airport Road Access at Planet Scale using Population Grid and Openstreetmap,March 2020,Xiaoqian Sun,Sebastian Wandelt,Mark Hansen,Unknown,Male,Male,Male,"The improvement of air transportation systems, a global challenge for economical growth as well as improving the quality of society, faces several demands and challenges: Increasing safety, resilience, and efficiency of operations, while reducing delay and travel times (Balakrishnan et al. 2016; Cook et al. 2015; Wandelt and Sun 2015; Verma et al. 2014). Meeting these high demands is critical, since air transportation already suffers from significant congestion and delays nowadays (Belkoura et al. 2016; Fleurquin et al. 2013), as well as hard criticisms on its negative environmental impacts by the society (Wolfe et al. 2014; Sun et al. 2017c), especially noises and emissions which are not just in the vicinity of the airports (Forsyth 2007), but also an environmental problem globally. Unbalanced utilization among multiple airports in a region could cause congestion problems, for instance, in the Jing-Jin-Ji metropolitan area in China, Beijing Capital International Airport (PEK) is often operating over-capacity; while for another airport which is 150 km away, Tianjin Binhai International Airport (TSN), only around 40% of its capacity is used. One major challenge for improving air transportation is to increase the road access of airports to the population (Sellner and Nagl 2010; O’Connor and Fuellhart 2016). The notion of access plays a critical role in understanding air transportation systems (Hansen 1995; Grubesic and Zook 2007; Allroggen et al. 2015; Wei and Grubesic 2015; Román and Martín 2011; Redondi et al. 2011), maritime transportation (Guo and Yang 2018; Canestrelli et al. 2017), railway sytems (Jiao et al. 2014, 2016; Wang et al. 2009, 2015), logistics (Malighetti et al. 2018; Holl and Mariotti 2017), commuting patterns (Garcia et al. 2018; Reggiani et al. 2011a), resilience of urban areas (Östh et al. 2018a, 2018b; Geurs et al. 2015; Reggiani et al. 2015), cities in digital economies (Tranos et al. 2013), job markets (Reggiani et al. 2011b), and regional land-use and transport planning (Büttner et al. 2018). The evaluation of access needs to consider different regional contexts for various economic activities (Reynolds-Feighan and McLay 2006). Existing studies on airport access, however, are often spatially constrained, mainly for regions inside US (Hansen 1995; Grubesic and Zook 2007; Allroggen et al. 2015; Wei and Grubesic 2015; Cho et al. 2015; Fu and Kim 2016; Park and O’Kelly 2016) or Europe (Grimme et al. 2010; Budd et al. 2016; Paliska et al. 2016; Gokasar and Gunay 2017). These studies often rely on data released by national authorities, such as the Bureau of Transportation Statistics or Eurocontrol, or data for very specific regions. The latter type of studies often includes a very detailed access analysis for few airports, often inside a so-called multiple-airport region only, where airports are highly competitive (Hansen and Du 1993). Note that there have been a few studies with global in scope. Particularly, Grubesic et al. (2008) analyzed hierarchical airside connectivity between 10,935 airports based on the Innovata’s Schedules Reference Service, with the average number of flights per week and the average number of seats per week between airport pairs as the weighting factor. They defined nodal regions of airports, where airports build hierarchies and the position of an airport indicated its access to the global network. They focused on airport airside connectivity, i.e., people located near these high tier airports can travel easier in the global airline network. However, the airport road access is not covered in their study. Moreover, as pointed out by Grubesic et al. (2008), “analyses of area subset (continents or regions) fail to account for city connections outside the bounded space” and “the worldwide scale of analysis provides unique insight that country-specific analyses are simply unable to capture”. Thus, it is a consensus that there are certain limits regarding the analysis of a sub-component of a larger system. In studies at a larger scale, for instance, at country level, the access of airports from the city center is usually analyzed, neglecting that the population of a city is not located at a single point, particularly given the increasing number of megacities with hotspots of higher population densities. Note that many local originating passengers, especially these business and government related travelers, may not necessarily start their trips from residential places, instead they might start their trips from workplaces which are located in the city center. Moreover, many visiting passengers would also travel to airports from the city center. Matisziw and Grubesic (2010) propose a new index for representing accessibility that takes into account a variety of factors, using driving (network-based) distances from Tracts to airports derived from a highway network. Their model can evaluate any measure of transportation cost, albeit distance or travel time. They found that levels of access to commercial airports in the US vary widely: while many rural locations are unable to support commercial airports; many urban locations have relatively good access to more than one airport. The resolution used by Matisziw and Grubesic (2010), the US census tract polygons, is indeed different than in other studies (city centers). Nevertheless, their methodology relies on line-by-sight distance from the census tract center, not estimated travel time. Obviously, in presence of highways and other transportation modes, the estimated travel time does not correlate strongly with line-by-sight distance (Buczkowska et al. 2019). Therefore, this study proposes to use a set of grid cells with a finer resolution of approximately 1 km at the equator, to estimate the travel time between grid cells and airports, by exploiting the Open Source Routing Machine with Openstreetmap for worldwide routing given real road infrastructure. This study contributes a more accurate estimation of (road) access to airports, not only in the US, but comparably among multiple continents at a worldwide scale. Previous studies often used passenger survey data to measure the access distance or access time to airports. This study aims to derive one road access index for worldwide airports, based on a set of freely available datasets. Most notably, the Gridded Population of the World (GPW) (Ciesin 2016; Lloyd et al. 2017) is used to derive population statistics at a fine-grained granularity, with cells of at most one square kilometer (around the equator). A set of grid cells (around 448 million) is obtained, for which the closest road travel distance is computed. The Open Source Routing Machine (Luxen and Vetter 2011) with Openstreetmap for worldwide routing between airports and grid cells is exploited. As a result, a fine-grained, comprehensive airport road access framework for all airports in the world is provided. Moreover, given that the methodology is the same for all regions, the results among different regions in the world can be easily and consistently compared. The remainder of this paper is structured as follows. The relevant literature on airport road access and airport competition is reviewed in Section 2. Section 3 introduces the main methodology for estimating airport road access at planet-scale. The results are presented in Section 4. This study is concluded with discussion and suggestions for future work in Section 5.",8
20.0,1.0,Networks and Spatial Economics,05 September 2019,https://link.springer.com/article/10.1007/s11067-019-09483-4,Reliable p-Hub Network Design under Multiple Disruptions,March 2020,Pouya Barahimi,Hector A. Vergara,,Unknown,Male,Unknown,Male,"Starting with O’Kelly (1986), numerous studies from different disciplines have addressed hub location and the design of hub networks with the objective of efficiently routing flows between many origins and destinations. The goal of the hub network design (HND) problem is to locate a number of hub nodes in a network to serve as transshipment, consolidation, or sorting points for flows between different origins and destinations using a reduced number of links. Non-hub nodes (also referred to as spokes in the literature) are assigned to hubs to complete the structure of the network, and the flows between origin-destination (O-D) nodes (which might or might not be hubs) are required to visit at least one hub to take advantage of economies of scale. Other basic assumptions have been made in the literature of the classic HND problem such as allowing flows to visit up to two hubs between origin and destination, having a complete network for inter-hub movements, a fixed discounted cost for inter-hub flows, infinite capacity at facilities and links, and either assigning a single hub or multiple hubs to each non-hub node (Campbell and O’Kelly 2012). Applications of hub networks are commonly found in telecommunications (Klincewicz 1998) and the transportation and logistics industries such as air transportation, postal and express package carriers, less-than-truckload freight carriers, and rapid transit systems (Contreras 2015). For extensive reviews of applications of hub-and-spoke networks, see Alumur and Kara (2008), Campbell and O’Kelly (2012), Contreras (2015), and Farahani et al. (2013). Regardless of the application, HND problems are a challenging class of optimization problems. The integration of interrelated decisions at two levels of decision making (i.e., hub location and network design/link selection/routing) is one of the main difficulties associated with these problems. They are generally formulated as mixed integer programming models and solved using sophisticated solution algorithms, especially for large-scale instances (Contreras 2015). However, there is a sense that most existing HND models do not really incorporate many real world elements as they are found in practice, and they rely heavily on the simplifying assumptions developed for the classic versions of the problem (Campbell and O’Kelly 2012). Topics that are now starting to being studied include different hub network topologies, flow dependent discounted costs, capacitated models, models with uncertainty, and dynamic models among others (Contreras 2015). In particular, most previous studies in the HND literature assume that all entities (e.g., nodes and links) within the network operate constantly at full capacity. In other words, the chance of inefficiencies or failures of an entity is completely ignored. However, due to either natural disasters or intentional disruptions, the efficiency of a network can drastically decline imposing irreversible loss in terms of financial costs, customers’ distrust, and opportunity costs. For instance, labor actions, changes of ownership, terrorist attacks (Snyder and Daskin 2005), and adverse weather around airports (An et al. 2015) or seaports (Kim and Ryerson 2013) are instances in which one or more parts of the hub-and-spoke network may fail. To address these instances, some recent research studies have focused on the reliability aspect of hub networks. The reliable hub network design (RHND) problem seeks to minimize the expected operating cost of a hub network while considering the failure probability of network elements (usually nodes). In this paper, a mathematical model for the reliable p-hub network design problem under multiple disruptions (RpHND-MD) is proposed which is able to incorporate additional practical elements that have not been considered in existing models in the literature. The major contribution of this study is to propose a method to solve the RHND problem under multiple disruptions of hubs when hubs have different failure probabilities and spokes may be assigned to multiple hubs. Considering a non-identical failure probability distribution for the hub nodes complicates the modeling process as it hinders formulating a linear programming mathematical model. The rest of this paper is organized as follows. Previous studies focusing on the RHND problem are reviewed in Section 2. Then, the mathematical formulation for RpHND-MD and the proposed solution procedure are presented in Section 3. Computational results for different instances are presented in Section 4. And finally, Section 5 shows relevant conclusions along with areas for future research.",1
20.0,2.0,Networks and Spatial Economics,09 September 2019,https://link.springer.com/article/10.1007/s11067-019-09479-0,Path-Based Dynamic User Equilibrium Model with Applications to Strategic Transportation Planning,June 2020,Babak Javani,Abbas Babazadeh,,Male,Male,Unknown,Male,"Intelligent transportation systems (ITS) are advanced applications that are mainly used for alleviating the problems associated with traffic congestion. Advanced traveler information systems (ATIS) are typical ITS applications, which provide the travelers and the traffic control system operators with information in order to enhance the safety and performance of roadway facilities. A basic requirement for evaluating ATIS is to make use of a dynamic traffic assignment (DTA) modelling approach. Examples include traffic message signs, traffic incident management, and in-vehicle route guidance systems. Given the time-varying travel demands, a DTA model captures the dynamic flow characteristics, and thereby propagates the travel demands more realistically than the conventional static traffic assignment (STA) models. This implies that the DTA model can be used to assess not only ATIS but also dynamic traffic demand management (TDM) strategies, like dynamic congestion pricing, provided that the required data are available. In this paper, a novel analytical DTA model together with an efficient path-based DTA algorithm are proposed. The model is formulated as a mathematical programming (MP) problem that makes use of differentiable link performance functions. So, the existing powerful non-linear optimization methods suit the model fine. Furthermore, to produce plausible solutions, a capacity constrained dynamic network loading (DNL) model is included whereby the queuing delays are considered through restricting link flows to the corresponding capacities. One of the special contributions of the paper is that the proposed algorithm is suitable for assessing some ITS plans since it works on the basis of a path data structure. For instance, it can be used to predict travelers response to ITS strategies (e.g. path changing in response to data provided by ATIS) following its ability to process the travel path information. The contribution becomes more apparent when we note that storing path information is burdensome and needs a large amount of random access memory (RAM), especially for large scale networks. In addition, obtaining ideal solutions for large scale networks is very difficult and time consuming, so that there are a few models and algorithms that can be applied to real scale networks (e.g. Boyce et al. 1997; Ziliaskopoulos et al. 2004; Florian et al. 2008; Ben-Akiva et al. 2012 and Han et al. 2019). The proposed DTA model is developed through reformulating the analytical DTA model proposed by Janson (1991a). This is a model based on the dynamic user equilibrium (DUE) condition with an interesting simple structure, where the temporal path-link incidence variables are used to explain the relationship between link and path flows and travel times. Janson (1991b) and Janson and Robles (1995) tried to solve the model: a convergent dynamic assignment algorithm was introduced in the former for the link-node formulation of the model, and a quasi-continuous algorithm was suggested in the latter for its quasi-continuous time representation. Although these algorithms have been applied to real case problems, they are link-based and consequently not well-suited for evaluating ITS strategies. Apart from this, the model is based on the assumption that each link lying on a path can be fully traversed by the path inside a time interval. Therefore, the length of time intervals should be considered much longer than the link travel times, prohibiting the use of adequately small time intervals. This in turn causes abrupt changes in link flows in consecutive time intervals. Besides, there is no constraint in the model to confine flows to the link capacities and so the queuing delays are discarded. In conclusion, the model by Janson (1991a) suffers from significant drawbacks in flow propagation, which stem from its simplifying assumption. Javani et al. (2019) suggested that the model can be refined to some extent by using a link segmentation strategy. This is to divide each link into a number of segments of equal length, so that shorter time intervals can be used, and therefore more accurate dynamic flows are obtained. They also proposed a path-based solution algorithm for their segment-based model which is properly applicable to large-scale instances. However, the problem remaining with this method is that, even after a very fine segmentation is done, there is always a positive probability that a link be traversed in two successive time intervals, violating the Janson’s assumption. In addition to this, the complexity of the algorithm increases as the number of segments grows. This paper makes three modifications to Janson’s model to circumvent its restrictions. First, flow propagation over the time intervals is enhanced by adding a new path-link fraction variable to the model, whereby the flow of a path that is assigned to a link is distributed among the time intervals during which the link is actually traversed by the path (instead of the time interval that the path reaches the link). Following this modification, a smoother path flow transition between the time intervals is obtained. Second, a link segmentation strategy is performed to divide the links into smaller segments, so that we can acquire more realistic dynamic flows and travel times. Third, the capacity constraints are added to the model and the queuing delays are taken into account. This paper also introduces a fast performance path-based DTA algorithm to solve the proposed model for the problems of real-world scale. The algorithm decomposes the problem into origin-destination (OD) pairs and departure time intervals, and utilizes a dynamic column generation technique to generate active paths for each subproblem. To perform the column generation, a dynamic shortest path (SP) finding algorithm is developed which is based on the ordinary shortest path algorithm by Golden (1976). The capacity constraints are applied using the so-called dynamic penalty function method, which is originally proposed by Shahpar et al. (2008) for the side constrained STA problem. The influence of the above modifications to the based model on the flow propagation over time intervals is evaluated through a simple example of a one-link network. Also, the solution properties of the proposed model are investigated using another small network. Furthermore, the numerical results of applying the proposed algorithm to a real life case study of a large scale network are presented in order to compare how quickly the algorithm converges at different time interval lengths. This network is also employed to illustrate the application of the algorithm to assessing some ITS plans.",
20.0,2.0,Networks and Spatial Economics,11 September 2019,https://link.springer.com/article/10.1007/s11067-019-09481-6,The Follower Competitive Location Problem with Comparison-Shopping,June 2020,Vladimir Marianov,H. A. Eiselt,Armin Lüer-Villagra,Male,Unknown,Male,Male,"Facility location researchers and practitioners deal with finding the best possible locations for all kinds of infrastructure. The book by Laporte et al. (2015) offers a comprehensive view of the field while the contributions in Eiselt and Marianov (2015) discuss selected examples of applications. An important area in the field of location theory deals with competitive scenarios. In his seminal paper, Hotelling (1929) studied a market in the shape of a line segment (a so-called “linear market”) with uniformly distributed demand, on which two competing facilities locate and sell the same product. Both firms locate a single facility each, and their decision variables are the location of their facility as well as the mill prices they charge. Customers are assumed to patronize the facility at which they will pay the lowest full price, i.e., the mill price plus the transportation costs, which are assumed to be linear in the distance. The main thrust of Hotelling’s work dealt with the question whether or not there is an equilibrium in the situation he described. A (Nash) equilibrium is defined as a pair of decisions of the competitors, which is stable, i.e., neither competitor can benefit by unilaterally changing his decision. Hotelling’s conclusion was that such equilibrium exists with both firm locating their facilities at the center of the market, a result that was dubbed the principle of minimum differentiation. In the early days following Hotelling’s paper, a number of his followers considered the principle of minimum differentiation the ultimate explanation for the frequently observed clustering or agglomeration of competitive facilities in practice. However, the decades following Hotelling’s paper showed many limitations of the analysis. First, the result was rather fickle similar to the equilibrium a ball is in when located on a plain surface: even a minor change of the slope of the surface will destroy the equilibrium (at least in the absence of friction). Secondly, it turned out that small changes in the assumptions could result in dispersed locations (see, e.g., Chamberlin 1933; Lerner and Singer 1937; Eaton and Lipsey 1975; Okabe and Suzuki 1987; Brown 1989). Thirdly, D’Aspremont et al. (1979) in their paper demonstrated that Hotelling’s conclusion with his own assumptions did not hold and that the original problem did not possess an equilibrium. Another strand of analysis dates back to the work of the economist von Stackelberg (1943). The competitive situations in his work comprise two (classes of) competitors: the one(s) to act first are the leaders, while those that act after the leader has made a decision and this decision has become public knowledge, are the followers. Once decisions have been made, they are irreversible. Note that the scenario is asymmetric: while the leader has to consider the potential decisions of the follower, the follower does not need such foresight; he only takes the situation as given and optimizes his own objective. It is noteworthy that the follower’s problem is a conditional location problem (“optimize your own objective, given that the leader has already located at known sites”), while the leader must include the follower’s reaction in each of his decisions, resulting in a bi-level optimization problem (see, e.g., Aras and Küçükaydın 2017). The first to apply such a leader – follower model to competitive location problems (albeit with fixed and equal prices) are Prescott and Visscher (1977). While these authors still worked on Hotelling’s linear market, Drezner (1982) considered competitive location problems in the plane, while Hakimi (1983) discussed similar problems on networks. Hakimi also coined the expression centroid for the leader’s problem and medianoid for the follower’s problem. Later, ReVelle (1986) solved the follower problem (dubbed the maximum capture or MAXCAP problem) on a network using an integer programming formulation. These contributions started a large body of research dedicated to the subject of competitive location by operations researchers, most of it solving one of the von Stackelberg problems, sometimes considering location and pricing, as in Kress and Pesch (2016). Berglund and Kwon (2014) present a different asymmetric approach, in which a von Stackelberg firm competes with Cournot-Nash firms. For reviews, see, e.g., Eiselt et al. (2015), Kress and Pesch (2012) for problems on networks; and Drezner (2014) for problems in the plane. Eiselt (2011) as well as Marianov and Eiselt (2016) analyze competitive location and agglomeration results from the point of view of location researchers. One of the key features in any competitive location problem concerns customer behavior. While Hotelling assumed that customers would purchase the good from the source with the lowest full price, most of his successors, who did not include price competition in their models, reduced this assumption to its simplified version, in which customers purchase from the source closest to them. A number of authors have made different assumptions, which essentially fall into two categories. First, there are those authors who have dropped the “individual trip” assumption made by almost all researchers in location analysis. In other words, consumers are not necessarily assumed to make special trips for each product they attempt to purchase. This bundling is typically referred to as multipurpose shopping or, as the case may be, multi-stop shopping. Dellaert et al. (1998) provide some insight into multipurpose shopping behavior from a conceptual point of view, while Hodgson (1990) introduced the concept of flow capturing (or flow interception). In it, customers no longer choose the facility closest to their respective home locations, but a facility closest to a trip they are on anyway, e.g., the daily trip to work. In other words, the relevant customer - facility distances are no longer point-to-point distances, but path-to-point distances. This concept applies when customers handle drop-offs (as in the case of childcare facilities) or pickups (such as gas fill-ups) along the way between home and work. Lately, there has been a revival of this idea when locating alternative fuel stations, as in Miralinaghi et al. (2017). This idea can be seen as a limited version of location-routing problems, see, e.g., Nagy and Salhi (2007). Marianov et al. (2018) investigate the effects of multipurpose shopping on store location. Another aspect of customer behavior concerns information gathering. In the case of retail location models, this would include internet searches, flyers, ads in media, and information collection by visiting the stores. Most authors in this subfield consider in price search. Contributions such as those by Guo and Lai (2014) include not only internet search, but also internet purchases in their model. The present paper belongs into this category, as we deal with customers comparing products in stores before making any purchase, i.e., comparison-shopping. More specifically, we assume a customer behavior that includes two levels of comparisons: in the first pre-trip planning stage, customers compare observable primary features of the products in which they are interested. This will include price as well as specifications that are quantifiable and typically published, such as the size of an item, its weight, its primary product features, etc. Given the results of the research, the consumer will plan a trip. Then, in the second during-trip stage, customers will examine the secondary product features (such as color, flavor, and specific fit in the case of clothing) in detail and make their decision to purchase or not to purchase accordingly. This behavior will be described in detail in the second section of this paper. The economic literature has addressed multipurpose and comparison-shopping, as in Eaton and Lipsey (1975, 1979, 1982), McLafferty and Ghosh (1987), Ghosh and McLafferty (1984), Arentze et al. (2005), Mulligan (1987), O’Kelly (1981, 1983), Thill (1982), and Wolinsky (1983). However, to the best of our knowledge comparison-shopping has never been dealt with in prescriptive facility location models see, e.g., Santos-Peñate et al. (2019) and Pelegrín et al. (2018). Our contribution to the literature consists in including, for the first time, comparison-shopping in a competitive facility location problem. We propose a model that solves the follower problem for a market-share-maximizing firm locating one or more stores in the presence of existing competitor’s stores. The follower problem has been addressed in the literature, and it has an importance by itself: it requires being solved by a new entrant in a market in which one or more incumbents are already present. It is also a sub-problem of the leader problem. Since comparison-shopping is a consumer behavior that has never been considered before in this context, it seems reasonable to solve this simpler problem first, to observe the effects of comparison-shopping without interference by other issues. We use a duopoly for the sake of simplicity and clean analysis. If more chains are involved, there could be effects related to e.g. the market size and border effects, which become undistinguishable from the effects of comparison-shopping alone. Our analysis compares the locations and demand captures that result from a model with comparison-shopping to those with single- purpose shopping trips. Using the simplest possible setting, we show that comparison-shopping results in a larger market, more frequent co-location, and stronger agglomeration of competing stores. There are similarities between the multipurpose shopping problem (Marianov et al. 2018) and the comparison-shopping problem investigated here. The first similarity is that both are in essence bi-level problems that we formulate at once as one-level optimization problems. Secondly, in both cases the stores of different firms tend to be located close to each other. However, there are also significant differences, which are shown in Table 1. In summary, the multipurpose and comparison shopping models have common properties, but there are significant differences in the concepts behind them. The main contributions of both papers are different. In both cases, though, the inclusion of more refined assumptions about customer behavior result in better representations of reality. The remainder of the paper is organized as follows. In Section 2, we describe the problem. Section 3 develops the model. Section 4 contains the computational experience, and Section 5 presents conclusions and future extensions.",11
20.0,2.0,Networks and Spatial Economics,10 October 2019,https://link.springer.com/article/10.1007/s11067-019-09478-1,Bounding the Inefficiency of the Reliability-Based Continuous Network Design Problem Under Cost Recovery,June 2020,Anny B. Wang,W. Y. Szeto,,Female,Unknown,Unknown,Female,"The price of anarchy (PoA), which was first termed by Koutsoupias and Papadimitriou (1999), measures the inefficiency of the traffic assignment problem. It reveals how much the system performance measure would exceed its theoretical minimum value when travelers choose routes selfishly. The PoA for traffic assignment problems has received great research attention. Four major lines of research have arisen (Roughgarden and Tardos 2002; Chau and Sim 2003; Correa et al. 2004; Roughgarden 2005; Xiao et al. 2007; Han and Yang 2008; Han et al. 2008; Guo et al. 2010; Huang et al. 2011; Wang et al. 2014; Szeto and Wang 2015), which are based on four considerations: arc capacity constraints; demand and link travel time/cost functions; road pricing; and extensions of traditional user equilibrium principles and multiple user classes. The PoA for traffic assignment problems is well understood by scholars. However, the PoA for other problems, e.g., network design problems (NDPs), has rarely been studied. The NDPs have broad definitions (Farahani et al. 2013). The most popular family of NDPs in the literature is the family of capacity expansion NDPs (Abdulaal and LeBlanc 1979; Dantzig et al. 1979; LeBlanc and Boyce 1986; Ben-Ayed et al. 1988; Friesz et al. 1993; Yang 1997; Yang and Bell 1998; Meng and Yang 2002; Chiou 2005; Szeto and Lo 2005; Szeto et al. 2010; Szeto et al. 2014), which optimizes the system performance measures of the road networks by determining the optimal capacity expansions (i.e., the additional capacities added to existing roads and/or the capacities of new roads) and the flow pattern (i.e., the traffic flow distribution in the road network). Some of these NDPs are also known as user equilibrium network design problems (UE-NDPs) because they capture the selfish routing behavior of travelers, which means that the flow pattern must satisfy the user equilibrium (UE) constraints. These NDPs also have one common feature—they assume that the travel demands and link capacities are deterministic. In reality, there are uncertainties in the travel demands and road supplies due to day-to-day travel demand fluctuation, special events, bad weather, road accidents, road construction activities, etc. The demand and supply uncertainties lead to system travel time and path travel time variations, which cannot be ignored by the system manager and travelers. The reliability-based user equilibrium network design problems (RUE-NDPs) are developed based on the deterministic UE-NDPs by considering demand uncertainty and/or supply uncertainty. Chen et al. (2011) conducted a detailed review of the family of RUE-NDPs (Chootinan et al. 2005; Chen et al. 2007; Ng and Waller 2009; Sumalee et al. 2009; Yin et al. 2009; Chow and Regan 2011; Szeto and Wang 2016). Most existing studies focus on the modeling, solution methods, and applications of the capacity expansion RUE-NDPs. However, the PoA for the capacity expansion RUE-NDPs, which is an important indicator for evaluating how much the design objective function value exceeds its theoretical minimum value when travelers chose routes selfishly, has rarely been studied. Szeto and Wang (2015) proposed the PoA for a capacity expansion RUE-NDP. Their study was the first attempt in the literature to examine the inefficiency of transport NDPs with capacity expansions. Szeto and Wang (2015) illustrated that the PoA for their proposed RUE-NDP reveals how much the system performance measure may exceed its corresponding theoretical minimum value due to the inefficient allocation of system resources (i.e., capacity expansions) and traffic flow, the latter of which is caused by the selfish routing behavior of travelers. They proved that the PoA has an upper bound, indicating that the inefficiency of the resource allocation of the network design is bounded above. The study of Szeto and Wang (2015) is far from complete. Firstly, they only considered one member of the capacity expansion RUE-NDP family. Their proposed PoA may not reflect the inefficiencies of resource allocations of the other RUE-NDPs that have different design objectives, decision variables, and constraints. Secondly, their study implicitly assumed that the RUE flow pattern is unique given the capacity expansions. Thirdly, most RUE-NDPs assume that the project cost does not exceed the available budget. However, the project cost can also be fully recovered by charging congestion tolls upon the travelers (Yang and Meng 2002; Lo and Szeto 2009). For RUE-NDPs that consider toll charges, the PoAs proposed by Szeto and Wang (2015) are not suitable. Thus, a general definition of the PoA for capacity expansion RUE-NDPs is required. This study expresses the family of capacity expansion RUE-NDPs in a generalized model formulation and proposes a general definition of the PoA for the capacity expansion RUE-NDPs. This study then considers a specific problem, which is a capacity expansion RUE-NDP under cost-recovery that considers supply uncertainty and road tolls. The problem is formulated as a min-max problem. The min-level problem aims to minimize the largest total system travel cost budget (TSTCB) plus the project cost. The TSTCB is a variant of the total system travel time budget and consists of the monetary cost of mean total system travel time and an extra cost associated with system travel time reliability. The max-level problem aims to determine the worst-case flow pattern that gives the largest TSTCB plus the project cost. The self-routing behavior and risk attitudes of travelers are captured by the reliability-based user equilibrium (RUE) constraints. In addition, travelers are charged with congestion tolls, which are used to recover the project cost. To guarantee that the project is self-financing or even profitable, a cost recovery constraint is incorporated. Based on the proposed model, this study proposes a novel approach to derive the analytical formula for an upper bound of the PoA. The contributions of this study are as follows: We propose a general definition of the PoA for capacity expansion RUE-NDPs to measure the inefficiency of the reliability-based transport NDPs with capacity expansion and cost recovery; We propose a new NDP, namely capacity expansion RUE-NDP under cost recovery, in which the project cost is fully recovered by charging travelers with congestion tolls. It is formulated by a min-max approach; and It derives an analytical bound of the PoA of the proposed capacity expansion RUE-NDP under cost recovery. The key findings regarding the upper bound of the PoA for the proposed RUE-NDP include the following: The upper bound depends on the travel time variations, the value of travel time, the value of reliability for system travel time, and the value of reliability for path travel time; The upper bound is independent of travel time functions, demands, and network topology; and The upper bound equals one if there are no travel time variations or/and the system manager and travelers are both risk-neutral, indicating that the PoA also equals one. This paper is organized as follows. In Section 2, we express the family of capacity expansion RUE-NDPs in a generalized model formulation and propose a general definition of the PoA for the capacity expansion RUE-NDPs. In Section 3, we describe our new problem. In Section 4, we examine the PoA for the studied problem and evaluate its upper bound. In Section 5, we provide a concluding remark and discuss the future research directions.",1
20.0,2.0,Networks and Spatial Economics,22 November 2019,https://link.springer.com/article/10.1007/s11067-019-09482-5,Disruptions in Spatial Networks: a Comparative Study of Major Shocks Affecting Ports and Shipping Patterns,June 2020,Laure Rousset,César Ducruet,,Female,Male,Unknown,Mix,,
20.0,2.0,Networks and Spatial Economics,27 November 2019,https://link.springer.com/article/10.1007/s11067-019-09484-3,Location-Price Competition with Delivered Pricing and Elastic Demand,June 2020,Phillip J. Lederer,,,Male,Unknown,Unknown,Male,"In the past, implementing delivered pricing (especially in attribute space) has been described as unrealistic because of information and practical difficulties in distinguishing between customers, determining an individual’s willingness to pay, and setting different prices to individuals. The rise of e-commerce has introduced the real possibility of third-degree price discrimination. Associated new information technologies have enabled more precise identification of customer segments, willingness to pay and competitors’ offerings/prices allowing competitive one-to-one pricing. The competitive consequences of the ability to price and locate under this regime was first studied by Lederer and Hurter (1986) but under the restricted condition of inelastic customer demand. This paper extends the literature by studying this problem under an important but unresearched situation of price elastic demand. Duopoly competition is modeled as a non-cooperative two staged game of location and delivered pricing. Lederer and Hurter showed that a Nash equilibrium always exists with inelastic demand but we show that it may not exist when demand is price elastic. A major focus of this paper is the study of existence and lack of existence of non-cooperative equilibria. Two equilibrium conditions are studied, one where the ordering of the firms on the line is fixed (an “ordered equilibrium”), and the other where the ordering is not fixed: one firm may leapfrog the other (“a Nash equilibrium”). Examples demonstrate that generally a Nash or even an ordered location equilibrium may not exist. In a Nash equilibrium, leapfrogging causes a serious technical problem, namely that there is a discontinuity in profit with respect to location. The lack of continuity causes the failure of all known fixed point theorems to assure the existence of a Nash equilibrium. A major contribution is a set of sufficient conditions that guarantee a Nash equilibrium. Given existence, equilibrium locations demonstrate properties quite unlike the inelastic case, for example, as transportation cost rises or firms’ production costs rise, each firm seeks to locate closer to its competitor. Also the interval spanning firms’ locations in equilibrium always contains a social welfare optimum pair. (In the inelastic case, a social welfare optimum is a location equilibrium!) Extensions present sufficient conditions guaranteeing existence of a Nash equilibrium for duopoly with cost asymmetric firms and sufficient conditions for a Nash n-player equilibrium. My setup follows Hoover (1927) by allowing spatially discriminatory prices where a firm can set a different delivered price for each location. In geographical space delivered pricing can occur when selling firms have lower delivery costs than any potential arbitragers, and in attribute space if customers have ideal points and firms can customize goods for each customer. Lederer and Hurter (1986) showed that a two staged game of competition in locations and delivered prices possesses a Nash equilibrium under perfectly inelastic demand. The underlying reason that an equilibrium exists is that each firm’s profit rises when reducing the total cost of all firms to serve the market. In particular, positioning firms to minimize the global total cost generates a Nash equilibrium. However, with elastic demand a firm’s attempt at profit maximization is not aligned with global cost minimization. This paper presents technical conditions that assure existence. These are general enough to admit any non-uniform customer distributions and logconcave demand functions. This is despite the fact that even under our sufficient conditions a firm’s profit function is not concave, quasiconcave or supermodular in location choices and in the Nash case, discontinuous. The key to existence or an ordered equilibrium is a structural property that causes each firm’s best response correspondence to increase in the other’s location despite supermodularity not holding. Technically speaking, the property is that the profit functions are quasisupermodular (Milgrom and Shannon 1994). In essence, an ordered equilibrium is the solution of a quasisupermodular non-cooperative game. (Topkis 1998). Because there are no general conditions that guarantee quasisupermodularity, the method of proof is somewhat unique. Also significant is that the sufficient conditions overcome the discontinuity in firm profits when proving existence of a Nash equilibrium. The theoretical and empirical literatures have identified many markets exhibiting third-degree price discrimination often referred to as “delivered pricing”. Markets identified include Haddock (1980) for plywood and steel; Lederer (1993) for firms offering transportation services on a network; Eaton and Schmitt (1994) for customized goods in company-to company-markets; Shaffer and Zhang (2002) in one-to-one promotions; Agarwal and Hauswald (2010) for small firm banking loans; and Numan and Willekens (2012) for accounting firms offering auditing services and positioning itself in attribute space of accounting skills. A diverse set of theory papers use Lederer and Hurter’s result such as Soper et al. (1991) discussing industry concentration, Gupta (1992) analyzing sequential entry into markets, Hamilton and Thisse (1993) considering delivered pricing with capacity contraints, Pal and Sarkar (2002) introducing Cournot pricing into a multi firm framework, Stuart (2004) allowing cooperative equilibria into spatial competition, Innes (2008) questioning the effect of technology to customize product for customers, Anderson and Wilson (2008) applying network design and pricing in transportation, Colombo (2010) seeking to understand the effect of taxation on location, Pelegrin et al. (2012) on locating multiple stores, Vogel (2011) on competing heterogeneous firms, and Meagher (2012) using a model to predict product variety. The ability to set customer specific prices has been recognized in the literature. Shaffer and Zhang (2000) cite Lexis-Nexis’ information service which sets different prices to virtually every user. Shiller (2014) demonstrates that consumer willingness to purchase a Netflix streaming subscription can be estimated using browsing history. Mikians et al. (2012) used price search data to study whether e-commerce sites quoted prices based upon customer characteristics observable via browsing history and found evidence of geography based pricing by Staples within Massachusetts and by Amazon for Kindle e-books in Europe. The reader will note that my approach to spatial competition is in contrast to literature following Hotelling’s (1929) seminal paper on competitive f.o.b. (or mill) pricing. d’Aspremont, Gabszewisz and Thisse (1979) extended Hotelling’s work to include competitive location and demonstrated that a competitive price equilibrium exists only if distance transport costs are convex but with the somewhat unnatural result that firms locate at the market’s extreme endpoints. (See Biscaia and Mota 2013 for a recent survey of the Hoteling location-price literature). Classic papers in this literature similarly made restrictions to avoid lack of existence, e.g. Schmalensee (1978), Novshek (1980) and Salop (1979). A significant limitation in this literature is that there are no general results about properties or existence of spatial equilibria in spaces higher than 1-dimension. The rest of the paper has the following structure. Section 2 presents the competitive situation with supporting notation, followed by Section 3 which defines our two solution concepts: the ordered equilibrium and the Nash equilibrium. Examples demonstrate that these equilibria may not exist along with an example having multiple asymmetric ordered and Nash equilibria. Another example shows that a firm’s profit function is not generally concave or quasiconcave. Section 4 shows that the firms’ profit functions are not generally supermodular but two propositions present sufficient conditions for supermodularity to hold. The presence of supermodularity enables proof of existence of an ordered equilibrium. Section 5 is the heart of the paper. We present more general assumptions that enable proof of existence of both ordered and Nash equilibria without requiring supermodularity. Lemmas 2 and 3 together show that the profit functions are quasisupermodular which means that the firms’ best response correspondences are increasing. Proposition 5 then proves the existence of ordered equilibria. Using the same assumptions, existence of a Nash equilibrium is proved in Proposition 6 but that proof requires a complex construction. Section 6 discusses technical issues that arise when the firms’ cost are asymmetric. Given some additional assumptions, existence is again assured. General properties are found, for example, higher transportation rates cause the firms to locate closer together, as do higher marginal production costs. A subsection explores sufficient conditions for the existence for n-firm ordered equilibria. Section 7 studies the location relationship between social welfare optima and the corresponding firm equilibria. First it is shown that the social welfare maximizing correspondences are increasing, and then that an ordered equilibrium is contained within social welfare optimizing locations. Section 7 explores the possibility of extending results by allowing customers to have more general demand functions and for firms to locate in higher dimensional spaces.",5
20.0,2.0,Networks and Spatial Economics,13 December 2019,https://link.springer.com/article/10.1007/s11067-019-09486-1,An Expanded Bipartite Network Projection Algorithm for Measuring Cities’ Connections in Service Firm Networks,June 2020,Miaoxi Zhao,Ben Derudder,Peiqian Zhong,Unknown,Male,Unknown,Male,"Over the last decade, the literature at the crossroads of network science and spatial science has boomed (cf. Evans 2010; Kuby and Church 2010; Ducruet and Beauguitte 2014; Derudder and Neal 2018; Broekel and Bednarz 2018; Neal 2018; Fritz et al. 2019). In this literature, the analysis of urban connectivity through the lens of the location strategies of producer services has become a major topic of research. For example, empirical research into the geographies of ‘world’ or ‘global’ cities has, to a substantial degree, zoomed in on the network connectivity of these cities in the globalized office networks of leading producer service firms (cf. Sassen 2001; Derudder et al. 2013; Neal 2014a; Hennemann and Derudder 2014; Taylor and Derudder 2016). Research in this spirit now extends over various scales (e.g. Lüthi et al. 2010) and also includes other types of multi-locational service firms (e.g. Hoyler and Watson 2013). The starting point of this research agenda is that Produce Service (PS) firms are key actors in the formation of urban-economic networks. However, in the original data there is no direct information on the connections between cities (i.e. a one-mode city-by-city network): the initial dataset details which firms are located in what cities (i.e. a two-mode or bipartite network; cf. Liu and Derudder 2012, 2013; Neal 2014a). A bipartite network can be formally defined as ‘a set of network nodes divided into two disjoint sets so that no links are present between two nodes within the same set’ (Ulusoy et al. 2015). However, it is possible to apply a ‘bipartite network projection’ that transforms the two-mode network (e.g. city/firm) into two separate one-mode networks (e.g. city/city and firm/firm). Different bipartite projection algorithms have been developed in the social network analysis literature in general (e.g. Neal 2014b) and in the urban network literature in particular (eg van Meeteren et al. 2016; Hennemann and Derudder 2014). In this paper, we seek to contribute to this literature by applying a new, complementary bipartite network projection algorithm for measuring urban connectivity in PS firm networks. The various bipartite network projection algorithms in urban network research essentially entail different ways of approximating the (probability of) linkages between cities based on data of the (importance of the) presence of firms in cities. A first approach is found in the work of Alderson and Beckfield (2004), Rozenblat and Pumain (2007), Wall and Knaap (2011), Krätke (2013), Martinus et al. (2015), and Sigler and Martinus (2017). Although this research does not specifically zoom in PS firms, it also entails applying an algorithm in which city-dyads are derived from locational information of firms in cities: inter-urban connections are defined as the (multiscalar) linkages between headquarters and subsidiaries of a firm. The second approach is Taylor’s (2001) ‘interlocking network’ algorithm, in which city-dyads are calculated by means of an interaction model: cities with a sizable presence of a particular firm are deemed to be strongly related, as this co-presence opens up the potential exchange of work flows (people, knowledge, capital, etc.). The third approach is from a group of algorithms, developed by Hennemann and Derudder (2014) and Martinus et al. (2015), builds on the interlocking network model, but adds a geographical dimension by hypothesizing regional hierarchies of knowledge exchange. And the fourth approach, and most innovatively,Footnote 1 is a range of algorithms introduced by Neal (Neal 2011; Neal 2014b; Neal 2017; van Meeteren et al. 2016), which collectively pay attention to the presence of different forms of ‘sorting process’. For example, connections can be conjectured as arising from above-expected levels of co-presence of firms in cities. The third and fourth approaches represent a rising level of arithmetic complexity in this research field, but are usefully premised on the idea of developing more nuanced and detailed measures of city-dyads. The potential of these algorithms is therefore not so much based on their state-of-the-art engagement in network analysis, but in their ability to paint complementary and nuanced pictures of the urban networks produced by PS firms. This implies that the underlying assumptions of the bipartite projection algorithms need to be clearly spelled out in the specific empirical context in which they are applied. Against this backdrop, in this paper, we develop a new bipartite network projection algorithm for measuring urban connectivity in PS firm networks. The algorithm is applied to a tailored dataset on the presence of PS firms in Chinese cities. We explain why this algorithm is pertinent in this particular case, and corroborate this through a discussion of some empirical results. The remainder of this paper is organized as follows. In the next section, we discuss bipartite projections in the context of the urban network literature. We present the interlocking network model (INM) and discuss its expansion in our locational recommendation model (LRM). This is followed by a description of the data and method, after which we apply the LRM to explore the urban network arising from the location decisions of PS firms across China. We conclude by discussing the relevance and added value of the LRM by comparing it to the INM in particular.",6
20.0,2.0,Networks and Spatial Economics,04 January 2020,https://link.springer.com/article/10.1007/s11067-019-09490-5,A Dynamic Hierarchical Bayesian Model for the Estimation of day-to-day Origin-destination Flows in Transportation Networks,June 2020,Anselmo Ramalho Pitombeira-Neto,Carlos Felipe Grangeiro Loureiro,Luis Eduardo Carvalho,Male,Male,Male,Male,"Given a geographic region, one of the main problems in planning and operating transportation systems is the estimation of origin-destination (OD) flows, i.e., the amount of trips made by people or freight between points in the region over a defined time interval. This is also referred to in the literature as the OD matrix estimation problem. OD flows are traditionally estimated through surveys, in which households or drivers are inquired about their daily journeys (Ortúzar and Willumsen 2011). However, direct surveys are expensive and they are in general carried out every decade (Cascetta 2009). This low frequency implies that planners may remain many years with no data on the evolution of OD flows over time. As an alternative, OD flows may be indirectly estimated from data on link traffic volumes gathered from traffic control systems. The idea is that we can extract information on OD flows from data on traffic volumes if we have a suitable statistical model which describes their relationships. The problem of estimating OD flows from observed link volumes in a transportation network can be classified as a statistical linear inverse problem (Hazelton 2010). Given observed link volumes, we want to estimate the corresponding OD flows which produced them. One distinguishing feature of this problem is that their statistical models are often nonidentifiable, which means that their parameters cannot be uniquely determined even if one has a sample of data of infinite size (Singhal and Michailidis 2007). In order to tackle the nonidentifiability, some form of regularization is often used, in which prior knowledge on the parameters is incorporated into the model. OD matrices obtained from previous survey studies are often used as prior knowledge. Another complicating aspect of the problem is the functional relationship between OD flows and link volumes. This is often modeled through an assignment matrix, whose elements give the proportion of OD flows which go through each observed link in the network. Assignment matrices can be estimated directly or indirectly through route choice models, which give the probabilities of the user choosing alternative routes in each OD pair. Over the years, many models have been proposed to estimate OD flows from link volumes. They can be broadly classified as static or dynamic, according respectively to the assumption of constant OD flows or varying OD flows over time. In addition, we can make a distinction between reconstruction of OD flows and estimation of the parameters of a model which describes the statistical pattern of OD flows (Lo et al. 1996; Hazelton 2001b). In the former, we try to recover the exact OD flows which generated the link volumes observed during a single time period, while in the latter we try to estimate the parameters of a statistical model which describes a “population” of OD flows from a sample of link volumes possibly observed during multiple time periods. In Section 1.1, we briefly review the literature on the problem, while in Section 1.2 we state our contribution. Early models had the purpose of reconstructing static OD flows given a single sample vector of observed link volumes on selected links in the network. They assumed low congestion and that the assignment matrix was known and independent of OD flows. Regularization was often applied by means of optimization of a particular objective function. Van Zuylen and Willumsen (1980) reconstructed the OD matrix by maximizing an entropy function subject to flow conservation constraints. Cascetta (1984) and Bell (1991) formulated generalized least squares models (GLS) whose solutions correspond to the minimization of the Mahalanobis distance to a prior OD matrix. Maher (1983) proposed a Bayesian model with a Gaussian prior as a form of regularization. Brenninger-Göthe and Jörnsten (1989) proposed a multiobjective approach which includes maximum entropy and GLS models as particular cases. In the case of congested networks, it has been argued that the assignment matrix depends on the level of OD flows. The main assumption in the case of static models is that the assignment matrix should correspond to an equilibrium state of the network, such as defined by Wardrop’s first principle (Wardrop 1952; Friesz et al. 2001). Fisk (1989), Yang et al. (1992), and Yang (1995) proposed frameworks based on bilevel optimization, in which the first level corresponds to an objective function (e.g., entropy maximization or GLS) subject to constraints which are obtained as a solution of a traffic assignment model in the second level. Both optimization problems are solved iteratively until convergence of OD flows and the assignment matrix. More recent developments in this class of models may be found in Cascetta and Postorino (2001), Nie and Zhang (2010), Shen and Wynter (2012), Abareshi et al. (2017), and de Grange et al. (2017). In contrast to models for the reconstruction of a single OD matrix, Vardi (1996) proposed a statistical model which describes a “population of OD matrices”. A sample of link volumes observed over multiple time periods corresponds to a sample of OD matrices. The purpose is then to estimate the parameters of the model from the sample of observed link volumes. Vardi assumed OD flows follow independent Poisson probability distributions and proposed maximum likelihood estimators to mean OD flows, which are identifiable under certain conditions on the link-path incidence matrix. Tebaldi and West (1998) and Li (2005) extended Vardi’s model and proposed Bayesian estimators. Hazelton (2000) and Hazelton (2003) proposed multivariate normal approximations to the Poissonian flows due to the computational intractability of the likelihood function in the case of discrete flows. Yang et al. (2018) proposed a general framework which encompasses both the reconstruction and estimation problems. A review of the main statistical models for static OD matrices is found in Pitombeira Neto et al. (2017). When considering dynamic OD flows, we should make a distinction between within-day and day-to-day dynamics (Hazelton 2008). Within-day dynamics refers to the variation of OD flows over the course of a day. The purpose of within-day models is to estimate the mean OD flow pattern or to forecast, possibly in real time, very short term OD flows. On the other hand, in day-to-day dynamics we would like to estimate OD flows over a sequence of days for the same reference time period, e.g. the morning peak hour. It is assumed that the trips in the study region start and finish within this reference time period. Most dynamic models are concerned with within-day dynamics. They can be generally classified in optimization-based models, which are more suited to offline estimation, or Kalman-filter-based models, which can be used both for offline and online estimation. Cremer and Keller (1987) proposed both optimization and Kalman filter models for estimation of dynamic OD flow proportions in traffic facilities in which total entering and exiting flows are observed. Cascetta et al. (1993) extended previous static optimization models for the dynamic case in which assignment matrices are estimated exogenously. Sherali and Park (2001) proposed a path-based constrained least squares model and assumed known path costs to regularize the model. Chang and Wu (1994) proposed a Kalman filter for estimation of dynamic OD flows in freeway corridors. Ashok and Ben-Akiva (2002) proposed a Kalman filter whose states are deviations to known historical dynamic OD flows. Cho et al. (2009) applied Kalman filter and Gibbs sampling to estimate both OD flows and assignment matrices, assuming known covariance matrices. Cascetta et al. (2013) developed a quasi-dynamic approach in which OD shares among destinations are assumed constant for a specified time interval. Lu et al. (2013) proposed a single level path-based nonlinear programming model assuming dynamic user equilibrium. Xie and Duthie (2015) reformulated the dynamic OD estimation problem as an excess-demand dynamic traffic assignment (DTA) problem which can be solved using DTA solution methods (Mahmassani 2001; Peeta and Ziliaskopoulos 2001). Models for estimating dynamic OD flows have also been proposed in the computer networks literature, in which this problem is known as network tomography (Vardi 1996; Benameur and Roberts 2004). Cao et al. (2000) extended Vardi’s model to time-varying flows in which OD flows are assumed to follow independent normal distributions and considered static within short time windows. Liang et al. (2006) proposed a dynamic model and an iterative proportional fitting algorithm in which data on partial observations of OD flows are added in order to improve estimation error. Singhal and Michailidis (2007) studied identifiability conditions of dynamic models in computer networks. Airoldi and Blocker (2013) proposed a hierarchical model in which mean OD flows vary dynamically at the top of the hierarchy, while OD flows at a given time are realizations of independent truncated normal distributions centered on the mean OD flows. It is worth noting that, unlike transportation networks, in computer networks assignment matrices may be inferred from message routing protocols and are often assumed known. This precludes the direct application of models developed for computer networks to transportation networks, since assignment matrices are often unknown in this case. Few models developed so far have addressed the day-to-day dynamics. Working on within-day dynamic models, Zhou and Mahmassani (2007) proposed one of the first models to explicitly incorporate some form o day-to-day dynamics. The authors proposed a Kalman filter for estimation of deviations of OD flows to historical OD flows, as in Ashok and Ben-Akiva (2002). In their method, they update the historical OD flows also with the use of a Kalman filter in which the observed data are the predicted deviations in the previous day. Also working on estimation of within-day OD flows, Lu et al. (2015) proposed a simple day-to-day update procedure by a weighting average of historical OD flows and estimated OD flows for the same day of week. Hazelton (2008) developed a path-based model for the estimation of day-to-day dynamic OD flows, in which the author assumed that mean OD flows on routes (paths) vary over time according to functions of a few parameters. Particular cases include constant demand, linear trend and weekday-weekend models. Pitombeira-Neto and Loureiro (2016) proposed a dynamic linear model to describe the evolution of day-to-day OD flows. Through computational experiments, they showed that the estimation error of OD flows decreases as one gather more link volume samples over time, given that route choice probabilities are known. Although the works in the preceding paragraph address day-to-day dynamics in OD flows, they do not model dynamic route choices. It has been argued that, in the day-to-day dynamics, users exhibit a learning mechanism regarding route choices (Watling and Cantarella 2013). As OD flows change over time, relative costs of alternative routes may change, so that users gradually learn these new costs and change routes. Under such a setting, the assumption that the transportation network is in equilibrium may not hold. Watling and Hazelton (2003). Parry and Hazelton (2013) and Hazelton and Parry (2016) are among the first works to address estimation of day-to-day OD flows taking into account a learning mechanism in route choices. They assumed that route flows vary according to a Markovian transition kernel and that route choices follow a logit model in which utility of routes are given as functions of route costs in past periods. They proposed Bayesian inference methods to estimate OD flows and route choice parameters. In this paper, we address the problem of estimating day-to-day OD flows taking into account the learning mechanism in route choices. In Section 1.2 we state our contribution. We develop a three-level dynamic hierarchical Bayesian model for day-to-day OD flows in a transportation network. Dynamic Bayesian models are state-space models of which Kalman filters are a particular case (West and Harrison 1997; Särkkä 2013). OD flows, route flows and route costs are represented as unob served states of the transportation network. The levels correspond respectively to OD flow generation, route flow assignment and link volume observation on links. According to Daziano et al. (2013), working in a Bayesian setting presents several advantages over traditional approaches in the transportation literature, for example: it allows the incorporation of regularization in a statistically principled manner via prior distributions; properties of estimators are valid for small samples; and predictive distributions allow flexibility in data analysis. At the first level, we set a non-stationary Markovian model which describes how OD flows vary over time. Unlike Hazelton and Parry (2016), who assumed flows as discrete variables following Poisson or multinomial distributions, we assume flows to follow multivariate normal distributions. This assumption has been made in other models for computational tractability. Covariance matrices are assumed to follow variance functions with dispersion parameters. This allows over or underdispersion relative to the Poisson assumption. The parameterization is parsimonious, so that we do not have to estimate the individual variances and covariances. At the second level, given OD flows, route flows are assumed to follow a multivariate normal distribution with a multinomial-like covariance structure. We set a multinomial logit route choice model which determines the probabilities of users choosing each route. In order to compute route choice probabilities, we must have estimates of route costs. One of the limitations of some models is that they assume known link costs or route costs, but in practice it is difficult to observe route costs at a daily basis. We tackle this problem by considering route costs as deterministic functions of route flows by means of link performance functions. Moreover, we take into account day-to-day route costs experienced by users in the route choice model. We do not assume the network is in an equilibrium state, but rather that users respond to past route costs by adjusting their day-to-day predictions. In order to model the learning mechanism of users, we make a distinction between user-experienced and user-predicted route costs. We assume that at any day users update their predicted route costs by exponentially smoothing predicted and experienced costs on the previous day. Unlike other works in the literature, this model has a single learning parameter which regulates the length of users memory, without the need of testing alternative models with different memory lengths. At the third level we set an observation model which relates observed link volumes and unobserved route flows. Observed link volumes are subject to measurement error whose covariance matrix is also given by a variance function with a dispersion parameter. As we do not assume an error-free relationship between observed link volumes and route flows, our model has the advantage that we do not have to sample route flows from the convex polytopes defined by the observed link volumes and the link-path incidence matrix, which is a computationally intensive task. Our purpose is to do inference on OD flows, route flows and parameters of the route choice model and the variance functions, given a time series of observed link volumes in a subset of links of a transportation network. For this, we develop a Metropolis-within-Gibbs algorithm to sample from the joint posterior distribution of OD flows, route flows, user-experienced route costs, user-predicted route costs, route choice probabilities and model parameters. OD flows are sampled in a forward-filtering-backward-sampling step, in which Kalman filter recurrences are applied forward in time and OD flows sampled backwards. Dispersion parameters of the variance functions are sampled from conjugate distributions, while route choice parameters are sampled in a Metropolis-Hastings step. We illustrate the application of our model and sampling algorithm through numerical studies on transportation networks from the literature.",9
20.0,2.0,Networks and Spatial Economics,17 January 2020,https://link.springer.com/article/10.1007/s11067-019-09489-y,Spatial Autocorrelation Panel Regression: Agricultural Production and Transport Connectivity,June 2020,Atsushi Iimi,Liangzhi You,Ulrike Wood-Sichra,Male,Unknown,Female,Mix,,
20.0,2.0,Networks and Spatial Economics,24 January 2020,https://link.springer.com/article/10.1007/s11067-019-09492-3,Multi-Trip Time-Dependent Vehicle Routing Problem with Soft Time Windows and Overtime Constraints,June 2020,Ampol Karoonsoontawong,Puntipa Punyim,Vatanavongs Ratanavaraha,Unknown,Unknown,Unknown,Unknown,,
20.0,2.0,Networks and Spatial Economics,24 January 2020,https://link.springer.com/article/10.1007/s11067-019-09487-0,Synchronizing Last Trains of Urban Rail Transit System to Better Serve Passengers from Late Night Trains of High-Speed Railway Lines,June 2020,Sihui Long,Lingyun Meng,Francesco Corman,Unknown,Unknown,Male,Male,"An urban rail transit (URT) system plays an important role in the urban transportation system to meet daily large passenger demand, due to the characteristics such as a high level of safety, wide accessibility, good energy performance, reliable service with sufficient punctuality. The URT lines are typically integrated with faster and longer distance modes, such as high-speed railway (HSR) lines at some transfer stations to provide seamless transfer services. For instance, lines 4 and 14 of Beijing URT system are connected with Beijing-Shanghai high-speed railway (HSR-BS) line and Beijing-Tianjin high-speed railway (HSR-BT) line at Beijing South Railway Station, to serve passengers from HSR trains. One key challenge of the above mentioned integrated transport system is to keep high reachability to stations in the entire URT network, for passenger flows from HSR lines, especially in late night. For instance, at Beijing South Railway Station, more than 2, 500 passengers get off trains of HSR lines after 11 pm, while the last trains of lines 4 and 14 of Beijing URT system have already ended operations at that time. Most of the passengers from HSR lines would prefer taking URT trains to finish their trips instead of taxies or buses, since it normally takes a long time period, e.g. more than one hour, waiting for a taxi or a bus in late night. The underlying problem is called last-train timetable synchronization (LTTS) problem, which has been studied so far in view of synchronizing last trains in feeding lines and connecting lines at transfer stations (Kang et al. 2015a). This paper tries to contribute to the literature in the following two aspects: (1) the transfer schemes (connections to keep) of passengers are dynamically generated instead of considering them as given fixed input data which is common in the literature, and (2) we consider variability of the transfer time of passengers, i.e. the minimal time that people need to walk from the place where they get off the high-speed train to the place where they get on the URT train. In the process of designing a last-train timetable, only some transfer connections can be maintained, since it is impossible to coordinate all transfer directions at one transfer station. We refer to Kang et al. (2014) for more information about the proof on the impossibility of connecting all transfer directions. The vast majority of the existing approaches have a common assumption, that a fixed transfer scheme is used for the network, i.e., pre-determined connections are fixed a-priori at transfer stations (e.g., Kang et al. 2014, 2015a, b; Kang and Zhu 2016; Yang et al. 2017a), as reviewed at Table 1 in Sect. 2. A fixed transfer scheme is typically built based on the internal flows of passengers, and neglects external demand of generators such as HSR stations. Moreover, passengers typically have different walking speed in stations. Hence there are inherent differences among transfer walking time (TWT) of passengers, which should be considered in the model. For instance, people taking a long distance trip would have luggage and typically walk at a slower pace, while people which are younger or taking a shorter distance trip might have less luggage and walk faster. Moreover, experienced people like commuters might know better about the route to their destinations, while inexperienced persons or tourists may walk slower in the station area. Figure 1 illustrates a comparative example of the last-train timetable with and without considering the difference among TWT of passengers, as a time-space diagram. Let us assume a fixed TWT for all passengers (i.e. 3 min for both passenger groups A and B), then the arrival time of the last train on L2 up as time 1 and the departure time of the last train on L1 up as time 4 will be a solution. In fact, TWTs are not the same for different passengers (i.e. 2 min for passenger group A and 4 min for passenger group B). If the timetable is scheduled neglecting the difference among TWT of passengers, group A will transfer successfully (i.e., group A needs to wait one minute after they arriving the platform of L1 up), but group B will fail to transfer (i.e., group B reaches the platform after the last train departing). If we design the timetable considering the difference among TWT of passengers, the departure time of the last train on L1 up, at time 3 and time 5, will be a feasible solution for passenger group A and B respectively. As a result, we need to decide which departure time is better, and how that relates to the total operation ending time. As for operation ending time, we mean the time at which operations can end; as we are tackling a problem with multiple lines, we consider the total operation ending time as the sum of the operation time of all lines. An illustration of a last-train situation with and without considering the differences among transfer walking times of different passengers An earlier departure time (i.e., time 3) is better for URT companies since the operation ending time will be earlier, and fewer costs will be required, but may not serve all passengers (i.e., passenger group B will fail to transfer). A departure at a later time (i.e., time 5) is favoured by passengers since all passengers could transfer successfully, but results in larger costs for URT operations. Therefore, we need to solve a trade-off between the number of passengers and operation ending time. To address these issues, this paper studies the under-investigated problem of synchronizing last trains of URT system to serve HSR passengers with consideration of differences among TWTs of different passengers. Passengers are assumed to have relatively few origins (i.e., platforms of HSR lines) and dispersed destinations (i.e., end stations of URT lines close to their homes). We do not make any assumptions regarding transfer connections to be kept in operations of the URT system. We develop a joint bi-objective optimization scheme to combine two objectives of travel time of passengers (related to penalties, discomfort, fares for taxis or buses, for very heterogeneous passenger groups) and operation ending time of URT (related to labor regulations, cost of operating a URT system, amount of personnel and their wage) which are hard to compare and put in a straightforward weighted objective function. The remainder of this paper is organized as follows. Section 2 provides a detailed literature review on timetable synchronization problem of non-last trains and last trains. In Sect. 3, the problem statement, illustrative example and model assumptions are given firstly, followed by a bi-objective model that formalizes the problem of designing transfer schemes and scheduling last trains of URT system. Then we use an ε-constraint method to solve the bi-objective model and obtain two modified formulations. Next, numerical tests based on a sample network and case studies based on a real-world network are given in Sects. 4 and 5 respectively, to evaluate the performance and effectiveness of the bi-objective model. Finally, conclusions and topics for future research are provided in Sect. 6.",10
20.0,2.0,Networks and Spatial Economics,12 November 2019,https://link.springer.com/article/10.1007/s11067-019-09488-z,Retraction Note: Appraisal of Science and Economic Factors on Total Number of Granted Patents,June 2020,Dušan Marković,,,Male,Unknown,Unknown,Male,,
20.0,3.0,Networks and Spatial Economics,30 March 2020,https://link.springer.com/article/10.1007/s11067-020-09494-6,Pareto-Optimal Sustainable Transportation Network Design under Spatial Queuing,September 2020,Wei Huang,Guangming Xu,Hong K. Lo,,Unknown,,Mix,,
20.0,3.0,Networks and Spatial Economics,01 June 2020,https://link.springer.com/article/10.1007/s11067-020-09497-3,Nonlinear Decision Rule Approach for Real-Time Traffic Signal Control for Congestion and Emission Mitigation,September 2020,Junwoo Song,Simon Hu,Chaozhe Jiang,Unknown,Male,Unknown,Male,"Urban traffic signal controls play an essential role for traffic management to reduce congestion and alleviate adverse environmental impacts. Different traffic signal control strategies have been developed and deployed in large scale in the past several decades (Sunkari 2004), ranging from traditionally pre-timed signal control systems based on historical traffic information to fully responsive systems that frequently update signal control parameters and/or phasing schemes according to real-time traffic conditions. Some typical examples of the latter include SCOOT (Hunt et al. 1982), SCAT (Lowrie 1982), OPAC (Gartner 1983), PRODYN (Henry et al. 1983), TRANSYT and RHODES. In the real world, traffic flows may vary significantly at road intersections even in the same time period of the day and day of the week. As a result, the capability to handle uncertain flow patterns on a network level is crucial in the design of adaptive signal controls (Yin 2008; Papatzikou and Stathopoulos 2015; Liu et al. 2015; He et al. 2014; Feng et al. 2015; Christofa et al. 2016; Smith et al. 2013). The objectives of adaptive signal control strategies include minimization of (weighted) vehicle/pedestrian delay (He et al. 2014; Sun et al. 2006; Zhang et al. 2010), minimization of passenger delay (Christofa and Skabardonis 2011; Christofa et al. 2016), minimization of number of stops (Lucas et al. 2000), maximization of total throughput (Chang and Sun 2004; Han et al. 2014). This paper focuses on real-time adaptive signal control on realistic traffic networks, while taking into account exhaust emissions including total carbon and Black Carbon (BC). Total carbon is closely related to the emission of CO2, the primary greenhouse gas contributing to the climate change. BC is produced through incomplete combustion of carbonaceous materials, and causes serious health concerns such as respiratory problems, heart attacks and lung canersFootnote 1 (Janssen et al. 2013). It is known that the total carbon emissions are highly dependent on the engine load and vehicle speed, while emissions of BC and NOx are more sensitive to vehicle dynamics (such as acceleration and idle) and vehicle technology (Zhang et al. 2011). Therefore, to accurately account for these different emission mechanisms in a dynamic and uncertain control environment poses a significant challenge. The accurate modeling of different species of exhaust emissions requires high-fidelity and high-resolution traffic model and data, which provide detailed and critical information on vehicle speed, acceleration, deceleration, fleet composition, and emission factors. However, the computational burdens associated with these models typically render real-time and large-scale application of signal control and optimization infeasible. On the other hand, the decentralization of controls, in which the signal control parameters are determined at individual intersections, offers viable solutions but do not guarantee global optimality due to the lack of coordination. In seek of a global optimal, real-time signal control strategy with multiple objectives including vehicle emissions with high fidelity and resolution, this paper proposes a novel nonlinear decision rule (NDR) approach based on feedforward neural network and recurrent neural network. The key novelty is that all the expensive computations are performed in an off-line environment through simulation-based optimization based on traffic microsimulation (S-Paramics) and high-fidelity emission modeling using AIRE and COPERT IV models (Mascia et al. 2016). The aim of the off-line optimization is to train the NDR such that its on-line (i.e. real-time) operation can be continuously improved. In addition, the on-line operation of the NDR is computationally efficient as all the optimizations are performed off line. As we shall see later, some other advantages of this framework include:
 flexible input structure: The system can accommodate a wide range of data types, spatial coverage and temporal resolution. This is a desirable feature for real-time signal control as most existing studies assume full knowledge of traffic states at all key intersections and their approaches, which is often not the case in real-world networks. As shown in the case study, flexible scope and resolution of controls: Different signal parameters (cycle time, green split, offset) at one or several intersections can be controlled simultaneously in real time; user defined objectives and priorities: As the training of the NDR is based on simulation, the proposed framework can include various traffic and environmental performance indicators; and explicit incorporation of uncertainties: Demand variations and uncertainties inherent in traffic dynamics can be accounted for during the training of the NDR, so that the resulting real-time controls are robust agains traffic uncertainties. While there has been numerous studies applying artificial intelligence models (such as neural networks, reinforcement learning) to real-time traffic signal controls (Arel et al. 2010; Sundaram et al. 2015; Castro et al. 2017), few have considered realistic traffic dynamics that give rise to accurate estimation of emissions on a network scale. This paper employs a microscopic traffic simulation model (S-Paramics), which has been thoroughly calibrated for a real-world test network (Mascia et al. 2016), and the AIRE instantaneous emission model to accurately calculate emissions in view of different vehicle dynamics and fleet composition. Furthermore, the nonlinear relationships between traffic and environmental performance indicators have not been explored fully in the literature. This paper investigates the potential trade-off traffic and environmental objectives both globally (network level) and locally (junction level), as well as for different degrees of network saturation. Moreover, the impact of sensor locations on the performance of the signal controls are assessed. Our findings provide valuable insights into the management of dynamic and complex traffic networks with environmental considerations. The proposed framework is tested for a real-world network located in Glasgow, Scotland, as part of the CARBOTRAF project (Mascia et al. 2016). Simulation-based validation of the signal controls in a real-time environment indicates a reduction of network-wide delay by up to 68%, total carbon and black carbon emissions by 3% and 2%, respectively, and 1% increase of network throughput. It is found that most of the emission reductions are concentrated at signal intersections, where local improvements can be up to 30%. In addition, it is shown that CO2 reductions, which took place primarily around traffic intersections, are correlated to delay reductions, while such correlation is weak for black carbon due to other factors like stop-and-go cycles and vehicle composition that contribute to BC emissions. Finally, the proposed NDR framework is tested with a different spatial configuration of traffic sensors, showing its robustness against sensor locations, which is a desirable property for real-world implementations. The rest of this paper is organized as follows. Section 2 offers an overview of real-time signal controls in the literature. Section 3 outlines the general model for the NDR approach as well as implementation details of its components. Section 4 details a case study of a real-world network and demonstrates the effectiveness of the proposed control strategies. Finally, Section 5 offers some concluding remarks.",2
20.0,3.0,Networks and Spatial Economics,13 March 2020,https://link.springer.com/article/10.1007/s11067-020-09493-7,Sustainable Management of Remanufacturing in Dynamic Supply Chains,September 2020,Sung Hoon Chung,Robert D. Weaver,Hyun Woo Jeon,,Male,,Mix,,
20.0,3.0,Networks and Spatial Economics,31 March 2020,https://link.springer.com/article/10.1007/s11067-020-09496-4,Origin-Destination Demand Reconstruction Using Observed Travel Time under Congested Network,September 2020,Chao Sun,Yulin Chang,Wenyun Tang,,Unknown,Unknown,Mix,,
20.0,3.0,Networks and Spatial Economics,07 July 2020,https://link.springer.com/article/10.1007/s11067-020-09500-x,"A Blood Bank Network Design Problem with Integrated Facility Location, Inventory and Routing Decisions",September 2020,Onur Kaya,Dogus Ozkok,,Male,Unknown,Unknown,Male,"Design of an efficient distribution network for blood, which is a very valuable product in healthcare systems, is extremely important for timely satisfaction of its demand at minimum cost. As stated by Osorio et al. (2015), availability of the right blood products at the right locations is critical, but at the same time another important factor is total cost, including processing, storage and distribution costs. Osorio et al. (2015) present a categorization of different decisions in blood supply chains as strategic, tactical and operational level decisions, and also explain their relations with each other. As stated by Miranda and Garrido (2006), traditionally, strategic, tactical and operational decisions are considered separately for modeling purposes. However this leads to non-optimal decisions, since in reality there is interaction between different levels. In this study, we integrate these decision levels in the design of a blood bank network for the blood distribution system. Specifically, we consider the facility location, inventory and routing decisions in the design of the distribution network of blood in Istanbul to satisfy the needs of hospitals. In the current blood distribution network, each hospital keeps a certain level of inventory, received by shipments from a main blood bank, which processes the blood collected from various sources. Blood is generally collected at donation stations or centers spread over a region and then it is processed at the blood banks or blood centers. Then blood banks supply hospitals within their regional area. Several vehicles with different routes, starting from the blood bank, fulfill the demand of the hospitals at certain intervals. Hospitals generally keep a certain level of their own inventory of blood, depending on the frequency of shipments from the main blood bank, that might lead to overstock and stockout risks. Furthermore managing inventory and maintaining a high service level is a problem for the entire system. We propose an alternative system, in which some of the hospitals are selected as local blood banks (LBB) and inventory of blood is only kept at these LBBs. In the proposed distribution network, LBBs will monitor and serve the nearby hospitals, such that the hospitals will not need to keep as much inventory as before. All the hospitals will be assigned to an LBB and more frequent shipments will be made from LBBs to these hospitals. The main blood bank will supply local blood banks at certain intervals and local blood banks will satisfy hospital needs with frequent deliveries. Since the demand for blood is highly random, and blood is a valuable product, pooling of the inventory of blood at these LBBs will lead to a better utilization of the resources. We suggest localization of blood banks to increase efficiency, and benefit from risk pooling advantages. The benefits of inventory centralization in the blood supply chain is acknowledged in the literature. For example, Hosseinifard and Abbasi (2018) state centralization of hospitals’ inventory can increase the sustainability and resilience of the blood supply chain. In this study, we aim to determine the optimal number and location of LBBs, assignment of hospitals to LBBs, the optimal inventory levels at each LBB and the routing decisions among the LBBs and hospitals. Even though these decisions are usually studied separately in the classical literature, they are related with each other and a system that integrates these decisions will provide better results in order to optimize the total system performance. Network design problems considering blood supply chains are studied by various researchers. Or and Pierskalla (1979) deal with the transportation of blood from regional blood banks to hospitals. Sahin et al. (2007) develop different mathematical models for the location–allocation decisions in regionalization of blood services in Turkey. Cetin and Sarul (2009) develop a mathematical model to decide on the locations of blood banks using a multi objective approach. Hemmelmayr et al. (2009) present a model based on vendor managed inventory systems to organize the delivery of blood products to hospitals and plan the routes, assuming that the blood bank makes the delivery programs based on stationary deterministic demand. Nagurney and Masoumi (2011) study the design of a sustainable blood banking system, consisting of collection sites, blood centers, testing and processing labs, storage facilities, distribution centers as well as demand points. Nagurney et al. (2012) develop a generalized network optimization model for a complex blood supply chain. Sahinyazan et al. (2015) design a mobile blood collection system for the collection of large amounts of blood at reasonable cost. They develop a mathematical model, as an extension of the Selective Vehicle Routing Problem, to determine the tours of the bloodmobiles. Osorio et al. (2018) present an analytical model for the optimal configuration of a generic blood supply chain. They aim to determine the optimal location of collection and production facilities and to assign hospitals to them to minimize total system cost. Khalilpourazari and Khamseh (2019) propose a multi-objective mathematical model to design a blood supply chain network in earthquake situations. Belien and Force (2012) and Osorio et al. (2015) present detailed literature reviews about blood supply chains. Even though we are motivated by the blood distribution system in Istanbul, the framework in this study is also applicable, with some modifications, to the optimization of other supply chain network problems. Hospitals can be seen as retailers, local blood banks as distribution centers and the main blood bank as a supplier. Our problem can be considered as a supply chain network design problem (SCNDP) with a goal to find the optimal number and locations of distribution centers, assignment of retailers to the open distribution centers (DCs), inventory levels that will be kept at the open DCs and the routing decisions between the facilities. There are many studies in the literature for network design problems considering location, routing and inventory decisions, however most of these studies analyze these decisions separately from each other. There are also several studies that consider these decisions in couples such as location-routing, location-inventory or inventory-routing decisions. Nagy and Salhi (2007), Prodhon and Prins (2014) and Schneider and Drexl (2017) present detailed surveys about the location-routing problems (LRPs) which integrate the location and routing decisions. Among the early studies, Balakrishnan et al. (1987) discuss various modeling approaches for location-routing problems that include mathematical programming formulations, approximations and modified models. Salhi and Fraser (1996) propose an iterative method that alternates between location and routing phases until a suitable stopping condition is met. Tuzun and Burke (1999) use a tabu search algorithm to find solutions for an LRP. Wu et al. (2002) employ a simulated annealing algorithm (SA) to solve the multi-depot location-routing problem. Bozkaya et al. (2010) develop a location-routing model for a firm that operates multiple facilities under competition. They solve the developed model using a hybrid genetic algorithm integrated with tabu search. Toyoglu et al. (2012) propose a node-based product-flow formulation in order to solve the problem more efficiently. Schneider and Loffler (2019) introduce a tree-based search algorithm in order to solve the large scale capacitated LRPs. They employ a granular tabu search in the routing phase, that uses a large composite neighborhood described by 14 operators. Koc (2019) analyses the impact of depot location and routing decisions on emissions in freight transportation by considering environmental objectives and time windows. He proposes an adaptive large neighborhood search metaheuristic for the solution of this problem. The early literature on location problems has mostly ignored the inventory related costs and has focused on finding the optimal number of facilities, their positions and assignment to these facilities. On the other hand, inventory models focus on finding optimal replenishment strategies and safety stock decisions by assuming that the number and location of the facilities are known. The early studies that combine these two decisions are defined by Barahona and Jensen (1998) and Erlebacher and Meller (2000). Daskin et al. (2002) and Shen et al. (2003) contribute to the inventory-location literature by adding working inventory and safety stock costs to distribution center location models. Miranda and Garrido (2004) propose a simultaneous approach to incorporate economic order quantity and safety stock decisions with facility location models. A nonlinear mixed integer programming model is developed considering stochastic demand and also risk pooling. They solve the model using a heuristic solution approach that is based on Lagrangian relaxation and a subgradient method. Sourirajan et al. (2007) propose a model to capture the tradeoff between risk-pooling and lead times. They introduce a model for a single product distribution network problem and solve it using a lagrangian relaxation heuristic. Mak and Shen (2009) develop a mixed integer nonlinear programming model in order to simultaneously determine the locations of service centers, the allocation of customers to centers and the base-stock levels in a two-echelon system, subject to service constraints. Naseraldin and Herer (2011) propose a location-inventory model in which the number and locations of retail outlets are determined simultaneously with the inventory replenishment and transshipment quantities. Shu et al. (2013) study the design of a logistics distribution network and aim to determine the set of warehouses to open from a list of potential locations, which warehouse is used to serve each retailer and the inventory replenishment policies at warehouses and retailers. Shahabi et al. (2013) aim to coordinate inventory control and facility location decisions, and develop mathematical models for a four-echelon supply chain network. They state that the hubs in the network can decrease transportation costs by consolidating products from multiple warehouses and directing the larger shipments to the retailer. Silva and Gao (2013) incorporate inventory replenishment costs into a distribution center location model and propose a Greedy Randomized Adaptive Search Procedure to solve the problem. Diabat et al. (2015) develop a lagrangian-relaxation based heuristic for a multi-echelon joint inventory-location problem that makes location, order assignment and inventory decisions simultaneously. Kaya and Urek (2016) present a mixed integer nonlinear facility location-inventory-pricing model to decide on the optimal locations of the facilities, inventory amounts and prices for products in order to maximize the total supply chain profit. They also develop heuristics for the solution of their model. Similar to our study, Punyim et al. (2018) also account for the probability of unfulfilled demand, and propose a mathematical model for the joint location-inventory problem on a two-level supply chain. The Inventory Routing Problem (IRP) is the integration of inventory control and vehicle routing decisions into a cost efficient distribution system. Moin and Salhi (2007) and Coelho et al. (2013) present detailed reviews of IRP in literature. One of the earliest studies is Federgruen and Zipkin (1986), in which they integrate inventory allocation and routing with random demand in retailers. They design a non linear mixed integer programming model and solve it by a generalized Bender’s decomposition approach. Anily and Federgruen (1990) analyze the problem with deterministic constant demand and aim to find inventory rules and routing patterns that minimize the inventory and routing costs in the long run. Anily and Bramer (2004) analyze the fixed partition policy for the inventory-routing problem, in which the retailers are partitioned into disjoint and collectively exhaustive sets and each set of retailers is served independently of the others at its optimal replenishment rate. Zhao et al. (2007) also propose a fixed partitioning policy for this problem by using the power-of-two principle to warehouse and retailer replenishment intervals and develop a tabu search algorithm to find the optimal retailer partitioning region. Toriello et al. (2010) present a time decomposition algorithm for inventory routing problems. Aksen et al. (2012, 2014) present a mathematical model and an adaptive large neighborhood search algorithm for a selective and periodic inventory routing problem for the collection of waste vegetable oil. Treitl et al. (2014) incorporate environmental aspects in an inventory routing problem by considering the carbon emissions in addition to other costs in their objective function. Chu et al. (2017) consider a multi-trip vehicle routing problem for daily inventory replenishment under stochastic travel times. They propose a two-stage heuristic algorithm for the solution of their model. Diz et al. (2018) propose a robust mathematical programming model for the maritime inventory routing problem considering the uncertainties about the time spent at the ports. Karoonsoontawong et al. (2019) present efficient insertion heuristics for IRP with time windows, allowing multi trips per vehicle. There are also some studies in the literature that consider location, inventory and routing decisions simultaneously. Shen and Qi (2007) consider a supply chain design problem where they decide on the number and locations of the distribution centers (DCs), considering random demand and a certain service level for the customers at each DC. Ahmadi Javid and Azad (2010) simultaneously optimize location, allocation, capacity, inventory, and routing decisions in a stochastic supply chain system. They propose a heuristic method based on a hybridization of Tabu Search and Simulated Annealing for the solution of this problem. Guerrero et al. (2013) develop a mixed-integer linear programming model and propose a hybrid heuristic to determine the depots to open, the quantities to ship from suppliers to depots and from depots to retailers per period, and the sequence to replenish the retailers. Guerrero et al. (2015) propose a relax-and-price heuristic in order to solve this problem. Yuchi et al. (2016) analyze the location-inventory-routing in forward and reverse logistic network design, in which new goods are produced and damaged goods are repaired by a manufacturer and then returned to the market to satisfy customer demands. Yuchi et al. (2018) propose hybrid heuristics for the location-inventory-routing problem in a closed-loop supply chain. Hiassat et al. (2017) consider a location-inventory-routing model for perishable products. They propose a genetic algorithm to determine the number and location of required warehouses, the inventory level at each retailer, and the routes traveled by each vehicle. Rafie-Majd et al. (2018) consider the integrated inventory-location-routing problem (ILRP) in a three-echelon supply chain, for delivering perishable products in a limited time horizon. In this study, we aim to determine the optimal selection of LBBs, assignment of hospitals to LBBs, optimal inventory levels at each LBB and routing decisions among the facilities. We develop a mixed integer nonlinear programming (MINLP) model that considers these decisions to minimize the total system cost under a certain performance criteria. We present the optimal solution of this model for small-sized instances and develop approximate solution methods to solve the medium- and large-sized problems. We propose a piecewise linear approximation method and a simulated annealing heuristic approach to find the solution of this problem. Our study differs from the literature by combining facility location, inventory and routing decisions in a complex supply chain network. The strategic location decisions, tactical inventory decisions and operational routing decisions are integrated in our proposed model. In addition, since the complex nature of the problem makes it impossible to solve medium- and large-sized instances in a reasonable time limit, we develop efficient heuristic methods to solve these instances. We also apply our models on a real life case study with real life data and observe that significant improvements can be obtained by the proposed system. The performances of the proposed solution methods are also analyzed and a sensitivity analysis related to the system parameters is done via detailed numerical experiments.",14
20.0,3.0,Networks and Spatial Economics,20 March 2020,https://link.springer.com/article/10.1007/s11067-020-09495-5,Residential Location Econometric Choice Modeling with Irregular Zoning: Common Border Spatial Correlation Metric,September 2020,José-Benito Pérez-López,Margarita Novales,Alfonso Orro,Unknown,Female,Male,Mix,,
20.0,3.0,Networks and Spatial Economics,01 July 2020,https://link.springer.com/article/10.1007/s11067-020-09498-2,Arrival Time Reliability in Strategic User Equilibrium,September 2020,Michael W. Levin,Melissa Duell,S. Travis Waller,Male,Female,Unknown,Mix,,
20.0,3.0,Networks and Spatial Economics,28 July 2020,https://link.springer.com/article/10.1007/s11067-020-09499-1,On the Net Neutrality Efficiency under Congestion Price Discrimination,September 2020,Sahar Fekih Romdhane,Chokri Aloui,Khaïreddine Jebsi,Female,Male,Unknown,Mix,,
20.0,3.0,Networks and Spatial Economics,29 July 2020,https://link.springer.com/article/10.1007/s11067-020-09501-w,Traffic Dynamics and Mode Choice’s Delay Effect Under Traffic Restriction in Two-Mode Networks,September 2020,Dong-Fan Xie,Xiao-Mei Zhao,,Unknown,,Unknown,Mix,,
20.0,4.0,Networks and Spatial Economics,26 August 2020,https://link.springer.com/article/10.1007/s11067-020-09507-4,Facility Dependent Distance Decay in Competitive Location,December 2020,Tammy Drezner,Zvi Drezner,Dawit Zerom,Female,Male,Unknown,Mix,,
20.0,4.0,Networks and Spatial Economics,08 September 2020,https://link.springer.com/article/10.1007/s11067-020-09504-7,Day-to-Day Evolution Model Based on Dynamic Reference Point with Heterogeneous Travelers,December 2020,Huijun Sun,Si Zhang,Lu Lou,Unknown,,,Mix,,
20.0,4.0,Networks and Spatial Economics,30 September 2020,https://link.springer.com/article/10.1007/s11067-020-09506-5,How to Optimize Train Stops under Diverse Passenger Demand: a New Line Planning Method for Large-Scale High-Speed Rail Networks,December 2020,Xin Zhang,Lei Nie,Yu Ke,,,,Mix,,
20.0,4.0,Networks and Spatial Economics,08 October 2020,https://link.springer.com/article/10.1007/s11067-020-09510-9,"Analysis of Technical, Pure Technical and Scale Efficiencies of Pakistan Railways Using Data Envelopment Analysis and Tobit Regression Model",December 2020,Khalid Mehmood Alam,Li Xuemei,Akber Aman Shah,Male,,Unknown,Mix,,
20.0,4.0,Networks and Spatial Economics,15 October 2020,https://link.springer.com/article/10.1007/s11067-020-09509-2,Modeling the Spatial Effects of Land-Use Patterns on Traffic Safety Using Geographically Weighted Poisson Regression,December 2020,Chengcheng Xu,Yuxuan Wang,Pan Liu,Unknown,Unknown,,Mix,,
20.0,4.0,Networks and Spatial Economics,18 October 2020,https://link.springer.com/article/10.1007/s11067-020-09502-9,Transmission Network Investment Using Incentive Regulation: A Disjunctive Programming Approach,December 2020,D. Khastieva,M. R. Hesamzadeh,J. Rosellón,Unknown,Unknown,Unknown,Unknown,,
21.0,1.0,Networks and Spatial Economics,09 November 2020,https://link.springer.com/article/10.1007/s11067-020-09505-6,Spatial Aggregation Issues in Traffic Assignment Models,March 2021,Ouassim Manout,Patrick Bonnel,François Pacull,Unknown,Male,Male,Male,"During the last 50 years, academic research has developed a plethora of transport modeling approaches (Bonnel 2004; Ortúzar and Willumsen 2011). From simplistic transport models of the 60s to current individual-centric modeling frameworks, research is pushing forward modeling sophistication and complexity. Despite this research agenda, transport models are still prone to bias. In several instances, these errors are seriously detrimental to the accuracy of transport models and thus to policy instruction. Ignoring these errors or neglecting their impacts may induce serious modeling errors, mislead policy decision, and ultimately endorse inefficient urban planning schemes with unpredictable economic and social costs (Dupuy 1975; Flyvbjerg et al. 2005; Skamris and Flyvbjerg 1997). The spatial aggregation problem is one of these enduring yet disregarded modeling issues (Manout and Bonnel 2019; Manout et al. 2019; Ortúzar and Willumsen 2011). The spatial aggregation problem is induced by the common practice of modeling continuous space as discrete. The majority of standard transport models, including some of the most sophisticated ones rely, by design, on this aggregate description. In the case of traffic assignment, this practice has 2 major consequences: centroid connectors and intrazonal trips. Centroid connectors are artificial network links introduced to attach zone centroids to transportation network. The definition of these links requires the choice of connection nodes and the computation of connectors’ travel times. This definition can have a major impact on assignment outcomes (Chang et al. 2002; Friedrich and Galster 2009; Jeon et al. 2012; Leurent et al. 2011; Manout et al. 2019; Sean Qian and Zhang 2012). Some of these impacts are: non-congested travel times when connectors have an unlimited capacity, artificial congestion around connection nodes when their number is insufficient, rerouting of flows, and an approximate loading of travel demand between adjacent-zones. These issues can be detrimental to traffic and transit assignment outcomes, including travel times and network flows. In the case of transit assignment, the extent of these implications has already been discussed and addressed in a previous paper (Manout et al. 2019). In this research, we focus on the impact of the unlimited-capacity assumption of connectors in traffic assignment. Intrazonal trips start and end at the same zone centroid. According to the standard assignment method, these trips cannot be assigned to the network since they have the same origin and destination. Some of the issues associated with intrazonal trips are: the omission of these trips from traffic assignment and the need for an off-line method to compute their travel times. Even if intrazonal journeys are short trips that count for little total mileage, these trips can account for an important part of travel demand. The impact of their omission from traffic assignment induces an artificial free-flow situation that biases main modeling outcomes including travel times and network flows. The importance of intrazonal trips and the impact of their omission have already been discussed in Manout and Bonnel (2019). The current paper builds on these findings to suggest new assignment models that address the omission of intrazonal trips. To avoid spatial aggregation issues, transportation research and manuals suggest using detailed zoning designs. Indeed, using fine-grained spatial units can minimize the loss of intrazonal trips and shorten the length of connectors and reduce their impact. Nevertheless, this approach avoids tackling the core of the problem, i.e. using an aggregate description of the continuous space, and addresses it only by minimizing its magnitude. Overmore, when refining the spatial design one should also refine the description of the network in order to ensure a minimum consistency between zonal and network descriptions (Bovy and Jansen 1983; Chang et al. 2002; Jeon et al. 2012). In this regard, spatial refinement comes at the price of increased costs of data collection, longer computational times, and tedious calibration efforts. Furthermore, the use of detailed spatial designs is not always possible nor desirable. In practice, data privacy issues are often a barrier to the use of micro-data. In other cases, modeling resolution is deliberately chosen to be coarse as in the case of strategic and regional transport models (Grange et al. 2008). In this case and others, one needs to directly tackle the problem of spatial aggregation. To address the spatial aggregation issue, this paper focuses on the omission of intrazonal trips in traffic assignment and the use of non-congested travel times in the definition of centroid connectors. In the next section, a brief literature review of solutions to the spatial aggregation problem in traffic assignment models is undertaken. 4 different traffic assignment strategies and 6 approaches are introduced in Section 3. These methods are designed to reduce the spatial aggregation bias by assigning intrazonal trips or by reducing the impact of their omission. In Section 4, 2 case studies are introduced to assess the contribution of these strategies. Section 5 explores in detail the contribution of each assignment strategy. Finally, findings of this research are discussed and some recommendations and future work suggestions are provided in Section 6.",1
21.0,1.0,Networks and Spatial Economics,11 November 2020,https://link.springer.com/article/10.1007/s11067-020-09512-7,Emergence and Dynamics of Short Food Supply Chains,March 2021,Arnaud Z. Dragicevic,,,Male,Unknown,Unknown,Male,"The present paper aims at analyzing the emergence and the dynamics of the French commodity production, marketed through short circuits, among which can be included the sectors of vegetable cultivation, beef and dairy cattle, honey products and of wine-growing. The study published by Pipame (2017) explains that a short distribution channel can be any marketing strategy based on maximum one intermediary between a producer and a final consumer. In the agribusiness, short food supply chains can thus be found in various forms such as producer-consumer, producer-retailer-consumer, producer-restaurateur-consumer or as that of producer-processor-consumer. The main objectives of short distribution channels—with respect to those involving more than one intermediary—is to provide local quality products to consumers and to supply products issued from environmentally sound practices or with a minimum carbon footprint. From the economic standpoint, short circuits ensure added value to the farmers and enable the consumers to support the local economy by sustaining small farms and businesses (ENRD 2012). According to Kneafsey et al. (2013), short circuits can be classified from the perspective of market structures. The so-called traditional short supply chain is farm-based and located in rural areas. The second category, termed neo-traditional, falls under the scope of complex collaborative networks. Those are off-farm based, with delivery schemes, in urban or peri-urban locations, where actors put forward their social and ethical values rather than the exclusive aim of making a profit. So as to encourage the development of such shortened supply chains, several European Union Member States have decided to implement ad hoc legal frameworks (Kneafsey et al. 2013). As an example of good practice, the French authorities have elaborated an Action Plan in 2009, the goal being to promote local food systems among the agricultural community (Barnier 2009). In terms of economic figures of sales made by small, medium and large French farming businesses, short circuit sales respectively accounted for 61.40%, 49.30% and for 29.40%. In 2010, the total value of agricultural production marketed through short food circuits was estimated to be of 6.70 billion euros. In 2015, its value weighed nearly 7.80 billion euros (Pipame 2017), up nearly 14.42% within five years. A relative proximity between a producer and a final consumer must also be taken account of. Inasmuch as the definition of short circuits does not explicitly highlight any local dimension, reducing the number of intermediate actors between producers and consumers implicitly encourages local consumption and therefore advocates opting for restricted geographical distances. Those enable to better integrate the agro-environmental issues and challenges incorporated by the European Common Agricultural Policy (Pipame 2017). Yet, the variety of circuits does not make it possible to affirm that they systematically have a better ecological footprint than the usual long circuits (Ademe 2017). In addition, economic stakeholders showcase the geographic proximity, in community-supported agriculture systems (Galt 2013), as a condition for creating social closeness and solidarity (Guiraud et al. 2014). Using economic sociology as a research framework, Chiffoleau et al. (2016) submitted an explanation, based on the network analysis of embeddedness, to the emergence of alternative local food networks. Thereby, the latter are embedded in close social relations, unlike the perception of agro-industrial production methods, which are gauged to disintegrate the food sector from social structures (Tregear 2011). Sociologists also refer to the works of White (2002), who explains, via the social construction of markets, that the simultaneous cooperation and competition between sellers have structured and provided stability to the local food market. From the historical viewpoint (Chiffoleau et al. 2016), the unexpected resurgence of short food supply chains demonstrates the need to better conceptualize the ways in which markets are constructed (Renting et al. 2003). This requires the availability of comprehensive, dynamic food network modeling tools, in particular by building on new analytic methods (Labarthe et al. 2007). If we now consider the field invested by the literature above-mentioned, a network consists of nodes which represent the entities of interest and of edges which embody their interactions. In this manner, Brinkley (2018) offered the first use of graph theory to explore relationships built through an alternative food network. If, additionally, we favor the objects of study as sets, such that the consideration is being given to the angle of a hypergraph based on collections of sets, local food systems can be represented as a tripartite matching market, built from hyperedges, between buyers, retailers and suppliers.Footnote 1 We then need to take into consideration a uniform hypergraph, where all edges have the same size (Alon and Yuster 2005). Such a system is regarded as efficient if it succeeds in coordinating the evolving behavior of economic agents through a steady pattern of connectivity. Connectivity can refer to any form of interaction or linkage (Dragicevic 2018). Its expression can thus take multiple forms, functioning as economic and commercial transactions (Stromquist 2002; Wenz and Levermann 2016). The performance of market networks, whether they be local food systems, hence depends on their ability to maintain their coordinative structure throughout the time span laid down by the beholder. Should the mechanism under consideration be envisaged as a trading partnership concluded at random, the resulting process is that of a stochastic matching, where the economic players become matched with some level of likelihood (Abreu and Manea 2012; Ashlagi et al. 2014; Che and Tercieux 2018). The stochastic matching comes from the fact that market exhibits significant supply and demand uncertainties (Ahumada and Villalobos 2011). This involves the construction of a random hypergraph (Ghosal et al. 2009), in which each hyperedge exists with some non-null probability. Stochastic matching markets are then in capacity to study the emergence of short circuits borne out from mutually beneficial trading activities. As well, the consideration of a time-evolving random matching, which leads to stochastic trading dynamics, is appropriate to study—over sufficiently long periods of time—their durability or, instead, their eventual briefness. Indeed, by taking into account their probabilistic resurgence, without omitting to subject them to time dynamics, our wish is to address the topic of short food supply chains in a holistic manner (Lozano 2007). In that, the emergence and the dynamics can behave both as stationary and nonstationary processes (Dragicevic and Shogren 2017). A better understanding of short-circuit architecture can also entail insights into its ability to grow (Brinkley 2018). De facto, by giving attention to random spatial patterns (Lepš and Kindlmann 1987), without forgetting to factor in the bounds springing up in such stochastic processes (Talagrand 2014), another of our objectives is to scrutinize the capacity of an established or hypothetical short-circuit market design to spread over some defined territory. Through the consideration of a stochastic matching model dependent on a probability function built from the Heron’s formula, we analyze the emergence and the dynamics of short agrifood sale circuits in form of time-evolving random hypergraphs. These marketing circuits, which are used as a supportive policy for promoting local food consumption, typify short distribution channels. Although bipartite matching can be easily derived from the framework, we move our focus on the matching of triplets of players representing buyers, retailers and sellers. Their conditional pairings with respect to standard and social preferences are both taken into consideration. The main advantage of tripartite matchings, that is, by introducing a retailer for example, is envisaged from the fact that consumers have difficulties in finding and spotting products on markets coming from short supply chains (Kneafsey et al. 2013). Our results show that the emergence of short food supply chains is triggered by a three-dimensional stochastic matching mechanism. Their time evolution is found to be governed by both stable and unstable dynamics, the latter being subject to bounded antiperiodic oscillations. Via the use of a Poisson process, we then redirect our interest toward spatial randomness and we provide information on the maximum number of circuits attainable in a defined territory. Their spatial evolution also happens to be ruled by stable and unstable dynamics. Furthermore, the model unveils a restricted spread of short circuits over the entire territory, which, in expectation, can only be a partial substitute for long supply chains. The outcomes show consistency with the agribusiness patterns currently observable in France. Section 2 provides a detailed description of the stochastic matching probability, the study of the time-evolution of its density function, and the evaluation of the capacity of short circuits to spread over a territory. Section 3 is devoted to illustrating simulation examples. Concluding remarks are given in Section 4.",5
21.0,1.0,Networks and Spatial Economics,20 November 2020,https://link.springer.com/article/10.1007/s11067-020-09503-8,Optimal Inventory Level Control and Replenishment Plan for Retailers,March 2021,Chih-Kang Lin,Shangyao Yan,Fei-Yen Hsiao,Unknown,Unknown,Unknown,Unknown,,
21.0,1.0,Networks and Spatial Economics,21 November 2020,https://link.springer.com/article/10.1007/s11067-020-09508-3,"Delay, Throughput and Emission Tradeoffs in Airport Runway Scheduling with Uncertainty Considerations",March 2021,Jianan Yin,Yuanyuan Ma,Hua Xie,Unknown,Unknown,,Mix,,
21.0,1.0,Networks and Spatial Economics,02 January 2021,https://link.springer.com/article/10.1007/s11067-020-09511-8,A Game-Theoretic Approach to the Freight Transportation Pricing Problem in the Presence of Intermodal Service Providers in a Competitive Market,March 2021,Mohammad Tamannaei,Hamid Zarei,Sajede Aminzadegan,Male,Male,Unknown,Male,"Freight transportation as an important part in supply chains plays a key role to coordinate the flow of shipments from suppliers to customers. Appropriate pricing policies in the freight transportation sector can help managers achieve an efficient and sustainable system, with acceptable levels of affordability and customers’ satisfaction. Changes in different parts of transportation costs such as fuel price and staff payments may lead to a change in demands, and consequently a change in the profitability of carriers. Therefore, service providers in freight transportation attempt to decrease their costs and set their prices in a way that maximizes their profits (Bhattacharya et al. 2014). Freight transportation pricing is a complicated issue, because it depends on various factors such as the type of products, the attributes of both the origin and the destination, available transportation modes, routes, and delivery factors (Azadian and Murat 2018). Nowadays, Logistics Service Providers (LSPs) are considered as major elements in supply chain management (Giri and Sarker 2017). An LSP performs logistics operations in place of its clients (Coyle et al. 1996; Panayides and So 2005). Transportation and warehousing are two common services of LSPs (Bourlakis and Melewar 2011). Suppliers need these providers to outsource some parts of their operations, concentrate on their main business, and effectively decrease their logistics responsibility burdens, costs, and delivery delays (Jharkharia and Shankar 2007; Aminzadegan et al. 2019; Tamannaei and Rasti-Barzoki 2019). Because of the competitive environment, survival of LSPs in the transportation market vastly depends on the level of satisfaction they can provide for their customers (Shi et al. 2016; Salleh and Dali 2009). Some criteria such as low price, appropriate credit, strong information technology infrastructure, and long-term relationships are regarded as necessary prerequisites that LSPs must prepare to attain their contributions in transportation markets (Gürcan et al. 2016). LSPs can be categorized by the transportation infrastructure they use. Some of them benefit from the competitive advantage of intermodal transportation infrastructures (Shinghal and Fowkes 2002). The establishment of such infrastructures requires huge initial investments but provides the advantage of reducing their costs and lets them play the role of “price-makers” in the transportation market (Lee et al. 2012). In this article, these LSPs are called “Intermodal Service Providers” (ISPs). On the other hand, there exist many local LSPs in the market that cannot benefit from intermodal services, and use only one freight transportation mode, usually the road transportation system. These local LSPs form a system called the Direct Transportation System (DTS). Since LSPs in the DTS cannot benefit from the advantage of intermodal infrastructures, their transportation costs are relatively high. In addition, the entry of a new LSP in the DTS does not require substantial investment, compared to the ISPs. Therefore, the DTS is populated with many LSPs, so there is intense competition in this market. Due to their high transportation costs and intense competition in the DTS, profit margins of LSPs are substantially low, and thus their prices are close to their transportation costs. In other words, LSPs in the DTS suffer low ability in making pricing decisions, and they are considered as “price-takers” in the transportation market (Nault and Dexter 2006; Berwick and Farooq 2003; Anderson and Wilson 2008). This article studies the pricing behaviours of ISPs for two different conditions in a competitive transportation market. Based on these conditions, mathematical programming models are proposed. In the first condition, there exists one ISP who competes with the DTS. In the second condition, the transportation market includes two ISPs, who compete with each other and with the DTS. During the past decade, many studies have focused on the freight transportation pricing problem. The interconnection of the problem with important aspects of transportation planning, such as mode selection, vehicle selection, route assignment, and allocation of products to vehicles, is one of the reasons that transportation pricing has become an attractive research topic. Some of the previous studies have developed optimization methods with considering LSPs in a supply chain system, in order to minimize the transportation costs or maximize the profit. In a study presented by Woo and Saghiri (Woo and Saghiri 2011), a fuzzy multiple-objective programming model is developed to consider suppliers and LSPs with main aspects of order assignment, holding, and ordering. Mahmoudzadeh, Mansour (Mahmoudzadeh et al. 2013) introduced a mixed-integer programming model for a product delivery problem by considering the location of distribution centers of LSPs, in order to decrease logistics service costs. Kuyzu, Akyol (Kuyzu et al. 2015) presented a bid price mathematical model to maximize the profit of the carriers by considering procurement auctions for a truck transportation system. They proposed an iterative coordinate search algorithm to solve their problem. Azadian and Murat (Azadian and Murat 2018) focused on financial sustainability of an air cargo transportation system by considering service location, and transportation pricing policies for a company. They proposed Benders decomposition and branch-and-bound methods to solve their problem. Shi, Zhang (Shi et al. 2016) developed a new structure of a transaction cost analysis for decreasing transportation service costs of LSPs. They examined third-party purchase as a value-added service offered by an LSP. In addition, some other studies modeled the intermodal transportation problem between air, road, rail, and maritime transportation modes by considering locations of distribution centers, and transportation pricing policies (Šakalys and Batarlienė 2017; Ghaderi et al. 2016; de Langen et al. 2017; Rodrigues et al. 2018; Gremm 2018). There exist some studies focusing on competition of different agents in freight transportation markets. In these studies, the game-theoretic approach is applied to model the pricing behavior of the LSPs and obtain the optimal pricing policies, aiming to maximize the logistics service profits of companies (Giri and Sarker 2017; Xu and Huang 2014; Jiang et al. 2014; Li et al. 2016; Yu and Xiao 2017; Jamali and Rasti-Barzoki 2019). The studies (Lim 2000; Zou et al. 2016; Genc and De Giovanni 2017; Nagurney et al. 2015) examined game theory to solve the pricing problems by considering LSPs for logistics operations to optimize their costs. Tsunoda (Tsunoda 2018) focused on competition between rail and air transportation systems with considering pricing policies for a government, a high speed rail operator, and an airline as the agents. He used the Bertrand and Hoteling models to solve the problem. Le Cadre, Mezghani (Le Cadre et al. 2018) modeled the interaction between distribution system operators (DSO) and transmission system operators (TSO), by considering both Nash and Stackelberg games. Lee, Boile (Lee et al. 2012) studied the competition between three sections of a freight transportation system. In their work, ocean carriers, port operators, and land carriers form a transportation system to serve customers. However, their competition is modeled to determine their equilibrium service prices and the routing decisions of carriers. The studies (Roy et al. 2016; Chen et al. 2018; Kuang et al. 2020) modeled the competition between dry ports to assess the strategic investment in a port–hinterland transportation network. The game-theoretic approach is applied in the studies (Sirikijpanichkul et al. 2007; Roumboutsos 2014; Xie et al. 2017; Holguín-Veras et al. 2006) to model the intermodal transportation problem by allocating distribution centers to LSPs. The game-theoretic approach is also used in the studies (Saeed 2013; Tawfik and Limbourg 2015; Zhang et al. 2018; Raturi and Verma 2020) to solve the intermodal transportation problem by considering transportation pricing and competition policies. In addition, some studies have developed bi-level programming approaches based on Stackelberg games, in order to model freight transportation in supply chain systems (Wang et al. 2017; Yue and You 2017; Amirtaheri et al. 2017; Goswami et al. 2020). A summary of the literature review is presented in Table 1. Due to cost heterogeneity of different modes of intermodal transportation, the unimodal pricing models are not appropriate to handle ISP pricing problems. According to our literature review, there exist few studies that consider the pricing strategy for intermodal transportation systems. In these studies, the problems are restricted to the competition between different providers of intermodal services. However, in the real-world condition, the freight between origins and destinations can be moved through either the services offered by ISPs, or a direct transportation system DTS without transshipment at any distribution center. Therefore, each ISP should select its equilibrium service price so as to maximize its profit while competing with both the rival ISPs and the DTS. To the best of our knowledge, no study has focused on the transportation pricing problem by considering competition between ISPs and DTS. The main contributions of the present article are as follows: A novel freight transportation pricing problem is developed, in which the competition between different ISPs with rail and road transportation services and the DTS is considered. Using a non-cooperative game-theoretic approach, multi-level mathematical models are proposed for two competitive conditions, in order to capture the interaction between two ISPs and a DTS. Based on a real-life case study and the concept of sustainability, some managerial insights are presented. The rest of the paper is structured as follows: the description of the problem is provided in Section 2. Model formulation and the proposed mathematical models are provided in Section 3. The discussions of the models based on a real-life case study are provided in Section 4. Finally, the article is concluded in Section 5.",16
21.0,1.0,Networks and Spatial Economics,13 January 2021,https://link.springer.com/article/10.1007/s11067-020-09514-5,Clean Water Network Design for Refugee Camps,March 2021,Özlem Karsu,Bahar Y. Kara,Aysu Ozel,Female,Female,Female,Female,"Post-disaster operations and related activities differ based on the nature and duration of the disasters (Holguín-Veras et al. 2012). For sudden onset disasters such as earthquakes, meeting urgent needs, reaching victims as soon as possible and handling the scarce resources under chaotic conditions are crucial. Therefore, mainly provisional solutions are found to respond to the urgency faster for shorter periods. For instance, shelter sites to be established after an earthquake are aimed to serve approximately one year. Thus, long-term infrastructures such as clean and drain water networks are not built, instead temporary solutions such as distribution of drinkable water and installation of toilet containers are implemented in these sites. On the contrary, recovery of long term disasters require more stable solutions which will respond to the needs for longer periods. Shelter sites to be established after a long term disaster necessitate additional and more durable features including infrastructure for clean and drain water. Due to the challenging conditions that the nature of disasters bring, effective planning of the available resources and operational efficiency are vital for long term solutions. The refugee crisis is one of the challenges of today’s world as a harsh long term disaster. It requires several actions that should be taken by countries providing asylum in co-operation with the Non-Governmental Organizations. One of the most important of those actions is establishing refugee camps in order to provide sheltering for refugees. Since the problems causing refugees to flee from their countries may remain for long periods, refugee camps may have to serve for many years. In the history, there are several examples of refugee camps that provide service for more than 20 years such as camps established in Bosnia and Herzegovina, Sudan and Afghanistan (UNHCR 2016). Therefore, refugee camps require long-term infrastructural planning. The purpose of this study is addressing the clean water distribution network design problem in refugee camps. The paper is organized as follows. In Section 2, the clean water network design problem for refugee camps is defined and the related literature is presented. Then, a bi-objective mathematical model is given and exact and heuristic solution methodologies are stated in Sections 3 and 4, respectively. In Section 5, a case study with real life data is presented including computational results. The paper ends with concluding remarks.",3
21.0,1.0,Networks and Spatial Economics,18 February 2021,https://link.springer.com/article/10.1007/s11067-021-09520-1,Latent Segmentation of Urban Space through Residential Location Choice,March 2021,Tomás Cox,Ricardo Hurtubia,,Male,Male,Unknown,Male,"Modelling households’ location decisions is key to understand past and future patterns of urban growth and change, which helps to plan transport and services infrastructure, and to design policies that guide towards a better and more sustainable urban development. The work by Alonso (1964) was the first to model the spatial distribution of different types of households according to their specific willingness to pay (WP) for a location as a function of its attributes, such as accessibility and built surface. In this model, each location is assumed to be auctioned, and the household with the highest bid (correlated with its WP) “wins” the location. This defines both the spatial distribution of households and the prices of real estate goods. Most present models of location choice are based on later formulations by McFadden (1978) and by Ellickson (1981). In both approaches, households evaluate each location in terms of its attributes and dwelling characteristics, and their probability of choosing the location depends on this evaluation (a utility function in McFadden’s and a WP function in Ellickson’s). In location choice models, WP functions are generally based on the interaction between a vector of location attributes and agent characteristics, and a vector of unobserved parameters that represent the marginal contribution of each attribute and characteristic to the WP. These parameters are specific to each type of agent and are modelled as to represent their preferences in the choice process. Besides demographic heterogeneity we can also find heterogeneity across (types of) places, known as spatial heterogeneity. As demographic or agent heterogeneity can be included by segmenting agents according to their characteristics, also spatial heterogeneity can be included by segmenting locations according to their spatial attributes and assigning segment-specific preferences. Spatial segmentation and how it affects valuation of attributes (preferences) is a complex issue, as it is part of the spatial cognition of city dwellers. If we want to achieve a spatial segmentation that maximizes the likelihood that parameters are representative of preferences in each zone, we cannot predefine zones only from differences in built or urban attributes as it is done, for example, in cluster analysis (Jain 2010; MacQueen 1967). Location preferences should play an active role in the spatial segmentation process, something that can’t be achieved when segmentation is defined before the estimation of preference parameters, but can be done with a joint estimation of both spatial segmentation and preferences. In order to do so, we propose a model based on the bid-auction approach, where agents not only have a WP function, but also have an heterogeneous perception of urban space which can be described by a spatial segmentation function, with parameters that are estimated jointly with the WP parameters, following a latent class modelling approach (Kamakura and Russell 1989). Latent classes in location choice have been used recently with interesting results (Cox and Hurtubia 2019; Ettema 2010; Liao et al. 2014; Lu et al. 2014; Olaru et al. 2011; Walker and Li 2007), but only applied to endogenous segmentation of decision makers (households, firms, real estate projects, etc.). The proposed approach is related to these models in its mathematical formulation but, since it uses a bid-auction framework, it enables to classify locations (space). This allows interpretation of preference parameters from a new perspective while, at the same time, provides a behavior-based method to build spatial submarkets, enabling a robust identification of spatially heterogeneous preferences in econometric models. This model presents a novel simultaneous approach to spatial segmentation and location choice, which we affirm is ahead of previous (two-step) zone-based segmentation methodologies, thanks to zones defined by a classification function based on parameters that are estimated in order to maximize the likelihood of reproducing the phenomenon. This method is also behavior-based, in a model formulation that follows agent segmentation process and is consistent with microeconomic theory. We apply the model to household location data for Santiago de Chile and compare results with those obtained when using a model with no segmentation, and other two models where segmentation is done in a first step (exogenous zones and cluster-based zones). Model comparison is done using a validation subsample. The paper is structured in eight sections. After this introduction, section two discusses the issue of spatial heterogeneity in general terms; section three presents the research opportunity and proposed approach; section four explains the mathematical formulation of the bid-auction localization model. Section five presents the proposed model, conceptually and mathematically. Section six presents the data and implementation of the proposed model, including the results and the comparison with other approaches. Part seven concludes the paper, and finally we include the estimation results of alternative models in the annex (section eight).",6
21.0,1.0,Networks and Spatial Economics,19 February 2021,https://link.springer.com/article/10.1007/s11067-020-09515-4,"Exploring Recovery Strategies for Optimal Interdependent
            Infrastructure Network Resilience",March 2021,Yasser Almoghathawi,Andrés D. González,Kash Barker,Male,Male,Unknown,Male,"Modern societies depend on the continuous and proper functioning of
                critical infrastructure networks such as transportation, telecommunications,
                electric power, natural gas, and water distribution, among others. Such
                infrastructure networks provide the fundamental services that support the economic
                productivity, security, and quality of life of citizens. However, infrastructure
                networks are subject to be affected by different types of disruptive events,
                including random failures, malevolent attacks, and natural disasters. Hence, for
                several years, the United States, as well as many countries around the globe, have
                been interested in effectively preparing for and responding in a timely manner to
                such disruptive events (e.g., “secure, functioning, and resilient critical
                infrastructures” (White House 2013)). Therefore, it is increasingly important to not only
                protect current infrastructure networks against disruption, but to be able to
                restore them once they have been disrupted. Several works in the literature have studied the restoration problem of
                an infrastructure network. They provide methods and algorithms with different
                objectives to restore the functionality of an infrastructure network following the
                occurrence of a disruptive event (e.g., Xu et al. 2007; Yan and Shih 2009; Matisziw et al. 2010; Nurre et al. 2012; Aksu and Ozdamar 2014; Vugrin et al. 2014; Kamamura et al. 2015; Fang et al. 2016; Hu et al. 2016; Fang and Sansavini 2017; Fu et al. 2017). However, infrastructure networks are not isolated from each
                other, but rather they rely on one another in different ways for their proper
                functioning. Hence, they exhibit interdependency, where two infrastructure networks
                are said to be interdependent if there is a bidirectional relationship between them
                through which the state of each infrastructure is dependent on the state of the
                other (Rinaldi et al. 2001). Rinaldi et
                al. (2001) classified the
                interdependencies between infrastructure networks into four categories: (i) physical
                interdependency, an output from an infrastructure network is an input to another one
                and vice versa, (ii) cyber interdependency, if an infrastructure network depends on
                information transmitted through an information infrastructure, (iii) geographical
                interdependency, if two infrastructure networks are affected by the same local
                disruptive event, and (iv) logical interdependency, all other types of
                interdependencies. In this paper, the authors consider the physical interdependency
                among different infrastructure networks. However, the work in this paper could be
                extended to consider other types of interdependencies such as geographical
                interdependency, which could be incorporated in this work by considering the
                co-location of disrupted components from multiple interdependent infrastructure
                networks. Moreover, other types of interdependencies (i.e., cyber and logical) could
                be incorporated as well as long as they can be described in a similar manner by the
                logic discussed in this work. Rinaldi (2004) categorized
                the models and techniques that consider interdependencies among infrastructure
                networks into six broad categories: (i) aggregate supply and demand tools (e.g., Lee
                et al. 2007; Min et al. 2007; Caschili et al. 2015), (ii) dynamic simulations (e.g.,
                Hernandez-Fajardo and Dueñas-Osorio 2013; Zhang et al. 2016), (iii) agent-based models (e.g., Panzieri et al.
                    2004; Oliva et al. 2010), (iv) physics-based models (e.g., An et
                al. 2003; Unsihuay et al. 2007), (v) population mobility models (e.g.,
                Casalicchio et al. 2009), and (vi)
                Leontief input-output models (e.g., Haimes and Jiang 2001; Reed et al. 2009). The model proposed in this paper falls in the aggregate
                supply and demand tools category by Rinaldi (2004), which evaluates the total demands, in the form of services
                or commodities, for an infrastructure network in a region and the ability to supply
                them. In addition, it is classified as a network-based approach according to a
                similar categorization by Ouyang (2014)
                for the approaches of modeling and simulation in infrastructure networks considering
                their interdependencies. The network-based approach (Ouyang 2014) describes the infrastructures as networks
                of nodes, links, and inter-links (i.e., nodes represent the different components of
                the infrastructures, links represent the physical relationship between the nodes,
                and inter-links represent the interdependencies among different
                infrastructures). Interdependencies across critical infrastructure networks can improve
                their operational efficiency since they generally lead to greater centralization of
                control, hence they play a significant role in the continuous, reliable operation of
                infrastructure network (Rinaldi et al. 2001). However, the proliferation of interdependencies among
                infrastructure networks may potentially cause them to be highly vulnerable to
                disruption. Consequently, if the operability of an infrastructure network is
                affected by the occurrence of a disruptive event, this could lead to cascading
                inoperability in some or all dependent infrastructure networks due to their
                interdependencies (Little 2002; Wallace
                et al. 2003; Buldyrev et al.
                    2010; Eusgeld et al. 2011). The high vulnerability of the
                infrastructure networks, due to their increased interdependencies, has been shown
                through several recent worldwide events, including the 1998 Canada ice storm (Chang
                et al. 2007), the 2001 US World Trade
                Center attack (Mendonça and Wallace 2006), the 2003 North American blackout (U.S.-Canada Power System
                Outage Task Force 2004), and the 2010
                Chile earthquake and tsunami (Wen et al. 2011), among others. Therefore, it is crucial for decision makers
                to account for interdependencies between infrastructure networks when preparing the
                plans for their recoverability to obtain a realistic analysis of their performance
                (Holden et al. 2013). In addition,
                performing restoration activities for each infrastructure network independently
                could lead to improper utilization of available resources, wasted time, and may even
                cause further disruptions when improperly scheduled (Baidya and Sun 2017). As a result, the restoration of such
                interdependent infrastructure networks following a disruptive event has become more
                challenging for decision makers as the increase in interdependency among
                infrastructure networks magnifies the complexity associated with planning for their
                post-disruption recovery and operation. The National Infrastructure Protection Plan (DHS 2013) highlights the importance of addressing
                the risks associated with the interdependencies among different infrastructure
                networks as being “essential to enhancing critical infrastructure security
                and resilience”. Hence, it is important to have resilient infrastructure networks accounting for the
                interdependencies between them, thus the motivation of this paper. The
                Infrastructure Security Partnership (2011) defined resilient infrastructure
                    networks as the infrastructure networks that would “prepare
                for, prevent, protect against, respond or mitigate any anticipated or unexpected
                significant threat or event” and “rapidly recover and reconstitute
                critical assets, operations, and services with minimum damage and
                disruption”. This paper addresses the interdependent networks restoration problem
                (INRP). INRP seeks to find the minimum-cost restoration strategy of a system of
                interdependent networks following the occurrence of a disruptive event that enhances
                its resilience considering the availability of time and resources. The goal of this
                paper is to help decision makers plan for recovery following the occurrence of a
                disruptive event, to procure strategies that center not only on recovering the
                system promptly, but also such that the weighted average performance of the system
                is maximized during the recovery process (i.e., enhancing its resilience).
                Accordingly, to solve the INRP, the authors propose a resilience-driven
                multi-objective optimization model, formulated using mixed-integer programming
                (MIP). Hence, the primary objective of this work is to: (i) prioritize the
                restoration of the disrupted components for each infrastructure network, and (ii)
                assign and schedule the prioritized networks components to the available work crews,
                such that the resilience of the system of interdependent infrastructure networks is
                enhanced considering a reduction in the performance of disrupted components based on
                multiple different disruption scenarios and taking into consideration the physical
                interdependency among networks. By studying the resilience of the interdependent
                infrastructure networks in this work, the authors unveil the effects on their
                performance of both the magnitude of the disruptive event (i.e., network
                vulnerability) and the trajectory of recovery of their disrupted components (i.e.,
                network recoverability). Note that in this paper we focus on improving the
                resilience of the system through actions that are made only after a disaster has
                occurred, as this mimics a plethora of realistic situations where disasters occur
                unexpectedly, and where all the decision center on actions that can recover the
                system efficiently. However, the proposed post-disaster model could be easily
                extended to also consider mitigation actions, such as retrofitting components or
                increasing the availability of resources. This paper builds upon initial work by Almoghathawi et al. (2019), which assumes (i) a binary status of the
                networks components (i.e., either fully disrupted or undisrupted), (ii) a
                restoration with a non-preemptive recovery process (i.e., work crews cannot switch
                between components during restoration), and (iii) completion dependence between
                nodes in interdependent networks (i.e., a dependent node cannot function unless the
                node or nodes that it depends on are completely functioning). This paper addresses
                these limitations and also explores recovery strategies based on different
                assumptions and considerations related to the assignment of the available work crews
                and the functionality of disrupted networks components. The remainder of the paper is organized as follows. Section 2 provides brief background about the restoration of
                interdependent networks, including an overview of network resilience and some of the
                most relevant works in the literature. Section 3 presents the proposed optimization model to solve the INRP,
                including notation, assumptions, objectives, and constraints used. In Section
                    4, an illustrative example is presented
                through a system of interdependent infrastructure networks in Shelby County, TN,
                affected by hypothetical earthquakes of different magnitude. Different
                considerations for the recovery process of the disrupted networks components are
                discussed in Section 5. Finally, Section
                    6 provides concluding remarks and
                suggests some relevant ideas for future work.",14
21.0,2.0,Networks and Spatial Economics,27 May 2021,https://link.springer.com/article/10.1007/s11067-021-09516-x,Uncertainties of Sub-Scaled Supply and Demand in Agent-Based Mobility Simulations with Queuing Traffic Model,June 2021,Aleksandr Saprykin,Ndaona Chokani,Reza S. Abhari,Male,Unknown,,Mix,,
21.0,2.0,Networks and Spatial Economics,03 March 2021,https://link.springer.com/article/10.1007/s11067-021-09517-w,New Inertial Projection Methods for Solving Multivalued Variational Inequality Problems Beyond Monotonicity,June 2021,Chinedu Izuchukwu,Yekini Shehu,,Unknown,Unknown,Unknown,Unknown,,
21.0,2.0,Networks and Spatial Economics,24 April 2021,https://link.springer.com/article/10.1007/s11067-021-09524-x,A Note on Solving Discretely-Constrained Nash-Cournot Games via Complementarity,June 2021,Dimitri J. Papageorgiou,Francisco Trespalacios,Stuart Harwood,Male,Male,Male,Male,"A Nash-Cournot game is a game-theoretical framework of imperfect competition in which multiple producers/players compete to optimize their individual objective functions, which also depend on other players’ production decisions. Traditional (i.e., purely continuous) Nash-Cournot problems have been extensively studied and it is well known that they can be expressed either as nonlinear complementarity or variational inequality problems (Facchinei and Pang 2007). Discretely-constrained Nash-Cournot (DC-NC) games arise when a subset of a player’s decisions are required to be discrete, for example, when a player must make a binary on/off decision. Such problems are becoming increasingly popular in day-ahead electricity markets where certain players, namely conventional generators like nuclear, coal, and natural gas plants, must submit bids that involve on/off decisions with substantial costs (Gabriel and Leuthold 2010; Garcia-Bertrand et al. 2005; Guo et al. 2020; Huppmann and Siddiqui 2018; Leuthold et al. 2012; Weinhold and Gabriel 2020). More general applications and theoretical investigations are discussed in (Fuller and Çelebi 2017; Gabriel et al. 2013; Guo et al. 2021; O’Neill et al. 2005; Pedroso and Smees 2014). Gabriel et al. (2013) approached discretely-constrained Nash-Cournot games by framing the problem as a discretely-constrained mixed complementarity problem (DC-MCP). We consider the same set up and, to the extent possible, the same notation as Gabriel et al. (2013). There are N players indexed by \(p \in \mathcal {P}=\{1,\dots ,N\}\). Player p optimizes her cost function \(f_{p}: {\mathbb {R}}^{n} \mapsto {\mathbb {R}}\) that depends on her decision vector \({\mathrm {x}_p} \in {\mathbb {R}}^{n_{p}}\) and the vector \(\mathrm {x}_{-p} = (\mathrm {x}_{1}, \dots , \mathrm {x}_{p-1}, \mathrm {x}_{p+1}, \dots , \mathrm {x}_{N})\) denoting the decisions of all other players besides player p. Here, \(n = {\sum }_{p \in \mathcal {P}} n_{p}\). Specifically, we assume that player p solves the following discretely-constrained optimization problem parameterized by x−p:
 where \(\mathcal {I}_p\), \(\mathcal {E}_p\), and \(\mathcal {D}_p\) denote the set of inequalities, equalities, and integer variables for player \(p \in \mathcal {P}\). Let \(\mathcal {X}_p = \{ \mathrm {x}_p \in {\mathbb {R}}^{n_{p}}\): Eq. 1b, 1c, 1d, 1e} denote the discretely-constrained feasible region for player \(p \in \mathcal {P}\) and let \(\mathcal {C}_p = \{ \mathrm {x}_p \in {\mathbb {R}}^{n_{p}}\): Eq. 1b, 1c, 1d} denote the continuous relaxation of \(\mathcal {X}_p\). A vector \(\hat {\mathrm {x}}\) is called a Nash equilibrium of this DC-NC game if \(\hat {\mathrm {x}}_{p} \in \mathcal {X}_p\) for all \(p \in \mathcal {P}\) and
 Gabriel et al. (2013) approach convex DC-NC games, i.e., games in which the continuous relaxation of each player’s optimization problem is a convex optimization problem, by applying the following four-step procedure: 1) relax the integrality constraints for each player; 2) write the KKT conditions for each player; 3) re-impose the integrality constraints; 4) solve the resulting DC-MCP. More concretely, since KKT conditions are neither necessary nor sufficient for a discrete optimization problem, Gabriel et al. (2013) attempt to find the set of Nash equilibria to Eq. 2 by appealing to the continuous relaxation of each player’s parametric optimization problem:
 Assume that the functions fp(⋅,x−p) are convex and a constraint qualification for the continuous relaxation \(\mathcal {C}_p\) holds. Then, the KKT conditions for player p’s relaxed problem (3) are to find \({\mathrm {x}_p} \in {\mathbb {R}}^{n_{p}}\), \(\lambda _{p} \in {\mathbb {R}}^{|{\mathcal {I}_p}|}\), \(\gamma _{p} \in {\mathbb {R}}^{|{\mathcal {E}_p}|}\) such that
 Gabriel et al. (2013) (p.313) then write:  “An interesting question is whether the set of xp that solves (4a), but with the discrete restrictions for \(x_{pr} \in \mathbb {Z}_{+}\) for \(r \in \mathcal {D}_p\), corresponds to the solution set of the original problem (2). The next result shows that this correspondence is correct.” Let \(\mathcal {S}^{\textrm {DC-Nash}}\) be the set of solutions to the discretely-constrained Nash-Cournot game (2) and \(\mathcal {S}^{\textrm {DC-MCP}}\) be the set of solutions to Eq. 4a for which \({\mathrm {x}_p}r \in {\mathbb {Z}}_{+}\) for \(r \in {\mathcal {D}_p}\). Then, \({\mathcal {S}^{\textrm {DC-Nash}}} = {\mathcal {S}^{\textrm {DC-MCP}}}\).",
21.0,2.0,Networks and Spatial Economics,19 March 2021,https://link.springer.com/article/10.1007/s11067-021-09525-w,Integrated Model for Timetabling and Circulation Planning on an Urban Rail Transit Line: a Coupled Network-Based Flow Formulation,June 2021,Pan Shang,Yu Yao,Pengli Mo,,,Unknown,Mix,,
21.0,2.0,Networks and Spatial Economics,26 March 2021,https://link.springer.com/article/10.1007/s11067-021-09526-9,Modeling Multi-Year Customers’ Considerations and Choices in China’s Auto Market Using Two-Stage Bipartite Network Analysis,June 2021,Youyi Bi,Yunjian Qiu,Wei Chen,Unknown,Unknown,,Mix,,
21.0,2.0,Networks and Spatial Economics,07 May 2021,https://link.springer.com/article/10.1007/s11067-021-09528-7,Maritime Traffic as a Complex Network: a Systematic Review,June 2021,Nicanor García Álvarez,Belarmino Adenso-Díaz,Laura Calzada-Infante,Male,Male,Female,Mix,,
21.0,2.0,Networks and Spatial Economics,25 May 2021,https://link.springer.com/article/10.1007/s11067-021-09529-6,How do All Roads Lead to Rome? The Story of Transportation Network Inducing Agglomeration,June 2021,Ahmed Saber Mahmud,,,Male,Unknown,Unknown,Male,"It is obvious from a cursory reading of U.S. economic history that part of the advantage of the manufacturing belt arose from the density of the railroad connecting the region’s cities, a density that was itself a product of the region’s manufacturing dominance. This transportation network deserves a little more attention (From Krugman (1991b) in “Geography and Trade” in pages 23-24). Despite being empirically evident, the theoretical literature associating urban agglomeration and transportation networks is scant.Footnote 1 We want to combine insights from network formation and economics of agglomeration to shed some light on this missing link. Urban areas are mainly production centers of manufacturing goods and rural areas of agricultural goods. Both regions need to connect to trade with one another. Network formation occurs in the presence of the cost of transporting manufacturing goods and the cost of forming links. The questions addressed in the paper are as follows: 
 What is the equilibrium rural-urban network when regions attempt to minimize transportation costs as well as the cost of forming links in a network? What is the incentive of manufacturing firms residing in urban areas to agglomerate within a given network? The paper illustrates how network structure itself can create agglomeration apart from the processes described in the literature. We also address the issue of necessary and sufficient conditions for agglomeration in a network. To the best of our knowledge, Mori and Nishikimi (2002) is the first model to combine transportation network and industrial agglomeration. The subsequent Mori (2012) illustrates how transport networks emerge due to scale economies in transportation among other factors. Their model illustrates how economies of transportation density induce agglomeration. Our framework describes how agglomeration occurs to minimize the costs of transportation and network.Footnote 2 We consider a nation as a collection of regions. Individuals inhabit each region, and every individual possesses the same amount of land in each region. The utility of the representative individual depends upon the consumption of agricultural goods (food) and manufacturing goods. The manufacturing good is a composite commodity consisting of many varieties. The agricultural good is homogenous. Agricultural production has diminishing returns in each region because land is an essential input; therefore, the production is not easily mobile. Manufacturing production, on the other hand can take place anywhere. Manufacturing goods have increasing returns to scale and require an abundant supply of labor. Transporting manufacturing goods from one region to another is costly. Since scale economies require a large pool of workers, urban areas emerge as production centers of manufacturing goods. The workers of urban regions have to be fed with agricultural products (food). However, the urban region cannot be self-sufficient because of the limited availability of land and diminishing returns. Hence, it is necessary to initiate trade amidst regions. To commence trade among regions, they need to form networks or build roads and highways, which is costly. The cost of transportation is higher if a manufacturing firm trades via other regions. Urban regions have two objectives in mind: to minimize the transportation cost of their products and minimize the cost of forming a network of the regions. The cost of transportation is minimal when a rural periphery is directly linked to all the manufacturing regions while the cost of forming links is the highest. What is the equilibrium network? In the first stage, the rural-urban network formation takes place. When the transaction cost saved from a direct link to a rural area is higher than the expense of the link itself, all regions are connected via direct links. If the cost of forming a link is higher, it becomes worthwhile to maintain a direct link to an urban area but not to a rural region. Therefore, rural areas are linked to a particular urban center, and the cities themselves form a complete network. When the cost of forming a direct connection rises even further, all regions become minimally connected. Even though network played a significant role in other social sciences such as sociology, equilibrium formation of network rose to prominence in economics after the landmark paper of Jackson and Wolinsky (1996). Jackson and Wolinsky (1996) had simple criteria to establish equilibrium network. According to their model, two agents establish a link if it is beneficial for both. They can unilaterally sever their link. Network formation is decided bilaterally, but the final payoffs depend upon the interactions of others. This approach has been widely applied to different contextsFootnote 3. What is the incentive of manufacturing firms to agglomerate? In the second stage, manufacturing firms decide to relocate. Previous models emphasized various agglomeration externalities such as the prospect of a better match, more efficient sharing of infrastructure, a larger pool of workers, and aggregate demand externality. The present endeavor focuses upon transportation network as the motivation behind agglomeration. One aspect of the transportation system is the transportation cost to rural regions. By locating to a single urban region, firms can access rural areas directly (without any intermediaries). Another impact of a mono-centric equilibrium stems from reducing the number of rural-urban roads and highways; i.e., reducing the network cost. The primary impetus behind agglomeration in our paper is to create a hub and spoke network. Therefore even when agglomeration externalities ceases to exist, urban agglomeration continues to occur via other channels. In urban economics and NEG, manufacturing firms concentrate on a single area because of agglomeration externality. According to NEG literature, the impetus for agglomeration lies in aggregate demand externality. The central message of this literature is best summarized by Krugman (1991b, page 15).Footnote 4 Given sufficiently strong economies of scale, each manufacturer wants to serve the national market from a single location. To minimize transportation costs, she chooses a location with large local demand. But local demand will be large precisely where the majority of manufacturers choose to locate. Thus there is a circularity that tends to keep a manufacturing belt in existence once it is established. Unlike the models of NEG, this paper focuses on the transportation network. Agglomeration in the present framework occurs even when the impact of aggregate demand externality is negated by other negative externalities such as congestion costs. One aspect of the transportation network is the transportation cost in rural regions. By locating to a single urban region, firms can access to rural areas directly (without any intermediaries). Another channel through which transportation network induces mono-centric equilibrium is from reducing the number of rural-urban roads and highways; i.e., reducing the network cost.",2
21.0,2.0,Networks and Spatial Economics,12 May 2021,https://link.springer.com/article/10.1007/s11067-021-09530-z,Assessing the Spatial Transferability of Freight (Trip) Generation Models across and within States of India: Empirical Evidence and Implications for Benefit Transfer,June 2021,Agnivesh Pani,Prasanta K. Sahu,Furqan A. Bhat,Unknown,Female,Unknown,Female,"In an era of limited survey resources and ever-growing demand for disaggregated travel data, investigating the spatial transferability of models has become a mainstay of travel demand analyses (Sikder and Pinjari 2013). The objective of such an investigation is to facilitate usage of formerly estimated model parameters and policy interventions in new application contexts with or without the usage of local data (Holguín-Veras et al. 2013). By transferring model parameters from an erstwhile estimation context, Metropolitan Planning Organizations (MPOs) can cut costs significantly in regions where they lack the institutional capacity and resources to undertake extensive data collection programs and model development (Wafa et al. 2016; Robson and Dixit 2017). These cost savings are particularly important for freight generation (FG) and freight trip generation (FTG) models, whose development typically requires substantial data inputs from establishment-based freight surveys (EBFS) and involves long production times (Giuliano et al. 2010). The proprietary nature of freight information, need for substantial interviewer training, falling response rates and large costs per unit response heighten the scarcity of the required EBFS data (Pani and Sahu 2019a), and by extension underline the importance of transferability assessment of FG/FTG models. While the recent literature abounds with model estimation studies for FG (Giuliano et al. 2010; Novak et al. 2011; Holguín-Veras et al. 2016; Pani et al. 2018), FTG (Holguín-Veras and Patil 2008; Holguín-Veras et al. 2012; van Lier et al. 2016; Alho and Silva J de 2017) and model diagnostic studies (Gonzalez-Feliu and Sánchez-Díaz 2019; Pani et al. 2019a; Pani and Sahu 2019b; Sahu et al. 2020), transferability assessment studies are a rarity. This research gap is particularly notable in large geographically diverse countries like India where models are only available for a handful of cities and regions (Pani et al. 2018, Pani and Sahu 2019b; Sahu and Pani 2019; Pani et al. 2020), despite having more than 40 cities with million-plus population (Arora 2018). In absence of local freight data, development of transport policies, mobility plans and logistics strategies for these cities hinges on transferring models from a base context that is “similar” to the local context. The similarity between local context and base context is traditionally identified exogenously using contextual determinants of travel, such as demographic characteristics (Mohammadian and Zhang 2007) and geographical determinants of travel such as spatial proximity (Johnston 2007) or existence within the same State (Sikder and Pinjari 2013). The problems with this approach of model transfer is that it presumes that models are inherently transferable, although there is contrasting evidence in literature about the spatial transferability of FG/FTG models. On one hand, ample empirical evidence highlight the presence of geographical disparities (Holl and Mariotti 2018; Pani and Sahu 2019c; Sahu and Pani 2019) and spatial effects (Sánchez-Díaz et al. 2016; Pani et al. 2018) that hinder the transferability of FG and FTG models. On the other hand, empirical investigations suggest that FG and FTG models are largely transferable across different States (Holguín-Veras et al. 2016). For instance, FTG models are found to be transferable between Seattle and New York since establishment-level models estimated are found to produce good estimates for FTG by super markets regardless of the dissimilarities in spatial context (Holguín-Veras et al. 2013). These contrasting findings may be linked to the fact that the similarity between estimation and application context is likely a multi-dimensional measure which latently dictates the spatial transferability of FG/FTG models. For instance, FMCG (fast moving consumer goods) establishments that pay for premium space to be located in a highly urbanized region may have different FG/FTG patterns than establishments located in less urbanized regions with lower land value. Similarly, a wood-product establishment located in a Port city can handle large volume of break-bulk commodities due to reduced logistics cost and, therefore, is likely to generate more freight movements than similar establishments in land-locked cities. Little is known, however, about the extent of transferability of FG/FTG models between different spatial contexts and performance of transferred models for various industry sectors. This paper contributes to this discernible gap in freight research by investigating the performance of directly transferred FG and FTG models (without updating the model coefficients) between three different regions in India. The effectiveness of transferability improvement techniques is subsequently assessed using locally collected freight data. The paper further examines transferability patterns across industry sectors, business size measures, and freight activity metrics. The remainder of the paper is structured as follows. The previous research on model transferability and the open questions that are addressed by this research are presented in the following section. The research context and data description are subsequently given in the third section, followed by a fourth section that elaborates the conceptual framework adopted for this study. The fifth section presents model estimation, transferability assessment and improvement and the final section concludes the paper.",11
21.0,3.0,Networks and Spatial Economics,07 May 2021,https://link.springer.com/article/10.1007/s11067-021-09535-8,A Study of Internet Development and Enterprise Financing in China,September 2021,Qiang Chen,Yabin Zhang,Lian Chen,,Unknown,,Mix,,
21.0,3.0,Networks and Spatial Economics,20 May 2021,https://link.springer.com/article/10.1007/s11067-021-09538-5,A New Flexible Parking Reservation Scheme for the Morning Commute under Limited Parking Supplies,September 2021,Wei Wu,Wei Liu,Vinayak Dixit,,,Unknown,Mix,,
21.0,3.0,Networks and Spatial Economics,20 May 2021,https://link.springer.com/article/10.1007/s11067-021-09536-7,A Game Theoretical Approach for Improving the Operational Efficiencies of Less-than-truckload Carriers Through Load Exchanges,September 2021,Baṣak Altan,Okan Örsan Özener,,Unknown,Male,Unknown,Male,"Among different modes of transportation, trucking carries the banner in the U.S. freight movement with a market share of 71.4%, which corresponds to $796.7 billion in 2018 (America Trucking Associations 2019). A broad classification of trucking services is based on shipment volumes: Truckload (TL) transportation and Less-than-Truckload (LTL) transportation. In TL transportation, carriers haul a full truckload of shipment that belongs to only one customer, on the other hand, in LTL transportation, carriers haul shipments from many different customers on one truck. The LTL transportation industry has reached annual revenue of $42.9 billion in 2018 with a revenue surge of 10.6% (Journal of Commerce 2019). The industry is still investigating opportunities to increase the yield to fuel up the growth further. To this end, this paper analyzes collaboration among LTL carriers to increase their operational efficiencies beyond their individual achievable levels. Collaborative approaches in transportation have attracted much attention both from practitioners and from academics. Several collaborative logistics practices that have been successfully implemented over the last decade. Nistevo, later acquired by IBMFootnote 1, and TransplaceFootnote 2 are two examples that offer Internet-based platforms for managing the logistics services. These platforms help companies to collaborate on day-to-day logistics activities both in TL transportation (identifying the best matching of loads with carriers’, minimizing the empty miles traveled by the carriers) and LTL transportation (collaborative load design and execution, maximizing the asset utilization with load consolidation with different shippers). Another example in practice is TrivizorFootnote 3, a European based company, which creates collaborative opportunities among the shippers in bundling and synchronizing their freight flows. All of these collaborative efforts require the application of well-designed and efficient optimization algorithms, as the underlying problems are usually more complex than the traditional ones. The literature addresses some of these collaborative problems. We review these studies in Section 2. Our study focuses on a collaborative problem under an LTL transportation setting, where carriers collaborate to improving their operational efficiencies via load exchanges. In LTL transportation, carriers consolidate shipments that do not have enough volume to utilize the vehicle capacity to the fullest. LTL carriers collect shipments from shippers, bundle the freight at a breakbulk terminal according to the destinations of the shipments, transfer it to another breakbulk terminal, and finally deliver each shipment to its consignee. Consequently, in LTL transportation, the network structure of the carrier has an immense impact on the cost of handling a shipment. Typically, the transit time and the distance of a shipment is higher than that of a direct shipment. Nevertheless, an LTL shipment is still the most cost-efficient option for most shipments under a certain volume due to the better utilization of the truck capacity. Needless to say, this cost efficiency depends on both the geographical and the volume-based synergies among the shipments. Unfortunately, neither shippers nor carriers can consider such synergies explicitly before making the shipment contracts as all depend on one another. Hence, a framework that reallocates the shipment requests among LTL carriers after the contractual phase might improve the operational efficiencies. In order to maximize operational efficiency, an individual LTL carrier should determine the best flow of shipments through its own network, which includes making two decisions simultaneously: optimal bundling of the shipments and optimal flow of these bundles through the hubs. Even the single carrier problem presents many challenges, and under a collaborative setting, these challenges significantly increase as in that case collective decisions should be made considering all the shipment requests and the hub locations of the carriers. The increased complexity is not only due to the increased size of the problem but also due to the difficulty in determining the best allocation of shipments to the carriers. For instance, a particular shipment may not be handled in the most cost-efficient manner by the carrier with the nearest hub locations to the shipment’s origin and destination due to insufficient shipment volume along the same route. Hence, for a collaboration of LTL carriers, determining the best assignment of shipment requests is not an easy task. A collaborative setting imposes two important structural problems other than identifying the optimal assignment of a shipment. The first problem is that, to foster voluntary participation, each carrier should receive a positive profit out of the collaboration. As each LTL carrier will attempt to collect the maximum profit, developing a desirable solution for all the participants requires a well-designed mechanism that maximizes the overall profit while considering the fact that each carrier is aiming to maximize its own profit. The second problem is about the trust issue among carriers, as these firms are competitors offering very similar services. The information sharing is crucial in collaborative approaches as without any information it is often quite difficult to identify beneficial collaborative opportunities. On the other hand, due to the trust issues, a well-designed framework should have minimum information requirements from the participating carriers. In this paper, we consider a setting where several LTL carriers try to identify shipments to be exchanged to improve the overall efficiency of their operations. The potential benefits of such a collaboration are due to better assignment of shipments considering their compatibilities and hence increased truck utilization across LTL carriers’ networks. Even though collective solutions are proven to provide higher profits to the participants by reducing the inefficiencies using a system-wide perspective, such solutions are often not attainable in real-life because the negotiating parties are seeking to maximize their individual profits rather than the overall profit. We propose a collaborative framework where the exchange interactions among the participants are modeled as a non-collaborative game. This game theoretical framework allows us to mimic the strategic behavior of the carriers in real-life where each participant is trying to force the exchange process to maximize the individual profits. In the end, the carriers reach an equilibrium point rather than a dictated solution. Due to the trust issues, the carriers might be unwilling to share confidential information, such as cost and load information, among each other. The exchange mechanism we propose is based on the cost estimation (expected cost) of the carriers rather than the actual cost values, hence no information on costs is required to be shared among participants. Additionally, the load/network information is assumed to be private for each carrier and the proposed mechanism only requires the information on the loads they offer at that particular exchange iteration. We also assume that the contracts are negotiated beforehand, and therefore the revenue received from the shipments belongs to the contractor carriers. However, the carriers may still have monetary exchanges among each other, which leads to two different exchange mechanisms. In the first exchange mechanism, we do not allow monetary exchanges between the participants. However, in the second exchange mechanism, the carriers are allowed to offer a side payment along with the load exchange proposal. Not surprisingly, the latter mechanism is more complex but offers higher profits to the participants. The final challenge in designing a collaborative framework is to identify satisfactory solutions within an acceptable computational time. Our benchmark for evaluating the solutions is the system-wide optimal solution. Unfortunately, in many cases, such a solution is not available due to the size of the problem. This is another reason why a centralized solution is not a viable alternative. In such cases, we use the lower bound as the benchmark, which underestimates the performance of our proposed method, especially on larger instances. Nevertheless, we show that our proposed method can identify significantly better solutions compared to individual management of the LTL networks and in fact in most cases, we find solutions within 3% of the optimal solution with a reasonable computational time.",
21.0,3.0,Networks and Spatial Economics,25 May 2021,https://link.springer.com/article/10.1007/s11067-021-09531-y,Exploring Multiple‐discreteness in Freight Transport. A Multiple Discrete Extreme Value Model Application for Grain Consolidators in Argentina,September 2021,Rodrigo J. Tapia,Gerard de Jong,Helena B. Bettella Cybis,Male,Male,Female,Mix,,
21.0,3.0,Networks and Spatial Economics,02 June 2021,https://link.springer.com/article/10.1007/s11067-021-09537-6,Household Activity Pattern Problem with Autonomous Vehicles,September 2021,Yashar Khayati,Jee Eun Kang,Chase Murray,Male,,Male,Mix,,
21.0,3.0,Networks and Spatial Economics,10 July 2021,https://link.springer.com/article/10.1007/s11067-021-09549-2,Incorporating Price-Dependent Demands into a Multi-Echelon Closed-Loop Network Considering the Lost Sales and Backorders: a Case Study of Wireless Network,September 2021,Behnam Vahdani,Elham Ahmadzadeh,,Male,Female,Unknown,Mix,,
21.0,3.0,Networks and Spatial Economics,10 July 2021,https://link.springer.com/article/10.1007/s11067-021-09527-8,The Core of the Global Corporate Network,September 2021,Ricardo Giglio,Thomas Lux,,Male,Male,Unknown,Male,"The structure and formation of corporate networks has intrigued researchers from diverse scientific fields and the public in general for a long time. Corporate governance structures can be cast into the format of a network due to the fact that often members of a corporate board are serving on the boards of two or more companies, thus generating a network of connections between different companies. This network has been characterized as belonging to the class of small world networks (see, for instance, Battiston and Catanzaro 2004, or Sankowska and Siudak 2016). Consequently, only a few degrees of personal connections are necessary to transverse the whole network.Footnote 1 Some questions naturally arise: Are the network structures the result of purposeful creation of links by some influential elite? Or are they just unintended outcomes from multiple individual and isolated choices? Regardless of the academic discussion, there is also a growing public concern that a relatively small group of people have close personal inter-connections while managing/controlling together a large share of the world economy. The basic empirical question posed by these different viewpoints is whether there is something unexpected in the network structure of board interlocks, i.e. structural features that could not be explained by a simple null model of appointments of board members and that would require the development of more involved sociological and/or economic models of how firms choose their board members with an eye on their connections to other boards. The present paper contributes to the recently burgeoning literature on the structural features of corporate board interlocks. In particular, we revisit the network properties of corporate boards using a nearly comprehensive worldwide database, that also allows us to analyze interlocks across national borders besides domestic ones. Namely, we have gathered all board members listed in Bloomberg’s archive of company profiles. The number of companies for which board information is available, amounts to almost 100,000. This is more than twice the number of listed companies worldwide. Although it seems impossible to identify by what criteria their large data set has been constructed from the even larger universe of all corporate entities worldwide, it is likely that it covers completely all firms beyond a certain size in countries that are integrated in the world economy to a certain degree and that it might constitute a more incomplete sample of the lower end of the size distribution of firms. However, we are mainly interested in those companies whose boards have overlaps with other firms and are not completely isolated. Their number in the database is almost 82,000 and since these will be the more visible ones it seems likely that the data base covers this interesting subsample to a very large extent, if not almost completely. Following the approach of Milakoviç et al. (2010), we confirm their results of a highly significant deviation from a random null model both for many of the countries included in our data base and for the aggregate worldwide board network as a whole. We also investigate this issue from a slightly different angle by studying the so-called “rich club” phenomenon. The “rich club” statistics quantifies the tendency of the dominant members of a network (in terms of their degree) to form tightly connected communities. Again, we find strong indication of the presence of this phenomenon in the international board network. Taken together, our results, thus, indicate that not only is the accumulation of multiple memberships a non-random event, but also that directors with a high number of seats are unexpectedly highly connected (i.e. have more joint appointments than expected by random assignments, even taking into account the higher chances of overlaps for directors with multiple positions). Beside these key results the paper also provides some additional insights into the features of the worldwide board network in terms of the role of different countries and economic sectors. The paper proceeds as follows: The following section offers an overview over related previous research on board interlocks from a network perspective. Section 3 provides information on the data set used and some descriptive statistics. Section 4 defines the network statistics used in our analysis, and Section 5 presents our empirical results on interlocks at the national and international level, and on the importance and degree of international integration of different countries and economic sectors in the worldwide board network. Section 6 discusses potential explanations of our findings and Section 7 concludes.",1
21.0,3.0,Networks and Spatial Economics,14 July 2021,https://link.springer.com/article/10.1007/s11067-021-09547-4,Multi-Attribute Community Detection in International Trade Network,September 2021,Rosanna Grassi,Paolo Bartesaghi,Gian Paolo Clemente,Female,Male,Male,Mix,,
21.0,3.0,Networks and Spatial Economics,14 July 2021,https://link.springer.com/article/10.1007/s11067-021-09548-3,Computing Dynamic User Equilibrium on Large-Scale Networks Without Knowing Global Parameters,September 2021,Duong Viet Thong,Aviv Gibali,Phan Tu Vuong,,Male,,Mix,,
21.0,3.0,Networks and Spatial Economics,02 July 2021,https://link.springer.com/article/10.1007/s11067-021-09544-7,Correction to: Strategic Assessment of Lisbon’s Accessibility and Mobility Problems from an Equity Perspective,September 2021,Camila Soares Henrique Fontenele Garcia,Rosário Macário,Carlos Felipe Grangeiro Loureiro,Female,,Male,Mix,,
21.0,4.0,Networks and Spatial Economics,17 July 2021,https://link.springer.com/article/10.1007/s11067-021-09545-6,Adaptive Park-and-ride Choice on Time-dependent Stochastic Multimodal Transportation Network,December 2021,Pramesh Kumar,Alireza Khani,,Unknown,Unknown,Unknown,Unknown,,
21.0,4.0,Networks and Spatial Economics,07 August 2021,https://link.springer.com/article/10.1007/s11067-021-09523-y,A Dynamic Tree Algorithm for Peer-to-Peer Ridesharing Matching,December 2021,Rui Yao,Shlomo Bekhor,,Male,Male,Unknown,Male,"Traffic congestion is one of the major issues in many cities around the world. Not only it increases travel times and travel costs directly, but also imposes severe environmental problems to the public. There are several possible ways to alleviate congestion, and one of them is to increase vehicle occupancy. For example, the average vehicle occupancy rate is only around 1.5 in the USA (Federal Highway Administration 2018), which implies that many vehicles are traveling with empty seats. Since infrastructure expansion will not solve traffic congestions entirely, it is important to utilize the road infrastructure more efficiently. Another way to alleviate congestion is to plan and operate public transport systems, which helps reducing private vehicle usage. Public transport vehicles typically take multiple passengers on board, and therefore can alleviate traffic congestions and vehicle emissions by reducing vehicle kilometers traveled (VKT). However, transit lines lack geographical and temporal flexibilities, because of their fixed routes and schedules. Moreover, it is costly and time consuming to increase public transit capacity and coverage. Although smarter, more efficient, and affordable new public transport options such as customized bus services (Qiu et al. 2014; Tong et al. 2017) are available, operators are still challenged to keep up with the demand. Taxi services, on the other hand, provide on-demand door-to-door transportation with high flexibility, but typically are more expensive and may worsen traffic congestion because of empty trips looking for passengers. Emerging communication technologies and smartphone applications boost new alternatives for urban transportation. Innovative shared mobility services address the gap of conventional alternatives. They aim to provide more flexible mobility options than conventional public transits, and more economical alternatives than taxis, by reducing capital investments and operational costs. One of the shared mobility alternatives is ride-sourcing. Service providers, such as Uber, Lyft, and DiDi, rely on smartphone applications to dispatch self-employed drivers. Essentially, ride-sourcing services are alternative taxi services with reduced fares, while online ride-sourcing companies use drivers’ personal vehicles instead of owning vehicles. Passengers pay directly through the application to the drivers, and service providers typically keep a percentage of each fare and some other commission fees. However, there has been criticism that current ride-sourcing services are contributing to the growth of vehicles kilometer traveled in cities, because of empty car trips to pick up passengers and the profit-driven objectives of their matching algorithms (Clewlow and Mishra 2017; Erhardt et al. 2019). Therefore, another form of shared mobility, namely peer-to-peer ridesharing, aims to provide flexible on-demand mobility with similar quality of service as ride-sourcing, while able to address traffic congestions and reduce vehicle emissions. Peer-to-Peer ridesharing is based on the concept of shared economy (Lessig 2008), in which collaborative consumption is split-up into single parts by the activities of sharing. In the context of peer-to-peer ridesharing, drivers are compensated for extra detours and to cover partially (or fully) their costs, the costs are then split between passengers who are provided with on-demand flexible transport with cheaper cost than taxi and reduce vehicle kilometer travel. Different from ride-sourcing, peer-to-peer ridesharing drivers have their designated destinations and want to arrive no later than some time. The trip purposes of peer-to-peer ridesharing drivers are to perform activities other than only pick up and drop off passengers, i.e., they are not dedicated drivers. Peer-to-peer ridesharing services are also provided by commercial companies, for example, Grab Hitch and DiDi Hitch. These services can be categorized into commercial, for-profit services, in which the commission fees are calculated based on the percentage of the fare. As a result, for-profit services may not maximize the potential of peer-to-peer ridesharing in reducing traffic and improving efficiency. To fully utilize the potential of peer-to-peer ridesharing in terms of social objectives to reduce traffic congestions and environmental impacts, non-profit (or fixed-fee) peer-to-peer ridesharing services can be provided by public (or private) entities to complement public transit by serving different trip types with private vehicles (Feigon and Murphy 2016; Hall et al. 2018), while being able to further reduce vehicle kilometer travel. A variety of technical challenges are imposed on the implementation of on-demand peer-to-peer ridesharing services. Specifically, the dynamic nature of peer-to-peer ridesharing makes the matching of drivers and passengers crucial to the success of the services. If we assume that drivers and passengers travel together between the same origins and destinations, the problem is relatively simple (Bahat and Bekhor 2016). However, if passengers are scattered in the network the problem becomes particularly complicated. The matching algorithm needs to determine which passengers are served by the driver and a service sequence that satisfies different constraints also should be provided. This problem is related to the well-known dial-a-ride problem (DARP), which also tries to find the optimal pickup and delivery sequence for a given set of passengers. This paper focuses on the peer-to-peer ridesharing matching problem. A mathematical formulation of on-demand peer-to-peer ridesharing matching problem based on a social objective to minimize the overall vehicle kilometer traveled is provided, and an efficient dynamic tree algorithm is catered for the on-demand ridesharing matching problem. Given the relatively extensive literature on the subject, we first provide a concise literature review in the next section, which will help to pinpoint the contributions of the present paper. The ridesharing matching problem can be viewed as a generalization of the DARP which is well studied in the literature. Psaraftis (1980) modeled the DARP with a single vehicle under static and dynamic settings. Savelsbergh and Sol (1995) formulated the basic form of DARP, where all vehicles depart and arrive at the same depot. In a more realistic DARP setting, time window constraints for the passenger requests (Psaraftis 1983), multiple depots for the vehicles (Cordeau and Laporte 2007), and heterogeneity of vehicles capacities and number of passengers in a request (Braekers et al. 2014) are included. Note that despite the similarities between DARP and ridesharing matching problems, there exists a key difference between on-demand peer-to-peer ridesharing services and dial-a-ride services: instead of being employees of the system in a dial-a-ride service, ridesharing drivers are considered as clients who have their distinct origins, destinations, and time constraints. Different approaches are developed in the literature to solve DARP. Cordeau (2006) introduced branch and cut (B&C) algorithm by applying several families of valid inequalities as cuts. Ropke et al. (2007) also introduced reachability constraints from the vehicle routing problem (VRP) with time windows to the B&C algorithm. Other heuristics and metaheuristics were further developed to handle large-size DARP. For example, simple insertion heuristics were proposed to quickly find feasible solutions where each request is inserted in the vehicle’s schedule by the cheapest insertion criterion (Jaw et al. 1986; Wong et al. 2014; Xiang et al. 2008). Cordeau and Laporte (2003) introduced Tabu search by locating requests in different neighbors with additional heuristic diversification strategies. Ridesharing systems are presented in different forms. In the simplest form of ridesharing, the system will try to match each driver with one passenger only (Agatz et al. 2011). Yan et al. (2019) also studied the single passenger ridesharing stochastic at a macroscopic level with user equilibrium. To fully utilize the unused capacity of the vehicles, more sophisticated ridesharing systems allow multiple passengers assigned to a single vehicle (Alonso-Mora et al. 2017; Herbawi and Weber 2012; Simonetto et al. 2019) or even allow passengers transfer between vehicles (Herbawi and Weber 2011; 2012; Masoud and Jayakrishnan 2017), solving these forms of ridesharing matching problem may require exploration of large decision space. In this paper, we focus on the multi-passenger ridesharing matching problem without the need for the passenger to transfer between vehicles. The ridesharing matching problem contains mainly the passenger-driver assignment problem and the VRP/DARP. Because of the similarity between DARP and ridesharing matching problem, solution algorithms for DARP can be applicable to the ridesharing matching problem. More efficient algorithms accounting for the dynamic nature of ridesharing matching problem were also developed in the literature. In terms of algorithmic implementations, solution approaches can be categorized into one-to-many algorithms and many-to-many algorithms. One-to-many algorithms are passenger-oriented, since they try to find the best vehicle with respect to the objectives for a given passenger, and adjust the route to serve the given passenger. Many-to-many matching algorithms consider a group of drivers and passengers at the same time, in which multiple passengers are matched with drivers while satisfying time window and capacity constraints. Since many-to-many matching algorithms optimize the global matching between drivers and passengers for a period of time, they have the potential to achieve better matches than one-to-many algorithms, which only optimize the local matching for a single passenger. However, since many-to-many algorithms are intrinsically more complex, efficient approaches to solve the problem still require further investigations. There are several one-to-many algorithms developed in the literature. The approach proposed by Huang et al. (2014) calculates the costs of all feasible permutations and finds the exact least-cost feasible vehicle schedule. Moreover, a dynamic tree structure that maintains only valid vehicle schedules were implemented. In this way, only the feasible subset of all possible permutations needs to be considered. Different heuristics were proposed to speed up the ridesharing matching process. For example, Ma et al. (2015) suggested nearest neighborhood vehicle search heuristic based on spatial indexing, and Jung et al. (2016) proposed a hybrid-simulated annealing method where Euclidean distance is used as an insertion heuristic to maintain the solution feasibility. Different approaches were proposed to solve the ridesharing matching problem using many-to-many algorithms. Early works were only able to solve small size problems in a reasonable amount of time. Baldacci et al. (2004) formulated the problem into two integer linear programs and proposed a bounding heuristic to solve the problem. Herbawi and Weber (2012) solve the matching problem using genetic algorithm. ﻿ Santos and Xavier (2013) applied greedy randomized adaptive search in the ridesharing matching problem by adaptively constructing diverse initial solutions. Recent works have developed more efficient algorithms in the context of real-time applications. In Agatz et al. (2011), the single-passenger ridesharing matching problem is formulated and solved by maximum weighted bipartite matching. To reduce the search space, Masoud and Jayakrishnan (2017) introduced the ellipsoid spatiotemporal accessibility method (ESTAM) to construct the passenger’s time expanded feasible network and solved the matching problem using dynamic programing; Najmi et al. (2017) proposed a clustering heuristic based on Euclidean distance to reduce the problem size. In Simonetto et al. (2019), the ridesharing matching problem is simplified to a single-passenger assignment problem and solved using linear programming. Alonso-Mora et al. (2017) proposed a general many-to-many dynamic multi-passenger vehicle assignment framework by reducing the problem to a passenger-combination (or trip) to driver matching problem. Their approach decouples the problem by first checking the shareability of passengers and drivers based on the idea of shareability networks (Santi et al. 2014). The algorithm starts by constructing a pairwise passenger-driver graph in which passengers connect to the same driver have the potential to share the ride; second, feasible passenger combinations are generated for each driver; third, an integer linear program (ILP) is solved to match passenger-combination to drivers. Recent studies extended the matching problem and corresponding solution algorithms. For example, investigating the stability of the matching between passengers and drivers (Wang et al. 2018), improving the traffic efficiency with advanced travel time feedback (Wu et al. 2019), and optimizing the many-to-many matching time interval and matching radius (Yang et al. 2020). Studies concerning the effects of transportation services on emergency events, such as epidemic disease spreading (Chen et al. 2020a; b) were also recently investigated in the literature. In this paper, we consider a flexible setting, in which drivers are willing to make a detour to cover both the pickup and drop-off locations of the passengers. Potential ridesharing participants announce their trips to the system as a passenger or a driver at a time close to their desired departure time. A trip announcement will include the origin-destination locations and its specific time windows. With this given information, the ridesharing system will try to assign multiple passengers to each vehicle and determine the detour to pick up and drop off the passengers. Our proposed dynamic tree algorithm falls in the category of many-to-many algorithms, in which multiple passengers are considered and assigned to drivers at the same time. Our proposed framework includes a preprocessing procedure and an efficient ridesharing VRP algorithm which makes our algorithm more efficient. The contributions of the present paper are outlined as follows: We propose a geometric pruning procedure to efficiently eliminate the infeasible passenger requests for each driver based on the spatiotemporal proximity of both drivers and passengers. This approach is able to accommodate the multi-passenger multi-driver problem with the introduction of reachable pickup region of the passenger as a circle. Candidate passengers are filtered by finding the intersection between reachable pickup regions and accessible regions of drivers. We adapt and integrate a dynamic tree structure to solve the ridesharing VRP. Within the many-to-many framework, our proposed dynamic tree algorithm is able to efficiently find high-quality matches for all drivers and passengers, instead of local matches for each passenger. Moreover, our algorithm is more general than Huang et al. (2014) in the sense that any number of passengers can be assigned to a vehicle (provided there are available seats). Additional passengers can be added to the vehicle that currently has no empty seat after a drop-off occurs and further utilize the vehicle capacity. The dynamic tree algorithm utilizes previous routing computations, and significantly improves computation times. Results obtained using simple networks show that the dynamic tree algorithm reaches the same objective function values of the exact algorithm, but with significantly shorter runtimes. In addition to the algorithmic contributions, we perform simulation studies to assess the impact of ridesharing services on the overall transportation network performance, using the well-known Winnipeg network.",10
21.0,4.0,Networks and Spatial Economics,08 October 2021,https://link.springer.com/article/10.1007/s11067-021-09550-9,Heuristic Methods for Minimum-Cost Pipeline Network Design – a Node Valency Transfer Metaheuristic,December 2021,Christopher Yeates,Cornelia Schmidt-Hattenberger,David Bruhn,Male,Female,Male,Mix,,
21.0,4.0,Networks and Spatial Economics,11 October 2021,https://link.springer.com/article/10.1007/s11067-021-09554-5,Extragradient Algorithm for Solving Pseudomonotone Equilibrium Problem with Bregman Distance in Reflexive Banach Spaces,December 2021,Lateef Olakunle Jolaoso,Christian Chibueze Okeke,Yekini Shehu,Unknown,Male,Unknown,Male,"Let C be a nonempty, closed and convex subset of a real reflexive Banach space E with dual E∗. We denote the norm and duality pairing between E and E∗ by ||⋅|| and 〈⋅,⋅〉 respectively. Let \(g:E \times E \to \mathbb {R}\) be a bifunction such that g(x,x) = 0 for all x ∈ E. We consider the Equilibrium Problem (shortly EP) in the sense of Fan (1972) and Brézis et al. (1972) which is defined as follows:
 We denote the set of solutions of the EP Eq. 1 by EP(g). It is well known that several problems arising in nonlinear analysis, such as optimization problems, variational inequalities, fixed point problems, complementarity problems, saddle point problems and Nash equilibria can be formulated as EP. More so, the EP has found several applications in many fields of applied sciences such as physics, engineering, game theory, transportation, economics, game theory, operation research, etc (see, Muu and Quoc 2009; Muu and Oettli1992). Equilibrium problems have applications to spatial price equilibrium models, computer and electric networks, market behavior, economic and financial network models. For example, the spatial price equilibrium models arising from equilibrium problems have provided basis for the analysis of competitive systems over space and time; and have fundamental contributions which have stimulated the development of new methodologies and uncovered vistas for applications in agriculture, energy markets, mineral economics, and finance. For more details on the applications of equilibrium problem theory to the study of networks and spatial economics, see, for example, Nagurney (1992). One of the important methods for solving the EP when g is pseudomonotone is the Extragradient Method (EM) which requires solving two strongly convex optimization problems in each iteration. The EM was originally designed for solving the saddle point problem by Korpelevich (1976) and Antipin (1976) independently. In 2010, Quoc et al. (2008) extended this idea to solves the pseudomonotone EP and introduced the following iterative scheme:
 where \(0 < \lambda < {\min \limits } \left \{\frac {1}{2c_{1}},\frac {1}{2c_{2}}\right \},\) c1 and c2 are the Lipschitz-like constants of the bifunction. They proved that sequence generated by Eq. 2 converges weakly to the solution of EP Eq. 1. This method has further been extended to infinite dimensional settings, see, for instance (Anh and An 2015; 2019; Bigi et al. 2009; Bigi and Passacantando 2015; Hieu 2018a; Hieu et al. 2018; Jolaoso and Aphane 2020a; ur Rehman et al. 2019; Vuong et al. 2013; Vuong 2018). Hieu (2017) introduced a subgradient extragradient method by considering a half-space in the second optimization problem. In Lyashko and Semenov (2016), the authors proposed a Popov’s type extragradient algorithm for solving the pseudomonotone EP as follows:
 Algorithm 1 was considered as an improvement of the extragradient and subgradient extragradient methods because in each iteration, the bifunction g was evaluated at a single point in yn in C. This makes the algorithm advantageous when the bifunction g has a complex structure. The authors also proved that the sequences {xn} and {yn} generated by Algorithm 1 converge weakly to a solution of the pseudomonotone EP. Hieu (2018b) further considered Algorithm 1 together with the subgradient extragradient and introduced the following new algorithm:
 Clearly, Algorithm 2 is most suitable than the previous algorithms if the feasible set C and the bifunction g have complex structures. The author in Hieu (2018b) proved that the sequences {xn} and {yn} generated by Algorithm 2 converge weakly to a solution of the EP provided that the stepsize satisfies \(0 < \lambda < \frac {1}{2(c_{1}+c_{2})}.\) Also, Kassay et al. (2018) modified Algorithm 2 and proved some weak and strong convergence results for solving the EP in real Hilbert spaces. More recently, Ogbuisi (2019) extends this result to a 2-uniformly convex and uniformly smooth Banach space using the following distance function \(\phi :E\times E \to \mathbb {R}\) defined by ϕ(x,y) = ||x||2 − 2〈x,Jy〉 + ||y||2, for all x,y ∈ E and J is the normalized duality mapping. He also proved that both sequences {xn} and {yn} generated by this process converges weakly to a solution of the EP Eq. 1 provided that the stepsize satisfies \(0< \lambda < \frac {\kappa }{4c_{2}+2c_{1}},\) where κ is the 2-uniformly convexity constant of E. We note that the convergence of the methods mentioned above require prior estimates of the Lipschitz-like constants c1 and c2 of the bifunction, which are not easy to be determined in general. Moreover, the stepsize defined by these processes are often too small and deteriorate the convergence of the algorithms. This observation leads us to the following natural question:  Question: Can we improve the above methods by considering a new Popov’s type extragradient algorithm which does not require prior estimates of the Lipschitz-like constants for solving the pseudomonotone EP in reflexive Banach spaces? In this paper, we give an affirmative answer to this question using a Bregman distance technique. We emphasize here that using a Bregman distance will allow the algorithm to be more flexible in computing the strongly convex optimization problems. More so, it generalizes the Euclidean norm distance and Lyapunov function used in the above-mentioned papers. The stepsize for our algorithm is selected self-adaptively and varies from each iteration to the other. This allows the algorithm to be computed more easily without prior estimate of the Lipschitz-like constants. Furthermore, this approach is more efficient that the line searching technique commonly used by some authors (see for instance (Hieu and Strodiot 2018; Jolaoso et al. 2020)) which uses inner-loops and may consume higher execution time. We also illustrate the performance of our algorithm using some numerical experiments. The rest of the paper is organized as follows: In Section 2, we give some preliminary results which will be used in the sequel. In Section 3, we present our algorithm and its convergence analysis. In Section 4, we give some application of our result to solving generalized Nash equilibrium problem in real Banach spaces. In Section 5, we discuss some numerical experiments to show the applicability and efficiency of the proposed method.",
21.0,4.0,Networks and Spatial Economics,15 October 2021,https://link.springer.com/article/10.1007/s11067-021-09552-7,Regularization Proximal Method for Monotone Variational Inclusions,December 2021,Dang Van Hieu,Pham Ky Anh,Nguyen Hai Ha,,Unknown,Unknown,Mix,,
21.0,4.0,Networks and Spatial Economics,30 October 2021,https://link.springer.com/article/10.1007/s11067-021-09555-4,Independent Retailer Restocking Choices in Urban Goods Movement and Interaction Effects with Traditional Markets,December 2021,Taufiq Suryo Nugroho,Chandra Balijepalli,Anthony Whiteing,Male,,Male,Mix,,
22.0,1.0,Networks and Spatial Economics,04 January 2022,https://link.springer.com/article/10.1007/s11067-021-09546-5,An Elastic Demand Model for Locating Electric Vehicle Charging Stations,March 2022,Xu Ouyang,Min Xu,Bojian Zhou,,,Unknown,Mix,,
22.0,1.0,Networks and Spatial Economics,24 January 2022,https://link.springer.com/article/10.1007/s11067-021-09553-6,A Granular Local Search Matheuristic for a Heterogeneous Fleet Vehicle Routing Problem with Stochastic Travel Times,March 2022,Ramon Faganello Fachini,Vinícius Amaral Armentano,Franklina Maria Bragion Toledo,Male,Unknown,Unknown,Male,"The freight distribution industry is not only essential to modern society but also responsible for significant revenues. For example, Fedex Ground reports revenue of 18.395 billion US dollars stemming from the pickup and delivery of small packages in the United States and Canada in 2018 (Fedex 2018). The vehicle routing problem (VRP) lies at the center of this industry and focuses on providing a minimum-cost service to a set of geographically dispersed customers by employing a limited and capacitated fleet of vehicles that is located at a central depot. Each customer can only be visited once by a single vehicle and all routes must start and finish at the depot (Dantzig and Ramser 1959). Given the VRP relevance, academics have developed a large amount of work targeting this problem and its extensions (Braekers et al. 2016; Golden et al. 2008; Toth and Vigo 2014). The VRP becomes more realistic when the fleets are heterogeneous, i.e., vehicles differ in their capacities and costs (Taillard 1999). Hoff et al. (2010) analyze industrial aspects of heterogeneous fleets and stress that vehicles usually present different characteristics in practice since fleets are acquired over long periods and automotive technology continuously evolves. Moreover, companies often keep a varied set of vehicle types due to the need for flexibility. For thorough reviews on heterogeneous fleet VRP (HFVRP), see Baldacci et al. (2008) and Koç et al. (2016). Another important feature of the VRP is the time window constraints that impose a time interval for collecting or delivering of goods at each customer (Desaulniers et al. 2014). The literature classifies these constraints as hard or soft. In the first case, vehicles can arrive early and wait, but the service must start within the time windows. On the other hand, the second case represents a more realistic situation in which the time windows can be violated at penalty costs for early or late customer service. A particular case of the VRP with soft time windows (VRPSTW), which intensifies its practical perspective, is the VRP with flexible time windows (VRPFlexTW). In this problem, introduced by Taş et al. (2014c), early and late service deviations at each customer are bounded by an outer or flexible time window generated with respect to the length of the original or hard time window. The carrier pays a penalty cost only if the arrival time takes place outside the hard time window, but within its deviation bounds. The interested reader is referred to Salani and Battarra (2018) for a recent survey of the VRPSTW. Both HFVRP and VRPFlexTW assume that all the information necessary to formulate the problems is known and readily available. However, in real-life applications, there are situations where the parameters of such problems have a stochastic nature (Gendreau et al. 2014). In particular, travel times are most subject to uncertainty caused by weather conditions, car accidents and traffic congestion (Gendreau et al. 2016). This uncertainty entails further feasibility conditions and additional costs. Therefore, ignoring the stochastic nature of travel times may result in arbitrarily bad VRP solutions. Despite the practical relevance of the previously mentioned VRP attributes, namely heterogeneous fleets, flexible time windows and stochastic travel times, no research has simultaneously addressed all of them. The first HFVRP variant that incorporates flexible time windows is proposed by Firouzi et al. (2018), whereas the only stochastic version of such a problem is the mixed fleet stochastic VRP investigated by Teodorović et al. (1995). The latter involves an unlimited fleet of heterogeneous vehicles and customers with stochastic demands. Apart from these studies, the closest related literature focuses on the VRPSTW under travel time uncertainty. Ando and Taniguchi (2006) suggest a model for this problem where each vehicle can make multiple routes per day within a given scheduling horizon. The objective is to minimize the sum of vehicle fixed costs, operating costs and expected earliness and lateness costs. Triangular distributions are estimated by using real data to describe the stochastic travel times, and a genetic algorithm is devised to handle the problem. Russell and Urban (2008) deal with a VRPSTW in which travel times are random variables modeled with the Erlang distribution, a special case of the gamma distribution with an integer shape parameter. The authors minimize a weighted average of three objectives, specifically: the number of vehicles employed, the total distance traveled, and the expected earliness and lateness penalties, calculated by closed-form expressions that reflect constant, linear or quadratic costs. The problem is solved in three phases. The first two phases build an initial solution and improve it through tabu search algorithms, respectively, while the third optimizes the waiting time before each customer using a generalized reduced gradient method. Li et al. (2010) propose two formulations of a VRP variant in which both travel and service times are normally distributed random variables. The first includes hard time windows modeled as chance constraints, whereas the second considers soft time windows and corresponds to a two-stage stochastic program with recourse which designs routes in the first stage aiming to minimize the expected costs of driver’s remuneration and late arrivals in the second stage. These models are solved by a tabu search metaheuristic that incorporates a Monte Carlo simulation procedure to estimate the expected values and probabilities of the stochastic parameters. The assumption that travel times follow a normal distribution is also employed by Thompson et al. (2011). The authors present alternative formulations based on stochastic programming and robust optimization of the VRPSTW under travel time uncertainty. Because the former requires expensive numerical integrations, the latter is used in a case study conducted by the authors. Another VRPSTW with stochastic travel and service times is analyzed by Zhang et al. (2013). They suggest a new stochastic programming model that incorporates service level constraints to ensure a minimum on-time arrival probability at each customer and a hierarchical objective function to minimize three components, namely: vehicle fixed costs, mean total travel time, and the weighted sum of costs stemming from earliness, lateness and route duration excess. An approximation method called \(\alpha\)-discrete, which estimates the vehicle arrival time distributions at customers, is embedded in an iterated tabu search algorithm to solve the problem. Experiments were conducted with travel and service times following lognormal and normal distributions, respectively. The more recent studies on VRPSTW with stochastic travel times are from Taş and colleagues (Taş et al. 2013, 2014a, b). Taş et al. (2013) develop a three-phase method. An initialization algorithm obtains a starting feasible solution in the first phase. A tabu search metaheuristic improves such a solution in the second phase, and a post-optimization procedure is called in the third phase to adjust the departure times of vehicles from the depot to minimize penalties due to time window violations. The same problem is exactly solved by Taş et al. (2014b) through a branch-and-price solution approach. Taş et al. (2014a) further extended the VRPSTW to incorporate time-dependent and stochastic travel times. They adapt the three-phase method given in Taş et al. (2013) and implement an adaptive large neighborhood search metaheuristic to tackle such a problem. In these three works, travel times are assumed to be gamma distributed. This paper deals with the industrially relevant variant of VRP called heterogeneous fixed fleet VRP with flexible time windows and stochastic travel times (HFFVRP-FlexTW-STT). The objective is to minimize the sum of the transportation costs and service costs. The transportation costs comprise the vehicle fixed costs and route variable costs, while service costs correspond to the penalty costs for violating customer time windows. These two costs provide an easy way of exploring the trade-offs between the expenses of the carrier company and the customer service reliability as pointed out by Taş et al. (2013). The main contributions of this paper are fivefold: i) to the best of our knowledge, the proposition of the first mathematical formulation for the HFFVRP-FlexTW-STT; ii) the development of a two-stage stochastic mixed-integer program with recourse, where the assignment of customers to vehicles make up the first stage, and recourse decisions are made in the second stage to find minimum-cost routes for vehicles according to observed travel times; iii) the suggestion of a scenario generation procedure which describes stochastic travel times using the Burr type XII distribution (Burr 1942), which has been shown to represent variations in day-to-day travel times better than other distributions such as lognormal, gamma, Weibull and normal (Susilawati et al. 2013; Taylor 2017); iv) the design of a novel granular local search matheuristic to solve the problem; v) extensive computational experimentation on benchmark instances and the assessment of benefits gained by flexible windows and stochastic travel times. The computational experiments show that our matheuristic outperforms a state-of-art mathematical programming solver in terms of solution quality and runtime. The proposed solution method also overcomes an alternative heuristic algorithm based on the augmented Lagrangian relaxation. Furthermore, the results underline the advantages of both flexible windows and stochastic travel times. The former drastically reduce the solution’s overall cost, whereas the latter are critical for their feasibility in a stochastic environment. The remainder of the article is organized as follows. Section 2 introduces the problem description. Section 3 describes the granular local search matheuristic. Computational results are reported in Sect. 4, and conclusions are outlined in Sect. 5.",
22.0,1.0,Networks and Spatial Economics,26 January 2022,https://link.springer.com/article/10.1007/s11067-021-09556-3,The Network Structure of Innovation Networks,March 2022,Shixun Wang,Lihong Yang,,Unknown,Unknown,Unknown,Unknown,,
22.0,1.0,Networks and Spatial Economics,07 March 2022,https://link.springer.com/article/10.1007/s11067-022-09558-9,Targeted Advertising in the Public Transit Network Using Smart Card Data,March 2022,Hamed Faroqi,Mahmoud Mesbah,Ali Khodaii,Male,Male,Male,Male,"Targeted advertising exposes relevant advertisements to groups of audiences at the right time and location. The targeted transit advertising model decides on optimum location and time slots for purposes of advertisements in the public transit network in a way that a maximum number of passengers watch advertisements relevant to their trip characteristics with minimum cost for advertising companies. Companies compete to attract more customers by improving their services and communicating that to the customers through advertising. Advertising companies try to target more potential customers at less cost. Also, from the customers’ viewpoint, it is difficult to find their needs among the advertisements that are implemented everywhere, unless they can find necessary advertisements when they need them (De Mooij 2013). Therefore, the targeted advertising model can simultaneously increase the efficiency and decrease the waste (exposing the advertisements to irrelevant audiences) of the advertisements through exposing the advertisements to relevant potential customers when/where they look for the advertisements. Targeted advertising in the public transit network could be beneficial for both public transit authorities and advertising companies. Public transit authorities have limited sources of revenue, such as fares, advertising, or government subsidies. Increasing fares or asking for more subsidies/budget has always been challenging. However, advertising, as one of the main revenue sources, can compensate for running and developing the public transit network (Santos et al. 2010; Paez et al. 2012). The authorities can make the transit infrastructure more attractive for the advertising companies to attract more investment. Also, on the other side, advertising companies are interested in transit passengers because they usually are a considerable part of potential customers. Therefore, targeted advertising can be an effective method to make the public transit network more attractive for advertising companies and attract more revenue for public transit authorities. Transit smart card data provide a unique opportunity to reconstruct passengers’ activities and trips in the public transit network. Although social and demographic characteristics of passengers are missing from most of the datasets, they can still reconstruct the activities and trips in the network. (Pelletier et al. 2011; Bagchi and White 2005; Hajdu et al. 2018; Faroqi et al. 2018a). Also, activities of individuals could be considered as a representative for individuals’ lifestyle since there are indirect relations between passengers’ activity and trips with their sociodemographic characteristics, which could be assumed as passengers’ lifestyle (Li and Du 2012; Zhang and Cheng 2018, 2019; Faroqi et al. 2018c). Therefore, passengers with similar activities can be targeted at the location and time of the activity or en route to the activity in accordance with their activity type. The public transit networks move millions of individuals (or potential consumers) daily in Australia, but the share of transit adverting is just around 1% of the total spending in the Australian advertising market (OMA Australia 2018). One of the reasons for the low share of transit advertising is the lack of consumer profiles, i.e. it is difficult to run targeted advertising (Wang et al. 2019). On the other hand, online advertising covers 44% of the total spending in the Australian advertising market, which is because it is easier to track users’ characteristics and behaviour on the Internet through developing online targeted advertising techniques. In addition, traditional transit advertising distributes advertisements according to the location of advertisements or time of day (not personalized according to the actual activity of individuals). For instance, a billboard in a more affluent area displays advertisements for more expensive products/services than a billboard in a poorer area, or during weekends, advertisements tend to display more recreational advertisements than weekdays. The traditional transit advertising ignores the fact that passengers may use a stop at a specific time of day for transferring purposes, not for performing their activities; however, the targeted transit advertising can consider the actual origin and destination stops of passengers besides activities of passengers at the destination stops. Therefore, connecting the public transit context and targeted advertising techniques would bring more interests to transit advertising from the marketing companies. Advertising companies desire to target more potential customers with less cost. Minimum cost with maximum coverage is an ideal option for the advertising companies; however, these two are conflicting objectives because increasing the coverage usually costs more for the companies. Also, activity-trip groups in the network might have a partial overlap. For instance, two groups might have a part of their trips, just between two stops, in common. Also, during a trip, an advertisement can be displayed inside the vehicle or at boarding/alighting stops. Different types of advertisements have different costs. For example, implementing an advertisement on a billboard is more expensive than at a bus stop. In addition, an advertisement on a bus or at a stop targets specific number of passengers, which is the coverage of the advertisement. For instance, targeting two groups in the overlapped part of their trips can be a better option (more coverage with less cost) than targeting them separately at their activity location. Therefore, advertisements must be distributed between the activity-trip groups in a way that has the maximum possible coverage with the minimum possible cost. This paper proposes a targeted advertising model in the public transit network (it could also be called “targeted transit advertising model”). It targets groups of passengers in the public transit network according to their activities and trips. Every targeted group includes passengers with similar activities (considering the type, location, and time of the activity) and trips (considering spatial and temporal dimensions of the trip). An agglomerative hierarchical clustering method is used to discover activity-trip groups of passengers according to the defined activity and trip similarity measures. Then, the model allocates advertisements to all discovered activity-trip groups aiming at maximizing the coverage and minimizing the cost of the advertisements. The targeted advertising problem is defined as a multi-objective optimization problem, and the NSGA-II algorithm is used to solve it. The main practical contribution of this paper is connecting targeted advertising techniques and public transit context. The main scientific contributions of this paper are formulating the targeted advertising in the public transit network problem, suggesting a solution approach for the problem, and extracting passengers’ profiles from the smart card data. The rest of the paper is structured as follows. The next section reviews the literature. Then, the targeted advertising problem is defined, after that the solution approach is described and followed by a section for presenting the results. Finally, the conclusions are presented.",2
22.0,1.0,Networks and Spatial Economics,30 March 2022,https://link.springer.com/article/10.1007/s11067-021-09557-2,Time Evolution of City Distributions in Germany,March 2022,Kiyohiro Ikeda,Minoru Osawa,Yuki Takayama,Unknown,,,Mix,,
22.0,1.0,Networks and Spatial Economics,08 April 2022,https://link.springer.com/article/10.1007/s11067-022-09562-z,Reflected Iterative Method for Non-Monotone Equilibrium Problems with Applications to Nash-Cournot Equilibrium Models,March 2022,Yekini Shehu,Lulu Liu,Qiao-Li Dong,Unknown,Male,,Mix,,
22.0,1.0,Networks and Spatial Economics,14 April 2022,https://link.springer.com/article/10.1007/s11067-022-09561-0,Associations Between Street Connectivity and Poverty,March 2022,Francisco Benita,,,Male,Unknown,Unknown,Male,"Streets and street networks are fundamental urban form elements that accommodate not only transit but also are source of social life. It is through the streets system that residents experience daily commuting interactions between their homes and the rest of the city. Areas with highly connected street networks have been associated with more densities of commercial and service activities (Porta et al. 2009; Rui and Ban 2014; Benita and Piliouras 2020); ease commuting between places as more destinations can be reached by more and shorter routes (Jiang and Claramunt 2004); and elevated residential property prices (Xiao et al. 2016). Alternatively, neighborhoods with poor accessibility to transit services combined with other forms of social and economic disadvantage, such as poor health or low rates of car ownership, can result in transport disadvantage (Currie et al. 2010; Büttner et al. 2018). Research into street networks have also been focusing on its negative effects on environment, human health, safety or livability. Ewing et al. (2003) found evidence that the urban sprawl directly related to traffic fatalities and pedestrian fatalities in the United States (US). Similarly, the relationship between the urban form and air quality has attracted the attention of urban planners and policymakers. Meanwhile studies (Lu and Liu 2016) have linked poor outdoor air quality to traffic processes, additional arguments explaining the associations between connectivity and the Urban Heat Island effect, e.g., the temperature difference between an urban area and its surrounding rural areas, are presented in Sobstyl et al. (2018). With respect to the safety of communities, many geometrical and topological morphologies are believed to relate with criminal behavior (Mao et al. 2021). The study of Haberman and Kelsay (2021) examined the case of Cincinnati, Ohio, and concluded that blocks located more central to the network were busier, possessed higher utilization potential and generated more robbery opportunities. All in all, the net effect of street connectivity is a trade-off between the positive and negative impacts. Important studies (Lucas 2011; Thornton et al. 2016) have found evidence of systematic practices to exclude poor neighborhoods from the urban fabric through renewal projects that create not only physical barriers but also economic and social isolation. To date, there is increasing interest among local transport planners and urban policymakers to carefully address such outspread phenomenon occurring throughout both the developed and developing world. Transport accessibility is widely regarded as vital to unlocking economic growth, and improving general livability and well-being (Cervero 2009). But probably is less clear the relationship between the urban form (also called built environment, land use development patterns or spatial development) and the geography of poverty. It translates to hidden complex and multifaceted situations which are result of a long process of construction developed over centuries. In the US, for example, the new urbanism practiced since 1940s has advocated for more transit-oriented spatial developments that are believed to be supportive of less car-dependent lifestyle (Srinivasan 2002). Nonetheless, meanwhile new urbanist features such as walkable neighborhoods or mixed land uses have become common in America, a large set of suburbs have experiencing rising rates of poverty (Markley 2018). This is partially explained by the gentrification process which has brought many changes in local landscapes by increasing goods, services and renovated buildings on the one hand, but also social displacement on the other. Large urban revitalization projects that have sized prominent parts of Washington, D.C., New York, Atlanta, or San Francisco have led to various forms of segregation among vulnerable population such as renters and low-income neighborhood residents (Zuk et al. 2018). Main metropolitan areas of Latin America, which are more compact and denser than their counterparts North America, have suffered from similar changes (Duque et al. 2019). The increase in the spatial concentration of poverty in the region over past decades has been associated with transport disadvantage and social exclusion (Guzman et al. 2017; Benita 2019) resulting in poor job accessibility, low employment rates, lack of social networking opportunities, and ultimately, a vicious circle of poverty difficult to break out. This spatial segregation of income classes has been subject of numerous in-depth social and urban research studies. The case of Mexico is relevant for at least two reasons. First, the country is southern neighbor of the US, and second, policies in both nations are closely aligned in terms of trade, economic growth, and urban development. In contrast to the US, however, more than half of Mexicans still live below the poverty line. In this regard, by computing indicators of street network connectivity as proxies for urban form, this study narrows the focus to the associations between the spatial development and poverty in the US and Mexico. We use official poverty data provided by the Census Bureau from each country and correlate poverty rates with six connectivity indicators, namely: street density, intersection density, regularity, betweenness, information and closeness centrality. As such, we contribute to the debate by offering new evidence and methodological elements for the study of how topological and geometrical features of the road network influence poverty. Although previous published research (Cervero 2009; Carpenter and Peponis 2010; Psarra et al. 2013; Thornton et al. 2016; Guzman et al. 2017; Büttner et al. 2018; Benita 2019; Duque et al. 2019) is likely to provide some guidance on the role the network plays in how cities prosper and thrive or shrink and decline, most of these studies are case or context specific, and backed by evidence limited to selected states, cities or neighborhoods. Ours is based upon comparable datasets and spatial statistical models that allowed us the identification of macroscopic and microscopic interactions. At the macro level, we looked at associations between the built space within counties/municipalities and its respective fraction of the population living below the poverty level. At the micro level, we looked at census tracts and their localized connectivity within metropolitan areas. This fine spatial resolution approach quantifies how Mexico, and potentially other Latin American nations, differs from the US. At the macro level, some of the street connectivity indicators correlate to poverty rates but perhaps more importantly, whether they increase or decrease the likelihood of poverty is different between the two countries. Although one might think that these neighboring countries would have similar results for how connectivity and poverty are linked, this paper shows an opposite relationship. In the case of Mexico, results suggest that poor neighborhoods suffer from low intersection density and low access to influential avenues compared to richer areas of the city. This difference in accessibility is far more skewed than the inequalities arising in US cities.",2
22.0,2.0,Networks and Spatial Economics,17 May 2022,https://link.springer.com/article/10.1007/s11067-022-09567-8,"Editorial Paper of the Special Issue on “Variational Inequalities, Nash Equilibrium Problems and Applications”",June 2022,Sebastiano Battiato,Patrizia Daniele,Laura Rosa Maria Scrimali,Male,Female,Female,Mix,,
22.0,2.0,Networks and Spatial Economics,20 March 2019,https://link.springer.com/article/10.1007/s11067-019-09458-5,Dynamic Spatial Equilibrium Models: an Application to the Natural Gas Spot Markets,June 2022,E. Allevi,A. Gnudi,G. Oggioni,Unknown,Unknown,Unknown,Unknown,,
22.0,2.0,Networks and Spatial Economics,30 July 2019,https://link.springer.com/article/10.1007/s11067-019-09475-4,Optimal Control of the Mean Field Equilibrium for a Pedestrian Tourists’ Flow Model,June 2022,Fabio Bagagiolo,Silvia Faggian,Raffaele Pesenti,Male,Female,Male,Mix,,
22.0,2.0,Networks and Spatial Economics,25 April 2019,https://link.springer.com/article/10.1007/s11067-019-09460-x,A Mathematical Network Model and a Solution Algorithm for IaaS Cloud Computing,June 2022,Gabriella Colajanni,Patrizia Daniele,,Female,Female,Unknown,Female,"According to the National Institute of Standards and Technology (NIST, see Mell and Grance (2011)): ‘Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This cloud model is composed of five essential characteristics, three service models, and four deployment models.” The essential characteristics are: On-demand self-service, Broad network access, Resource pooling, Rapid elasticity and Measured service (see He and Peeta (2014) and (Moya-Gomez et al. 2017) for Dynamic Accessibility using Big Data and Dynamic Resource Allocation Problem). Cloud Computing offers all the advantages of a cost-effective system, in terms of convenience, flexibility, and proven delivery platform for providing business or consumer IT services over the Internet. However, Cloud Computing presents an added level of risk because essential services are often outsourced to a third party so there are still some challenges to be solved, not least of which are: privacy and cybersecurity (see Chen and Zhao (2012), Nagurney (2015), Nagurney et al. (2017), and Pearson et al. (2009)). According to NIST there are three Service Models (see Calheiros et al. (2011), Litoiu et al. (2010), Mell and Grance (2011), and Zhang et al. (2010)): 
 Software as a Service (SaaS): a software distribution model in which applications are hosted by a vendor or a service provider and made available to customers over a network, typically the Internet. Users gain the access to application software and databases but they do not manage or control the underlying cloud infrastructure and platform where the application runs such as the network, the servers, the operating systems, the storage. Platform as a Service (PaaS): it can be defined as a computing platform in which developers can build and deploy web applications quickly and easily on a hosted infrastructure using programming languages, libraries, services, and tools supported by the provider but without the complexity of buying and maintaining the software and infrastructure underneath it. In other words, PaaS allows them to leverage the seemingly infinite compute resources of a cloud infrastructure. Infrastructure as a Service (IaaS): it is a way of delivering Cloud Computing infrastructure such as servers, storage, network and operating systems (usually in terms of virtual machines) that provides virtualized computing resources over the Internet as an on-demand service. Rather than purchasing servers, software, datacenter space or network equipment, clients, on the contrary, buy those resources as a fully outsourced service on demand. Therefore, IaaS refers to online services that abstract the user from the details of infrastructure like physical computing resources, location, data partitioning, scaling, security, backup etc. Virtualization is the essential technological characteristic of clouds, therefore, in this paper, we focus our attention on resource management that is one of the most important issues in Cloud Computing for IaaS (see Manvi and Shyam (2014)), including allocation, provisioning, mapping and adaptation in a multi-tenancy environment, where users share the same resource. Analyzing the Cloud Computing network, we take into account resources utilization, monetary cost and energy consumption as optimization objectives at the same time, while most of the researchers have dealt with them separately (see Pietri and Sakellariou (2016)). Virtualization consists of sharing computer hardware by partitioning the computational resources; often many services need not the total available resources but only a small portion of them. Energy consumption is playing an increasing role in the Cloud Network because of its costs and, for this reason, many researchers use the Server Consolidation (an approach according to which it is better mapping Virtual Machines on fewest possible physical servers) to prevent the wastage of resources (see Usmani and Singh (2016)). In this paper, taking into account also the physical resources heterogeneity, we aim at optimizing cost of running servers (cost of turning on or off a server, power consumption) and resource wastage. In Mann (2015), the author underlines that in the single-data centre problem, the usual formulation used by researchers is about mapping Virtual Machines to Physical Machines, while, in the multi-IaaS problem, it is more common to investigate the mapping of tasks (platform) to Virtual Machines. In the model we present, the provider can accept or reject a platform execution request, can establish the revenue and can make the decision about the allocation, ensuring the quality of service (determined in a particular agreement). The paper is organized as follows. In Section 2 we present the mathematical model of cloud computing and we describe the role of the different layers. Then, we analyze the behavior of the typical IaaS provider and derive its optimality conditions given by the desire to maximize its profit while minimizing its operational costs. We also present a linearization of the constraints and of the objective function in order to compare the optimal solutions. In Section 3 we describe the computational procedure for the calculus of the nonlinear model which will be applied in Section 4 to two numerical examples. Section 5 is dedicated to the conclusions.",4
22.0,2.0,Networks and Spatial Economics,07 August 2019,https://link.springer.com/article/10.1007/s11067-019-09473-6,Special Issue on Variational Inequalities: Consistent Conjectural Variations Coincide with the Nash Solution in the Meta-Model,June 2022,Viacheslav Kalashnikov,Nataliya Kalashnykova,José G. Flores-Muñiz,Male,Female,Male,Mix,,
22.0,2.0,Networks and Spatial Economics,13 March 2019,https://link.springer.com/article/10.1007/s11067-019-09459-4,Properties of a Variational Model for Video Inpainting,June 2022,Riccardo March,Giuseppe Riey,,Male,Male,Unknown,Male,"The mathematical problem of video inpainting can be formulated as follows. Let the spatial domain \({\Omega }_{s}\subset \mathbb {R}^{2}\) be an open bounded set with Lipschitz boundary, let the interval [0,T] be a temporal domain, and let Ω = Ωs × [0,T] be the spatiotemporal domain where a video sequence is defined. A point belonging to Ω is denoted by (x,t). A gray-value video content is a function \(u:{\Omega }\rightarrow \mathbb {R}\). A degraded gray-value video is a function \(f:{\Omega }\setminus D\rightarrow \mathbb {R}\) where the subset D ⊂Ω denotes a spatiotemporal region where the video data is lost. We assume that D is a known open set with Lipschitz boundary. An application of video inpainting consists in the recovery of the missing data in archive film materials which suffer from several kinds of degradations due to physical aging, and present regions where the original data may be entirely lost. Such regions are represented by the set D and the content of the degraded film is represented by the function f known over Ω ∖ D. Then the mathematical problem of video inpainting consists in looking for a video u which is defined on the whole set Ω, matches f outside D, and has a content inside D which satisfies suitable mathematical constraints. Such constraints consist in spatial piecewise smoothness and, in the case of motion compensated video inpainting here considered, coherence with the apparent motion of the video data f in Ω ∖ D. Apparent motion can be estimated through gray-value variations of the video data f in Ω ∖ D and it is represented by a vector field of velocities \(\sigma :{\Omega }\rightarrow \mathbb {R}^{2}\) denoted optical flow. We require the simultaneous estimation inside D of both the gray-value video u and the optical flow field σ. Variational methods have been widely used for image and video analysis (Ambrosio 1989; Aubert et al. 1999; Aubert and Kornprobst 1999, 2006; Bertalmio et al. 2001; Cocquerez et al. 2003; Brox et al. 2004). Actually, the mathematical theoretical and computational tools related to minimization problems (appeared in connection with mechanics, physical sciences and the related engineering problems) have been applied over recent years always more to several and different new (and unexpected until few years ago) frameworks related to social and environmental sciences (just to give a short and non exhaustive list of examples: economics, Internet congestion control, dynamic traffic assignment (Carlsson et al. 2016; Low and Srikant 2004; Peeta and Ziliaskopoulos 2001)). In the present communication we consider a variational model for motion compensated inpainting which has been analyzed in March and Riey (2017) and which is based on a model previously proposed by Lauze and Nielsen (2004). A model of this type has been also applied to the problems of video deintarlacing and super-resolution (Keller et al. 2008, 2011). The variational method consists in the minimization of a functional defined on a space of vector valued functions of bounded variation. Such functions are well suited for applications to video inpainting since a video content which is discontinuous along the boundaries of moving objects in the scene can be reconstructed. The functional is built by resorting to the relaxation method (Buttazzo 1989) of the Calculus of Variations. In order to recover properties of minimizers and to perform numerical experiments, a key step is to compute the Euler equation of our functional. Tools useful for this purposes can be found, for instance, in Amar et al. (2013), Anzellotti (1985, 1986). In the present paper we study some asymptotic properties of the functional which are related to the accuracy of the estimate of the optical flow σ in Ω ∖ D, which are relevant for the joint reconstruction of the video content inside the set D. The paper is organized as follows. In Section 2 we specify the notation and we give mathematical preliminaries. In Section 3 we describe the variational model and the issue of reconstruction of the optical flow. In Section 4 we give the properties of the relaxed functional. In Section 5 we state and prove the main results of the paper.",1
22.0,2.0,Networks and Spatial Economics,04 January 2020,https://link.springer.com/article/10.1007/s11067-019-09491-4,Restricted Participation on Financial Markets: A General Equilibrium Approach Using Variational Inequality Methods,June 2022,Maria Bernadette Donato,Monica Milasi,Antonio Villanacci,Female,Female,Male,Mix,,
22.0,2.0,Networks and Spatial Economics,29 November 2019,https://link.springer.com/article/10.1007/s11067-019-09485-2,Modified Projection Methods for Solving Multi-valued Variational Inequality without Monotonicity,June 2022,Xin He,Nan-jing Huang,Xue-song Li,,Unknown,Unknown,Mix,,
22.0,2.0,Networks and Spatial Economics,16 March 2019,https://link.springer.com/article/10.1007/s11067-019-09461-w,On the Stability of Coalitions in Supply Chain Networks via Generalized Complementarity Conditions,June 2022,Laura Scrimali,,,Female,Unknown,Unknown,Female,"A supply chain is a network of suppliers, manufacturers, distributors, retailers and consumers. A decentralized supply chain, in which all partners act competitively to maximize the profit, leads to inefficient performance due, for instance, to information asymmetry or double marginalization. For this reason, recently, there has been an increasing interest in supply chain coordination. Supply chain integration is a large-scale business strategy that brings as many links of the chain as possible into a tight relationship with each other. The goal is to improve response time, production time, and reduce costs and waste. Every level in the chain benefits. More and more companies are integrating their supply chains to improve their efficiency and keep up with increasing competition (see, for instance, Shahabi et al. 2013). Integrated supply chains allow firms to compete better on costs, by eliminating wasted time and materials, and having fewer middlemen. It also enables organizations to shorten their product life cycles, by having fewer links in the supply chain from back to front. Finally, it makes it possible to respond faster to changes in the market. Vertical or horizontal integration is at the heart of a company’s operations strategy. Horizontal integration is the process of acquiring or merging with competitors on the same level of the supply chain. Integration could include firms that supply similar products or manufacturer buying another one in order to serve a larger market share. The concept of vertical integration dates back to 1880 when Andrew Carnegie had the idea to acquire different levels of the supply chain to improve productivity and reduce costs. Vertical integration is an approach for increasing or decreasing the level of control of firms over their inputs and distribution outputs. This paper presents fully vertical integration in a supply chain network that consists of three layers of decision-makers, namely, suppliers, manufacturers and retailers, with prices and shipments that evolve in time. In particular, we assume that a supplier, a manufacturer, and a retailer make a joint venture to confront the other players. In this situation, the retailer is considered as the dominant player of the coalition and acts as a profit-maximizer. Most of the related literature studies two-layer supply chain or integration between two levels of a supply chain, see for instance (Amoozad Mahdiraji et al. 2014). Another typical approach is to consider contracts to force the cooperation. In Ding and Chen (2008), the authors study the coordination of a three-level supply chain selling short life cycle products in a single period model. They show that the supply chain can be fully coordinated with appropriate contracts as enticements to integration. In He and Zhao (2012) the authors focus on a multi-echelon supply chain with both demand and supply uncertainty. They use a Nash bargaining analysis to provide contract terms that lead to win-win situation. In Lin and Hsieh (2012), the authors show that cooperation is possible without introducing contracts and discuss both vertical integration and horizontal competition in a duopolistic supply chain model with three layers of agents in a static framework. They present the optimization problem related to the model and provide an equivalent finite-dimensional variational inequality formulation. In Scrimali (2018), the author extends the model in Lin and Hsieh (2012) and proposes to study the problem in a time-dependent setting and establishes the evolutionary variational inequality describing the equilibrium conditions. In this paper, we return to the supply chain network model as in Scrimali (2018). We present an alternative formulation of the evolutionary variational inequality in order to conduct a deeper analysis of the coalition behavior. In particular, we focus on the Lagrange multipliers associated with the production capacity constraints. We give the equilibrium conditions governing the model as complementarity conditions and prove the equivalence with the evolutionary variational inequality. The role of Lagrange multipliers is of great importance: they correspond to the marginal profits of the coalition and hence reveal if the coalition is feasible or not. They can be considered as indicators of the effectiveness of the coalitions. We also provide an a priori estimate of the range of values of the Lagrange multipliers which leads to an a priori estimate of the equilibrium solutions. Moreover, we state some analytic conditions for the efficiency and the stability of a coalition. We emphasize that the estimates based on the Lagrange multipliers make it possible to evaluate a priori the efficiency and the stability of any potential coalition. There is a large stream of literature devoted to the study of the solutions to variational inequalities and quasi-variational inequalities describing equilibrium models by means of the Lagrange multipliers. For instance, we refer to Barbagallo et al. (2014) and Daniele et al. (2016) for the financial equilibrium problem, Scrimali (2012) and Mirabella and Scrimali (2018) for the pollution control problem, Oggioni et al. (2012) for the electricity market, Giuffrè et al. (2015) for the elastic-plastic torsion problem and Daniele et al. (2017) for cybersecurity investments. This paper is organized as follows. In Section 2, we present the supply chain network model and recall the evolutionary variational formulation of the equilibrium conditions governing the model. In Section 3, we give an alternative formulation of the variational inequality by means of the Lagrange multipliers and provide the analysis of the marginal profits. Section 4 shows an application to a duopolistic quadratic model. In Section 5, we discuss some efficiency and stability conditions and derive some merging rules. Finally, Section 6 summarizes our results.",4
22.0,2.0,Networks and Spatial Economics,14 March 2019,https://link.springer.com/article/10.1007/s11067-019-09457-6,The Global Exponential Stability of a Dynamical System for Solving Variational Inequalities,June 2022,Phan Tu Vuong,,,,Unknown,Unknown,Mix,,
22.0,3.0,Networks and Spatial Economics,14 September 2022,https://link.springer.com/article/10.1007/s11067-022-09576-7,"Vulnerability, Resilience and Complex Structures: a connectivity perspective",September 2022,Ivano Cardinale,Aura Reggiani,Roberto Scazzieri,Male,Female,Male,Mix,,
22.0,3.0,Networks and Spatial Economics,17 May 2022,https://link.springer.com/article/10.1007/s11067-022-09563-y,"The Architecture of Connectivity: A Key to Network Vulnerability, Complexity and Resilience",September 2022,Aura Reggiani,,,Female,Unknown,Unknown,Female,"Natural and man-made disasters, economic recession and financial dynamics, geo-political changes, ruptures, and collapses in the infrastructural and communication networks, terrorist attacks, and sudden critical events of high impact, such as the current contagious diseases due to Covid-19, are not only causing enormous socio-economic damage but also creating an era of uncertainty, which is leading researchers and scientists to focus on concepts and themes such as the vulnerability, and hence the complexity and, hopefully, the resilience, of spatial economic networks. In this context, the concept of connectivity, closely linked to vulnerability, complexity, and resilience, needs particular attention. First, connectivity has been highlighted as complex (network) connectivity in the context of globalisation studies. For example, Tomlinson refers to globalisation as: ‘an empirical condition of the modern world: what I shall call complex connectivity. By this I mean that globalisation refers to the rapidly developing and ever-densening network of interconnections and interdependencies that characterizes modern social life’ (1999, p. 2). In his discussion, Tomlinson also emphasises that: ‘the broad task of globalization theory is to understand the sources of this condition of complex connectivity and to interpret its implications across the various spheres of social existence’ (1999, p. 3). Along these lines, connectivity has been firmly embedded in past studies and in different disciplines, such as physics, sociology, politics, economics, etc., where the network concept came to the fore for its important features: ‘The modern spatial economy has a global ‘networked’ character that is generating important socio-economic and political changes. In this respect, new forms of connectivity play a significant role through their dynamic and complex interplay with the economic and political driving forces behind globalization’ (Reggiani 2021, p. 325). In particular, in recent years, connectivity has been considered mainly with reference to transport, telecommunication, and social networks, where the role of network connectivity is tangible (Caldarelli and Vespignani 2007; Reggiani 2012). It should be noted that in 1962 the Nobel Laureate in Economics, Herbert Simon, wrote an article entitled “The Architecture of Complexity”, where he emphasised the role of hierarchical structures in complex systems: ‘Thus my central theme that runs through my remarks is that complexity frequently takes the form of hierarchy, and that hierarchic systems have some common properties that are independent of their specific content. Hierarchy, I shall argue, is one of the central structural schemes that the architect of complexity uses’ (Simon 1962, p. 468). Not surprisingly, 45 years later the physicist Albert-László Barabási wrote an article with the same title “The Architecture of Complexity”, where he underlined, first, the role of networks intrinsically linked to complex systems: ‘Rather, in complex systems, the interactions form networks, where each node interacts with only a small number of selected partners whose presence and effects might be felt by far away nodes’ (Barabási 2007, p. 33); and second, the relevance of hierarchical structures: ‘…a thorough understanding of complex systems requires an understanding of network dynamics as well as network topology and architecture’ (p. 34). So, both these scientists, although from different disciplines, stressed how the study of a complex system should consider not only the nature of dynamical processes, but also its network architecture, in other words its interacting connectivity structure. In this vein, it should be noted the term ‘system’ used in this paper embeds the network concept. Inspired by these two works, in the present paper we go a step further, by considering a ‘nested approach', i.e., by paying attention not only to connectivity, but also to the architecture of connectivity for vulnerability and resilience issues. More specifically: if we interpret connectivity as the (weighted) degree of network linkage of a given node in a network, its architecture also has a relevant role for both vulnerability and resilience. In parallel to complex networks, the concepts of vulnerability and resilience have been widely investigated in recent years. Nevertheless, their importance is still increasing as they are often being applied in economics, in transport, urban and regional studies, and, more generally, in the social sciences. Vulnerability analysis essentially refers to the propagation of shocks in a network, while resilience analysis refers to the speed at which a network returns to its equilibrium after a shock, as well as to the perturbations/shocks that can be absorbed before reaching new equilibria. Thus, vulnerability and resilience are clearly related to the complexity of network evolution in the presence of shocks. However, connectivity and its impact on network vulnerability and resilience has not yet received ‘full’ attention, despite the huge number of studies on vulnerability and resilience. Given this background, the present paper highlights methodological considerations on the role and interpretation of connectivity – and its architecture – in vulnerability and resilience in complex networks. Attention will also be paid to a complementary analysis of these two (dynamic) concepts. In sum, on the basis of the existing literature, it will be argued that connectivity and its architecture can be considered as a useful (and analytical) framework for understanding and interpreting the concepts of network vulnerability and resilience. In this regard, we can distinguish the following aspects of connectivity, which will be discussed in the course of the paper: Explicit connectivity: network vulnerability and shock propagation. Hidden connectivity: complexity in the space-economy. Relevance of connectivity: complexity in network analysis. The architecture of connectivity: resilience in spatial and transport economics. Sections 2, 3, 4, and 5 will, in turn, explain these four aspects – and the related scientific challenges – in the light of a possible integrated approach. Finally, Sect. 6 concludes with some considerations on the need for an interdisciplinary perspective provided by complexity science.",2
22.0,3.0,Networks and Spatial Economics,04 May 2022,https://link.springer.com/article/10.1007/s11067-022-09564-x,Collaborative Network Topologies in Spatial Economies,September 2022,Shaun Lichter,Terry Friesz,Amir Bagherzadeh,Male,,Male,Mix,,
22.0,3.0,Networks and Spatial Economics,22 April 2022,https://link.springer.com/article/10.1007/s11067-022-09559-8,A Circular Economy Model of Economic Growth with Circular and Cumulative Causation and Trade,September 2022,Kieran P. Donaghy,,,Male,Unknown,Unknown,Male,"Several paragraphs of material in the introduction (Sect. 1) and the review of the relevant literature (Sect. 2) are taken from Donaghy (2021b). The idea of a circular economy, as first discussed by Kenneth Boulding in the 1960s and ‘70 s (see, e.g., Boulding 1966) and reintroduced by environmental economists David Pearce and R. Kerry Turner in 1990, is a characterization of how goods and services can be produced and consumed in an ecologically sound and environmentally sustainable manner that meets concerns of overuse of resources, waste management, and climate change, inter alia, through the conscious interlinking of disparate economic activities. The idea has been adopted by a number of EU countries (Denmark, in particular, with its ‘Clean Tech’ programs) and by China (in its two most recent five-year plans) as the basis of their approaches to economic development. It has also been promoted by the World Economic Forum as a model for urban economic development in the 21st Century (World Economic Forum 2018). There is now an extensive literature on how more circular production can be organized in specific industries or clusters of industries. The notion of circular and cumulative causation, through which certain positive and negative effects are promoted and reinforced by positive feedbacks, also is not new. An intrinsic network externality, it has played a prominent explanatory role in the development economics of Veblen (1915) and Myrdal (1957) and the growth theories of Kaldor (1967) and Thirwall (2003), but also in the New Economic Geography of Paul Krugman (1995) and Baldwin et al. (2001). However, little attention has been given to how circular and cumulative causation (or CCC)—an emergent property of complex nonlinear dynamical systems—might be harnessed to bring about desirable systems properties, such as those of a circular market economy. The principal objective of Donaghy (2021b) was to begin a formal examination of how CCC might be exploited to manage a transition to a more circular macro-economy—i.e., how we might use our understanding of causal mechanisms to manage change (Runde 1998; Cartwright 2007; Donaghy 2020). Related to this endeavor is a forthcoming publication by Scazzieri (2021), which provides a welcome unpacking of the work of Simon (1962) on the architecture of complexity in terms of subsystems that are relatively invariant within networks of interdependent activities. He relates this framing of complexity to further developments by Laszlo Barabasi that focus on how systems complexity can be managed and its dynamics anticipated. Of particular interest to social scientists is Scazzieri’s assessment of structural economic theory and his review of theories of structural economic dynamics that help identify fundamental casual mechanisms for explaining the evolution of complex socioeconomics structures. Reggiani [forthcoming] focuses in a complementary fashion on the architecture of connectivity (network structures). Viewing systems complexity from these perspectives has implications for policy analysis: doing so directs our attention to where system interventions are needed. This paper extends the analysis of Donaghy (2021b) by introducing to circular economic growth modeling frameworks trade between three national or regional economies in an environmentally polluting resource, other materials, and recycling technologies. The following section will review developments in thinking about circular economics, CCC, and a perceived need for macroeconomic modelers and environmental economists to embrace nonlinear dynamic modeling of systems with complex behavior, including emergent properties. The third section will consider in detail the circular economy model of economic growth of George et al. (2015). The fourth section will review in brief the model of circular economic growth of Donaghy (2021b) that embodies three sources of CCC and its implications. The paper’s fifth section presents a new circular economy model of economic growth with trade in the resources and technologies alluded to above, calibration and numerical simulations of the model (with and without trade), and its solutions. Numerical simulations suggest that there would be gains from trade in terms of progress towards a circular growth economy. The paper concludes with suggestions of how other aspects of the Pearce and Turner model of a circular economy can be incorporated in circular economy models of economic growth with trade and discusses possible next steps in a research agenda.",5
22.0,3.0,Networks and Spatial Economics,10 June 2021,https://link.springer.com/article/10.1007/s11067-021-09540-x,"State-Owned Enterprises’ Reforms and their Implications for the Resilience and Vulnerability of the Chinese Economy: Evidence from the Banking, Energy and Telecom Sectors",September 2022,Roberto Cardinale,,,Male,Unknown,Unknown,Male,"The existing literature uses the concept of vulnerability to indicate a system’s exposure to the risk of disruption due to weakness and defenselessness from potentially destabilizing factors such as disturbances or shocks of endogenous or exogenous nature (Berdica 2002; Adger 2006; O’Brien et al. 2007; Caschili et al. 2015b). By contrast, the concept of resilience indicates the system’s ability to react to such factors. Therefore, a system is defined as resilient if a disturbance or shock (i) does not alter its functioning and distinctive features, (ii) affects its functioning temporarily but the system is able to restore its operability in a relatively short period of time, (iii) alters the system’s original functioning but the reaction leads to a change that allows to fulfill the original goals although in a different way (Reggiani et al. 2002; Briguglio et al. 2009; Gibson and Tarrant 2010; Folke et al. 2010). Although the two concepts are related to an extent, as they both refer to the risks and potentially existential challenges faced by a system, vulnerability indicates the ex-ante condition of exposure to risk, while resilience refers to the ex-post reaction to a destabilizing factor, namely when the risk has materialized itself into a factor of disturbance. The distinction is important and may prove relevant for policy purposes. In fact, it makes it possible to distinguish between (a) policy interventions that minimize vulnerability and prevent disruptions from occurring, for example by promoting regulatory measures to safeguard against potentially destabilizing (exogenous) effects for the economy; and (b) policy interventions that create the long-term (endogenous) conditions for resilience, for example by increasing the stock and quality of critical assets, which are necessary for the system to react positively to different types of shock. In every system, connectivity plays an important role for the system’s functioning and operability (Goyal 2007; Reggiani et al. 2015). Connectivity may contribute to strengthening the system’s resilience or expose it to increasing vulnerability, depending on the quality of infrastructure and networks connecting its main actors and structures, and on the system’s ability to manage their interactions, which evolve over time and are subjected to constant changes (Kuroda 2015; O’Kelly 2015). Infrastructure and physical networks are the backbones of economies, and are fundamental for the functioning of social and political systems as well. They ensure a smooth functioning of markets by connecting intermediate and final producers, and final producers to consumers across different locations within or outside the national borders (Cardinale 2019b). The connecting role of infrastructure and physical networks has social and political implications as well because they make it possible to supply the population with final goods and grant access to Services of General Interests (SGI) (Florio 2013; Cardinale 2017). Infrastructure and networks play an important role for national security and to preserve political stability against domestic and external threats (Millward 2011). Their resilience or vulnerability to internal or external disturbances and shocks, in turn, are crucial to guarantee the functioning and security of the economy and society, and to prevent the economy from underperforming or experiencing disruptions. Critical infrastructure and networks are usually owned and managed by large companies. Financial, technological and managerial features of these companies determine the extent to which critical infrastructure and networks are able to respond to disturbances or shocks in the ways described in points (i), (ii) and (iii) above. The contribution of such companies to the resilience of the economy becomes even greater when they are vertically integrated, namely when their business is not limited to the management of infrastructure and networks, but it involves other critical phases across production, transport and sales. Ultimately, their contribution to the economy’s resilience is fundamentally linked to their ability to carry out and perform a connectivity role, which consists in the coordination and management of key phases of the value chain and in the supply of essential inputs to the economy in abundant and affordable ways. The systemic effect of companies operating in network industries has historically justified various forms of State influence in their management, especially through the retention of substantial shares in the ownership, but also through regulation or concessions with obligations attached (Bauer 2005; Florio 2013; Cardinale 2019a). However, starting from the 1990s, privatization and liberalization policies around the world have also targeted network industries, although with substantial delay as compared to sectors with lower systemic relevance. The reshuffling of the governance of network industries has raised concerns in academic and policy debates regarding the potential impact for the economy.Footnote 1 This is the main question addressed in the paper. More specifically, the paper focusses on how policy changes affect the resilience and vulnerability of companies operating in key sectors of the economy, and whether their response to policy changes is adequate to maintaining quality and continuity in the supply of key goods or services to the economy and society. The focus is on banking, energy and telecom sectors in China. The choice is based on their systemic relevance for the economy, as each sector uses financial capital, energy, and telecom services as inputs in their production of final goods and services. The importance of these sectors to the economy’s resilience is proven by the fact that when they underperform, because the quality of their output is low or the infrastructure securing supplies to interdependent sectors is not adequately developed, the economy is negatively affected and is also likely to underperform. Technical failures, mismanagement and serious disruptions in the production and supply of these inputs could even lead to large-scale disruptions in final productions and in fundamental services, causing serious damages to the economy and society. The Chinese context is suitable for the purpose of this research because of the magnitude of policy changes occurred in recent decades (Hu and Wang 2017), particularly liberalization and privatization policies. Policy changes are a major test for the economy’s resilience because they cause substantial changes in previous patterns of production and distribution. The change has potentially destabilizing effects for the system when it occurs in sectors producing key inputs for the economy (Andreoni and Scazzieri 2014; Kuroda 2015; Scazzieri et al. 2015), as in the case of banking, energy, and telecom. In particular, liberalization policies are a form of sectoral regulation and have direct impact on the business of companies, especially when these retains monopolistic power (Bianchi 1998), which is often the case with network industries. The entry of new competitors in the market results in changes in the strategies for procurement, production and sales (Bianchi and Labory 2013), and in investment in current and future infrastructure and networks. Privatization of State-Owned Enterprises (SOEs) is considered as a subset of liberalization policies, and results in a change in the ownership structure of the company. The transition from State to private ownership is likely to induce major changes in the management and in the decisions about production and infrastructure investment (Cardinale 2019a, 2020; Barca et al. 2020). If private shareholders advocate for implementing investment strategies that prioritize the pursuit of short-term profitability, long-term investments on production and infrastructure may be reduced (Cardinale 2017). This strategy may in turn negatively affect connectivity within the fundamental structure of the economy and, in the long term, the ability of these companies to supply key inputs to the economy, thus weakening and exposing the economy to a condition of increasing vulnerability. The paper shows that the reforms of Chinese SOEs were implemented to increase the resilience of key sectors of the economy while minimizing the factors of vulnerability arising from the reforms. More specifically, resilience has benefited from the increasing capitalization of partially privatized SOEs and the possibility to invest in infrastructure and other industrial assets that are critical for the economy. At the same time, reforms made new aspects of vulnerability emerge, such as financial instability due to volatility of financial markets, conflicts between public and private shareholders, and rivalry between incumbent and emerging firms.Footnote 2 To strengthen key sectors’ resilience while containing newly emerging vulnerabilities, liberalization and privatization policies in China have been designed to address respectively (i) the short-term financial instabilities that may be caused by listing SOEs in stock exchanges, particularly when these go public in markets that are not subjected to domestic regulation; and (ii) the reduction of long-term investments and the potential deterioration and/or inadequate development of assets of systemic interest such as infrastructure and networks. The paper is structured as follows. Section 2 analyses how a gradual process of reform of SOEs in the banking sector was a solution to reconcile growth and stability in the financial sector. Section 3 shows that SOEs’ reforms in the energy sector has also contributed to their growth, although the ambition of the reforms has in some cases posed risks to energy security. Section 4 reconstructs the challenges faced during the years of reforms in telecom SOEs, and how a new industrial policy vision is trying to address and reconcile the development of technological leaderships with market competition. Section 5 summarizes the main findings, and Section 6 provides suggestions for further research.",6
22.0,3.0,Networks and Spatial Economics,27 May 2021,https://link.springer.com/article/10.1007/s11067-021-09533-w,Vulnerability and Resilience in the Caribbean Island States; the Role of Connectivity,September 2022,Edwina E. Pereira,Albert E. Steenge,,Female,Male,Unknown,Mix,,
22.0,3.0,Networks and Spatial Economics,19 March 2021,https://link.springer.com/article/10.1007/s11067-021-09521-0,"Resilience, Performance and Strategies in Firms’ Reactions to the Direct and Indirect Effects of a Natural Disaster",September 2022,Davide Antonioli,Alberto Marzucchi,Marco Modica,Male,Male,Male,Male,"An earthquake on May 20 and 29, 2012 hit one of the leading regions of the Italian economy, Emilia-Romagna, which is one of the driving forces of the Italian industrial system. The regional added value recorded in the year before the earthquake corresponded to about 9% of the national figure (Source: ISTAT data). Its dynamism is the result of an evolution that has its origins in the famous ‘Emilian model’ (Brusco 1982), made from interactions between the production and social system and which has developed into its current state in which companies, institutions and research are widely interconnected in a solid innovation system (e.g. Marzucchi et al. 2015). Within the region, the earthquake hit specifically the provinces of Reggio Emilia, Modena, Bologna and Ferrara, whose added value in 2011 represented 5.36% of the national aggregate and 7.01% when the industrial added value is considered in particular (Source: ISTAT data). This paper aims to analyse the effects of the earthquake on Emilia-Romagna companies and specifically on their capacity to adapt after a shock, focusing on two types of impacts triggered by the same shock (i.e. the earthquake): on the one hand, the effect of a natural shock on the overall economic performance of firms and, on the other hand, the outcome of the companies’ strategic choices. In line with the literature on resilience (Boschma 2015; Martin 2012; Martin and Sunley 2015; Modica and Reggiani 2015), we discriminate between different interpretations of the said concept and we look at two different perspectives. Namely, drawing on the first perspective (see Martin 2012), we focus on the so-called ecological resilience – that is, the capacity of firms to resist a stress – and we investigate the impact on the performance of the firms. Drawing on a second perspective, we focus on the capacity of firms to adapt to a new ‘environment’ (see Boschma 2015) as the result of the stress caused by a shock, and we consider the strategic reaction and choices of the companies. While the analysis of the impact on economic performance has an intrinsic significance for the assessment of the impact on the production system affected by the earthquake, the focus on strategic choices is aimed at a short-term analysis of the introduction of practices and behaviours that can contribute to the revitalisation of the affected companies. Two recent perspectives in the field of applied economics determine the reference framework for this research. A first group of studies focuses on the effect of natural disasters on production, investment and productivity both at the micro and aggregate levels (e.g. De Mel et al. 2012; Hallegatte and Dumas 2009; Hochrainer 2009; Leiter et al. 2009; Miao and Popp 2014; Skidmore and Toya 2002). These works do not lead to clear conclusions about the expected sign of the effects of such extreme events. In fact, while a natural disaster such as an earthquake necessarily entails negative destructive effects, the need to rebuild can induce companies to increase investments and modernise plants with positive consequences on company performance. A second perspective considers the concept of resilience, which might include the ability to keep output close to its potential (Duval et al. 2007; Reggiani et al. 2002), the ability of an entity to preserve its functions (Rose 2007) or the ability of an enterprise to adapt its structure to maintain a path of acceptable growth of production (Martin 2012) following a shock. Therefore, the focus shifts from considering only the magnitude of the disaster and its effects to encompassing the specific actions and policies that companies put in place to increase their resilience and the preparation to face possible future shocks. In this paper, we rely on original data collected through a survey that involved a sample of about 550 companies from Emilia-Romagna. The survey allowed us to gather information on the status of the ‘treatment’ (having suffered damages) of the 2012 earthquake, as well as variables regarding strategic choices, economic trends and key characteristics, such as the business structure and the orientation towards innovation and human capital. Particular attention was paid to the distinction between direct and indirect damages suffered by the firm or by the productive and commercial partners of the company (e.g. suppliers and customers) and to the inclusion of ‘counterfactual’ companies that have not suffered any damage from the earthquake. The results of the econometric analysis show that, in addition to the damage directly suffered by companies, the indirect effects also had a negative impact on their economic performance. The evidence confirms the difficulty of highlighting a clear effect on the investment trend. Nonetheless, this study provides evidence of the resilient characteristics of the firms in terms of their capacity to adapt to and to cope with shock. In fact, in the presence of short-term negative effects on economic performance, the earthquake also acted as an incentive for the introduction of reconstruction strategies which concerns the increase of flexibility in production, the search for new markets, the reduction of environmental impacts, increased safety and higher compliance with existing regulations. In all, we underline a negative effect of direct damages on the economic performance of firms, and we show a positive effect (mainly) coming from the indirect damages on the firms’ strategies. This latter result, even if further work is needed, might be the result of the complexity and the interconnection that plays an important role in the industrial environment of Emilia-Romagna, where the presence of districts and industrial clusters is relevant. The remainder of the paper is structured as follows. Section 2 presents a review of the relevant literature. Section 3 discusses the data used and the methodology applied. Section 4 describes the results. Finally, Section 5 presents the paper’s conclusions and its main implications.",2
22.0,3.0,Networks and Spatial Economics,04 February 2021,https://link.springer.com/article/10.1007/s11067-021-09518-9,The Role of Bike Sharing in Promoting Transport Resilience,September 2022,Lu Cheng,Zhifu Mi,Dongfeng Chang,,Unknown,Unknown,Mix,,
22.0,3.0,Networks and Spatial Economics,20 November 2020,https://link.springer.com/article/10.1007/s11067-020-09513-6,Change of Scene: The Geographic Dynamics of Resilience to Vehicular Accidents,September 2022,Timothy C. Matisziw,Mark Ritchey,Robert MacKenzie,Male,Male,Male,Male,"Modern society is hugely dependent on the availability of networked infrastructures, such as those facilitating the movement of people, freight, energy and data, with the performance of these systems directly associated with an array of social, economic, and environmental costs. As such, the vulnerability of networks to events that can potentially degrade performance and increase costs is of particular planning concern. In this respect, greater resiliency to disruptive events is a desirable characteristic of a system. Resilience has been conceptualized in a variety of ways, but in general refers to the ability of a system to maintain an acceptable level of performance in light of disruptive activities, whether by resisting change and/or by returning to a suitable state (either prior or new) (Modica and Reggiani 2015; Reggiani et al. 2002). While higher system resiliency is advantageous, providing the mechanisms whereby which resiliency can be enhanced is a resource intensive task, as is examined in this article with respect to transportation systems. Like other complex networks, transportation systems have an array of vulnerabilities related to factors such as their topology, use, interdependency, physical and operational condition, and potential threats that can vary greatly over space and time (Grubesic and Matisziw 2013; Matisziw et al. 2012a; Matisziw et al. 2007). Vehicular crashes are common sources of disruption to transportation systems, whose impacts often transcend many vulnerabilities, contributing to a range of social and economic costs. As most vehicular crashes involve damage to personal, private or public property, assistance with navigating legal, medical, safety, traffic, repair, and other issues that can emerge is often needed. Timely emergency response to such events is therefore essential to mitigate their impacts, assist with system recovery, foster a greater sense of social security, and to increase the overall resiliency of the transportation system and all of the other socio-economic systems to which it is tied (Klimek et al. 2019). Resilience in this sense depends in part on the way in which resources for responses are utilized as well as the adaptability of individual responders to changing conditions (Comes 2016). Given that provision of emergency response services involves a tremendous investment in resources, ensuring that resources are used as effectively and efficiently as possible are priorities of the agencies/entities charged with maintaining those services as well as the individuals whom they support (Engel and Eck 2015). Though there have been a variety of metrics proposed for assessing effectiveness and efficiency of response services, the response time required for emergency personnel to arrive at the scene for a call for assistance has long been used as a measure of service performance in this respect (Pate et al. 1976; Stevens et al. 1980). The time involved in responding to emergencies depends on numerous factors. Given the geographic difference between the location of an incident and that of the responder, response time can be directly influenced by the availability of a responder, dispatch time, as well as travel speed and distance (Pate et al. 1976). Other factors such as such as time of day, location of the crash with respect to the lanes of traffic, and number of vehicles involved are also thought to have a notable effect on response times (Lee and Fazio 2005). However, the actual resources available for emergency response and the policies underlying their use (e.g., policies for response prioritization) are also known to exert a considerable influence on response times (Levine and McEwen 1985). For instance, responses to crashes involving injuries may be prioritized over non-injury crashes (Lee and Fazio 2005). As vehicular accidents are relatively commonplace, spatial and temporal patterns of crash activity often become discernable to those familiar with a region. Therefore, emergency responders are increasingly able to leverage their experiences with their regions of responsibility to better position themselves for more effective response to potential events (e.g., hot spot policing (Braga et al. 2019)). However, the spatial and temporal distribution of crashes is known to vary considerably as likely do the factors underlying the events. These instabilities in the spatial and temporal dimensions of crash incidence can lead to problems in the application of many statistical approaches to accident prediction (Mannering 2018). Those tasked with planning for and implementing emergency response are likely affected by these instabilities as well in their efforts to enhance system resilience. To this end, this research examines the extent to which resilience of a transportation system changes with respect to geographic distribution of accidents over time through an analysis of accident responses recorded by law enforcement. First, background literature related to the spatiotemporal dimensions of vehicular crashes is examined as are analysis techniques that have been applied to evaluate the dimensions of emergency response. Next, a methodology for assessing spatiotemporal variations in resilience to accidents in transportation systems is outlined. Based upon an extensive set of crash records, the analysis methodology is then applied to evaluate the geospatial dynamics of system resilience to accidents over time.",3
22.0,3.0,Networks and Spatial Economics,06 February 2021,https://link.springer.com/article/10.1007/s11067-021-09522-z,In Search of Concerted Strategies for Competitive and Resilient Regions,September 2022,Kamila Borsekova,Samuel Koróny,Peter Nijkamp,Female,Male,Male,Mix,,
22.0,3.0,Networks and Spatial Economics,16 June 2021,https://link.springer.com/article/10.1007/s11067-021-09519-8,Decomposability and Relative Invariance: the Structural Approach to Network Complexity and Resilience,September 2022,Roberto Scazzieri,,,Male,Unknown,Unknown,Male,"Any given network as a structure of mutually related components rests on the fundamental distinction between interdependence (a positional characteristic of elements belonging to that network or structure, such as the relationship between pieces in a jigsaw puzzle) and connectivity (a functional characteristic of elements involved in a domain of mutual responses, such as the relationship between actors in an action field).Footnote 1 Elements belonging to a given economic and spatial network may be ‘positionally dependent’ on one another (in the sense that displacement of one element involves displacement of one or more elements within the same topology of possible positions) even in the absence of direct interaction between elements within that structure. On the other hand, elements belonging to a given field of interaction may be mutually responsive to one another even in the absence of positional changes within that field. Positional interdependence is central to studies highlighting what has been called the ‘architecture of complexity’ (Simon 1962), while functional interdependence is central to studies investigating connectivity in social networks and strategic action fields (Goyal 2007; Fligstein and McAdam 2011, 2012). This paper argues that a structural approach to economic complexity based on the distinction between positions and interactions could account for features of resilience that may otherwise remain unexplained. From a more general point of view, this paper aims to contribute to the interdisciplinary dialogue between the positional (spatially embedded) approaches to network analysis and the approaches addressing the complex dynamics triggered by nonlinearities in the interacting components of the system under investigation (Ducruet and Beauguitte 2014; Tsiotas and Polyzos 2018). Section 2 outlines the fundamentals of the structural approach to economic and spatial complexity by highlighting the role of the hierarchical arrangement of system elements as a central feature of system identity. This section considers the positional distribution of system elements as a fundamental characteristic of complex systems and a most important factor in determining the dynamics of complex systems through a principle of relative structural invariance. This principle states that not all system elements can change at the same time and/or at the same speed. Section 3 carries this argument into dynamic analysis by connecting the working of relative structural invariance with the aggregation criterion followed in assigning system elements to specific subsystems (such as clusters of firms, industries, or vertically integrated productive sectors). This section highlights that different aggregation criteria may be associated with different bottlenecks and/or windows of opportunity due to the greater salience of certain invariances relative to others. As a result, important features of the dynamics of economic and spatial networks depend on which type of aggregation is empirically and operationally more important in specific contexts. Section 4 builds on the analysis of the previous section and examines the relationship between the aggregation criterion adopted in assigning network elements to specific subsystems and the resilience properties of the economic system with respect to specific dynamic impulses. This section argues that a structural approach to economic and spatial complexity may account for features of vulnerability and resilience that may otherwise remain unnoticed and unexplained. Section 5 brings the paper to close by highlighting the need to combine the analysis of connectivity with the investigation of positional interdependence seeing that the configuration of interdependence may be central in determining which patterns of connectivity are more likely to arise (and which ones are excluded) due to the invariance of certain relative positions of network elements. This section also highlights that the distribution of relative invariances at a given point of time and over time, by determining which changes of relative positions are feasible and which ones are not, is an important factor in explaining the routes taken by the structural dynamics of economic and spatial systems as they steer between different and sometime opposed patterns of resilience.",7
22.0,3.0,Networks and Spatial Economics,30 August 2021,https://link.springer.com/article/10.1007/s11067-021-09551-8,Leontief Meets Markov: Sectoral Vulnerabilities Through Circular Connectivity,September 2022,Ariel L. Wirkierman,Monica Bianchi,Anna Torriero,Male,Female,Female,Mix,,
22.0,3.0,Networks and Spatial Economics,18 April 2019,https://link.springer.com/article/10.1007/s11067-019-09462-9,"Vulnerability, Resilience and ‘Systemic Interest’: a Connectivity Approach",September 2022,Ivano Cardinale,,,Male,Unknown,Unknown,Male,"The concepts of vulnerability and resilience are often used to understand properties of social-ecological systems, i.e. systems that “[reflect] the idea that human action and social structures are integral to nature and hence any distinction between social and natural systems is arbitrary” (Adger 2006, p. 269; see also Reggiani et al. 2002; Turner Jr. et al. 2003).Footnote 1,Footnote 2 More specifically, vulnerability describes “states of susceptibility to harm, powerlessness, and marginality of both physical and social systems” (Adger 2006, p. 269; see also O'Brien et al. 2007). Resilience refers to “the magnitude of disturbance that can be tolerated before a socioecological system (SES) moves to a different region of state space controlled by a different set of processes” (Carpenter et al. 2001, p. 765) as well as “the capacity to self-organise and the capacity for adaptation to emerging circumstances” (Adger 2006, pp. 268–9; see also Holling 1973; Folke 2006). And while vulnerability and resilience are widely seen as related concepts that “have common elements of interest—the shocks and stresses experienced by the social-ecological system, the response of the system, and the capacity for adaptive action” (Adger 2006, p. 270), the connections between them are yet to be fully fleshed out (Modica and Reggiani 2015; see also Janssen and Ostrom 2006; Reggiani 2013; Caschili et al. 2015). For the purposes of the present article, vulnerability and resilience can be treated jointly because they both have an important socio-political element. In fact, vulnerability “does not exist in isolation from the wider political economy of resource use. Vulnerability is driven by inadvertent or deliberate human action that reinforces self-interest and the distribution of power in addition to interacting with physical and ecological systems” (Adger 2006, p. 270). Analogously, resilience “relates to the response of a system to disturbance or change, whether that disturbance is sudden and shocking or more gradual. When a system is subjected to disturbance, these are the only possible outcomes: it withstands the disturbance, maintaining the specified features of interest, or not; if not, it either recovers the features of interest in an acceptable time frame or not; if it does not maintain the specified features of interest and does not recover them, but ends up in a different condition following disturbance, then the question is whether or not the change in the system is considered desirable, or even an improvement” (Adger 2006, p. 270, emphasis added). In fact, the “policy implications of vulnerability and resilience are profound and contested. Policies and strategies, which reduce vulnerability and promote resilience change the status quo for many agencies and institutions and are frequently resisted” (Adger 2006, p. 278). The socio-economic dimension of resilience and vulnerability requires that we analyze the conflicting interests of stakeholders. In particular, we must study the conditions under which it is possible to specify dimensions of vulnerability and resilience that are compatible with stakeholders’ pursuit of their own interests. In fact, since any change in the system would favor some stakeholders over others, any policies that change the status quo would be contested by at least some stakeholders (Cardinale 2015).Footnote 3 In this sense, it is true that “what constitutes improvement or detriment is observer dependent” (Helfgott 2018, p. 853; see also Carpenter et al. 2001). However, this paper argues that the concept of “systemic interest” (Cardinale 2015, 2017, 2018b) can help identify a dimension of vulnerability and resilience that is likely to be shared across stakeholders. Systemic interest is the interest of stakeholders to preserve the viability of the socio-economic system within which they act. In fact, because of connectivity, changes that favor some stakeholders but might make the system unviable would end up jeopardizing the interests of those very stakeholders. Therefore, a view of vulnerability and resilience defined in terms of systemic interest is likely to be shared across stakeholders, because it is connected with the ability of the system to remain viable. It is important to note that this approach does not impose a unique direction of desirable change but a range within which change does not jeopardize viability. In other words, defining vulnerability and resilience on the basis of systemic interest provides not a univocally determined objective, but a constraint on the pursuit of particular interests on the part of stakeholders. In the approach proposed in this paper, features of connectivity are fundamental for understanding which social-ecological systems are likely to afford a systemic interest.Footnote 4 In fact, connectivity determines whether a shock to a part of the system is more or less likely to affect systemic viability; it also has a crucial influence on whether a sufficiently wide spectrum of stakeholders have an interest in counteracting the effects of such a shock. In particular, the paper explores which features of connectivity are likely to generate an interest in maintaining the viability of the system by addressing shocks through policies (resilience) and in addressing exposure to shocks (vulnerability). It also discusses what institutional features are likely to favor or hinder such outcomes. The paper is organized as follows. The next section introduces the “Structural Political Economy” approach to explore how structural economic analysis can be revisited to define stakeholders through the study of connectivity in social-ecological systems. Section 3 suggests a route to identify conflicting interests and systemic interest. Section 4 presents the central contribution of the paper: exploring what features of connectivity are more likely to be associated with systemic interest, and how this impinges on resilience and vulnerability. A short section concludes.",14
22.0,4.0,Networks and Spatial Economics,19 May 2022,https://link.springer.com/article/10.1007/s11067-022-09565-w,Proportional-Switch Adjustment Process with Elastic Demand and Congestion Toll in the Absence of Demand Functions,December 2022,Lie Han,,,,Unknown,Unknown,Mix,,
22.0,4.0,Networks and Spatial Economics,23 May 2022,https://link.springer.com/article/10.1007/s11067-022-09560-1,The Piecewise Constant/Linear Solution for Dynamic User Equilibrium,December 2022,František Kolovský,Ivana Kolingerová,,Male,Female,Unknown,Mix,,
22.0,4.0,Networks and Spatial Economics,25 June 2022,https://link.springer.com/article/10.1007/s11067-022-09571-y,Multi-Objective Decision Method for Airport Landside Rapid Transit Network Design,December 2022,Danwen Bao,Shijia Tian,Ting Zhu,Unknown,Unknown,,Mix,,
22.0,4.0,Networks and Spatial Economics,25 June 2022,https://link.springer.com/article/10.1007/s11067-022-09568-7,A New Projection-type Method with Nondecreasing Adaptive Step-sizes for Pseudo-monotone Variational Inequalities,December 2022,Duong Viet Thong,Phan Tu Vuong,Le Dung Muu,,,,Mix,,
22.0,4.0,Networks and Spatial Economics,30 June 2022,https://link.springer.com/article/10.1007/s11067-022-09574-9,Rumor Transmission in Online Social Networks Under Nash Equilibrium of a Psychological Decision Game,December 2022,Wenjia Liu,Jian Wang,Yanfeng Ouyang,Unknown,,Unknown,Mix,,
22.0,4.0,Networks and Spatial Economics,30 June 2022,https://link.springer.com/article/10.1007/s11067-022-09566-9,Day-to-Day Signal Retiming Scheme for Single-Destination Traffic Networks Based on a Flow Splitting Approach,December 2022,Xiaozheng He,Jian Wang,Henry X. Liu,Unknown,,Male,Mix,,
22.0,4.0,Networks and Spatial Economics,04 July 2022,https://link.springer.com/article/10.1007/s11067-022-09570-z,Spatial Network Analysis of Container Port Operations: The Case of Ship Turnaround Times,December 2022,César Ducruet,Hidekazu Itoh,,Male,Male,Unknown,Male,"The rise and fall of ports have long relied on their ability to ensure efficient vessel accommodation and cargo handling (Jackson 1985). Seaports of the nineteenth century were already competing by providing fast transit between sea and land (Marnot 2005). Such aspects are even more crucial in recent decades, with the acceleration of global trade and the advent of containerization (Bernhofen et al. 2013). Containerization was specifically applied to maritime transport to facilitate cargo handling and save time and cost (Levinson 2006). The time that ships spend in a port thus has become increasingly crucial, especially for shipping companies, although it remains poorly documented in official reports (de Langen et al. 2007). The current COVID-19 pandemic had tremendous impacts in terms of supply chain disruption and port congestion worldwide (Merk et al. 2022), thereby confirming how the speed of port operations is vital for global transport and economic development. While it is recognized that “port efficiency” as a whole may facilitate trade (Clark et al. 2004) and local economic development (Doi et al. 2001; Haddad et al. 2010), the time factor lags behind other port performance indicators in the academic literature (Tongzon 2001; Itoh 2002). It is often discussed in broader researches on supply chain efficiency (Hummels 2001; Nordas et al. 2006), port choice behavior (Itoh et al. 2002; Tiwari et al. 2003; Tongzon and Sawant 2007), and congestion issues in ports (see Notteboom 2006; Vernimmen et al. 2007; Yan et al. 2009; Jones et al. 2011; Leachman and Payman 2012), but systematic empirical studies remain scarce (Suarez-Aleman et al. 2014). This article wishes to tackle this lacuna by providing a spatiotemporal analysis of vessel turnaround times across ports of the world in the last four decades. The main objective is to further understand the determinants of ship times in ports. In particular, this research innovates by adopting a relational, or network, perspective. Port connectivity studies have become popular in the last decades (Ducruet et al. 2020b), but the relationship between maritime centrality and ship times has not been investigated yet. It is based on the idea that port operations are increasingly influenced by exogenous realities, such the position of ports in value-driven chain systems (Robinson 2002). One first hypothesis is that a strong centrality will accelerate port operations. Another facet of this relational perspective is the possibility to put in relation port time and delays at sea, namely the difference between expected berthing time and actual berthing time (see Premathilaka 2018), in the global container shipping network considered as one comprehensive system. Related to this, a second hypothesis is that sailing delays increase bottlenecks, congestion, and thus port time, as a cascading effect throughout the network, which is made of interconnected ports and dependency chains (Stergiopoulos et al. 2018; Talley and Ng 2016). Another innovation of the present research is to confront ship times with the territorial attributes of places in which ports operate. Although port competition studies considering ship times may include such elements, like policy measures and hinterland connections, they often remain theoretical or focused on a small sample of ports (see Zondag et al. 2010). Maritime networks belong to the class of spatial networks (Barthelemy 2015; Ducruet 2020), with nodes being characterized by geographic and socio-economic characteristics at different levels. Those include the national economy, in terms of investment potential in efficient port infrastructure. It also includes more local attributes, especially about the urban location itself. The urbanized area may act as a constraint for port operations, but at the same time, the urban economy constitutes a crucial market for maritime trade (Ducruet et al. 2020a). Ports situated within dense urban environments have higher probabilities to face congestion than ports situated in smaller urban settlements. Other locational factors also play a role, ports being in a more or less favorable situation to accommodate larger vessels. Spatiotemporal models of port evolution well depicted the demise of upstream seaports (Bird 1963), while modern transshipment hubs, which provide state-of-the-art facilities and adequate berth depth, often locate on peninsulas and small islands (Fleming and Hayuth 1994; Rodrigue and Notteboom 2010). Last but not least, ship time is thought to be differentiated across world regions, depending on socio-economic development levels, but its geographic distribution is not well-known. Our research covers the period between 1977 to 2016, namely since the Open Door Policy of China and a few years before the current pandemic. Despite the latter event, which is affecting ports and supply chains to such an extent that it fosters a paradigm shift in container shipping (Merk et al. 2022), the search for regularities in the distribution and evolution of port time remains necessary. One main reason is that there is hope for the pandemic to cease and for port operations to resume, thereby going back to a state of global “synchronization” among transport terminals (Rodrigue 1999). Past regularities may survive to shocks, as it will be examined in this research from diverse angles, namely the escalation of ship size and the 2009 global financial crisis. The remainders of this article are organized as follows. The second section reviews the existing literature on ship time in diverse scientific disciplines. It is followed by a third section introducing the data and methodology serving the global analysis of ship time in container ports. The fourth section provides preliminary results of ship time evolution and its geographic distribution. Main results lie in the fifth section, where the determinants of ship time are analyzed. The last section discusses the lessons learned for research and practice and provide conclusions as well as pathways for further research.",2
22.0,4.0,Networks and Spatial Economics,11 July 2022,https://link.springer.com/article/10.1007/s11067-022-09572-x,The Perpetual Trouble with Network Products Why IT Firms Choose Partial Compatibility,December 2022,Manfred Stadler,Céline Tobler Trexler,Maximiliane Unsorg,Male,Female,Female,Mix,,
22.0,4.0,Networks and Spatial Economics,20 July 2022,https://link.springer.com/article/10.1007/s11067-022-09573-w,Including Right-of-Way in a Joint Large-Scale Agent-Based Dynamic Traffic Assignment Model for Cars and Bicycles,December 2022,Mads Paulsen,Thomas Kjær Rasmussen,Otto Anker Nielsen,Male,Male,Male,Male,"One of the most important purposes of traffic assignment models is to give a realistic representation of congestion for all relevant modes. As such, in urban contexts it is relevant to include bicycle traffic, but due to fundamental differences between car traffic and bicycle traffic, the underlying methodologies to model congestion for these modes may differ (Paulsen et al. 2019). In cities with a high level of segregation between modes, this may be dealt with by implementing separate methods for modelling on-link travel times for car and bicycle traffic, respectively. However, a joint model including both car and bicycle traffic is necessary when modelling intersections, as using two separate models would ignore the considerable interactions occurring between the two types of traffic due to yielding and right-of-way. Proposing such a model and integrating it into a joint traffic assignment model for otherwise separated car and bicycle traffic is the purpose of this paper. In the past two decades, dynamic traffic assignment has been an important topic in research (Peeta and Ziliaskopoulos 2001; Chiu et al. 2011; Friesz et al. 2013; Friesz and Meimand 2014; Friesz and Han 2019; Han et al. 2015a, b, 2019). However, only in recent years has it been possible to model congestion for dedicated bicycle traffic realistically with large-scale applicable approaches. Wierbos and Knoop (2019) modelled congestion for bicycle traffic through congestion effects arising when sharing road infrastructure with motorised transport modes along links. Although suitable for integration within a traffic assignment model, such integration was not carried out in the study. Agarwal et al. (2019) also modelled congestion of cyclists in an agent-based simulation of mixed traffic in the Indian city of Patna. Congestion arose both from riding in shared traffic, but also on dedicated bicycle infrastructure assuming homogeneity among cyclists and a fixed maximum number of cyclists per hour. Paulsen et al. (2019) introduced an advanced agent-based network loading model for large-scale modelling of congestion on bicycle paths. The model, tailor-made for bicycle traffic, was based on speed heterogeneity among cyclists and capable of gradually inducing congestion on certain cyclists even before reaching a traditional congested regime. Such properties remain unprecedented for large-scale applicable methodologies on bicycle traffic. Paulsen and Nagel (2019) integrated the methodology into a full traffic assignment model with feedback mechanisms between demand and supply. The model was applied to a large-scale bicycle network of Metropolitan Copenhagen, however, without modelling excess travel time arising from interactions between vehicles at intersections. Such delays at intersections typically constitute a considerable amount of the total travel time in the network, especially for bicycle traffic where delays on the links themselves are limited due to good opportunities to overtake. Modelling intersections is particularly relevant in urban areas with a high concentration of bicycle traffic, as the presence of bicycles not only causes delays for other bicycles, but also for turning cars. Likewise, car traffic also delays cyclists having to cross larger roads. In cities with a high level of segregation between modes, inter-modal delays can be ignored on links as the two traffic streams are completely separated but are still important to model at intersections in order to obtain realistic travel times for both modes. Microsimulation tools, such as AIMSUN (Dandl et al. 2017), SUMO (Krajzewicz et al. 2014) and VISSIM (Fellendorf 1994), are capable of modelling delays at intersections for both car and bicycle traffic in great detail but are not applicable to a large scale. In the last twenty years, large-scale flow-based car traffic assignment models have often included some form of intersection delays (Nielsen et al. 1998a). Such models can indirectly include the effect of having to yield to bicycle traffic by reducing the capacity of relevant turn moves by applying relevant adjustment factors. Estimating such adjustment factors for bicycle traffic has received quite some attention in the literature (Allen et al. 1998; Brilon and Miltner 2005; Chen et al. 2007, 2014; Li et al. 2009, 2011; Guo et al. 2012; Preethi and Ashalatha 2018) and is a common way to include car travel time delays at intersections caused by prioritised traffic in flow-based models. Similar adjustments can be made for other types of non-motorised traffic (Mondal and Gupta 2020) such as pedestrians (Niittymäki and Pursula 1997; Milazzo et al. 1998; Chen et al. 2008, 2015; Roshani and Bargegol 2017). Although such methods can be used for modelling car traffic delays caused by bicycle traffic, they are not suitable for modelling the opposite case where cyclists have to yield to car traffic, as the large heterogeneity among cyclists makes flow-based approaches inappropriate (Paulsen et al. 2019). As such, using flow-based methods is not a viable option when wanting to perform realistic traffic assignment for both car and bicycle traffic while modelling their interaction at intersections. The only existing large-scale applicable network loading model capable of modelling congestion on dedicated bicycle infrastructure already under moderate traffic intensities, is the agent-based methodology from Paulsen et al. (2019) applied in Paulsen and Nagel (2019). As such, in order to be able to model on-link congestion, any proposed intersection model that can capture the delays caused by yielding at intersections need also to be agent-based for the two to be compatible. This study contributes to literature by modelling delays caused by conflicting moves at intersections directly in the mobility simulation of a large-scale agent-based traffic assignment model for joint car and bicycle traffic. Modelling such conflicts is not new in itself, e.g. Dandl et al. (2017), neither is it new to model an entire metropolitan area in an agent-based transport simulation setup, e.g. Raney et al. (2003). However, doing both simultaneously, i.e. modelling conflicts at intersections while also simulating and modelling on-link congestion as well as route choice and traffic assignment of both bicycle and car traffic of an entire metropolitan area is. The study does this by extending the open-source agent-based transport simulator MATSim (Horni et al. 2016). As MATSim is already capable of simulating a large geographical area in a feasible time, the specific objective of this study is to replace the existing, simplistic intersection (node) model of MATSim with a detailed one that obeys multi-modal right-of-way at intersections and apply it to a large-scale case study with a large proportion of bicycle traffic. The remainder of the paper is structured as follows. Section 2 presents the network pre-processing stage of our proposed algorithm, in which every node of the network is classified into one of five types, where conflicting moves, prioritisation of different moves, and left turns are determined. Section 3 presents how vehicles and bicycles are modelled when travelling across such nodes. A case study of Metropolitan Copenhagen is presented in Sect. 4 alongside results. Section 5 discusses the findings and outlines directions for future research. Finally, the conclusions of the study are summarised in Sect. 6.",
22.0,4.0,Networks and Spatial Economics,04 August 2022,https://link.springer.com/article/10.1007/s11067-022-09575-8,A Relaxed Forward-Backward-Forward Algorithm with Alternated Inertial Step: Weak and Linear Convergence,December 2022,Yekini Shehu,Lulu Liu,Jen-Chih Yao,Unknown,Male,Unknown,Male,"Suppose H is a real Hilbert space with inner product \(\langle .,.\rangle\) and norm \(\Vert .\Vert\). Let \(B:H\rightarrow 2^{H}\) be a maximal monotone operator and \(A:H\rightarrow H\) be a Lipschitz continuous monotone operator. We consider the following inclusion problem: find \(x\in H\) such that Throughout this paper, we denote the solution set of the inclusion problem (1) by \((A+B)^{-1}(0)\). The inclusion problem (1) contains, as special cases, convexly constrained linear inverse problem, split feasibility problem, convexly constrained minimization problem, fixed point problems, variational inequalities, Nash equilibrium problem in noncooperative games, and many more. See, for instance, Bauschke et al. (2005); Chen and Rockafellar (1997); Combettes and Wajs (2005); Contreras et al. (2004); Dinh and Kim (2016); Lions and Mercier (1979); Moudafi and Thera (1997); Passty (1979); Peaceman and Rachford (1955); Quoc et al. (2012) and the references therein. A popular method for solving problem (1) in real Hilbert spaces, is the well-known forward–backward splitting method introduced by Passty (1979), Lions and Mercier (1979). The method is formulated as where \(J_{\lambda _k}^B:=(I+\lambda _k B)^{-1}\) denotes the resolvent of maximal monotone operator B. It was shown, see for example Chen and Rockafellar (1997), that weak convergence of (2) requires quite restrictive assumptions on A and B, such that the inverse of A is strongly monotone or B is Lipschitz continuous and monotone and the operator \(A + B\) is strongly monotone on Dom(B). Tseng in (2000), weakened these assumptions and included an extra step per each step of (2). In particular, Tseng (2000) introduced the following forward-backward-forward splitting method: and obtained weak convergence result in real Hilbert spaces when the step-sizes \(\lambda _n \in (0,\frac{1}{L})\) and when \(\lambda _n\) is obtained using Armijo line search (see, Tseng (2000), (2.4)). The forward-backward-forward algorithm (3) has been studied extensively in the literature. See, for example, Alves and Geremia (2019); Boţ and Csetnek (2016); Cholamjiak et al. (2018); Gibali and Thong (2018); Khatibzadeh et al. (2017); Latafat and Patrinos (2017); Sahu et al. (2015); Shehu (2019, 2020); Shehu and Cai (2018); Shehu and Iyiola (2020); Thong and Vuong (2019); Thong and Vinh (2019); Wang and Wang (2018) and the references contained therein. Recently, weak convergence of inertial-type forward-backward-forward splitting methods have been studied in Abubakar et al. (2020); Padcharoen et al. (2021); Thong and Van Hieu (2018) and most of other papers. In general, these papers study inertial forward-backward-forward splitting method of the form: Boţ et al. in (2020) obtained weak convergence of (4) when \(\theta \in [0,1)\) and \(0<\rho <\frac{2(1-\theta )^2}{(1+\mu )(2\theta ^2-\theta +1)}\). Abubakar et al. (2020) obtained weak convergence result when \(\rho \in (0,1)\). Padcharoen et al. Padcharoen et al. (2021) gave the weak convergence analysis of (4) when \(\rho =1\), \(0\le \theta <\frac{\sqrt{1+8\epsilon }-1-2\epsilon }{2(1-\epsilon )}, \epsilon :=\frac{1-bL}{1+bL}\) and \(0<a\le \lambda _k\le b<\frac{1}{L}\). The weak convergence in Thong and Van Hieu (2018) was obtained under the same condition as in Padcharoen et al. (2021) for a variational inequality problem. Weak convergence of another version of an inertial forward-backward-forward splitting method has also been studied in Boţ and Csetnek (2016). Quite recently, forward-backward-forward splitting method of the form (4) has been studied for variational inequality problem by Boţ et al. in (2020) when \(\theta =0\), and \(\rho \in (0,2)\). Boţ et al. in (2020) obtained both weak and linear convergence results under some conditions. Our aim in this paper is to study a modified form of inertial forward-backward-forward splitting method (4) and propose an alternated inertial forward-backward-forward splitting method. Specifically, our contributions in this paper are: We propose two alternated inertial forward-backward-forward iterative methods for inclusion problem (1). We give weak and linear convergence analysis of our proposed methods. Our weak and linear convergence analysis are given for both under-relaxation and over-relaxation cases of our proposed methods. Some recent forward-backward-forward iterative method in Boţ et al. (2020); Thong and Vuong (2019); Tseng (2000) can be recovered as special cases of our method. In one of our proposed methods, the variable step-sizes are self-adaptive which do not depend on the Lipschitz constant of A and without any line search procedure. Furthermore, we obtain Fejér monotonicity of the generated sequence with respect to the set of solutions to a certain extent. This property is lost in other inertial type forward-backward-forward method in Boţ et al. (2020); Padcharoen et al. (2021); Thong and Van Hieu (2018). Numerical implementations from test problems including optimal control problems show that our proposed method is competitive with some inertial forward-backward-forward splitting methods in Abubakar et al. (2020); Boţ et al. (2020); Padcharoen et al. (2021); Thong and Vinh (2019); Thong and Van Hieu (2018).",
23.0,1.0,Networks and Spatial Economics,09 September 2022,https://link.springer.com/article/10.1007/s11067-022-09577-6,"Integrated Multi-Level Intermodal Network Design Problem: A Sustainable Approach, Based on Competition of Rail and Road Transportation Systems",March 2023,Shima Taheri,Mohammad Tamannaei,,Female,Male,Unknown,Mix,,
23.0,1.0,Networks and Spatial Economics,25 October 2022,https://link.springer.com/article/10.1007/s11067-022-09569-6,A Novel Method for Finding Minimum-norm Solutions to Pseudomonotone Variational Inequalities,March 2023,Duong Viet Thong,Pham Ky Anh,Do Thi My Linh,,Unknown,,Mix,,
23.0,1.0,Networks and Spatial Economics,04 November 2022,https://link.springer.com/article/10.1007/s11067-022-09579-4,Cost-Efficient Urban Areas Minimising the Connection Costs of Buildings by Roads: Simultaneous Optimisation of Criteria for Building Interval and Built Cluster Size,March 2023,Hiroyuki Usui,,,Male,Unknown,Unknown,Male,"As urban populations increase, urban areas are expanding to provide residents with places to live. In urban economics, the expansion of urban areas generally results from not only a growing population but also rising household incomes and falling commuting costs (Brueckner 2000; Mieszkowski and Mills 1993). Increasing incomes enable residents in urban areas to aim for more living space in suburban areas. This causes urban areas to expand as dwelling sizes increase. The expansion of urban areas is reinforced by investment in road networks, and public transportation networks make travel faster and more convenient, thus reducing the cost of commuting to a workplace in a business district (Brueckner 2000). Although urban areas need to be expanded to accommodate the increasing population, too much expansion of urban areas results in a set of problems called urban sprawl: low-density, dispersed, suburban-style development, often described as the result of rapid, unplanned and/or uncoordinated urban growth (Carruthers and Ulfarsson 2003; Ewing 1997; Nelson and Duncan 1995). Urban sprawl creates the following prominent issues: (1) central city decline without investment in urban core areas; (2) reliance on the use of private cars and therefore road congestion; and (3) public transportation services becoming unviable or inefficient in suburban areas (Belzer and Autler 2002; Dieleman and Wegener 2004; Ewing et al. 2002). From a planning perspective, these issues are triggered by market failures such as subsidies for automobiles and public transport commuters (in Japan) (encouraging long-distance commuting) and local land use regulations (discouraging high-density housing and mixed use) (Ewing and Hamidi 2015). In urban economics, the following three market failures are considered to cause urban sprawl: (1) the failure to take into account the social value of open space when land is converted into urban use; (2) the failure on the part of individual commuters to recognise the social cost of congestion created by their use of road networks, which leads to excessive commuting and urban areas that are too large; and (3) the failure of developers to take into account the entire cost of urban public infrastructure generated by their projects (Brueckner 2000). In general, the expansion of urban areas is accompanied by a large number of road networks to provide residential places (buildings) with urban public infrastructure, such as a water supply system, sewage system and electric power supply, alongside access to anywhere along the road network (Bertaud 2018; Hortas-Rico and Solé-Ollé 2010; Marshall 2009). Urban public infrastructure along road networks is thus simply called road networks. The cost of road networks is proportional to their total length, which depends on the pattern of buildings forming urban areas. The failure of developers mentioned above is critical because the road network-related tax burden on new building owners is typically less than the actual infrastructural costs that they generate (Brueckner 2000). This is because the cost of road networks is shared among all the residents in urban areas rather than being charged directly to those who require them. In fact, the price of road networks is approximately at average cost rather than at marginal cost, and far-flung development patterns increase the average cost due to the large investments required to extend road networks to reach relatively few people (Brueckner 2000; Carruthers 2002; Carruthers and Ulfarsson 2003; Ewing and Hamidi 2015). The way to correct this problem is to adopt a system of impact fees that are calculated specifically to offset the road network cost of the new development (Brueckner 2000). Solving this problem during a period of urban decline is more difficult than doing so during urban growth, and answering the question of how to manage urban public infrastructure including road networks that represent over engineering in relation to the present urban population is an urgent matter (Faust et al. 2016). Conventionally, urban public infrastructure has been designed according to an assumption of either growing or static populations, without future design scenarios that allow for unexpected developments and flexibility. Urban public infrastructure designs have thus been unable to accommodate decreased demand effectively (Faust et al. 2016; Hollander et al. 2009; Schilling and Logan 2008). Inflexible urban public infrastructure designs mean that shrinking cities face obstacles in controlling rising per capita costs for services while continuing to provide services to sparsely distributed urban areas (Faust et al. 2016). The management of urban public infrastructure during an urban growth or decline process is important for sustainable urban space in terms of cost efficiency (Haase et al. 2017; Radzimski 2016). A compact city policy has been attempted through the government regulation of expanding urban areas to mitigate issues of urban sprawl and shrinkage. Compact urban areas are expected to be provided with high-quality services at relatively lower costs than scattered ones (Carruthers 2002). The conventional tools for compact city promotion are zoning regulation, by drawing urban growth boundaries (UGBs), and transit-oriented development (TOD). Zoning regulation is the most straightforward way of achieving a compact city (Carruthers 2002). TOD improves the physical and functional connection of public transportation systems with surrounding mixed land use development. The pros and cons of a compact city policy and its related implementation tools have been discussed from an economic and a planning perspective (Belzer and Autler 2002; Brueckner 2000; Dieleman and Wegener 2004; Ewing 1997; Ewing and Hamidi 2015; Ewing et al. 2002; Gordon and Richardson 1997). Although drawing UGBs is convenient for local policy makers due to its easy implementation, it is not directly connected to the underlying market failures responsible for urban sprawl (Brueckner 2000). It actually consists of remedies based on market mechanisms such as development taxes and impact fees that are directly linked to such market failures (Brueckner 2000; Carruthers 2002; Ewing and Hamidi 2015; Gordon and Richardson 1997; Peiser 1989). TOD has revitalized once-declining city districts that were originally accessible to public transportation, work and amenities, and their populations have grown (e.g. Portland, USA) (Dieleman and Wegener 2004). According to the National Surveys on Communities in USA by Belden and Stewart (2011), this can be explained by changes in residential preferences as people tend to prefer a mix of housing types, with pedestrian amenities and nearby public transportation, to detached single-family homes on large plots in suburban areas where there is no public transportation (Ewing and Hamidi 2015). Tokyo is also regarded as a leading city in TOD practices in terms of plans and policies, actors and implementation (Liu et al. 2022; Thomas and Bertolini 2020). Although Tokyo has been seen as a compact city through TOD since the late nineteenth century, the Japanese central government began to require local governments to make their urban areas spatially compact to reduce the management costs of urban public infrastructure in 2014 (Asami 2017; Usui 2019). In the late twentieth century, Japan was also facing urban sprawl, characterised by sets of sparsely distributed buildings along road networks with no urban planning. Japan entered an era of depopulation at the beginning of the twenty-first century and today faces an urban perforation phenomenon, which involves vacant plots tending to be generated and accumulated randomly when the number of vacant plots without reconstruction after demolition increases in urban areas (Usui and Perez 2022). Although local governments in Japan try to make their urban areas spatially compact by drawing UGBs, the relationship between present and future urban areas in terms of morphology, focusing on road networks connecting buildings and the management cost of road networks, remains unclear and has yet to be considered theoretically. Given that contemporary urban areas are officially delineated by merging predetermined basic spatial units (e.g. census units) that meet the population density criterion (e.g. metropolitan statistical areas (MSAs) and densely inhabited districts (DIDs)Footnote 1) (hereafter called the top-down approach), they depend on how the basic spatial units and the population density criterion are set. However, thresholds such as population density cannot be determined objectively. More importantly, urban areas delineated in this way do not reflect the composition of road networks and the buildings along them (Finance and Swerts 2020). Urban areas should be delineated by focusing on the locations of buildings and road networks (hereafter called a bottom-up approach). In the urban growth and decline process, understanding the relationship between the urban areas formed by road networks connecting the locations of buildings (called morphological urban areas) and the management costs of road networks in a consistent way is fundamental for consistently comparing the current and future urban areas morphologically and evaluating the cost reductions before and after morphologically changing the urban areas (Arribas-Bel et al. 2019; Behnisch et al. 2019; Caruso et al. 2017; Finance and Swerts 2020; Montero et al. 2021; Usui 2019). Urban sprawl undermines the cost-effective provision of urban public infrastructure services, including road networks, and economies of scale for other services (e.g. police protection and public education) by lowering the density of individual consumers (Carruthers and Ulfarsson 2003). This issue provides a practical point of departure for debates about the role that governments should play in regulating the outcome of urban growth (Carruthers and Ulfarsson 2003; Ewing 1997). However, the relationship between morphological urban areas and road networks’ management costs has yet to be revealed (Carruthers 2002; Carruthers and Ulfarsson 2003; Ewing 1997; Ewing and Hamidi 2015). Although methods have been developed for delineating morphological urban areas (see the literature review in the next section), they are not related to the management costs of road networks: this is a research gap. In general, the more spatially compact urban areas are in terms of the road networks and the buildings along them, the more their management costs can be reduced. In sprawling urban areas, a set of sparsely distributed buildings becomes a small settlement, and the nearest-neighbour distance (NND) of a building tends to be greater here than in compact urban areas, which means that the urban management costs are inefficient because the management costs of road networks in sprawling urban areas are higher. Conversely, when the NND and settlement size are shorter and larger, delineated morphological urban areas are spatially compact and cost efficient. A consistent method for delineating morphological urban areas needs to be developed to evaluate whether current urban areas are spatially compact and cost efficient, focusing not only on the locations of buildings and the road networks connecting them but also on their management costs (hereafter simply cost). To fill this research gap, we focus on the relationship between the cost of road networks and the following two key parameters for delineating the morphological urban areas formed by buildings, namely the NND of each building and the number of buildings forming clusters (called built cluster size). The objective of this study is therefore to answer the following question: how can one simultaneously optimise the criteria for the maximum NND of each building, rc, and the minimum built cluster size, mc, to minimise the average cost of the road networks connecting buildings? By solving this optimisation problem, we can delineate optimal morphological urban areas in terms of cost efficiency that minimises the average total cost per building; these are called cost-efficient urban areas. Such optimal urban areas are expected to provide urban planners with a norm that can be compared not only with the current but also with the future image of morphological urban areas in terms of cost efficiency. This paper is organised as follows. The previous literature is reviewed in the second section, focusing on the methods for delineating urban areas and their cost. In the third section, the data processing and the definitions of the notations used in this paper are explained. Built clusters are then modelled and the number of buildings is formulated as the function of rc and mc. This model is applied to an empirical study region in Chiba prefecture, east of Tokyo. In the fourth section, the optimisation problem that minimises the average total cost of the road networks connecting buildings, from which optimal solutions are derived, is formulated. In the fifth section, optimal solutions are computed for Chiba prefecture, and the current optimal urban areas are proposed. The optimal urban areas are compared with natural cities and applied to the locations of buildings and the present DIDs. A discussion of how to make the current dispersed distribution of buildings compact in terms of cost efficiency is also presented. In the final section, we proffer concluding remarks and directions for future research.",1
23.0,1.0,Networks and Spatial Economics,08 November 2022,https://link.springer.com/article/10.1007/s11067-022-09578-5,Dimension Reduction in the Topology of Multilayer Spatial Networks: The Case of the Interregional Commuting in Greece,March 2023,Dimitrios Tsiotas,Vassilis Tselios,,Male,Unknown,Unknown,Male,"Geographical space, transportation, and regional development lie under a symbiotic relation (Tsiotas and Ducruet 2021): from one side, geographical space imposes friction (cost) to movements between places, affecting the structure and functionality of transportation networks and their relevant ability to serve spatial economic development (Barthelemy 2011; Rodrigue et al. 2013). From another side, regional development contributes to the increase (or change) of demand for geographical space and therefore supports the further evolvement of spatial and transportation networks (Rodrigue et al. 2013). In the light of this interdependence, the spatial property is by default determinative for the components of this symbiotic scheme, affecting (a) the shape, form, and usage (urban, rural, regional, etc.) of geographical space; (b) the topology and functionality of transportation networks (Ducruet and Beauguitte 2014); and (c) the centripetal or centrifugal developmental dynamics of market places (Brakman et al. 2005), in the context of the new economic geography. Spatial constraints are immanent at all levels of network aggregation (neighborhood, local, regional, national, and global), assigning distance costs in the development of connections (Barthelemy 2011; Rodrigue et al. 2013). Moreover, spatiality suggests a major force in the configuration of the regional problem (Capello 2016), which regards the asymmetric development observed either between different geographical places, at a fixed time, or at different times for the same place, or jointly. Although spatial constraints are (either directly or indirectly) evident in all aspects of socioeconomic activity, their modeling and study is a complex task submitted to high level of relativity. Such relativity can be witnessed by (a) the uneven spatial distribution of regional variables and indicators observed at various geographical scales (Garretsen et al 2013); (b) the differences in the topology of transportation networks observed due to node aggregation (Tsiotas and Polyzos 2018) or geographical scale (Tsiotas and Ducruet 2021); (c) the emergence of top-down and bottom-up theories (Crescenzi and Rodriguez-Pose 2011) of regional economics and policy; (d) the need of applying diversified regional policies at different levels of geographical agglomeration (Brakman et al. 2005) and market integration (Ottaviano 2003); and more. Recently, network science (Barabasi 2013; Brandes et al. 2013), a discipline studying communication systems with the use of network paradigm, contributed to geographical and regional research by describing the structure of spatial networks beyond their geometry (Barthelemy 2011; Ducruet and Beauguitte 2014; Tsiotas and Ducruet et al. 2021), allowing thus incorporating topological variables in the modeling of spatial systems. This modern discipline has been already proven fertile in the modeling of spatial networks (Barthelemy 2011) and has promoted relevant research in transportation (road, rail, maritime, air transport) and other infrastructure networks. In the context of network paradigm, the regional science, economic geography, and relevant disciplines can deeper conceptualize topological aspects of interconnected systems configuring spatial hierarchies (Ducruet et al. 2011; Ducruet and Beauguitte 2014), and have more options available for the multilayer modeling of spatial and regional economic systems. However, although the coupling of network and regional science is very promising, Marshall et al. (2018) note that network topology is yet a new and undiscovered concept in the context of regional and geographical sciences, and there is still a way to go to integrate topological analysis into current spatial analysis protocols. For instance, Tsiotas and Ducruet (2021) observe that decomposition techniques in network analysis are not yet applied in a comprehensive context to unravel the relationship between space and network topology. Such reasoning motivates this paper to question whether there are also norms or common features ruling the effect of spatial constraints on the configuration of network topology at different degrees of spatial or administrative aggregation, contrary to the undoubted variability describing spatial inequalities at different levels of geographical scale. This research question is inspired by recent approaches (Tsiotas and Polyzos 2018; Tsiotas and Ducruet 2021) examining (one major finding in economics) the “puzzling effect of distance” on spatial economic interaction using the network paradigm. However, this study goes one step further by examining this research question in the context of land (instead of maritime) transportation. Towards answering to this research question, this paper models the interregional commuting system in Greece (GCN) into a multilayer spatial network (Barthelemy 2011; Kivela et al. 2014; Boccaletti et al. 2014) of national geographical scale. It further studies its multilayer topology both (a) within and (b) between its layers, which are configured at different levels of geographical and administrative resolution (at the national, regional, and capital city level) and different types of structure and functionality (geometric, accessibility, and commuting flow). The study builds on methods of complex network analysis (Barthelemy 2011; Barabasi 2013; Kivela et al. 2014; Boccaletti et al. 2014) and dimension reduction (Norusis 2008; Walpole et al. 2012) to examine the topological properties of GCN across its layers and extract the major topological components of this multilayer model. This is done by applying a principal component analysis (PCA) on a collection of topological variables from all layers. This dimension reduction method is chosen because (Wold et al. 1987; Norusis 2008): (a) it properly serves the research question of detecting common topological features amongst different layers of the GCN; (b) is popular, well-established, and empirically tested; and (c) has been already proven useful in multidisciplinary applications. The overall approach in this paper conceptualizes the utility of commuting flows and the cost of spatial distance as major determinants of network topology and performs an interlayer analysis to detect the important topological features across the network layers, which represent different aspects of utility and cost of the interregional commuting market. In the context of a spatial-economic interpretation, the GCN is an integrated utility-cost model with multifaceted structural reference and economic functionality. In this light, the construction and study of a multilayer model of interregional commuting can promote scientific research and policy practice. For instance, the modeling approach proposed by this study can provide insights into spatial and transportation planning, and sustainable development, where it is quite assisting for the planners to conduct strategic national plans based on integrated models incorporating utility-cost (Kulmer et al. 2014) and spatial-economic information (Clinch and O’Neil 2009; Vigar 2009; Kulmer et al. 2014). As a market of immanent (and periodically alternating) supply and demand forces, the interregional commuting network can provide insights into the detection of dipole or polycentric structures in urban and regional systems (Li et al. 2018; Tsiotas et al. 2021). Such structures illustrate either corporate or competitive relations in the geographical space and thus can point out axes and directions toward spatial development (Davoudi 2003), applicable at all geographical scales (Tsiotas et al. 2021). Also, in the model, the availability of layers at different levels of geographical and administrative resolution, can enlighten into a better comprehension of how spatial distance and spatial aggregation (Wegener 2001; Jeon et al. 2012; Tsiotas and Polyzos 2018) affects the structure and functionality of transportation systems. This asset can facilitate more efficient strategic planning based on more accurate and better targeted geographical and administrative levels (Albrechts 2006; Cavaco and Costa 2020). Generally, this paper promotes regional and transportation research by providing an empirical case study and methodological approach examining the interrelation between geographical resolution and spatial hierarchy in transportation networks, which is a major concern in spatial and transportation planning (Rodrigue et al. 2013; Capello 2016). Finally, provided that relevant empirical research mainly builds on comparisons between network layers, this study advances computational network science by configuring a methodological framework for dimension reduction in multilayer networks to detect principal components of network topology and to examine topological consistency across networks. The remainder of this paper is organized as follows; Sect. 2 provides a literature review. Section 3 presents the methodological and conceptual framework of the study. Section 4 shows the results of the analysis and discusses them within the context of regional and geographical sciences, and finally, in Sect. 4 conclusions are given.",
23.0,1.0,Networks and Spatial Economics,28 November 2022,https://link.springer.com/article/10.1007/s11067-022-09581-w,The Impact of Seat Resource Fragmentation on Railway Network Revenue Management,March 2023,Xiang Zhao,Xinghua Shan,Jinfei Wu,,Unknown,Unknown,Mix,,
23.0,1.0,Networks and Spatial Economics,16 December 2022,https://link.springer.com/article/10.1007/s11067-022-09582-9,Traffic Graph Convolutional Network for Dynamic Urban Travel Speed Estimation,March 2023,Huan Ngo,Sabyasachee Mishra,,,Unknown,Unknown,Mix,,
23.0,1.0,Networks and Spatial Economics,19 December 2022,https://link.springer.com/article/10.1007/s11067-022-09580-x,Approximation of a Continuous Core-periphery Model by Core-periphery Models with a Large Number of Small Regions,March 2023,Minoru Tabata,Nobuoki Eshima,,,Unknown,Unknown,Mix,,
23.0,1.0,Networks and Spatial Economics,17 January 2023,https://link.springer.com/article/10.1007/s11067-022-09585-6,"Bicycle-sharing in Beijing: An Assessment of Economic, Environmental, and Health Effects, and Identification of Key Drivers of Environmental Performance",March 2023,Haotian Ma,Xinlu Chen,Qian Wang,Unknown,Unknown,,Mix,,
23.0,1.0,Networks and Spatial Economics,28 January 2023,https://link.springer.com/article/10.1007/s11067-022-09584-7,Designing Metro Network Expansion: Deterministic and Robust Optimization Models,March 2023,Lebing Wang,Jian Gang Jin,Yi Wei,Unknown,,,Mix,,
23.0,2.0,Networks and Spatial Economics,27 July 2023,https://link.springer.com/article/10.1007/s11067-023-09597-w,Intersections Between Mobility and Communications,June 2023,Gregory D. Erhardt,Ke Han,,Male,,Unknown,Mix,,
23.0,2.0,Networks and Spatial Economics,23 June 2021,https://link.springer.com/article/10.1007/s11067-021-09534-9,"Commuting to the future: Assessing the relationship between individuals’ usage of information and communications technology, personal attitudes, characteristics and mode choice",June 2023,Dea van Lierop,Francisco J. Bahamonde-Birke,,Female,Male,Unknown,Mix,,
23.0,2.0,Networks and Spatial Economics,01 July 2021,https://link.springer.com/article/10.1007/s11067-021-09539-4,Analysing Spatial Intrapersonal Variability of Road Users Using Point-to-Point Sensor Data,June 2023,F. Crawford,D. P. Watling,R. D. Connors,Unknown,Unknown,Unknown,Unknown,,
23.0,2.0,Networks and Spatial Economics,12 June 2021,https://link.springer.com/article/10.1007/s11067-021-09542-9,Routine Pattern Discovery and Anomaly Detection in Individual Travel Behavior,June 2023,Lijun Sun,Xinyu Chen,Luis F. Miranda-Moreno,Unknown,Unknown,Male,Male,"With the rapid development of information and communications technology (ICT), large amounts of spatiotemporal data—such as GPS trajectory data, license plate recognition (LPR) data, call detailed records (CDR), and transit smart card transactions—are generated continuously through individuals’ mobility and travel activities. The high spatial/temporal resolution of these data sets sheds new lights on advancing our understanding of human mobility and travel behavior, which have been shown to be highly consistent and predictable over time in previous studies. The intrinsic regularity of travel behavior allows planners and practitioners to design better transport systems and services based on simplified and aggregated patterns (Kitamura et al. 2006; Schönfelder and Axhausen 2016; Sun et al. 2013). However, given the increasing amount of individual-based spatiotemporal data, we may observe anomalous behavior frequently in contrary to individual regularity. For instance, a common assumption of smart card data is to consider each ID a unique user, while in reality, two users may share one card for their daily transit use. The same applies to vehicle usage, for which we consider license plate number a proxy to a unique driver, while a car in a household can be shared among many members. When this happens, we may observe anomalous behaviors with regard to the regularity and stability of individual travel behavior. Moreover, in the long term, an individual’s travel pattern may also change over time, such as changing jobs and moving to a new address. For example, Zhao et al. (2018a) studied pattern changes in individual travel behavior using long-term smart card transactions. The authors define travel pattern changes as “abrupt, substantial, and persistent changes in the underlying travel patterns” and apply a moving kernel to measure the degree of changes over time for frequency, spatial, and temporal features, respectively. Detecting such anomalies in travel behavior is essential to many transportation planning, traffic operation, and law enforcement applications. At a collective level, the anomaly may come from intrinsic variations resulted from incidents, events, policy implementation, and infrastructure constraints. For instance, travel behavior of students may change substantially from a spring semester to the following summer vacation. Transport agencies should take this collective behavior shift into consideration in their daily traffic management practices. At an individual level, the anomaly may arise from factors such as change of home/work location, road space rationing policy, or even sharing one’s car with a friend. However, given the high complexity and large volume in emerging spatiotemporal behavioral data, it is infeasible to examine the data manually to identify the behavior anomalies mentioned above. This paper attempts to provide a unified framework for characterizing meaningful patterns in travel behavior and then detecting anomalies based on the learned patterns. We define individual behavior anomalies as cases where it is difficult to predict one’s current/future activity given her past travel behavior. The contribution of this paper is two-fold. First, we develop a generative model for individual spatiotemporal travel records. Specifically, we extend the latent Dirichlet allocation (LDA) model Blei et al. (2003) to generate spatiotemporal records and use a two-dimensional probability distribution to model a traveler’s topic distribution. The model can (1) uncover meaningful spatiotemporal travel behavior patterns by sharing information from a large number of users and (2) summarize an individual’s complex mobility data into a low-dimensional latent space, which not only provides a powerful way to predict one’s future mobility patterns but also allows us to quantify the similarity between users and cluster users. This knowledge is critical to many smart transportation applications such as trip planning and car sharing. The compact representation obtained for each individual can also be used in other applications such as clustering travelers with similar behavior patterns. Second, on top of the generative mobility model, we propose a probabilistic framework for detecting anomalies in individual travel behavior using perplexity as a scoring function. A similar approach has been developed in Xiong et al. (2011) and Yu et al. (2014), but in this paper we focus on individual behavior: we measure the degree of behavior anomaly for each individual traveler by using the “predictability” of future mobility records under the trained model. A high perplexity indicates that an individual’s future behavior cannot be well reconstructed by her past behavior. And a low perplexity score suggests that the model can well predict an individual’s future travel behavior. To verify the effectiveness of this framework, we conduct an empirical analysis based on a large-scale LPR data set collected from Guangzhou, China. Note that the general framework can be applied to a variety of mobility data with spatial and temporal information encoded (e.g., smart card data and GPS data). The remainder of this paper is structured as follows. In Section 2, we review relevant literature on spatiotemporal mobility modeling at both individual and collective levels, and recent work on anomaly detection in travel behavior. In Section 3, we introduce the LPR data used in this study. Section 4 presents the key LDA-based model for spatiotemporal mobility modeling, including the inference algorithm and the anomaly detection framework. In Section 5, we conduct extensive numerical experiments to demonstrate the effectiveness of this framework. Section 6 concludes this study and provides some future research directions.",2
23.0,2.0,Networks and Spatial Economics,03 December 2021,https://link.springer.com/article/10.1007/s11067-021-09541-w,An Open-Source Framework to Implement Kalman Filter Bus Arrival Predictions,June 2023,Sean Óg Crudden,Simon Berrebi,,Male,Male,Unknown,Male,"Using Automatic Vehicle Location (AVL) technologies, transit agencies can predict vehicle arrivals in real-times. These real-time predictions are necessary to support live decision-making for both passengers and transit agencies (Cats and Jenelius 2014). From the passenger’s perspective, knowing when a transit vehicle will arrive informs travel time, mode, and route choice. For transit agencies, real-time information can be used to control transit operations dynamically in order to improve service capacity and reliability, which are both determinants of transit ridership. Research shows that providing real-time information through mobile applications improves the ridership experience, and in some cases increases ridership. Providing real-time information reduces both perceived and actual wait time of passengers at stops and increases perceived safety (Watkins et al. 2011, Ferris et al. 2010). Real-time information affects the way passengers plan their trips, when they decide to travel, and which routes they choose to take (Cats et al. 2011). Mobile applications with real-time information can encourage single-occupancy drivers to start riding transit, and current riders to make more trips by transit. Several studies have found a significant increase in transit ridership following real-time information availability (Zhang et al. 2011; Tang and Thakuriah 2012; Brakewood et al. 2015). Vehicle arrival predictions can also be used by transit agencies to control operations. On frequent transit routes, transit vehicles have a tendency to bunch together, which causes undue waiting time for passengers in the form of delays and reduced service frequency (Verbich et al. 2016). Several methods in the literature use real-time information to dynamically adjust headways (i.e. time between transit vehicles) according to current operating conditions (Daganzo and Pilachowski 2011, Bartholdi et al. 2012, Berrebi et al. 2015). When provided accurate vehicle arrival predictions, these methods have been shown to outperform the schedule in a wide range of environments (Berrebi et al. 2018a). The real-time prediction methods used in practice, however, are still at an elementary stage, and do not fully support real-time decision-making for passengers or transit agencies. Using data from the main transit agency in Stockholm, Sweden, (SLL) Cats and Loutos (2016a) evaluated prediction accuracy. They found that at the 10-min horizon (when predicting the arrival time of a vehicle that is 10 min away), the standard deviation of predictions is 3 min. In other words, just two-thirds of vehicles are predicted to arrive between 7 and 13 min. Cats and Loutos found systematic underestimation of vehicle arrivals. These errors limit the value of real-time information for passengers, who cannot fully rely on them to plan trips and make it impossible to control operations in real-time. The typical prediction method assumes that deviations from the schedule remain constant as a vehicle makes its way to downstream stops. While this approach can alert passengers of upstream delays, it does not consider the tendency for delays to propagate. Particular circumstances such as the weather, traffic congestion, or surges in demand can impact travel times between stops. There are highly sophisticated methods in the literature that use cutting-edge multivariate forecasting and machine learning methods to predict vehicle arrival times. These methods have been shown to provide more accurate predictions than the schedule-based method used in practice. The main barrier to implementation, however, is the lack of framework to process the data coming from different sources in real-time. This paper presents a novel system architecture capable of employing prediction methods that require multiple data inputs in real-time. A refined Kalman Filter algorithm based on Reinhoudt and Velastin (1997) is implemented on the entire Metro Transit network for one month. The software is implemented on the open-source platform TheTransitClock. Each of the main requirements for Kalman Filters and functionalities provided by TheTransitClock are described. Prediction accuracy results are compared to a schedule-based prediction method by route and on the entire network. Finally, the use of real-time passenger counts to expand the accuracy of the prediction tool is discussed.",2
23.0,2.0,Networks and Spatial Economics,24 June 2021,https://link.springer.com/article/10.1007/s11067-021-09543-8,Optimization of Station-Skip in a Cyclic Express Subway Service,June 2023,Jingfeng Yang,Hai Wang,Jiangang Jin,Unknown,,Unknown,Mix,,
23.0,2.0,Networks and Spatial Economics,31 May 2021,https://link.springer.com/article/10.1007/s11067-021-09532-x,Stability and Convergence in Matching Processes for Shared Mobility Systems,June 2023,Roger B. Chen,Christopher Valant,,Male,Male,Unknown,Male,"With a wider adoption in mobile information and communications technologies (ICT), innovative shared mobility solutions, such as car-sharing and ridesharing, have begun to feasibly emerge from private companies like Uber and Lyft. These ridesourcing services rely on matching agents such that each agent. This paper examines the stability and convergence of ridesharing markets modeled as a two-sided matching market, following a similar perspective in recent studies (Mittendorf 2018; Djavadian and Chow 2017). Recent studies have also considered how to match agents towards an equitable goal (Lyu et al. 2019). If successfully implemented, these ridesharing would greatly benefit low and medium density communities that face many challenges for implementing successful conventional public mobility systems (Kuby et al. 2004). Planning organizations and public mobility service providers need to consider these alternatives. Services such as Uber and Lyft further improve the prospects of ridesourcing for communities with high personal vehicle ownership, but few feasible alternative travel modes. The flexibility of ridesharing organically provides an alternative to the fixed nature of conventional public transit (Frei et al. 2015; Frei and Mahmassani 2013). Rather than operating on relatively fixed routes and timetables, shared mobility systems are untethered to any fixed infrastructure or schedules, and with enough participants, can respond to varying demand. These contexts underscore the motivation for this paper, which focuses on the stability and convergence of matching in ridesourcing systems (Wang and Yang 2019; Yang et al. 2020). However, the behavioral insights broadly apply to matching processes in other shared systems as well. We view ridesourcing systems as a matching market and describe the matching process from the viewpoint of an assignment game. The economics literature contains a large volume of research on the matching problem; many are related variants on the marriage problem (Gale and Shapley 1962). In the marriage problem, players have ordinal preferences for matching with members of the other population. The “core” consists of matchings, such that no pair would prefer other participants to their current partners; matchings are typically stable (Roth and Sotomayor 1992). Similar results exist for many-to-one and many-to-many matching perspectives, referred to as the roommate problem with non-transferable utility in the literature (Klaus and Klijn 2007; Kojima and Unver 2008). Furthermore, we view the matching process as a self-coordinating system where drivers match with riders, each evaluating their own resulting payoffs relative to current matches and preferences. The ridesharing matching problem examined here differs from both the marriage and roommate problems. The latter problems are framed in terms of nontransferable (typically ordinal) utility. We frame potential matches in terms of a transferable value. The difficulty in using the known literature results from the marriage and roommate problems is that payoff improving deviations from a match result from players’ current matches and preferences. In an assignment game, match payoffs result from splitting of the value of the matches, similar to an outcome from a negotiation process. Stability and convergence in matching in the assignment game arises is harder to reach, being more constrained relative to the marriage or roommate problems from the literature (Roth and Vande Vate 1990; Naxy and Pradelski 2016). Recently, within the transportation literature, researchers have contributed strongly the topic of ridesharing and shared services, from both a modeling (Djavadian and Chow 2017; Qian and Ukkusuri 2017; Lee and Savelsbergh 2015; Hall and Qureshi 1997) and empirical perspective (Sanchez et al. 2016; Chan and Shaheen 2012). Several day-to-day simulation studies on ridesharing capture its dynamics using an agent-based simulation to reach an assumed SUE and address the stochastic nature of these systems (Djavadian and Chow 2017). Agent-based simulations of day-to-day traffic equilibrium models have a long intellectual history the transportation literature (Horowitz 1984; Cantarella and Cascetta 1995; Friesz et al. 1994; Zhang et al. 2001; Watling and Hazelton 2003; Chen and Mahmassani 2004; Yang and Zhang 2009; Bie and Lo 2010) and typically assume an adjustment process for travelers. Extensions to systems with adjustments on both sides for travelers and service providers (Cantarella et al. 2015; Li and Yang 2016) fit naturally with the current scenario of shared mobility services with adjustments for both riders and non-traditional third-party hailing taxi service drivers with variable fares (Djavadian and Chow 2017). Synergistic with develops in transportation, two-sided markets have a strong presence in the economics literature (Rochet and Tirole 2003, 2006), defined as markets with platforms enabling interactions between end-users; the goal is to get both sides “on board” by appropriately charging each other. The literature indicates the social optimum in a two-sided market may differ from a conventional one-sided market (Rochet and Tirole 2003). Modeling current mobility services, such as Uber and Lyft, should naturally follow a two-sided market perspective for representing network externalities between travelers and service providers. For services, such as Uber, the fleet size varies day-to-day based on the number of drivers available, which in turn depend on the availability of customers, operating costs and profit. Djavadian and Chow (2017) specifically consider a broad set of flexible transport services matching service providers with riders in the context of a two-sided market. The authors model an endogenous adjustment between service operators (sellers) and user demand in a single modeling framework. Using an agent-based stochastic day-to-day adjustment process under ride-sourcing setting. This paper follows this body of work to consider a different adjustment process from a behavioral perspective, where agents, both driver and riders, adjust a time-dependent aspiration level for matching that endogenously varies with the history and experience of past encounters. Conventionally, ridesourcing platforms are two-sided markets and matching demand and supply, or riders with drivers, is a critical factor in level-of-service. Consequently, matching and order dispatching algorithms affect the overall performance of these system. Good matching algorithms provide better service for customers, but also efficient fleet (Wang and Yang 2019; Yang et al. 2020). This paper contributes towards developing good matching algorithms with these customer service goals. In this paper, we specifically examine the stability and convergence in a ridesharing matching market, where a single driver matches with a single rider and adjust a time-dependent aspiration level, given an initial willingness-to-pay (WTP) and willingness-to-accept (WTA), for riders and drivers respectively. A series of agent-based simulation experiments illustrate the different rates of convergence and robustness in stability from assuming: (i) varying the size of the system with respect to number of drivers and riders; (ii) range of potential match payoffs between agents and (iii) different ranges for the final transaction price that favor the driver, rider or neither. This paper departs from previous studies by considering an adjustment process where agents on both sides of the market have an endogenous aspiration level adjusted over time as new experiences are gained. Additionally, the model allows for evolving aspirations on both the driver and rider sides of the market. Matching and order dispatching pose fundamental challenges for ridesourcing platforms. One main barrier is the highly dynamic and time-varying stochasticity, and uncertainty these systems face. In this paper, by modeling the evolution of these systems in terms of individual aspirations and their long-term role in ensuring of more satisfactory matches in ridesourcing systems, we address this barrier. We distinguish the ridesourcing system in this work and its matching operations from ride-pooling services which operate as shared ride-hailing options (Ke et al. 2020). In the next section, we present the modeling framework next, followed by a description and discussion of the simulation experiment results. We conclude with a summary of results and future directions for investigating this matching process for resourcing systems.",1

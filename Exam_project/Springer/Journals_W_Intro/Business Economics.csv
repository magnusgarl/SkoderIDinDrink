Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
41,1,Business Economics,01 January 2006,https://link.springer.com/article/10.2145/20060101,Potential Growth of the U.S. Economy: Will the Productivity Resurgence Continue?,January 2006,Dale W Jorgenson,Mun S Ho,Kevin J Stiroh,,,Male,Mix,,
41,1,Business Economics,01 January 2006,https://link.springer.com/article/10.2145/20060102,Information Technology and the Data Jungle,January 2006,Rosemary D Marcuss,,,Female,Unknown,Unknown,Female,,
41,1,Business Economics,01 January 2006,https://link.springer.com/article/10.2145/20060103,Saving for Retirement: Understanding the Importance of Heterogeneity,January 2006,Andrew A Samwick,,,Male,Unknown,Unknown,Male,,13
41,1,Business Economics,01 January 2006,https://link.springer.com/article/10.2145/20060104,India and the Global Economy,January 2006,Beth Anne Wilson,Geoffrey N Keim,,Female,Male,Unknown,Mix,,
41,1,Business Economics,01 January 2006,https://link.springer.com/article/10.2145/20060105,Using Neural Nets to Forecast the Unemployment Rate,January 2006,Rolando F Peláez,,,Male,Unknown,Unknown,Male,,12
41,1,Business Economics,01 January 2006,https://link.springer.com/article/10.2145/20060106,A Chain-Type Price Index for New Business Jet Aircraft,January 2006,Dong W Cho,,,,Unknown,Unknown,Mix,,
41,1,Business Economics,01 January 2006,https://link.springer.com/article/10.2145/20060107,Regulatory Reform and the U.S. Manufacturing Sector,January 2006,Thomas A Hemphill,,,Male,Unknown,Unknown,Male,,
41,1,Business Economics,01 January 2006,https://link.springer.com/article/10.2145/20060108,Housing Costs in the CPI: What Are We Measuring?,January 2006,Joseph G Carson,David S Johnson,Charles Steindel,Male,Male,Male,Male,,5
41,2,Business Economics,01 April 2006,https://link.springer.com/article/10.2145/20060201,Enhancing Fed Credibility,April 2006,Janet L Yellen,,,Female,Unknown,Unknown,Female,,8
41,2,Business Economics,01 April 2006,https://link.springer.com/article/10.2145/20060202,Financing Retirement: The Private Sector,April 2006,David A Wise,,,Male,Unknown,Unknown,Male,,
41,2,Business Economics,01 April 2006,https://link.springer.com/article/10.2145/20060203,Improving Incentives in Health Care Spending,April 2006,Katherine Baicker,,,Female,Unknown,Unknown,Female,,3
41,2,Business Economics,01 April 2006,https://link.springer.com/article/10.2145/20060204,Subjective Probability Forecasts for Recessions,April 2006,Kajal Lahiri,J George Wang,,Female,Unknown,Unknown,Female,,14
41,2,Business Economics,01 April 2006,https://link.springer.com/article/10.2145/20060205,The Unemployment Effects of Proposed Changes in Social Security's “Normal Retirement Age”,April 2006,Franklin A Michello,William F Ford,,Male,Male,Unknown,Male,,9
41,2,Business Economics,01 April 2006,https://link.springer.com/article/10.2145/20060206,Whose Capital is It? Putting Owners Back in Control,April 2006,John Bogle,,,Male,Unknown,Unknown,Male,,
41,2,Business Economics,01 April 2006,https://link.springer.com/article/10.2145/20060207,BJK Research LLC,April 2006,Bruce Kratofil,,,Male,Unknown,Unknown,Male,,
41,2,Business Economics,01 April 2006,https://link.springer.com/article/10.2145/20060208,Employment Dynamics: BLS and Census Bureau Use Administrative Records to Provide New Data,April 2006,Robert P Parker,,,Male,Unknown,Unknown,Male,,1
41,2,Business Economics,01 April 2006,https://link.springer.com/article/10.2145/20060209,The U.S. Market for Power Lawn and Garden Equipment,April 2006,Michael A Deneen,Andrew C Gross,,Male,Male,Unknown,Male,,2
41,3,Business Economics,01 July 2006,https://link.springer.com/article/10.2145/20060301,Private Equity Finance as a Growth Engine: What It Means for Emerging Markets,July 2006,Florence Eid,,,Female,Unknown,Unknown,Female,,5
41,3,Business Economics,01 July 2006,https://link.springer.com/article/10.2145/20060302,Domestic Implications of a Global Labor Market,July 2006,John E Silvia,,,Male,Unknown,Unknown,Male,,4
41,3,Business Economics,01 July 2006,https://link.springer.com/article/10.2145/20060303,A Temporary Tax Rebate in a Recession: Is it Effective and Safe?,July 2006,Laurence S Seidman,Kenneth A Lewis,,Female,Male,Unknown,Mix,,
41,3,Business Economics,01 July 2006,https://link.springer.com/article/10.2145/20060304,When Will Social Security Shortfalls Begin to Pinch?,July 2006,Jerry H Tempelman,,,Male,Unknown,Unknown,Male,,
41,3,Business Economics,01 July 2006,https://link.springer.com/article/10.2145/20060305,Consumer-Driven Health Plans: New Developments and the Long Road Ahead,July 2006,Richard M Scheffler,Mistique C Felton,,Male,Unknown,Unknown,Male,,3
41,3,Business Economics,01 July 2006,https://link.springer.com/article/10.2145/20060306,"Medicaid, State Finances, and the Bottom Line for Businesses",July 2006,Louis F Rossiter,Randy F Neice,,Male,,Unknown,Mix,,
41,3,Business Economics,01 July 2006,https://link.springer.com/article/10.2145/20060307,Modeling NABE Members' Compensation,July 2006,Peter Jaquette,,,Male,Unknown,Unknown,Male,,5
41,3,Business Economics,01 July 2006,https://link.springer.com/article/10.2145/20060308,"MillicentCox, LLC",July 2006,Millicent Cox,,,Female,Unknown,Unknown,Female,,
41,3,Business Economics,01 July 2006,https://link.springer.com/article/10.2145/20060309,The Global Market for Power Tools,July 2006,Michael A Deneen,Andrew C Gross,,Male,Male,Unknown,Male,,6
41,4,Business Economics,01 October 2006,https://link.springer.com/article/10.2145/20060401,The Monetary Policy Model,October 2006,William Poole,,,Male,Unknown,Unknown,Male,,
41,4,Business Economics,01 October 2006,https://link.springer.com/article/10.2145/20060402,Reflections of a Professional Life-Long Fed Watcher,October 2006,Stuart G Hoffman,,,Male,Unknown,Unknown,Male,,
41,4,Business Economics,01 October 2006,https://link.springer.com/article/10.2145/20060403,Strengthening Globalization's Invisible Hand: What Matters Most?,October 2006,Thomas F Siems,Adam S Ratner,,Male,Male,Unknown,Male,,2
41,4,Business Economics,01 October 2006,https://link.springer.com/article/10.2145/20060404,Pricing of Mutual Fund Services in Retirement Plans: Evidence from Open-End Equity Funds,October 2006,Jacob De Rooy,,,Male,Unknown,Unknown,Male,,
41,4,Business Economics,01 October 2006,https://link.springer.com/article/10.2145/20060405,Productivity and Wages,October 2006,Edward P Lazear,,,Male,Unknown,Unknown,Male,,5
41,4,Business Economics,01 October 2006,https://link.springer.com/article/10.2145/20060406,Expected Future Budget Deficits and the U.S. Yield Curve,October 2006,Lloyd B Thomas,Danhua Wu,,Male,Unknown,Unknown,Male,,2
41,4,Business Economics,01 October 2006,https://link.springer.com/article/10.2145/20060407,Original Equipment Suppliers Association,October 2006,Dave Andrea,,,Male,Unknown,Unknown,Male,,
41,4,Business Economics,01 October 2006,https://link.springer.com/article/10.2145/20060408,Energy Information Administration Provides Extensive Data on Petroleum Supply and Demand Conditions,October 2006,Robert P Parker,,,Male,Unknown,Unknown,Male,,
42,1,Business Economics,01 January 2007,https://link.springer.com/article/10.2145/20070101,"U.S. International Deficits, Debt, and Income Payments: Key Relationships Affecting the International Outlook",January 2007,John Kitchen,,,Male,Unknown,Unknown,Male,,1
42,1,Business Economics,01 January 2007,https://link.springer.com/article/10.2145/20070102,Borrowing Without Debt? Understanding the U.S. International Investment Position,January 2007,Matthew Higgins,Thomas Klitgaard,Cédric Tille,Male,Male,Male,Male,,14
42,1,Business Economics,01 January 2007,https://link.springer.com/article/10.2145/20070103,The Valuation of Hidden Assets in Foreign Transactions: Why “Dark Matter” Matters,January 2007,Ricardo Hausmann,Federico Sturzenegger,,Male,Male,Unknown,Male,,3
42,1,Business Economics,01 January 2007,https://link.springer.com/article/10.2145/20070104,Financial Literacy and Retirement Preparedness: Evidence and Implications for Financial Education,January 2007,Annamaria Lusardi,Olivia S Mitchelli,,Female,Female,Unknown,Female,,744
42,1,Business Economics,01 January 2007,https://link.springer.com/article/10.2145/20070105,Does Prepayment Risk of Mortgages Affect Excess Returns of Bank Stocks?,January 2007,Ling T He,,,,Unknown,Unknown,Mix,,
42,1,Business Economics,01 January 2007,https://link.springer.com/article/10.2145/20070106,The Link Between Gasoline Prices and Vehicle Sales,January 2007,Walter McManus,,,Male,Unknown,Unknown,Male,,13
42,1,Business Economics,01 January 2007,https://link.springer.com/article/10.2145/20070107,Cost-Benefit Analysis: Regulatory Reform or Favoring the Regulated?,January 2007,Thomas A Hemphill,,,Male,Unknown,Unknown,Male,,
42,1,Business Economics,01 January 2007,https://link.springer.com/article/10.2145/20070108,Windows and Doors Around the World—The Global Market for Fenestration Products,January 2007,Kenneth Long,Andrew Gross,,Male,Male,Unknown,Male,,2
42,2,Business Economics,01 April 2007,https://link.springer.com/article/10.2145/20070201,Housing Activity and Consumer Spending,April 2007,Jonathan McCarthy,Charles Steindel,,Male,Male,Unknown,Male,,5
42,2,Business Economics,01 April 2007,https://link.springer.com/article/10.2145/20070202,Dynamic Adjustments in Transfer Pricing Agreements,April 2007,David Broomahll,,,Male,Unknown,Unknown,Male,,2
42,2,Business Economics,01 April 2007,https://link.springer.com/article/10.2145/20070203,The Propensity To Sue: Why Do People Seek Legal Actions?,April 2007,Fredrick C Dunbar,Faten Sabry,,Male,Female,Unknown,Mix,,
42,2,Business Economics,01 April 2007,https://link.springer.com/article/10.2145/20070204,"Consumer-Driven Healthcare: Information, Incentives, Enrollment, and Implications for National Health Expenditures",April 2007,Paul Hughes-Cromwick,Sarah Root,Charles Roehrig,Male,Female,Male,Mix,,
42,2,Business Economics,01 April 2007,https://link.springer.com/article/10.2145/20070205,New Dimensions of Financial Liberalization in Japan,April 2007,Masaharu Takenaka,,,Male,Unknown,Unknown,Male,,1
42,2,Business Economics,01 April 2007,https://link.springer.com/article/10.2145/20070206,New Monthly Hours and Earnings Measures from the Bureau of Labor Statistics' Current Employment Statistics Program,April 2007,Robert P Parker,,,Male,Unknown,Unknown,Male,,2
42,3,Business Economics,01 July 2007,https://link.springer.com/article/10.2145/20070301,The Changing Dynamics of Inflation,July 2007,Randall S Kroszner,,,Male,Unknown,Unknown,Male,,1
42,3,Business Economics,01 July 2007,https://link.springer.com/article/10.2145/20070302,The Competitiveness of European Financial Markets,July 2007,Gertrude Tumpel-Gugerell,,,Female,Unknown,Unknown,Female,,2
42,3,Business Economics,01 July 2007,https://link.springer.com/article/10.2145/20070303,Germany's Post-Wirtschaftswunder Struggle,July 2007,Jörg Decressin,,,Male,Unknown,Unknown,Male,,
42,3,Business Economics,01 July 2007,https://link.springer.com/article/10.2145/20070304,Demystifying Japan's Economic Recovery,July 2007,Jun Kurihara,,,,Unknown,Unknown,Mix,,
42,3,Business Economics,01 July 2007,https://link.springer.com/article/10.2145/20070305,Making Health Care More Affordable Through Health Insurance Finance Reform,July 2007,Katherine Baicker,,,Female,Unknown,Unknown,Female,,
42,3,Business Economics,01 July 2007,https://link.springer.com/article/10.2145/20070306,Seeking a Replacement for the Medicare Physician Services Payment Method,July 2007,Martey S Dodoo,Robert L Phillips Jr.,Larry A Green,Unknown,Male,Male,Male,,
42,3,Business Economics,01 July 2007,https://link.springer.com/article/10.2145/20070307,The U.S. Television Set Market,July 2007,Shawn G DuBravac,,,,Unknown,Unknown,Mix,,
42,4,Business Economics,01 October 2007,https://link.springer.com/article/10.2145/20070401,The Explanatory Power of Monetary Policy Rules,October 2007,John B Taylor,,,Male,Unknown,Unknown,Male,,4
42,4,Business Economics,01 October 2007,https://link.springer.com/article/10.2145/20070402,The Incredible Shrinking Banking Industry,October 2007,Carl R Tannebaum,,,Male,Unknown,Unknown,Male,,
42,4,Business Economics,01 October 2007,https://link.springer.com/article/10.2145/20070403,Forecasting Components of Consumption with Components of Consumer Sentiment,October 2007,James A Wilcox,,,Male,Unknown,Unknown,Male,,28
42,4,Business Economics,01 October 2007,https://link.springer.com/article/10.2145/20070404,An Empirical Model of the Sources of Innovation in the U.S. Manufacturing Sector,October 2007,Jeremy A Leonard,Cliff Waldman,,Male,Male,Unknown,Male,,8
42,4,Business Economics,01 October 2007,https://link.springer.com/article/10.2145/20070405,Analyzing the Link Between Real GDP and Employment: An Industry Sector Approach,October 2007,Barbara Sawtelle,,,Female,Unknown,Unknown,Female,,9
42,4,Business Economics,01 October 2007,https://link.springer.com/article/10.2145/20070406,Comparing Aggregate Housing Price Measures,October 2007,Jordan Rappaport,,,Male,Unknown,Unknown,Male,,2
42,4,Business Economics,01 October 2007,https://link.springer.com/article/10.2145/20070407,The Global Market for Agricultural Machinery and Equipment,October 2007,Anand Mehta,Andrew C Gross,,Male,Male,Unknown,Male,,3
43,1,Business Economics,01 January 2008,https://link.springer.com/article/10.2145/20080101,Forecasting U.S. Recessions with Probit Stepwise Regression Models,January 2008,John Silvia,Sam Bullard,Huiwen Lai,Male,,Unknown,Mix,,
43,1,Business Economics,01 January 2008,https://link.springer.com/article/10.2145/20080102,Forecasting Real Inventories and the Anomaly of Money Illusion,January 2008,Anthony Joseph,Maurice Larrain,Eshwar Singh,Male,Male,Unknown,Male,,1
43,1,Business Economics,01 January 2008,https://link.springer.com/article/10.2145/20080103,Nigeria in the Global Economy,January 2008,Temitope W Oshikoya,,,Unknown,Unknown,Unknown,Unknown,,
43,1,Business Economics,01 January 2008,https://link.springer.com/article/10.2145/20080104,Is Inequality Growing as American Workers Fall Behind?,January 2008,John A Tatom,,,Male,Unknown,Unknown,Male,,1
43,1,Business Economics,01 January 2008,https://link.springer.com/article/10.2145/20080105,Economic Research in a Financial Group in Mexico,January 2008,Alberto Gomez-Alcala,,,Male,Unknown,Unknown,Male,,2
43,1,Business Economics,01 January 2008,https://link.springer.com/article/10.2145/20080106,Regional Economic Data from Federal Government Agencies: Availability and Access,January 2008,Robert P Parker,,,Male,Unknown,Unknown,Male,,
43,1,Business Economics,01 January 2008,https://link.springer.com/article/10.2145/20080107,The U.S. Wine Industry,January 2008,Barbara Insel,,,Female,Unknown,Unknown,Female,,8
43,2,Business Economics,01 April 2008,https://link.springer.com/article/10.2145/20080201,A Behavioral Life-Cycle Approach to Understanding the Wealth Effect,April 2008,Diane K Schooley,Debra Drecnik Worden,,Female,Female,Unknown,Female,,
43,2,Business Economics,01 April 2008,https://link.springer.com/article/10.2145/20080202,The Depoliticization of Monetary Policy,April 2008,Jerry H Tempelman,,,Male,Unknown,Unknown,Male,,
43,2,Business Economics,01 April 2008,https://link.springer.com/article/10.2145/20080203,The Puzzle of Manufacturing Sector Investment,April 2008,Donald A Norman,,,Male,Unknown,Unknown,Male,,
43,2,Business Economics,01 April 2008,https://link.springer.com/article/10.2145/20080204,"The Great Inflation: Inflation, Inflationary Expectations, and the Phillips Cycle 1960–2002",April 2008,Thomas W Synott,,,Male,Unknown,Unknown,Male,,
43,2,Business Economics,01 April 2008,https://link.springer.com/article/10.2145/20080205,The Effects of Education on the Natural Rate of Unemployment,April 2008,Albert E DePrince Jr,Pamela D Morris,,Male,Female,Unknown,Mix,,
43,2,Business Economics,01 April 2008,https://link.springer.com/article/10.2145/20080206,Fiscal Realities for the State and Local Governments,April 2008,Roger E Brinner,Joyce Brinner,Megan Leahey,Male,Female,Female,Mix,,
43,2,Business Economics,01 April 2008,https://link.springer.com/article/10.2145/20080207,Regulatory Rules and Estimating Economic Growth: Two Perspectives on Expensing Employee Stock Options,April 2008,Cynthia A Glassman,David N Beede,,Female,Male,Unknown,Mix,,
43,2,Business Economics,01 April 2008,https://link.springer.com/article/10.2145/20080208,"The Global Market for Buses, 2000-2010",April 2008,Lance A Ealey,Andrew C Gross,,Male,Male,Unknown,Male,,
43,3,Business Economics,01 July 2008,https://link.springer.com/article/10.2145/20080301,The Fed's New Communication Strategy: Is It Stealth Inflation Targeting?,July 2008,Michael Woodford,,,Male,Unknown,Unknown,Male,,
43,3,Business Economics,01 July 2008,https://link.springer.com/article/10.2145/20080302,Subprime Credit: The Evolution of a Market,July 2008,John Silvia,,,Male,Unknown,Unknown,Male,,
43,3,Business Economics,01 July 2008,https://link.springer.com/article/10.2145/20080303,Shifting Trends in Semiconductor Prices and the Pace of Technological Progress,July 2008,Ana Aizcorbe,Stephen D Oliner,Daniel E Sichel,Female,Male,Male,Mix,,
43,3,Business Economics,01 July 2008,https://link.springer.com/article/10.2145/20080304,Economic Implications of Mexico's Sudden Demographic Transition,July 2008,Fernando Sedano,,,Male,Unknown,Unknown,Male,,
43,3,Business Economics,01 July 2008,https://link.springer.com/article/10.2145/20080305,Porter's Model of Generic Competitive Strategies,July 2008,Orges Ormanidhi,Omer Stringa,,Male,Male,Unknown,Male,,
43,3,Business Economics,01 July 2008,https://link.springer.com/article/10.2145/20080306,Pharmaceutical Economics,July 2008,George Chressanthis,,,Male,Unknown,Unknown,Male,,
43,3,Business Economics,01 July 2008,https://link.springer.com/article/10.2145/20080307,"Detailed Industry, Product, Geographic Data from the 2007 Economic Census Become Available in 2009",July 2008,Robert P Parker,,,Male,Unknown,Unknown,Male,,
43,4,Business Economics,01 October 2008,https://link.springer.com/article/10.2145/20080401,Current Economic and Financial Conditions,October 2008,Ben S Bernanke,,,Male,Unknown,Unknown,Male,,4
43,4,Business Economics,01 October 2008,https://link.springer.com/article/10.2145/20080402,Reforming Mortgage Finance,October 2008,Sheila C Bair,,,Female,Unknown,Unknown,Female,,
43,4,Business Economics,01 October 2008,https://link.springer.com/article/10.2145/20080403,Encouraging Growth and Stability,October 2008,Edward P Lazear,,,Male,Unknown,Unknown,Male,,
43,4,Business Economics,01 October 2008,https://link.springer.com/article/10.2145/20080404,The Baltics: Continuing Boom or Bursting Bubble?,October 2008,Matthias S Fifka,,,Male,Unknown,Unknown,Male,,1
43,4,Business Economics,01 October 2008,https://link.springer.com/article/10.2145/20080405,The Economics of Private Business Jet Travel,October 2008,Claire Starry,Gerald W Bernstein,,Female,Male,Unknown,Mix,,
43,4,Business Economics,01 October 2008,https://link.springer.com/article/10.2145/20080406,Globalization and the U.S.Defense Industrial Base: The Competition for a New Aerial Refueling Tanker,October 2008,Nayantara Hensel,,,Unknown,Unknown,Unknown,Unknown,,
43,4,Business Economics,01 October 2008,https://link.springer.com/article/10.2145/20080407,Economists in a World of Financial Ruin,October 2008,Richard Koss,,,Male,Unknown,Unknown,Male,,1
43,4,Business Economics,01 October 2008,https://link.springer.com/article/10.2145/20080408,The Global Management Consulting Sector,October 2008,Andrew C Gross,Jozsef Poor,,Male,Unknown,Unknown,Male,,22
44,1,Business Economics,25 February 2009,https://link.springer.com/article/10.1057/be.2008.10,From the Editor,January 2009,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"Needless to say, in January of 2009 and for the last half of 2008, Topic A is the financial crisis and its spillover to the real economy in the United States and worldwide. Right now, it is not clear that there is a Topic B, although there are many candidates clamoring on the sidelines, waiting for the economy to right itself. This issue is mostly given over to Topic A. Michael Mussa, the 2008 recipient of NABE's Adam Smith Award, examines Smith's contribution to our understanding of financial crises and finds that much of it is still relevant today: the same mistakes that have been made for centuries keep recurring. However, each crisis has it own characteristics, also; and Mussa examines the particular roles of moral hazard and immoral result in prescribing policies to be followed in the future. Although Michael Mussa looks back over 200 years to Adam Smith, Ellen Hughes-Cromwick's presidential address is concerned with the growing complexity and imbalances that have emerged over the past 50 years, the period since NABE was founded. She finds that the increasing risk in the financial system has not been offset by gains in the real sector and calls for means of restraining the excesses of financial innovation. The small business sector accounts for half of private U.S. GDP and over half of private sector employment. Thus, how it reacts to monetary policy is an important question for the entire economy. William C. Dunkelberg and Jonathan A. Scott find that small business does not react the way that conventional theory suggests and trace the reasons for this to be so. William Poole, like Mussa and Hughes-Cromwick, investigates the financial crisis and its causes. He cautions against overreacting to the crisis by excessive and counterproductive regulation and urges that government policies be transparent and on-budget rather than unfunded mandates, which have had an important role in contributing to the current crisis. He also raises the question of whether government incentives to borrow contribute to financial instability. Certainly one of the candidates for Topic B (or C, or….) will be to increase the efficiency of health care delivery. Information technology has not reached its potential, and Stephen T. Parente believes that much of this shortcoming is because it is not organized around the patient. He proposes a system of “medical banking” that will increase efficiency and acceptance for the patient and all other participants in medical information technology. In this issue's Forum on Emerging Issues, Thomas A. Hemphill examines current and future developments in social regulatory policy—policy that is primarily designed to address issues related to health, safety, and the environment. He reviews what has been done over the past several years and what is likely in the incoming Obama Administration. In the Book Reviews, Robert A. McLean finds Edward M. Gramlich's Subprime Mortgages: America's Latest Boom and Bust to be a valuable contribution to understand what went wrong and what can be done in the future to prevent recurrence. John C. Goodman reviews Robert J. Samuelson's The Great Inflation and Its Aftermath: The Past and Future of American Affluence. The “Great Inflation” covers basically the 1970s, culminating in the serious recession of 1981–82. Samuelson traces the mistaken beliefs and policies that contributed to the Great Inflation and the corrections that led to the “great stability” that followed and persisted until the current crisis. Goodman finds that it is important to heed the lessons of the Great Inflation, lest attempts at recovery from the current crisis recreate its conditions. This issue does not include Focus on Statistics, Focus on Industries and Markets, or Economics at Work. They do, however, remain as important elements of Business Economics and will appear in subsequent issues.",
44,1,Business Economics,25 February 2009,https://link.springer.com/article/10.1057/be.2008.9,Adam Smith and the Political Economy of a Modern Financial Crisis,January 2009,Michael Mussa,,,Male,Unknown,Unknown,Male,"Adam Smith recognized that stock of money was an essential part of the economy's total stock of useful capital. However, unlike other forms of capital, money was not directly useful either in consumption or in production. Rather, money played the essential role of the “great wheel of circulation” that made possible the distribution of labor and material inputs to their various productive uses and the means by which “every individual in the society has his subsistence, conveniences, and amusements distributed to him in their proper proportion.” More specifically, Smith argued, “The great wheel of circulation is altogether different from the goods which are circulated by it. The revenue of society [gross or net national product in today's terminology] consists altogether in those goods, and not in the wheel that circulates them.” Nevertheless, Smith maintained that “… the stock of money which circulates in any country must require a certain expense, first to collect it, and afterward to support it, both [of] which … are … deductions from the neat [or net] revenue of society. A certain quantity of gold and silver and of very curious labor [i.e., bankers] … is employed in supporting that great but expensive instrument of commerce …” instead of in directly productive activities. The stock of “money” to which Smith refers in this discussion is comprised of gold, silver, and lesser coins and “… several sorts of paper money, [most importantly] the circulating notes of banks and bankers.” The key characteristic of circulating bank notes that makes them useful as money is their ready acceptability as payment in a wide range of transactions on an essentially equal footing with gold or silver money. “When the people of any particular country have such confidence in the fortune, probity, and prudence of a particular banker, as to believe that he is always ready to pay upon demand such of his promissory notes as are likely to be at any time presented to him; these notes come to have the same currency as gold and silver money, from the confidence that such money can at any time be had for them.” Smith repeatedly emphasizes the virtues of paper money both to the banks that create it and to society as a whole. This virtue, in both cases, fundamentally reflects a violation of the principle that there is no such thing as a free lunch. By creating paper money at very little cost that may be used with essentially equal convenience in place of metallic money (gold or silver) that can only be obtained at considerable expense, the banker creates essentially out of nothing something that has considerable value for himself and for society. Smith explains this remarkable phenomenon as follows:
 The banker who advances to the merchant whose bill he discounts, not gold or silver, but his own promissory notes, has the advantage of being able to discount the greater amount of the whole value of his promissory notes, which he finds by experience, are commonly in circulation. He is thereby enabled to make his clear gain of interest on so much a larger sum. It is not by augmenting the capital of the country, but by rendering a greater part of that capital active and productive than would otherwise be so, that the most judicious operations of banking can increase the industry of the country. … The gold and silver money which circulates in any country, and by means of which the produce of its land and labour is annually circulated and distributed to proper consumers, is … all dead stock …. The judicious operations of banking, by substituting paper in the room of a great part of this gold or silver, enables the country to convert the great part of this dead stock into active and productive stock. Smith also emphasized that there were limits to the amount of paper money that could prudently be created by an individual bank and within a country, and that serious dangers ensued if too much paper money was created. In Smith's view, the effective limit on the amount of paper money was an amount somewhat less than the amount of metallic money that would be held if no paper money were available. This was based on Smith's analysis that sound paper money could efficiently be used in domestic commerce in substitution of most metallic money. Individual bankers, however would need to hold some fractional reserve of gold and silver to maintain their commitments to convert their paper money into metallic money on demand. And for the country as a whole, metallic money would also be needed in transactions with foreigners who ordinarily would not accept domestic paper money. If an individual bank expands its issue of paper money too much, it could face a run in which holders of its money demand immediate conversion into gold and silver. If the bank could not meet the run out of its own reserves or with metallic money obtained by rediscounting some of the bills it holds (against which it had advanced its own money), the bank will fail. The customers of the bank will be hurt by such a failure, in addition to the loss incurred by the bank. If the general supply of paper money becomes too large, then many banks may face runs in a general panic. Unless additional gold and silver (or freely convertible notes of a central bank like the Bank of England) can be obtained to stem the panic, the financial crisis will create general economic distress—a phenomenon that had been observed on many occasions. Smith proposed several solutions to the problem of excessive creation of paper money, but his analysis here is somewhat confusing and contradictory. At the end of his chapter on “Money … ,” Smith embraces, with two provisos, the solution dear to the hearts of today's ardent deregulators—freely competitive banking. “If bankers are restrained from issuing any circulating bank notes or notes payable to the bearer, for less than a certain sum; and if they are subjected to the obligation of an immediate and unconditional payment of such banknotes as soon as presented, their trade may, with safety to the public, be rendered in all other respects perfectly free.” However, Smith's discussion earlier in the chapter makes clear that the actual experience with banking and paper money in Scotland, England, and the American colonies raised substantial doubts about whether the private incentives for prudent banking under the discipline of competition would provide adequate safeguards against the dangers of excessive money creation. Indeed, Smith embraces constraints on banks’ creation of paper money and extension of credit that would be anathema to modern advocates of financial deregulation. In particular, Smith's explicit proviso that paper money issued by banks should not be “for less than a certain sum” is far from innocuous. The “certain sum” that Smith recommended was five pounds sterling. Although today, five pounds sterling is worth about eight U.S. dollars and will pay for no more than a short taxi ride in London, in the late 18th century, five pounds sterling was the monthly wage of a skilled craftsman, equivalent in today's money to about $5,000. As Smith made clear, paper money issued by banks was for use in business transactions, among dealers and merchants who were presumably sophisticated about the ways of business and finance. Paper money was not supposed to be a substitute for the coinage used in everyday retail transactions. More generally, Smith argued forcefully that the total issuance of paper money within a country should be constrained by what later came to be known as “the real bills doctrine.” Specifically, paper money should be paid out only on bank credit extended in the discounting of “real bills” that corresponded to actual trade among merchants. Smith argued that such lending was fundamentally safe and would not involve an undue expansion of paper money. But that any other bank lending should only be to such customers where the bank can “… observe with great attention, whether in the course of a short period … the sum of repayments it receives from them, is … fully equal to that of the advances which it [the bank] commonly makes to them.” Another constraint on bank issuance of paper money and creation of credit that Smith recommended was an 18th century version of the Glass-Steagall Act. Banks should limit their extensions of credit and associated issuance of paper money to short-term lending primarily related to the discounting of real bills. All forms of longer-term lending [generally secured by bonds or mortgages] should be the domain of “… such private people as propose to live upon the interest of their money … and who upon that account [are] willing to lend … to such people of good credit as are likely to keep it for several years.” Closely related to Smith's advocacy of the real bills doctrine and the preclusion of banks from longer-term lending is Smith's abhorrence of what I would term “money-credit merry-go-rounds.” Smith describes the abhorrent phenomenon as follows:
 … trader A in Edinburgh … draws a bill upon B in London, payable two months after date. In reality B in London owes nothing to A in Edinburgh; but he agrees to accept of A's bill, upon condition that before the term of payment he shall redraw on upon A in Edinburgh for the same sum, together with the interest and commission, another bill payable likewise two months after date. … A in Edinburgh … before the expiration of the second two months, draws a second bill upon B in London, payable likewise two months after date; and before the expiration of the third two months, B in London redraws on A in Edinburgh another bill, payable also two months after date. This practice has sometimes gone on, not only for several months, but for several years, the bill always returning upon A in Edinburgh, with the accumulated interest and commission or all former bills …. Though the bills upon which this paper had been advanced, were all of them repaid in their turn as soon as they became due; yet the value which had been really advanced upon the first bill, was never really returned to the banks which advanced it …. The paper which was advanced upon those circulating bills of exchange, amounted on many occasions, to the whole fund destined for carrying out some vast and extensive project of agriculture, commerce, or manufactures …. The greater part of this paper was, consequently, over and above … what the circulation of the country could easily absorb and employ …. Smith emphasizes that this merry-go-round of money and credit becomes even more dangerous when it becomes opaque through the involvement of many different banks. “When two people, who are continually drawing and re-drawing upon one and other … with the same banker, he must immediately see what they are about …. But this discovery is not altogether so easy when they discount their bills sometimes with one banker and sometimes with another, and when the same two persons do not constantly draw and redraw upon one another, but occasionally run the round of a great circle of projectors, who find it in their interest to assist one another in this method of raising money …. [Even] when a banker has made this discovery, he might sometimes make it too late, and might find that he had already discounted the bills of those projectors to so great an extent, that, by refusing to discount even more, he would necessarily make them all bankrupts, and thus, by ruining them, might perhaps ruin himself.” Smith concludes his argument for prudent limits on money creation by describing what happened in Scotland when the issue of paper money and credit became unsustainably large and prudent banks began to cut back. Protests by those who desired even further extensions of money and credit led the establishment of a new bank “… for the express purpose of relieving the distress of the country …. This bank was more liberal than any other had previously been, both in granting cash accounts and in discounting bills of exchange … This bank, no doubt, gave some temporary relief to those projectors [who favored and supported its establishment], and enabled them to carry on their projects for about two years longer than they could otherwise have done. But it thereby only enabled them to get so much deeper into debt, so that when ruin came, it fell so much the heavier upon both them and their creditors.”",3
44,1,Business Economics,25 February 2009,https://link.springer.com/article/10.1057/be.2008.7,Economic and Financial Climate Change: A Business Economist's Perspective,January 2009,Ellen Hughes-Cromwick,,,Female,Unknown,Unknown,Female,"The crossroad and disequilibrium between the nonfinancial and financial sectors during 1958 to 2008 evolved in ways no one could have predicted. In 1958, the global economy was nascent but growing at a rate that was boosted by recoveries following World War II. Europe was bearing fruits from growth in manufacturing output. In fact, the 1950s were part of the European “Golden Age,” aided by postwar stimulus, technological advances, and currency devaluations as part of the Bretton Woods system, that allowed export growth to accelerate. Meanwhile, the U.S. economy fell into recession in August 1957 and troughed in April 1958. This was followed by a very brief expansion that ended in April 1960. With a $2 trillion U.S. economy, the first quarter of 1958 fell at an annual rate of 10.4 percent—steep by any measure. Consumer spending fell over five percent. Fifty years ago, over 50 percent of the $7 trillion global economy was represented by U.S. and European output, as shown in Figure 1. World Output Shares: 1958 and Today Source: World Bank, in constant 2000 U.S.$. In contrast, today's global economy of $39 trillion is powering ahead with substantial growth in emerging markets. United States and Europe now represent less than 50 percent of global output, a much different profile than 50 years ago. Most importantly, as shown in Figure 2, the structure of the U.S. economy has changed during this period. The financial sector (finance and insurance) of the economy has gained in prominence during the last 50 years. With the economy growing in real terms from $470 billion to $14 trillion today, the share of value added from manufacturing has shrunk from one-quarter of output to 12 percent in 2007, due to substantial gains in productivity and increased international competition, particularly from Japan and emerging market economies. At the same time, the share of finance and insurance has gone from under four percent to over eight percent during this period. Table 1 provides this historical comparison for employment as well. Finance employment was three percent of all jobs in 1958—as compared with 4.5 percent today. At the same time, the manufacturing job share has declined from 29 to 10 percent. The value-added “role reversal” between finance and manufacturing is a development that is now being reassessed in light of the significant deflation in the value of financial sector output. Shares of GDP, 1958 and 2007 Source: U.S. Department of Commerce, NAICS GDP by Industry Value Added. Compare this to the output and employment shares of the rest of the economy as represented by the third column of data for output and employment shares in Table 1. This is the nonfinancial sector excluding manufacturing (but including government.). All of these industries represented 71 percent of output in 1958, and grew to just over 80 percent by 2007. At the same time, these industries employed about 69 percent of workers in 1958 and expanded this share to over 85 percent by 2007. This brief synopsis of shifting shares between the U.S. sectors during the last 50 years is a useful backdrop to today's turmoil and what it means for the challenges the United States may face during the next 50 years. Although quite arguable, it may be asserted that the excessive risk-taking undertaken by the financial sector has produced two results. First, it ballooned the value-added of the sector beyond what was now known to be a sustainable level; and the profits of the sector doubled during the last 50 years, from 13 percent of all U.S. corporate profits in 1959 to 26 percent in 2007.Footnote 2 As financial intermediaries, it is not clear why this sector's profitability should grow in relation to the size of aggregate economic activity. However, this growth is partly related to the broader growth of global economic performance and the role that U.S. financial firms have played in facilitating this growth. Second, excessive financial risk-taking led to repercussions on the nonfinancial sector and adverse consequences for employment. In some sense, the risky activity of the financial services sector is akin to a tax on economic activity with the incidence of the tax borne by workers in the nonfinancial sector. This tax incidence, if you will, is a form of negative externality associated with the production of structured financial products with no effective “protection” against the adverse consequences that are and will be incurred by workers in the nonfinancial sector.",
44,1,Business Economics,25 February 2009,https://link.springer.com/article/10.1057/be.2008.6,The Response of Small Business Owners to Changes in Monetary Policy,January 2009,William C Dunkelberg,Jonathan A Scott,,Male,Male,Unknown,Male,"There are several schools of thought regarding the channels through which monetary policy exercises its influence on real economic activity. Recently popular is the asset price view [Taylor 1995]. This view focuses on the impact of changes in interest rates on the spending decisions of businesses and households through changes in asset values and exchange rates. The higher the market rate of interest, the fewer the number of investment opportunities whose rate of return exceeds the cost of capital and, consequently, the lower the level of investment spending. Also, rising interest rates change the price of current consumption and reduce financial market wealth, adversely affecting current consumption. Except for new housing (treated as part of gross private domestic investment), little empirical evidence is available to support the notion that interest rate changes have a strong direct effect on consumer spending. However, it appears that changes in asset values that are viewed as permanent do have a modest effect on consumer spending. A second view focuses on the supply of credit and the lending criteria of banks. In this view, banks may not allocate credit simply through changes in the price of credit. They may also refuse to take on certain credit risks and not lend at any price (rationing). This credit supply view sees monetary policy producing changes in credit standards at lenders as well as in rates charged as the monetary transmission channel [Bernanke and Gertler 1995]. If the transmission channel is through the supply side (changes in banks' risk tolerance and lending standards), many firms will not notice the effect of changes in monetary policy until they apply for a new loan or a renewal (which many never do or do irregularly).Footnote 3 But, a simple model for affecting economic agents is not likely to be all-encompassing, leading to the observation that “… monetary policy works at least in part through ‘credit’ (i.e., bank loans) as well as through ‘money’ (i.e., bank deposits)” [Bernanke and Blinder 1992]. Other views rely on various structural rigidities in the economy that influence the transmission of monetary policy effects (the friction transmission view) to the real sector. Rigidities in the wage structure, price setting, or the ability of economic agents to reallocate assets in their portfolios explain why changes in nominal variables like the money supply or credit can affect real variables [Christiano, Eichenbaum, and Evans 1997]. These models typically depend on expectations of future inflation or nominal returns to drive decision-making as the means of transmitting the effects of monetary policy to real variables. It is less clear how the friction transmission model works at the micro level. According to Christiano and others: “The first friction is that some firms do not immediately adjust prices in response to monetary policy shocks while ex post, output is demand determined. The effect of this friction is that aggregate output falls in response to a monetary contraction. The second friction is that households do not immediately adjust their nominal saving in response to monetary policy shocks. The effect of this friction is that monetary contractions disproportionately affect the reserves of banks and, hence, the supply of loanable funds. The result is a rise in interest rates which induces firms who need working capital to cut back on their scale of operations and aggregate output declines” [Christiano, Eichenbaum, and Evans 1997, p. 1203]. The limited participation model is driven by “assuming that, in any given period, households must determine how much money to deposit with financial intermediaries prior to the realization of the monetary shock” [Christiano, Eichenbaum, and Evans 1997, p. 1203]. The sticky price version requires that intermediate goods producers set their prices first, then the policy change occurs, and output is demand-determined based on prices set before the policy change. Ultimately, it appears that firms are confronted with interest rate or credit availability changes that affect real variables. If a firm does not borrow in the period (or ever), these changes will not matter. Another channel for monetary policy transmission is suggested by rational expectations theory. Decision makers use the information provided by policymakers' actions to predict future values of important variables such as company sales and then to make relevant spending decisions in the current period, not just in future periods. This view is especially pertinent to owners of firms that are continuously making revenue, price, and labor cost forecasts. These forecasts form the basis for plans to hire and spend in current and in future periods. If changes in monetary policy announced to the public affect these forecasts, then changes in policy will immediately affect spending and hiring, long before business owners react when they apply for a loan or before the effect of policy shows up as a change in the number of customers coming in the front door (for example, fewer home buyers in response to higher long-term rates). Furthermore, many business owners have no debt, do not use credit, and have no assets other than their homes and businesses. If these owners use the information conveyed by changes in Federal Reserve policy to formulate forecasts of future economic activity and act on those expectations, monetary policy can still have an effect on these firms. This approach, however, does not produce clear and reliable predictions of the response to changes in monetary policy [Juks 2004], because the response to a change in the Federal Funds target may depend not on the direction and size of the interest rate change but on how the change is interpreted in the context of economic conditions. With the exception of the rational expectations perspective, the other views of how monetary policy transmission occurs have some shortcomings when applied to the small business sector. Although banks are not the primary source of capital for starting a new firm, they are the primary source of funds for small firms once started, providing working capital and funding for investment in plant and equipment [Dunkelberg and Cooper, 1983; Berger and Udell 1998]. Changes in the cost and availability of funds at banks that result from changes in monetary policy could have an important effect on small firm spending. But changes in loan terms and owner responses to these changes as they come to banks for capital take time to develop. Changes in interest rates will have no direct impact on firms that do not borrow and those that borrow irregularly. (However, their customers might be impacted by the policy change.)Footnote 4 If loans only re-price every five years, the effect of interest rate changes on many decision makers is muted, limited only to owners borrowing or re-pricing in the current period and to those with variable priced loans (increasing in frequency and perhaps making the economy more sensitive to monetary policy shifts). Just how these less active credit market participants would be directly affected by changes in monetary policy through the asset price/interest rate mechanism is not clear in traditional models of the transmission of monetary policy effects. A rational expectations model may more accurately describe how small firms react to (unexpected) changes in monetary policy; but this model, as noted above, does not provide clear predictions of the owner response. Some proportion of the population of small business owners follows the news and uses that information to make forecasts of future values of important variables (sales, input prices, wages, and so on) and ultimately acts on these forecasts. Expectations are modified immediately and spending plans changed in response to an announced change in monetary policy. Thus, real variables as well as prices will respond to changes in policy, possibly quite quickly. However, a given policy change—say a rate cut (that arguably should affect long-term rates as well)—may not immediately affect spending in the manner predicted by the investment or credit channel views. Rate cuts might be followed by cuts in investment spending, for example, rather than increases, as conventional theory suggests, if the policy change is interpreted as a signal of a weakening economy. This perspective does not in any way invalidate the importance of interest rates or bank lending policies as vehicles for transmitting monetary policy effects. It does, however, broaden the potential effect of policy changes by including agents who are not active participants in capital markets but that generate significant amounts of output and jobs. This perspective also accommodates a more rapid response to changes in monetary actions, independent of the degree of capital market participation of firms. The NFIB data do not lend themselves to longitudinal studies of the longer term effects of changes in monetary policy because each survey is based on a new random sample of the membership, not re-interviews; and the questions used in the survey instrument have very short horizons (six months at the most). Summary statistics from each monthly survey have been shown to be significantly related to macroeconomic measures of spending and hiring over time [Dunkelberg, Scott, and Dennis 2003], but the microeconomic level responses of business owners to policy changes from this data have not been documented. This paper will show that:
 there is an immediate response to announcements of unexpected changes in monetary policy; the responses immediately affect hiring and spending activity; and the immediate responses of business owners are often inconsistent with those implied by conventional theory or by time series macroeconomic models of the impact of monetary policy on real variables. Response lags to monetary policy can be long and variable (with appropriate signs), but there are significant responses that are immediate and often contradictory to the longer term objectives of the Federal Reserve. This will be illustrated by documenting the changes in important business owner plans and expectations that occur within days of a Federal Reserve announcement and persist for a month or more. Identifying owner responses that are uniquely tied to a particular Federal Reserve action for more than a month becomes complicated by the intrusion of other economic events, including the fact that rate changes typically do not occur as one-off events but as part of a succession of rate changes. Although the results below suggest that small business reactions to monetary policy changes are often contrary to conventional theory, we regard them as an expansion of our understanding of how monetary policy is transmitted to the real economy: they do not invalidate the importance of interest rates and bank lending policies in the transmission process and provide insight into the question raised by Sims and Zha [2006].",6
44,1,Business Economics,25 February 2009,https://link.springer.com/article/10.1057/be.2008.1,The Credit Crunch of 2007–08: Lessons Private and Public,January 2009,William Poole,,,Male,Unknown,Unknown,Male,"Anyone familiar with the scholarly literature on financial crises knows that our recent—and unfortunately ongoing—experience has many general characteristics familiar from past crises. How can the private sector make the same mistakes over and over again? The current crisis, in particular, is in some ways a supersized version of what happened to Long-Term Capital Management in 1998, which was not that long ago and certainly familiar to all who were in senior management in major financial firms at the outset of this crisis. The root of the current problem is poor credit quality. Too many financial institutions loaded up on too much subprime mortgage paper without adequate assessment of the credit risk. But the issue goes beyond poor credit evaluation, because the strategies behind these portfolios were risky from the beginning. Credit evaluation is hard, and we cannot depend only on better credit evaluation in the future to maintain financial stability. The financial system needs to be robust with respect to credit risks. I reluctantly conclude that the market is all too often shortsighted—not as shortsighted as the public sector typically is but shortsighted nonetheless. The problem is well known in the finance literature. The probability distribution of possible outcomes has fat tails—the probability of extreme events is much higher than managers of financial firms and investors in those firms seem to understand. Financial firms pursue risky strategies that yield apparently high returns year after year, until the crisis hits and the strategies fail catastrophically. A common feature of financial strategies that fail is a duration mismatch—assets on the balance sheet have longer maturities than liabilities. Financial firms can indeed earn reliable returns from maturity transformation, provided they have robust strategies to deal with the risk. What seems to have happened is that firms have relied on thick asset markets to allow them to borrow short-term by putting up their longer-term assets as collateral. Once lenders come to distrust the value of the collateral and ask for more, the strategy is in great difficulty. Selling the collateral at a price anywhere close to its value on the balance sheet may not be possible when the market comes to distrust the paper. The possibility that asset markets would close could and should have been foreseen. The only protection a firm has in such a case is to fall back on its capital. A firm pursuing this strategy should hold a substantial secondary reserve of assets of high quality, such as Treasury bills, that could be sold when necessary. Strong secondary reserves are simply banking 101. With more capital, firms could have been more patient holders of distressed debt and could have pursued work-out strategies to maximize its value. The greater the mismatch of asset and liability duration, and/or the weaker the credit quality of assets, the greater should the firm's capital be. Unfortunately, entering the crisis many large financial firms were very heavily leveraged. The subprime paper they held had weak credit quality and relatively long duration. Some large financial firms, especially hedge funds and investment banks, ostensibly had three or four percent capital, but that cushion was quickly exhausted when assets dropped in value. Another part of the typical strategy was to rely on derivatives to provide hedges. Interest rate swaps, for example, could be used to create synthetic long-duration liabilities out of short-term liabilities. That strategy still requires that the firm be able to roll over its short-term liabilities, which proved difficult and expensive once the market came to distrust the solvency of the borrowing firm. Incidentally, from the beginning of the crisis we heard that the problem was illiquidity and lack of confidence. Both, in fact, flow from market assessments of risk of insolvency. Strongly capitalized firms, such as Berkshire Hathaway and Microsoft, never had any problem borrowing in the money and capital markets. From the beginning, the issue was not liquidity per se but the underlying problem of the threat of insolvency. Another problem with derivatives strategies is that derivatives contracts are not a substitute for safe assets such as Treasury bills because of counterparty risk. Once a major seller of credit default swaps (CDSs) such as AIG got into trouble, the value of a hedging strategy based on CDSs went south. In the years ahead, every financial firm should examine risks in light of what has happened over the past 18 months. A critical part of the risk management strategy must be to stress-test models using data from the past few years. And if the markets being studied do not include collateralized debt obligations (CDOs) secured by subprime mortgages—and they probably won’t—risk managers should make up hypothetical data based on what has happened to CDOs. Another lesson for the private sector, and one I do not know much about, is the need to design compensation policies that provide the correct long-run incentives for managers. This is a difficult topic because the pressure to hire and retain high-performing employees is real, and yet intense competition for such employees makes it difficult to structure compensation that reflects risks of bad outcomes that may not occur for a decade or more.",2
44,1,Business Economics,25 February 2009,https://link.springer.com/article/10.1057/be.2008.2,Health Information Technology and Financing's Next Frontier: The Potential of Medical Banking,January 2009,Stephen T Parente,,,Male,Unknown,Unknown,Male,"The goal of medical banking is to enhance financial transactions associated with health care with information on the patient's condition, treatment outcome, and adherence to patients making their own human capital investments. If we consider Michael Grossman's [1972] landmark household production function specification, where medical care and other factors affect the production of health, medical banking explicitly makes it possible to collect the data necessary for a household production model to become a day-to-day reality and a part of the current financial transaction system in health care markets. As an illustration of the capability of medical banking, consider the development of the personal finance risk score. Consumer banking transactions, whether through checking accounts or credit cards, have produced an externality that constitutes an entire secondary market of data to be used for risk score information. Imagine that the same thing is possible for medical care at the individual patient level. Health risk scoring already happens today, though fairly crudely, for risk prediction by insurers of their current or prospective patients. Enhancement would provide metrics for personal health production akin to what Grossman described theoretically. These metrics could also be aggregated by associating physicians (without suggesting causality) with their aggregate medical productivity, pharmacies, group practices, or hospitals to the health outcomes of the patients who visited them. How could simple financial transactions of medical providers that are no more than debits and credits yield sufficient data for measuring medical care production? The simplest answer is that the financial transaction system itself becomes a conduit for the transfer of not only bank account transfer instructions, but also clinical data from new and existing health IT platforms of the providers seeking payment. For example, if a laboratory testing company bills for blood work on a diabetes patient that produces numerical values describing the result of a specific test, a medical banking platform would require the attachment of the lab result as evidence of the service performed to receive the negotiated fee as reimbursement. The current practice is simply to pay for the test without additional information transmitted. This made sense in 1966, when all results were recorded on paper. But today, all results are digital and easy to transmit. This one innovation would provide a line of demarcation between the fee-for-service health insurance financial transactions of the last 60 or more years and the new medical banking platform described in this paper. To understand the potential for medical banking, it is important to understand the status quo. Today, the overwhelming majority of health care financial transactions occur through third-party insurers that are private (such as Aetna or Cigna) or public (such as Medicare and Medicaid). The third-party insurance primary business model is the facilitation of a fee-for-service transaction system between employers, governments, and insurers who hold risk contracts and the providers of medical care—such as physicians, hospitals, and pharmacies—on behalf of the patient. Take for example, an insured person who breaks his leg and goes to the emergency room of a local hospital. The hospital will seek reimbursement from the insurer of this injured patient by submitting a claim for reimbursement with specific line items for use of the facility, physician time, medical equipment used to set the bone, pain medications prescribed, and x-rays taken. A consulting orthopedic surgeon, retail pharmacy, anesthesiologist, and radiologist will all invoice separately. The insurer will receive these requests for payment and negotiate final payment over the course of 30 to 120 days following the emergency room event. Where do banks enter the picture now? If this person works for a large firm where he has signed up for health insurance, the firm is likely to be self-insured through the Employee Retirement Income Security Act (ERISA); and this firm will instruct the insurer to pay the medical providers using the bank account of the employer following the negotiation of final payments to the providers. Thus, health insurance here is simply “negotiated” fee-for-service. If the injured patient worked for a small company that could not afford to be self-insured under ERISA, the payment would originate from the bank account used by the insurer associated with the patient. This would be the case if the injured bought their health insurance in the individual insurance market as well. If the patient had no insurance and was not in a public insurance program such as Medicare or Medicaid, they would be responsible to pay the charges of the hospital, which are likely to be at least twice the negotiated rate between insurers and the providers mentioned above.",
44,1,Business Economics,25 February 2009,https://link.springer.com/article/10.1057/be.2008.3,American Business and the New Social Regulation,January 2009,Thomas A Hemphill,,,Male,Unknown,Unknown,Male,"The 110th U.S. Congress has passed significant new social regulatory policy legislation. The most important environmental/energy regulatory policy enacted was the “Energy Independence and Security Act of 2007” in December of 2007. This Act includes a mandated increase in the Corporate Average Fuel Economy (CAFE) standard for the total fleet of passenger automobiles manufactured for sale, from the present standard of 27.5 miles per gallon (the goal established by Congress effective in 1985) to 35 miles per gallon (a 27.3 percent increase) by 2020 (with the fuel efficiency standard gradually phased in beginning with the 2011 automobile model year). Not surprisingly, this bill was passed in spite of intensive lobbying by the automotive industry.Footnote 3 Under the new CAFE standard legislation, automobile manufacturers will be able to earn energy efficiency credits when they exceed the new CAFE standards and have a choice of either “banking” the credits when not needing them or selling them to other automobile manufacturers that not meet the CAFE standard. On April 22, 2008, the U.S. Department of Transportation (DOT) announced an even more ambitious CAFE standard proposal in the administrative rule-making process: accelerating implementation of CAFE standards for passenger vehicles and light trucks over a five-year period ending in 2015. For passenger vehicles, CAFE standards would reach 35.7 miles per gallon (a 29.2 percent increase in fuel efficiency) in 2015, and light trucks would increase from 23.5 miles per gallon in 2010 to 28.6 miles per gallon (a 21.7 percent increase in fuel efficiency) in 2015 [U.S. Department of Transportation 2008]. For cars and light trucks, the DOT would will require a fleet-wide average of 35.0 miles per gallon by 2020 [Power 2008]. The automobile industry, or individual manufacturers, may still legally challenge the proposed administrative rule on the grounds that the agency is exceeding the implementation timetable (and passenger vehicle miles per gallon CAFE standard) included in the original legislation. Automobile manufacturers are already calling the DOT's CAFE standards too aggressive, especially since this year will be the worst year for U.S. automobile sales in a decade, and the industry cost of implementing the fuel efficiency regulations are estimated at approximately $46 billion [Power 2008]. Many Democrats, however, believe that the CAFE standards should be raised even further, as this would reduce gasoline demand, save consumers $100 billion in gasoline fuel costs, and reduce the need to drill in environmentally sensitive wilderness areas [Power 2008]. In late September 2007, with the support of the Pharmaceutical Research and Manufacturers of America [2007], the U.S. Congress passed, and President Bush signed into law, the Food and Drug Administration Amendments Act of 2007 (Public Law No. 110-85), which renews and extends the Prescription Drug User Fee (PDUF) Act of 1992. The PDUF Act grants the FDA the authority to collect fees from pharmaceutical firms to be used for laboratory-based safety review and approval of new ethical drugs and medical devices. Under the original legislation, Congress must re-authorize PDUF every five years. Under the Act's user fee program, the FDA will increase the annual amount of user fees it collects from pharmaceutical firms from $305.4 million to $392.8 million, a 26.9 percent increase in funding [U.S. Food and Drug Administration 2007]. In addition, the legislation evolved into a broader-based FDA reform bill, including provisions: that strengthen agency authority over drugs already on the market (including requiring postmarket safety studies and risk evaluation mitigation for drugs exhibiting adverse effects); increasing regulatory oversight of direct-to-consumer pharmaceutical advertising (although not granting the authority to ban false or misleading drug-to-consumer advertising); granting agency authority to order drug firms to change labeling for a drug; establishing a publicly available clinical trial database; modernizing its Adverse Events Reporting system (used to collect and aggregate safety data and reporting on new safety concerns); and granting authority to establish new standards and definitions, and if necessary, recall tainted pet food. Consumer product safety, with a focus on imported products, has received intense scrutiny by Congress. In fiscal year 2007, there were 473 product recalls involving toys and jewelry containing excessive lead, toys with dangerous magnets that can cause stomach and intestinal injuries to children, and cribs with hardware and construction failure that could potentially cause death and injury to babies, to name a few [May 9th Coalition 2008]. In response to this onslaught of product recalls [Shin 2008], the resulting Consumer Product Safety Improvement Act of 2008 includes major provisions that will reform the operations of the Consumer Product Safety Commission CPSC, including:
 increasing the budget of the agency to $136 million for FY2009 from $80 million to $136 million (allowing for the hiring of at least 500 full-time employees and 50 port-of-entry and overseas production facility inspectors); increasing the number of CPSC members from three to five; requiring the CPSC to maintain a publicly available, searchable Web-based site that includes any reports received by the agency of injuries, illness, death, or risk of such harm from the use of consumer products; requiring the CPSC to establish protocols and standards regarding design certification of products, continuing safety compliance with standards, and accrediting third-party laboratories for testing of certain products used by children seven years or younger; effectively banning lead content and three categories of phthalates (chemical ingredients used to make such plastic products as teethers and pacifiers for infants and toddlers) in toys and other children's products; allowing for the state attorneys general to have the authority to help enforce federal product safety laws, with companies failing to report safety hazards or violating product safety laws facing up to $15 million in financial penalties (the previous financial penalty “cap” per company being $1.8 million). President Bush signed the bill into law on August 14, 2008. The issue of federal preemption clauses (superseding state law) has been a topic of recent scrutiny by the U.S. Supreme Court, with it ruling overwhelmingly in favor of federal preemption authority.Footnote 4 On February 20, 2008, the Supreme Court ruled 8-1 in Riegel v. Medtronic Inc. (No. 06-179) that the preemption clause in the Medical Device Amendments of 1976 to the Food, Drug and Cosmetics Act of 1938 21 U.S.C. §360k(a) bars product liability claims in state courts that challenge the safety or effectiveness of medical devices granted premarket approval by the FDA [Greenhouse, Feder, and Harris 2008]. Medtronic Inc, along with the Bush administration, successfully argued that allowing state personal injury lawsuits against medical device manufacturers amounts to a state “requirement” that differs from FDA requirements because such complaints are based on state law, which is preempted by the federal Medical Devices Amendments of 1976 [NewsInferno.com 2008]. This Supreme Court decision bolsters the use of federal preemption clauses in administrative rule-making, but it will have a limited effect on many state medical device product liability lawsuits, as most medical devices now commercially available entered the market through a different regulatory process, one in which the FDA found them to be “substantially equivalent” to those medical devices marketed before the 1976 law took effect [NewsInferno.com 2008]. Several lawmakers, including Representative Henry Waxman, Chairman of the House Oversight and Government Reform Committee, denounced the Supreme Court's decision and promised a legislative response to its ruling [Greenhouse, Feder, and Harris 2008]. Along with the Riegel decision, the Supreme Court ruled the same day on two other nonmedical federal preemption cases: Rowe v. New Hampshire Motor Transport Authority (No. 06-457) and Preston v. Ferrer (No. 06-1463). In Rowe, the Supreme Court ruled 9-0 that a preemption clause in the Federal Aviation Administration Authorization Act of 1994 preempted a Maine law (enacted for public health purposes) requiring state regulation of tobacco deliveries. In Preston, the Supreme Court ruled 8-1 that the Federal Arbitration Act of 1925 9 U.S.C. §1 et. seq., supersedes a California state law (the California Talent Agencies Act) under which the California Labor Commissioner conducts an initial review of disputes before they are submitted to arbitration. In the fall term, the Supreme Court will hear Levine v. Wyeth (No. 06-1249), whereby in the case of FDA approved drugs, the Bush administration (again supportive of pharmaceutical manufacturers) will argue that preemption against state product liability lawsuits is implicit in the structure of the statute, although the Food, Drug and Cosmetics Act of 1938 does not specify a preemption clause [Greenhouse, Feder, and Harris 2008].",
44,1,Business Economics,25 February 2009,https://link.springer.com/article/10.1057/be.2008.4,Subprime Mortgages: America's Latest Boom and Bust,January 2009,Robert A McLean,,,Male,Unknown,Unknown,Male,,
44,1,Business Economics,25 February 2009,https://link.springer.com/article/10.1057/be.2008.5,The Great Inflation and Its Aftermath: The Past and Future of American Affluence,January 2009,John C Goodman,,,Male,Unknown,Unknown,Male,,
44,2,Business Economics,19 June 2009,https://link.springer.com/article/10.1057/be.2009.11,From the Editor,April 2009,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"I am sad to report that Edmund A. Mennis, my predecessor as editor of Business Economics, died on March 18. I knew Ed through the Conference of Business Economists as well as NABE and always respected his generous spirit and his abilities as an applied economist. Much of whatever success that Business Economics now enjoys is due to Ed, who helped me greatly when I took over as editor. Elsewhere in this issue, Gerald L. Musgrave has written a memorial to him.",
44,2,Business Economics,19 June 2009,https://link.springer.com/article/10.1057/be.2009.1,The Need to Return to a Monetary Framework,April 2009,John B Taylor,,,Male,Unknown,Unknown,Male,"A wide variety of monetary policy frameworks have been discussed and debated over the years. Among frameworks that focus on the instruments of monetary policy, there are two broad types: those that focus on the interest rate and those that focus on the quantity of money. Among the latter, we have seen various quantity measures proposed, including the monetary base, several of the monetary aggregates, reserves, free reserves, and borrowed reserves. Because the overnight federal funds rate has a lower bound of zero, even a monetary policy framework that usually focuses on the interest rate needs to make use of a quantity measure when the interest rate hits zero. Figure 1 summarizes a conceptual way to think about such a framework. I originally designed this diagram for the Bank of Japan while an adviser in the 1990s [Taylor 2001]. The inflation rate is on the vertical axis and the output gap is on the horizontal axis. The numbers in italics are the interest rate settings according to the Taylor rule.Footnote 1 The downward sloping line shows when the lower bound on the interest rate is hit according to such a rule. It is obtained by substituting a zero interest rate into a Taylor rule. For the area below the line, the interest rate is zero and policymakers must look at some quantity, such as the money supply or the monetary base; this is the region of quantitative easing. In this lower region, policymakers could use Milton Friedman's famous constant growth rate rule, or the money base rule proposed by McCallum [1988]. Or policymakers could design another procedure for determining the quantity based on economic principles. For the Friedman rule, a monetary aggregate would grow at a constant rate, say 4 percent. For the McCallum rule the growth of the monetary base is flexible, but would be around 4 percent according to conditions near the end of 2008. A Monetary Framework that Incorporates Quantitative Easing Figure 1 illustrates that one should not think of quantitative easing as a separate or different framework for monetary policy, but rather as part of a broader framework. Any point in the region below the line is clearly undesirable, and the hope is that policy will help take the economy back above the line as soon as possible. Christiano and Rostagno [2001] have shown how such a broader framework could work technically, incorporating it into a macroeconomic model, although they assume that policymakers start targeting money growth when actual money growth falls below a certain level rather than when a combination of inflation and the output gap falls below zero. The federal funds rate went to zero in the last quarter of 2008, so the Federal Reserve has gone into the quantitative easing region. In the next section, I examine the Federal Reserve's balance sheet and try to characterize the type of quantitative easing that has been followed. I start by showing that the increases in reserve balances are orders of magnitude greater than the money growth rates proposed by McCallum or Friedman.",21
44,2,Business Economics,19 June 2009,https://link.springer.com/article/10.1057/be.2009.5,A Two-Headed Dragon for Monetary Policy,April 2009,James Bullard,,,Male,Unknown,Unknown,Male,"Recall that President Carter nominated Paul Volcker to be Chairman of the Board of Governors in August 1979. At the time, Volcker was serving as the President of the Federal Reserve Bank of New York, and the nation was facing a daunting policy challenge: about a decade of dramatically high interest rates and inflation. At the start of the 1970s, the yield on long-term Treasury securities was around 6.5 percent, and CPI inflation was running at a little more than 6 percent. During the decade, the real economy was exceptionally volatile by the standards of the two-and-a-half decades after 1984. By August 1979, the economy was in the throes of stagflation: The inflation rate was about 11.75 percent, the long-term interest rate was nearing 8.75 percent, and the unemployment rate was beginning its climb from 6 to 10.8 percent by November 1982, as shown in Figure 1. The misery index, the sum of the inflation rate and the unemployment rate, would eventually rise well past 20. The poor economic performance in the 1970s was partly due to two oil shocks and a downward shift in trend productivity growth, but persistently high inflation could not have occurred without the Federal Reserve's assistance. Inflation, Long-Term Interest Rate and UnemploymentSource: Federal Reserve Board and Bureau of Labor Statistics. Volcker's “monetarist experiment” of 1979–82 was an important moment in U.S. economic history. In essence, the Federal Reserve's Open Market Committee (FOMC) decided to abandon its long-held policy of targeting the federal funds rate and switch to a regime that instead targeted nonborrowed reserves for the purpose of controlling the supply of money. The purpose of this policy change was to “slay the inflation dragon.”Footnote 1 Although controversial and painful—the U.S. economy endured a significant recession—the policy switch has come to be viewed as quite successful. By 1982, inflation had fallen to about 4.5 percent, and it averaged a little less than 3.75 percent for the remainder of the decade. This set the stage for the long economic booms of the 1980s, 1990s, and the current decade, punctuated by just two relatively mild recessions. Monetary policymakers learned a great deal from the 1960s and 1970s. Today's situation is certainly not 1979—inflation is low, for instance. Still, the drama today is similar to the drama of the late 1970s in some respects. In an atmosphere of heightened macroeconomic uncertainty, the Federal Reserve has taken unprecedented actions to help stabilize credit and financial markets. And in particular, the departure from nominal interest rate targeting as the primary focus of monetary policy has a clear parallel with the October 1979 monetarist experiment. This time, however, the decision to drop the nominal interest rate target was pushed on the Federal Reserve by events associated with financial market turmoil and the zero bound on nominal interest rates. And like 1979, attention has appropriately turned to quantitative measures of the thrust of monetary policy. The new Federal Reserve programs, as Figure 2 indicates, have fueled an astonishing expansion of the Federal Reserve's balance sheet. Federal Reserve Bank CreditSource: Federal Reserve Board. One of my key concerns in this new reality is how to keep medium- and longer-term inflation expectations anchored. Taking nominal interest rate adjustments off the table is a significant development. When thinking about the definition and direction of monetary policy, financial market participants and other economic actors in the private sector are completely accustomed to thinking in terms of nominal interest rates movements—so much so that I am not sure they can readily think in any other way. Of course, nominal interest rate targeting, as espoused by leading academics such as John Taylor and Michael Woodford, works relatively well in normal times. During exceptional times like today, though, the zero bound means that the Federal Reserve has lost the ability to signal its intentions to the private sector through nominal interest rate movements. If the FOMC is not clear and careful, medium-run expectations for inflation could begin to drift in a manner that is inconsistent with the Federal Reserve's primary goals of price stability and sustainable economic growth. Even within the community of monetary policy specialists, my sense is that today there is wide-ranging uncertainty about the future of the macroeconomy, the role for monetary policy, and the medium-term impact on inflation. Some financial market participants and observers predict substantial inflation in the coming years, while others think continued disinflation and possibly deflation is the more likely scenario. In short, the Federal Reserve is in new territory, using new policy tools at a time when we are face-to-face with a two-headed dragon. In one instance, we face the medium-term risk of a deflationary trap, like the one observed in Japan during the 1990s and into the current decade. In the other instance, the Federal Reserve simultaneously faces the risk of a 1970s-style inflation stemming from a failure to control monetary base growth.",1
44,2,Business Economics,19 June 2009,https://link.springer.com/article/10.1057/be.2008.8,Is China's Exchange Rate Policy a Form of Trade Protection?,April 2009,Anthony J Makin,,,Male,Unknown,Unknown,Male,"This section develops an international macroeconomic framework for examining China's external account surplus and the counterpart deficits of its trading partners. The main focus is the role played by the exchange rate, as primarily determined by trade flows in the context of very limited private capital mobility, though as becomes apparent in subsequent analysis, public sector capital flows are in fact highly mobile. There is a wide range of exchange rate models in extant exchange rate theory, as surveyed by Isard [1995] and Sarno and Taylor [2002]. At one extreme there is the traditional flow equilibrium model of the exchange rate exposited in most international economics textbooks, in which the exchange rate simultaneously equilibrates net demand and supply of foreign currency arising from trade account transactions. This approach contrasts markedly with later asset stock equilibrium approaches that stress capital account transactions [Branson 1983; Frenkel and Mussa 1985] that are usually deemed more appropriate in the context of high international capital mobility. Yet, the traditional flow approach remains a useful starting point for examining trade-oriented economies like China that retain restrictive capital controls and have less developed financial markets and institutions. This flow approach underpins the following analysis. Nominal and real exchange rate movements directly influence aggregate output and exports and aggregate expenditure and imports. For China, let us define the real exchange rate as:  where e is the nominal effective exchange rate, P is China's domestic price level, and P* is the weighted price level of its trading partners. (In what follows, all macroeconomic variables for trading partners that correspond to the same variables for China are similarly denoted with an asterisk.) Hence, for China's trading partners, the real exchange rate is defined as  where E is the weighted nominal effective exchange rate of China's trading partners, the inverse of trading partners’ exchange rate against the yuan, or E=1/e, so that a depreciation of the yuan is an appreciation of trading partner currencies. In what follows, it is assumed that foreign and domestic price levels are stable in the short run, consistent with low worldwide inflation. Thus, movements in the nominal exchange rate therefore account for short-term real exchange rate variation. International macroeconomic accounting dictates that trade and current account imbalances reflect regional output-expenditure gaps [Alexander 1952], and these are strongly influenced over the short to medium run by overall competitiveness, as measured by real exchange rate movements. Because the exchange rate is a shared variable, China's external surplus reflecting excess production over expenditure has implications for the external deficits of Western trading partners that signify excess of expenditure over production. Aggregate output or supply functions for both regions are specified as  where L, K, and T are the factor inputs labor, capital, and technology, respectively, used to produce national output, Y. Nominal exchange rate depreciations improve competitiveness and hence encourage higher short-run production in anticipation of increased exports of goods and services. Following substantial foreign direct investment in China over past decades that has directly augmented the capital stock, multinational corporations produce a sizable part of the economy's annual output for export. On the aggregate demand or expenditure side,  where A, A* is absorption or aggregate demand, C, C* is consumption, and I, I* is investment. The trade account balance measures the divergence between national output and expenditure, such that  where Y, Y* is national output or income, TS is the trade surplus, and TD* is the counterpart trading partner deficit. Aggregate expenditure or demand functions for both regions can be written as  Aggregate demand is influenced by domestic interest rates, i, i*, the fiscal policy stance, γ, γ*, (with a rise in γ implying fiscal expansion), and residents’ wealth, φ, φ*. In turn, domestic interest rates are a function of short-run monetary conditions, M, M* (a rise in M implying monetary expansion). Total expenditure in an open economy includes spending on imported goods and services, whose prices are initially set in foreign currency. A stronger exchange rate lowers the domestic currency price of imports, increasing import demand and total expenditure. Hence, a downward sloping aggregate expenditure schedule AD for China can be drawn in exchange rate-expenditure space as shown in the left panel of Figure 1. Exchange Rates and Output-Expenditure Imbalances As China's national product includes that part of output sold abroad as exports, total output is positively related to competitiveness, which, given the assumption about short-run price level stability, reflects nominal exchange rate movements. Hence, it is depicted by an upward sloping AS schedule in exchange rate-output space, as also shown in the left panel of Figure 1. Divergences between domestic expenditure and output manifest not only as external imbalances but also as excess demand or supply of foreign currency. In the absence of capital flows, the trade account is balanced and the nominal exchange rate is at initial equilibrium at the point where national expenditure equals national output. The weaker is the exchange rate, the higher is the excess supply for goods and services, and hence the greater the economy's net supply of foreign currency due to trade surpluses. Keeping in mind that the effective exchange rate for China is E=1/e, the vertical axis in the right panel measures the same exchange rate as the left panel but is read from the top down. Thus, the aggregate expenditure and production schedules for trading partners in the right panel have opposite slopes to those shown in the left panel. The trade accounts of China and its Western trading partners balance where aggregate supply and demand schedules intersect at nominal effective exchange rates, e0, E0.",5
44,2,Business Economics,19 June 2009,https://link.springer.com/article/10.1057/be.2009.4,The Value of Private Businesses in the United States,April 2009,Patrick L Anderson,,,Male,Unknown,Unknown,Male,"Most news media provide daily news about the major stock markets in the United States. However, the “stock market” includes only a small fraction of the firms in the United States. These are generally, though not always, larger and longer-established firms that have chosen the corporate form of organization for the advantages it offers to professionally managed firms that need capital investments on a large scale. As we will demonstrate below, far more firms, with larger earnings, are outside this well-publicized subset of American businesses. We start our analysis of privately held firms by looking at the available data on firms and their value. The U.S. Census Bureau conducts an economic census every five years, which provides the base data for much of what we know about private businesses. However, the economic census does not ask for “value” of businesses. However, excerpts from this census provide invaluable information on how firms are financed, who owns them, and the trends in the number of firms and the number of their employees. The 2002 census data are the most current available. The Census Bureau also reports County Business Patterns (CBP) data, which primarily focuses on business activity in a particular area. The CBP data generally support the estimate of the Small Business Administration (SBA), discussed below, on the number of firms. IRS data on the number of returns filed are also instrumental in understanding which firms could be private vs. public. Public firms generally fall into one category of IRS filings: “C” corporations (C corps). The “C corp” is what people commonly call a “corporation.” It is distinguished by equity shares that can be held and traded by domestic or foreign persons or entities, the ability to have multiple classes of stock, and by the application of federal income tax to the earnings of the firm itself. In contrast, “S” corporations (S corps) and sole proprietorships cannot be publicly traded. S corps are restricted in terms of both number and type of shareholders, and are pass-through entities in terms of federal income taxation.Footnote 1 Some noncorporations (such as “limited liability companies” or LLCs) may also elect to be taxed as an S corp. Real estate investment trusts (REITs) and regulated investment companies (RICs) also file corporate tax returns, although most do not pay income tax.Footnote 2 REITs and RICs are designed, as their names imply, to be pass-through entities for earnings on real estate and other investment rather than operating businesses. Partnerships are not separate entities from their partners, but are an aggregate of the individual partners. In general, LLCs file as partnerships unless they have elected to be taxed as an S corp. Most partnerships are privately held. However, there are a limited number of publicly traded partnerships or PTPs (sometimes called “master limited partnerships”) in which units are traded on exchanges, like shares of stock.Footnote 3 The owners of these units generally become limited partners, in that their risk of liability is limited to their ownership interest in the partnership. Some LLCs also function like limited partners in a larger partnership. With the foregoing definitions in mind, we summarize in Table 1 the number of firms operating in the United States. Note that the raw number of firms reported by the Census, 23 million, is slightly smaller than the IRS data on business income tax returns filed in 2002. This is consistent with the observation that some companies may file multiple returns, including specific returns for wholly owned subsidiaries. One frequent division of businesses is between large, publicly traded companies and “small businesses.” This classification, as we describe briefly below, is defective in at least two ways. However, it is useful to review the available data on “small business” for clues as to the characteristics of private firms. What might be called the stylized facts about small business was recently summarized in a Federal Reserve staff paper:
 Small businesses—nonfarm entities with fewer than 500 employees—are an integral part of the U.S. economy. They account for about half of private sector output, employ more than half of private-sector workers, and have generated 60 percent to 80 percent of net new jobs annually over the past decade. [Mach and Wolken 2006]. However ubiquitous these stylized facts are, they are difficult to confirm. The bases are estimates made by the Office of Advocacy for the Small Business Administration for the nonfarm sector [Small Business Administration, current]. The SBA conducts annual research on “small” businesses and summarizes the results in multiple reports, including an annual report to the President. The 2006 Annual Report cites the “under 500 employee” definition for small businesses, noting that this threshold “means about 99.9 percent of all businesses are small [Small Business Administration 2006, p. 8].” The report also notes that among the “maze” of federal statistics, it is difficult to find even a reliable estimate of the raw number of private firms.Footnote 4 We noted above the common division of businesses into small, privately held firms and large public corporations. One clear defect in this notion is the fact that some of the largest U.S. companies are privately held. These firms, unless they trade public debt, are not required to file financial data with the Securities and Exchange Commission. Forbes magazine compiles a much-watched list of the largest firms annually [Reifman and Wong 2006]. Their 2006 list includes Koch Industries (revenue of $90 billion), Cargill (revenue of $70 billion), PricewaterhouseCoopers (revenue of $21 billion), and Publix Super Markets (revenue of over $20 billion). There were 394 private companies with revenues in excess of $1 billion that made the 2006 “Forbes Largest Private Companies” list [Reifman and Wong 2006].Footnote 5 Forbes estimates that these firms employ 4.4 million people and generate $1.25 trillion in sales annually. In general, a firm that is “publicly held” is one in which the equity interests are traded on an exchange that is open to all investors. The major stock exchanges, such as the NASDAQ and New York Stock Exchange (NYSE), establish listing requirements for such firms that generally limit the listed firms to large, well-established firms. Although this point is widely misunderstood, the U.S. Securities and Exchange Commission does not set listing standards [Securities and Exchange Commission current]. At a first approximation, we estimate there are 6,000 to 7,000 publicly held firms in the United States, and over 7 million active, bona fide firms in total.Footnote 6 Thus, the term “publicly held” excludes approximately 99.9 percent of the firms in the United States. A gray area in the definition of this category encompasses the large number of companies with shares that are thinly traded through a market-maker of some type. These trades occur in what is commonly called the over-the-counter (OTC) or “pink sheets” markets.Footnote 7 Some of these firms meet the threshold of publicly traded firms. Some, especially those without any recent transactions, do not. “Privately held” does not mean “equity not traded.” Many private firms have equity that trades hands on a regular basis. Medium and large professional service firms, for example, typically have partners join and leave each year.Footnote 8 Some industries (notably franchised industries such as auto dealers, hotels, restaurants, and alcoholic beverage distributors) have experienced significant consolidation among franchisees over the past two decades, resulting in fairly large companies with many equity owners. It is interesting to note that private firms with relatively frequent equity transactions create a second gray area in the boundary between “private” and “public” firms. Indeed, there are probably many privately held firms that have larger blocks of equity changing hands in various years than for a good portion of the corporations with OTC-traded stocks.",11
44,2,Business Economics,19 June 2009,https://link.springer.com/article/10.1057/be.2009.3,A New Metric to Gauge Household Economic Stress: Improving on the Misery Index,April 2009,Robert A Dye,Chad Sutherland,,Male,Male,Unknown,Male,"In order to more accurately assess consumer credit delinquencies and give a more detailed picture of the economic pressures being felt by American households, we developed the PNC Financial Services Group's Household Economic Stress Index (HESI). The HESI combines the two factors of the Misery Index (the rate of inflation and the unemployment rate) with today's new drag on households, declining real estate values. Specifically, the value of the index equals the rate of change in consumer prices over the past year plus the current unemployment rate minus the rate of change in house prices over the past year (as reported by the Office of Federal Housing Enterprise Oversight (OFHEO) now part of the Federal Housing Finance Agency)Footnote 1, or:  This simple metric combines three powerful forces that shape the economic lives of households on a daily basis. We feel that the addition of change in house prices to the Misery Index will be a more effective metric in evaluating overall household economic stress as well as assessing the potential for consumer credit defaults.",2
44,2,Business Economics,19 June 2009,https://link.springer.com/article/10.1057/be.2009.8,Updated Economic Statistics in 2009,April 2009,Robert P Parker,,,Male,Unknown,Unknown,Male,"As a result of Congress funding most of the federal government for fiscal 2009 at fiscal year 2008 spending levels, with a so-called “continuing resolution,” through March 11, the major statistical agencies already had made program cuts prior to that date. For example, BEA eliminated the survey of new foreign direct investment in U.S. companies by foreign companies and reduced the level of detail published for regional estimates. BLS has discontinued several service sector prices from its international price program and stopped work on developing new indexes. It also cut the size of the sample used for the National Compensation Survey, reducing the reliability of the employment cost index and the employee benefits survey, and it may eliminate the publication of all metropolitan area hours and earnings data and the publication of all employment data for the 65 smallest metropolitan areas in the Current Employment Statistics (CES) program. With the passage of a budget, work on several key programs that had been placed on hold is now underway. These programs include BEA's data improvement plan for research and development activities and a health care measurement initiative; the updating by BLS of two of the samples used to construct the CPI—the geographic sample and the housing sample used for tracking price change in that component (both samples had been based on the 1990 Decennial Census); maintaining by BLS the sample size and reliability of the national unemployment rate and other key indicators obtained from the Current Population Survey; and the Census Bureau's expansion of collection of data on the service sector.Footnote 2",
44,2,Business Economics,19 June 2009,https://link.springer.com/article/10.1057/be.2009.9,The CEO Share Of Earnings: A New Approach To Evaluating Executive Compensation,April 2009,Kevin M Zhao,Charles L Baum,William F Ford,Male,Male,Male,Male,"This article presents a very preliminary analysis of how a new metric, the CEO share of earnings, might be used to shed new light on the agency theory question of whether non-owner mangers of large publicly-held companies are overcompensated vis-à-vis the earnings they produce for the owners of their companies over time. When earnings is used as a crude first approximation of returns to the shareholders, our findings do not validate the widely assumed hypothesis that CEOs are increasingly overpaid for their work. We hasten to add that these are very crude and preliminary conclusions. One of the major limitations of our work is that we excluded from the analysis companies that had negative earnings or failed during the sample period and thus did not address the hot-button issue of whether departing CEOs of failing companies were over-compensated for their efforts. We should also note that, in this initial run through the data bases cited above, we made no attempt to correct for sample selection bias flowing from the omission of unprofitable, failed, and merged-out firms, etc. Those sources of bias will be addressed later when we generate a balanced panel of companies to study. We also made no attempt to disaggregate our sample by industry. That is potentially important, especially in particular industries, such as the financial sector, which may behave differently. As always, our findings are of course affected by the specific start and end points of the data we analyzed. In that connection, adding on the 2008 recession year, when the data becomes available, will sharply impact the trend of the CEO's share of earnings discussed above. Also, as noted at the outset, the earnings performance measure we use is not the only or even the best measure of the total returns to shareholders generated by professional managers. Nevertheless, we hope our introduction of this new executive performance metric will encourage others to join us in exploring its usefulness in analyzing key agency issues. Finally, in that connection, we should add that the same logic can be applied to evaluate the total compensation of top management teams of executives (not only CEOs), as reported in Securities & Exchange Commission (SEC) filings, to evaluate their compensation trends vis-a-vis the profits they generate for shareholders.",1
44,2,Business Economics,19 June 2009,https://link.springer.com/article/10.1057/be.2009.6,The Black Swan: The Impact of the Highly Improbable,April 2009,Gerald L Musgrave,,,Male,Unknown,Unknown,Male,,
44,2,Business Economics,19 June 2009,https://link.springer.com/article/10.1057/be.2009.7,The First Billion is the Hardest: Reflections on a Life of Comebacks and America's Energy Future,April 2009,John C Goodman,,,Male,Unknown,Unknown,Male,,
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.20,From the Editor,July 2009,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.21,In Memoriam: Edmund A. Mennis,July 2009,Gerald L Musgrave,,,Male,Unknown,Unknown,Male,,
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.14,Fiscal Policy and Economic Recovery,July 2009,Christina D Romer,,,Female,Unknown,Unknown,Female,"Let me start with the issue of the effects of fiscal policy in general. If we cut taxes by 1 percent of GDP or increase government spending by a similar amount, what will that typically do to real GDP or employment? That is, what are the fiscal policy multipliers? I will be the first to point out that estimating these multipliers is difficult and that there is surely substantial uncertainty around any estimate. But, I feel quite confident that conventional multipliers are far more likely to be too small than too large. David Romer and I have argued that omitted variable bias is a rampant problem in estimating the effects of fiscal policy. One good way to illustrate this is to discuss Robert Barro's approach to estimating the spending multiplier. Barro has argued that a reasonable way to estimate the effects of increases in government spending is to look at the behavior of spending and output in wartime. But, consider one of his key observations—the Korean War. If he were using just this observation, Barro would basically divide the increase in output relative to normal by the increase in government purchases relative to normal during this episode. When one does this, one gets a number less than one. From this Barro would conclude that the multiplier for government spending is less than one. But, other things were going on at this time that also affected output. Most importantly, taxes were raised dramatically; indeed, the Korean War was largely fought out of current revenues. That output nevertheless rose substantially is evidence that the effects of increases in government spending are, in fact, very large. In estimating the effects of the recovery package, Jared Bernstein and I used tax and spending multipliers from very conventional macroeconomic models. Indeed, many people in this room, as well as those at key government agencies, kindly provided simulations of the effects of standardized changes in taxes and government spending in their models. One of the things that both struck us and reassured us was that the estimates were quite similar across forecasters. In most models, a tax cut has a multiplier of roughly 1.0 after about a year and a half, and spending has a multiplier of about 1.6. These policy multipliers are surely more accurate than the simple calculations Barro suggests because big macro models try to take into account the other factors driving output. In their estimation, what is happening to all the policy variables is considered, as well as factors such as international developments, commodity prices, and consumer sentiment. However, because it is difficult to fully control for all of these factors, and because policy inherently has a large endogenous component, the issue of omitted variables is almost surely still present and important. For this reason, David Romer and I [Romer and Romer 2008] proposed an alternative way of estimating the effects of tax changes.Footnote 1 We found that the short-run effect of a permanent tax cut of 1 percent of GDP is to raise output by between 2 and 3 percent over the next three years. Furthermore, the nature of the responses suggests that the short-run effects of a tax cut operate mainly through aggregate demand: unemployment falls quickly and sharply, and inflation tends to rise. Unfortunately, doing the same kind of narrative analysis for government spending would be very difficult: there are vastly more spending changes than tax changes, and the motivations for them are less easily classified. But, the same issue of omitted variables is surely present. As the Korean War example illustrates, spending changes are often taken at the same time as tax changes that push output in the opposite direction. Also, spending increases are often taken in recessions, where other factors are clearly reducing output. As a result, it is likely that conventional estimates of spending multipliers are also biased downward. Furthermore, there is every reason to believe that if we could do the same kind of careful study for government spending, the usual relationship between tax and spending multipliers would be maintained. That is, measured correctly, I would expect the spending multiplier to be larger than the tax multiplier. The reason is the conventional one: all of an increase in government purchases goes into spending, whereas only some fraction of a tax cut is spent. The key conclusion of this analysis of general fiscal policy multipliers is that fiscal stimulus typically has a substantial effect. Moreover, existing estimates are more likely to biased downward than overstated.",6
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.12,Government Lending and Monetary Policy,July 2009,Jeffrey M Lacker,,,Male,Unknown,Unknown,Male,"Views about the role of government credit in promoting financial and macroeconomic stability tend to be shaped by views about the role of credit in the business cycle. In one popular view, credit market disturbances, such as the recent rise in losses on mortgage-backed securities, cause banks and other credit intermediaries to pull back credit supply as they attempt to repair their balance sheets. The reduction in lending to households and firms forces them to reduce their spending on goods and services and creates an additional drag on growth. An alternative view is that shocks to the economy affect spending more directly and that as growth declines, the creditworthiness of households and firms deteriorates, causing credit flows to fall and spreads to widen. These two views represent opposite directions of causality between credit and aggregate spending. In reality, both of these directions may well be in operation at the same time, and determining the quantitative importance of each is very hard. But, my reading of recent events emphasizes the second view, in which the effect of slowing growth on credit conditions predominates. This view has received much less attention than it deserves, I believe; so let me say a few words about the current cycle in light of this issue. The antecedent of the contraction we are in was the boom in home sales, prices, and construction coming out of the last recession. Untangling the causes of that boom poses research challenges that will launch a thousand dissertations, I expect. The most plausible suspects at this point include financial innovation, regulatory laxity, accommodative monetary policy, and a global savings glut; but all worked through the expanding availability of mortgage credit. Even with favorable financing conditions, the increasingly leveraged purchases of homes would not have made sense without confidence—in hindsight misplaced confidence—in a continued upward path for home prices. Whatever the causes of the boom, the result was what turned out to be a glut of housing, which, as people's beliefs about demand growth adjusted, led to historic declines in prices. The most immediate effect was a collapse in residential investment and large consequent declines in employment in construction and related sectors. The reduction in home owners’ wealth as home prices declined, together with growing uncertainty about labor market prospects, caused household spending to slow beginning in mid-2007, and then declined outright in mid-2008. The dimming outlook for consumer spending also dampened business investment spending in turn and spread the employment slowdown beyond the residential construction sector. This, in turn, further dampened the consumer spending. These trends reduced prospects for, and increased uncertainty about, household incomes and firm revenues. As a result, households and firms are riskier, lending prospects than they were a couple of years ago, given the change in the overall macroeconomic environment. Note that surveys that ask lenders whether they have “tightened terms” in recent months do not really get at this question. Any given profile of borrower characteristics—income, balance sheet, and credit score, for example—is likely to translate into a riskier loan now, so banks are likely to have tightened qualification cutoffs even without any reduction in their risk appetite.Footnote 1 The downturn in home prices in many regions has resulted in increased losses on home mortgages, particularly subprime mortgages. Uncertainty about the ultimate depth of the decline in home values has meant ongoing uncertainty about the magnitude of aggregate losses that will be realized on mortgage-related assets. Financial market participants have also faced uncertainty about where these losses will turn up. Mortgage risks were split up and spread widely, both within United States and abroad, through securitization and use of the insurance capabilities provided by credit derivative contracts, making it difficult to assess any individual institution's share of the aggregate exposure. In addition, financial market participants have at times faced uncertainty about prospective public sector intervention. The disparate responses to potential failures at several high-profile organizations may have made it difficult for market participants to forecast whether official support would be forthcoming for a given counterparty, and where in the capital structure that support would land. Most of what have been observed in financial markets since the summer of 2007 seem readily intelligible as a consequence of the increased uncertainty facing market participants resulting from the significant economic downturn. Apprehension about potential losses caused lenders to demand higher risk premia in interbank credit markets for institutions with at least some presumed mortgage-related exposure. Market participants became especially concerned about the heightened risk associated with lending at longer maturities, and so risk premia became especially elevated for term lending. Some borrowers were unwilling to pay higher premia for term loans, and shortened the tenor of their funding. Others sought to protect themselves against an erosion in counterparties’ perception of their creditworthiness by paying the unusually high premia to “lock in” funding or by hoarding liquid assets despite high opportunity costs. More broadly, the proliferation of intermediation channels in recent years has meant that for many borrowers, the next best financing option may not be much more costly. For example, many commercial paper issuers have back-up lines of credit with banks that they can draw on in the event they are unsatisfied with market pricing. Thus, observing that a given intermediation channel is “frozen,” “clogged,” or “dried up” may not indicate dysfunction, but may indicate instead just a portfolio reallocation in response to a shift in risk assessments.",1
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.13,Four Long-Term Fiscal Realities,July 2009,Alan D Viard,,,Male,Unknown,Unknown,Male,"Numerous studies have documented the long-run imbalance between federal revenue and federal spending. Notably, the Congressional Budget Office (CBO) [2007] projects that, under current policies, federal spending on the three major entitlement programs—Social Security, Medicare, and Medicaid—will rise from 8.4 percent of GDP in 2007 to 14.5 percent in 2030, 18.6 percent in 2050, and 25.7 percent in 2082. Other noninterest federal spending declines slightly (from 9.9 percent of GDP in 2007 to 7.6 percent in 2082), but the net result is a surge in total federal spending. In contrast, federal revenue will rise only modestly under current policies, inching up from 18.8 percent of GDP in 2007 to 20.9 percent in 2082. The imbalance between revenue and spending will cause the federal debt to grow explosively.Footnote 1 Over the long haul, Medicare and Medicaid account for nearly all of the entitlement growth; from 2007 to 2082, their share of GDP rises by 15.2 percentage points, but Social Security's share rises by only 2.1 percentage points. In the early part of this period, however, Social Security's relative importance is somewhat greater. From 2007 to 2030, Social Security accounts for 1.8 percentage points of the total 4.1-percentage-point rise in these programs’ share of GDP. The near-term expansion of Social Security partly reflects the retirement of the baby-boomer generation. Over the longer haul, all three programs are affected by the ongoing rise in longevity. Medicare and Medicaid are also affected by a relentless rise in the price of health care, relative to the price of other goods and services. As suggested by the rapid projected growth of the two medical programs, the increase in the relative price of health care is the single most important driver of the long-run entitlement growth. The key implication of the fiscal imbalance is a reduction in the wealth of future generations. Future generations will face a heavier fiscal burden as they service the large stock of government debt. Unless private saving rises to offset the government borrowing, national saving will fall and future generations will inherit a smaller amount of net wealth. The distribution of the impact across the various future generations depends upon when the fiscal imbalance is addressed. If the policies described by CBO actually remain in place through 2082, generations alive in 2082 and those born thereafter will face a very large decline in wealth. If policies to narrow the fiscal imbalance are adopted before that time, the impact on those generations will be reduced, with part of the burden falling instead on earlier generations. Although some discussions of the fiscal imbalance emphasize its potential effect on interest rates, any such effects are of secondary importance. In a closed economy, a restricted supply of national saving would be expected to drive up interest rates and reduce domestic investment. Interest rates need not rise, however, if the economy is sufficiently open to international capital flows, as an inflow of foreign savings can then offset the reduction in national saving and avert a decline in domestic investment. Even in that case, however, the decline in national saving still reduces the wealth of future domestic residents. Although the inflow of foreign savings props up the domestic capital stock, part of that capital stock belongs to foreign savers rather than domestic residents, implying that the wealth of the latter group still declines. Although access to foreign savings can ease the macroeconomic adjustment to a decline in national saving, it cannot change the fundamental economic fact that lower saving today leads to lower wealth tomorrow.Footnote 2 Other discussions emphasize disastrous consequences that may result if policymakers wait too long to address the fiscal imbalance: Foreign lenders may eventually be unwilling to buy additional Treasury securities or the government may eventually default on its debt, either explicitly or implicitly through inflation. These outcomes can, should, and probably will be avoided, however, by addressing the fiscal imbalance before such crises arise. In this paper, I adopt a different emphasis, focusing on developments that are likely to occur over the upcoming decades as policymakers address the fiscal imbalance. In the following four sections, I describe four long-term fiscal realities pertaining to the trajectory of revenue and entitlement spending, the distribution of the fiscal burden, and the shape of the federal tax system.Footnote 3",
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.15,Trade Policy and the Obama Administration,July 2009,Jeffrey J Schott,,,Male,Unknown,Unknown,Male,"What will be the U.S. priorities for trade negotiations? Overall, the agenda will be constrained in the near term because of the economic crisis. It is hard to generate political support for new trade liberalization in times of economic stress. As we have seen in recent months, it is extremely difficult even to hold the line on existing levels of market access. As a practical matter, the administration will have its hands full in the near term with unfinished business of the Bush era—particularly the Doha Round and the free trade pacts with Colombia, South Korea, and Panama that have been signed but not yet ratified by Congress. But the Obama administration will want to put its own mark on each of these initiatives and refocus attention on issues of high priority to core Democratic constituencies, such as environmental issues in the WTO talks, auto issues with South Korea, and labor problems in Colombia. Accelerating the Doha negotiations needs to be an important part of the overall response of the G20 nations to the global economic crisis—with the aim of deflecting new measures that distort trade and investment in the short run and preparing the final package of trade reforms for political decision in late 2010 or 2011. It is now clear that the status quo is not sustainable, and that if multilateral negotiations fail—or even continue to drift—markets will become less open to international competition, and the viability of the multilateral negotiating process, including the WTO's valuable dispute settlement mechanism, will be called into question. By grasping the leadership mantle in the WTO, U.S. officials can both pursue important economic objectives while demonstrating a renewed U.S. commitment to multilateralism. By contrast, we will likely see a dramatic shift in the U.S. approach to bilateral FTAs. The U.S. strategy of the past decade, which has produced trade pacts with more than a dozen countries, has become politically unpopular and is unlikely to be emulated by the Obama administration. In any event, the queue of potential new FTA partners is short and comprised mostly of small trading partners with a predominantly political agenda (for example, Taiwan, Egypt, Ukraine). Instead of negotiating new bilateral FTAs, I think the Obama administration will likely give more attention to melding existing pacts in the Western Hemisphere and across the Asia-Pacific region (including talks on a Trans-Pacific Partnership launched by former USTR Susan Schwab in September 2008). In addition, the USTR may also explore prospects for targeted deals with the European Union, Brazil, Japan and other major trading partners that focus on services, energy, and the environment. But this exploration will take some time to develop and will depend importantly on what happens in the Doha Round. The more immediate task for the Obama administration will be to find a way to implement the deals awaiting ratification, in particular the FTAs with Colombia and South Korea. For both foreign policy and national security reasons, the United States needs to ratify these pacts. It is inconceivable that the Congress would outright reject a critical U.S. regional ally, but it will take some political skill to get the Congress to say “yes.” In addition, as already discussed in bilateral summits with the leaders of Mexico and Canada, President Obama will want to “upgrade” NAFTA to better meet the challenges of border security, energy security, and climate change, and to fulfill his campaign promises on labor and environment.",1
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.18,Globalization of the U.S. Food Supply: Reconciling Product Safety Regulation with Free Trade,July 2009,Thomas A Hemphill,,,Male,Unknown,Unknown,Male,"Public regulations, also called administrative laws (in contrast to statutory and case law), “are specific standards or instructions concerning what can or cannot be done by individuals, businesses, and other organizations, with the explicit threat of sanction or fine for transgressions” [Dudley 2005, p. 1]. Food safety regulations are generally classified as product or process standards: product standards specify characteristics that a product must attain before it is considered safe to enter commerce, but process standards specify techniques that must be used by the industry to process or package foods [Mitchell 2003]. The FDA and the Food Safety Inspection Service (FSIS) of the U.S. Department of Agriculture (USDA) are the primary public agencies responsible for enforcing the U.S. government's imported food regulatory system—although a number of other federal and state agencies also have food health and safety regulatory oversight roles, including the Imported Food Initiative entered between the FDA and the New York State Department of Agriculture & Markets [Becker and Porter 2007].Footnote 2 The FDA is charged with regulatory responsibility for the safety of approximately 80 percent of the U.S. food supply (including nonred meat and certain poultry) and enforces the U.S. Food, Drug, and Cosmetics Act [Becker and Porter 2007]. Based on FDA value-of-shipment data for 2003, the agency regulates $417 billion in domestic food and $49 billion in imported food [U.S. Food Drug Administration 2007a]. The FSIS is responsible for enforcing the federal safety standards for major red meat, poultry, and egg products, which makes up most of the remaining 20 percent of the nation's food supply [Becker and Porter 2007]. Although food products entering U.S. commerce must meet the same safety standards as domestically manufactured food products, under the WTO's SPS Agreement, a foreign country is permitted to apply its own regulatory authorities and institutional systems in meeting U.S. standards under an internationally recognized concept known as “equivalence” [Becker 2006]. To insure that imported food in fact complies with U.S. standards, the FDA relies on a system of prior notification of incoming food products by importers with U.S. Customs, as well as document reviews at points of entry [U.S. Food Drug Administration 2004]. This system helps FDA inspectors to determine a food shipment's relative risk and whether it requires actual examination and/or testing of a food sample at one of the agency's 13 regional laboratories—whose testing results may indicate noncompliance with U.S. regulations and a refusal of the food shipment's entry into U.S. commerce [U.S. Food Drug Administration 2004]. The FSIS utilizes a very different regulatory system than the FDA, including a different approach to addressing equivalence, for inspecting imported food products [U.S. Department of Agriculture 2007]. Under the FSIS regulatory system, foreign countries must pass a comprehensive review process before being considered eligible to export to the United States [United States of America 2004]. The FSIS requires that countries exporting into the United States supply certificates attesting to compliance with U.S. food safety laws and regulations [United States of America 2004]. Working collaboratively with the food inspection service of the exporting government, the FSIS must certify that the country from where the food is imported has a U.S. equivalent level of safety protection (the FSIS presently allows only 32 countries this privilege) [U.S. Department of Agriculture 2007].Footnote 3 The FSIS will thereafter rely on the foreign country's inspection system to carry out daily inspections and certify individual exporting establishments—although the agency will regularly conduct on-site audits of the eligible inspection systems [U.S. Department of Agriculture 2007]. Furthermore, FSIS personnel re-inspect food products at U.S. border entry points before allowing them to enter into U.S. commerce, as well as reviewing all import records (assisted by a statistical sampling program) [Becker and Porter 2007]. Although food safety risks from imports are similar to the kind and extent of risks from domestic producers, the United States has less food safety oversight over imports, which are increasingly from developing countries [Buzby and Unnevehr 2003]. Recent agency regulatory oversight statistics for imported food products released by the FDA have not been encouraging. Rapid growth of imports meant that in fiscal year 2006, the FDA was able to inspect 1.3 percent of all the imported food (over 10 million shipments) it is charged to regulate—down from 1.7 percent (2.8 million shipments) in fiscal year 1996 [Parker 2007]. Moreover, only 2 percent of imported food shipments inspected in fiscal year 2006 were analyzed in a laboratory as a part of the FDA's inspection process [Barrionuevo 2007]. In addition, FDA inspection resources declined between 2001 and 2006, with the number of FDA food import inspectors at U.S. ports of entry reduced by approximately 20 percent, but the dollar volume of food imports has risen by about 50 percent [Schmit 2007].Footnote 4 Furthermore, the FDA lacks food import data, such as information on where the product was manufactured and the process used during manufacturing [Eyre 2007]. Lastly, of 326 available ports of entry (ports, border crossings, and postal facilities) in the United States, which process over nine million entries of imported food and food-related products annually [Lutter 2007], only 90 of these locations have FDA inspectors on-site—leading some importers to target port of entry locations without FDA regulatory oversight [Schmit 2007]. How has this deteriorating imported food safety situation in the United States been reached? Nucci, Dellava, and Cuite [2008, pp. 22–23] offer one possible explanation:
 This situation is likely due, at least in part, to import volumes rising more quickly than the budget for inspections. Between 1996 and 2005, the USDA saw an 87 percent increase in meat and poultry imports (in millions of pounds). Over the same timeframe, total imported food shipments under FDA control increased 257 percent, resulting in decreases in import inspections of 10.3 percent by the USDA, and 0.7 percent by the FDA. Neither the USDA nor the FDA has been able to adequately respond to the increased demand for import protection. In 2003, the majority of expenditures of both the FDA and the USDA related to food safety were for inspection and enforcement of both domestic and imported foods. However, the FDA, which is responsible for inspecting 80 percent of the food supply, received only about 25 percent of federal safety funding. And the monies allocated by FDA towards inspection accounted in 1997 for only 23.5 percent of the total agency budget. In 1997 $203 million of the total FDA budget $997 million was spent on food safety issues. The largest share of FDA's budget is devoted to its nonfood drug, cosmetics and medical devices responsibilities (emphasis added). And even if there were a compelling increase in budget allocation, there is no guarantee that an increase in budget would successfully result in inspections (emphasis added). Furthermore, in 1999 the FSIS had a budget of $468 million allocated for inspection of slaughterhouses, processing, and import facilities—yet only $7 million was budgeted for inspection of import/export facilities [Nucci, Dellava, and Cuite 2008]. One conclusion seems inevitable: the U.S. government has given short shrift to funding regulatory inspections of imported food. For the FSIS, the agency's inspectors provide visual inspection to 11 percent of red meat imports at the border (and a smaller subset receives a laboratory inspection) [Food Safety and Inspection Service 2007]. In 1998 and 1999, the FSIS conducted initial country audits and concluded that 32 foreign trading partners had “equivalent” food safety systems (as permitted under the SPS Agreement) to the United States. Self-regulation exists when a firm, an industry, profession, or the business community establishes its own standards of behavior, such as through a code of conduct or ethics, either self-established or adopted, where no such statutory or regulatory requirements exist and when such standards assist in complying with or exceeding existing statutory or regulatory requirements [Hemphill 1992]. Self-regulation may be practiced at the firm-level, industry-level, or business (or transindustry)-level of economic organization. Industry self-regulation is typically through a trade association or a professional society that sets and enforces rules and standards relating to the conduct of firms as well as individuals in the industry [Gupta and Lad 1983]. Because nations have little control over the behavior of firms in locations outside their direct regulatory control, and the adverse consequences of such business activities do not respect national political boundaries, many business leaders are increasingly recognizing the need for private regulation [King 2007]. Wilson [2003, p. 148] has identified the following self-regulation framework:
 First-Party Certification: Corporations set their own code of practice and monitor its implementation. Second-Party Certification: The industry's trade association sets and monitors standards. Third-Party Certification: Standards are set in consultation with nongovernmental organizations (NGOs) or other forms of nonprofit organizations, which then take on responsibility for monitoring them. Fourth Party Certification: International regulation is undertaken by governmental bodies. According to the Consumers Union, a nonprofit, consumer advocacy group, industry self-regulation rarely protects consumers and often provides the food industry with “cover” when food contamination occurs [Consumers Union 2007]. Moreover, since industry self-regulation is voluntary, it does not ensure that all products that enter the marketplace are safe, because meeting such safety standards is costly to processors who may interpret safety standards differently [Consumers Union 2007]. Often, some form of governmental oversight and threat of public regulation co-exist alongside industry self-regulation [Gupta and Lad 1983, p. 423-424]. In fact, researchers have found that the greatest potential for industry self-regulation lie with a mixed form of industry rule-making and government oversight [Gupta and Lad 1983; Lagace 2007].Footnote 5 An example of such a food safety management program is the Hazard Analysis Critical Control Point (HACCP) initiative, initially developed by Dr. Howard Bauman of the Pillsbury Company in the early 1960s, and later championed by the nation's largest food processing industry association, the Grocery Manufacturers Association/Food Products Association (GMA/FPA) [Grocery Manufacturers Association 1998]. According to the Committee to Ensure Safe Food from Production to Consumption of the Institute of Medicine and National Research Council:
 HACCP programs use a systematic approach to identify microbial, chemical, and physical hazards in the food supply, and establish critical control points that eliminate or control such hazards … . The HACCP system institutes methods to control food safety hazards, whereas traditional inspection and testing procedures are not designed to detect and control contaminants that are sporadically distributed throughout foods and are not visible. [Institute of Medicine and National Research Council 1998, p. 30] The HACCP food safety management program involves seven principles [U.S. Food and Drug Administration 2001]:
 Analyze hazards Identify critical control points Establish preventive measures with critical limits for each control point Establish procedures to monitor the critical control points Establish corrective actions to be taken when monitoring shows that a critical limit has not been met Establish procedures to verify that the system is working properly Establish effective recordkeeping to document the HACCP system The HACCP food safety management program was adopted by the FDA as a regulatory tool—in 1995 for the U.S. seafood industry (fish and seafood products), and in 2001 for the U.S. juice industry (processing and packaging). In 1998, the FSIS established a similar program for red meat and poultry processing plants [FDA Backgrounder 2001]. As mentioned earlier, self-regulation is also practiced at the company-level, as federal and state laws create incentives—in the form of criminal and civil legal liability—to encourage firm managers to abide by state-of the-art food safety management practices. The presumption of corporate and industry responsibility is explicitly recognized by the U.S. government in a recent letter from the FDA to food manufacturers in which the agency announced it is “their [food manufacturers] legal responsibility to ensure that all ingredients used in their products are safe for human consumption (italics added)” [Bracket and Sundlof 2007]. Furthermore, the FDA encourages food manufacturers to ensure the safety of their ingredients, packaging, and processes and “not wait for possible FDA testing of their materials as manufacturers bear the responsibility of ensuring only safe products are put on the market” [Bracket and Sundlof, 2007]. According to a 2001 USDA study of 175 food poisoning lawsuits brought in U.S. courts between 1988 and 1997, juries awarded civil damages to plaintiffs in one-third of the cases, with average awards of $133,280 (although the median award was $25,560 and does not include other expenses, including legal and expert witness testimony) [Buzby, Frenzen, and Rasco 2001].Footnote 6 Such lawsuits can also engender negative public relations and impact long-term corporate reputation. With the FDA not able to enforce “equivalency” requirements adequately on exporting countries’ food processing facilities and these countries’ domestic regulation of food safety not meeting American standards, this places an increased burden on importing U.S. companies to self-regulate the safety of imports. This is not an enviable position for many U.S. food importers, as the FDA reports that in July 2007 there were 188,936 food processing facilities in 174 countries that export to the United States—presenting a daunting task for company/industry self-regulation enforcement [U.S. Food Drug Administration 2007a]. While company-inspired HACCP systems exist in some of these foreign food processing facilities, the food safety contamination incidents are rarely HACCP failures. According to William Sperber [2005], food contamination incidents are usually “failures of cleaning and sanitation practices or the lack of management awareness and commitment to provide the necessary training and resources.” Most importantly, says Sperber, “HACCP is a necessary, but insufficient condition to assure food safety.” This requires a more intensive effort by the importing food processing and retailing companies to guarantee that a “farm-to-kitchen-table” food safety system be designed and implemented for each country of origin. Figure 1 shows the “industry regulatory spectrum” developed by David Garvin [1983] and expanded by Hemphill [2003]. It identifies forms of regulation, or “regulatory strategies,” that can be proactively considered for design purposes, ranging from government ownership to an unregulated industry.Footnote 7 Garvin [1983] notes that the forms of regulation that tend toward a mixture of public involvement and oversight and self-regulation are the regulatory strategies most overlooked in designing new regulatory regimes. By comparing the industry regulatory spectrum of Figure 1 to the present system of imported food safety regulation in the United States, one can reasonably conclude that it involves a relatively high degree of public involvement. The regulatory strategies that characterize the food processing industry include “Self-Regulation Plus an Autonomous Government Agency with Rule-Making Authority,” involving a combination of industry and company approaches (HACCP) and government oversight (FDA and FSIS) and “Self-Regulation Embodied in Transnational, Federal or State Statutes” (including, for example, direct reference to HACCP in certain FDA/FSIS industry regulations). Sources: Garvin [1983, p. 48] and Hemphill [2003, p. 342]. The use of food safety regulation as a potential trade barrier or restriction against imported food products was initially (and weakly) addressed in Article XX of the 1947 General Agreements on Tariffs and Trade (GATT), although it and all subsequent GATT revisions failed to formulate and define an international agreement regarding food safety [Schmidt and others 2003].Footnote 8 It was not until 1994, when international food safety (along with animal and plant health) principles and rules were formulated in the SPS Agreement (a direct result of the 1986–1993 Uruguay Rounds of Multilateral Trade Negotiations), which is a mandatory part of the agreement establishing the WTO in 1995 [Buzby and Unnevehr 2003]. The SPS Agreement, because of its transparency and international framework for food safety, often resolves many disputes before reaching the Agreement's formal dispute settlement mechanism, which is an integral part of the foundation of the international trade order [Buzby and Unnevehr 2003]. Moreover, the SPS Agreement specifically references international standards as the benchmark against which national regulations are evaluated, although the SPS Agreement, under Article 2, affirms the rights of WTO members to adopt their own SPS measures to the extent necessary to protect human or animal life or health without imposing arbitrary standards [Athkorala and Jayasuriya 2003]. Under the SPS Agreement, WTO members, such as the United States, must agree to the following principles [Buzby and Unnevehr 2003, p. 4]:
 Transparency: Member countries must publish their food safety regulations and offer a mechanism for replying to enquiries from trading partners. Equivalence: Member countries must recognize the SPS measures of a trading partner nation as equivalent if they offer the same level of health and safety protection. Science-based Measures: SPS measures must be based upon scientifically based risk assessments (including control strategies) and must be chosen to minimize market distortions to trade. In the case of food safety, the international harmonization standards-setting body is the Food and Agriculture Organization/World Health Organization's Codex Alimentarius Commission, which has issued the revised Codex Standards since 1962 [Henson 2003]. The Codex Standards have helped facilitate discussions of internationally accepted standards, but the SPS Agreement has contributed to a more conciliatory climate of negotiations concerning food safety in international commerce [Buzby and Unnevehr 2003]. Interestingly, while many developing countries have adopted Codex Standards to improve competitiveness in international commerce, many developed countries have not embraced these standards [Schmidt and others 2003]. In addition to its WTO undertakings, the U.S. government has also entered into six regional trade agreements to encourage trade liberalization, including the North American Free Trade Agreement (with Canada and Mexico) and the Southern African Customs Union (with Botswana, Lesotho, Namibia, South Africa, and Swaziland). Furthermore, with protracted difficulties slowing the attainment of global free trade in the most recent Doha Round of multilateral WTO negotiations [Arnold 2003], bilateral negotiations resulting in Free Trade Agreements (FTAs) or similar Trade Promotion Agreements (TPAs) have been entered into by the U.S. government with 14 countries [International Trade Administration 2007].Footnote 9 According to the Office of the U.S. Trade Representative [2006], U.S. export growth with the 12 countries with which FTAs were implemented between 2001 and 2005 is twice as fast as U.S. export growth to the world. As of 2006, the countries that the United States has engaged in regional or bilateral free trade agreements account for 42 percent of total U.S. exports [International Trade Administration 2007].Footnote 10 Arnold [2003, p. 3], in his analysis of benefits and costs associated with FTAs for the U.S. Congressional Budget Office, specifically addresses their economic dynamics:
 In the case of an FTA, however, the reductions in trade barriers increase the competitiveness of imports from the other parties to the agreement not only relative to domestic production but also relative to imports from other countries. Consequently, any resulting rise in imports from parties to the agreement may displace either domestic production or imports from other countries. Economists refer to the displacement of domestic production as trade creation because it results in a net increase in trade. They call the displacement of imports from other countries trade diversion since it does not increase trade overall but rather amounts to a diversion of existing trade. Although FTAs/TPAs can be a challenge in the area of sanitary and phytosanitary issues, the U.S. government trade strategy followed when negotiating the SPS section of an FTA/TPA has generally been to require no new SPS rights or obligations, as the U.S. government's position is that the existing WTO SPS Agreement is adequate to meet food safety requirements [APHIS Trade Support Team 2004]. Furthermore, U.S. FTA/TPA negotiators will discuss only issues that are outstanding and not allow the introduction of new topics to the negotiations [APHIS Trade Support Team 2004]. Thus, according to the Office of the U.S. Trade Representative [2007], all FTAs/TPAs negotiated and approved by the U.S. Congress allow the U.S. government to determine the appropriate level of food safety protection for imported products (on the basis of a science-based assessment of specific risk), and no exporting country can force the U.S. government to accept their food products or ingredients that do not meet these food safety standards [Office of the U.S. Trade Representative 2007]. All imported food products entering U.S. commerce are required to meet the same level of food safety standards that are applied to domestically produced food products [Office of the U.S. Trade Representative 2007]. Moreover, there is no language included in these negotiated FTAs/TPAs that precludes FDA/FSIS and state regulators in the United States from instituting necessary surveillance and enforcement measures when such officials deem it necessary to ensure safe food for U.S. consumers [Office of the U.S. Trade Representative 2007]. Although there was little or no scientific evidence that food imported into the United States posed a greater risk than domestically produced food in the late 1990s [Zepp, Kuchler, Lucier 1998], the same statement cannot be definitively echoed a decade later.",
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.2,World Mining Machinery,July 2009,Michael A Deneen,Andrew C Gross,,Male,Male,Unknown,Male,"Mining activities have an adverse impact on both human health and the natural environment, both during and after mining operations. Human health issues range from poor working conditions, even at the surface level, to hazardous air and cave-ins when the mining is underground. Damage to the environment can run from mountain-top removal to erosion of land, from reduction of biodiversity to acidic runoff polluting streams. Such considerations affect both mining companies and makers of mining machinery. However, both mining and use of machinery in such operations improved worldwide. Of course, when mining disasters occur they still rate headlines regardless of location. Lately, Russia and other nations in eastern Europe along with some African, Asian, and Latin American mines have made the news; but North America also had its share of fatalities and injuries. The states of Kentucky and West Virginia have major environmental scars from mining operations; and Wyoming may be next, as its lower sulfur coal is much in demand. While laws and rules affect mining companies directly, they have an impact on makers of mining machinery. High cost of compliance with regulations can discourage companies from opening new mines or limit operations of existing facilities. Standards for mining equipment safety are generally toughest in western Europe. In the EU, mining machinery makers must comply with specific directives that range from the first date of operation all the way to decommissioning. A revised “Machinery Directive 2006/42/EC” is scheduled to come into force by the end of 2009. Among the changes are new, more precise requirements for noise and vibration. Standards are generally lower in developing nations where governments rely on mining for a major portion of their GDP. Mining machinery makers will face even more stringent standards in the coming years in Western nations. Although the emphasis in the past has been on higher safety levels in underground mines, surface machinery is coming under more regulation. The EU and the United States have set deadlines for implementing off-road vehicle emission standards for nitrogen oxide and particulate matter that will come into force by 2014–15. In Japan, two ministries have jointly established similar limits that will apply by 2009–10. We expect a longer horizon in developing nations. Enactment of regulations and enforcement are likely to vary on a country-by-country basis. It is possible that some treaties and/or voluntary action by large, multinational firms will result in commonality among nations.",3
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.17,An Economist's Role in Disaster Mitigation at FEMA,July 2009,Keith Burbank,,,Male,Unknown,Unknown,Male,"Jules Dupuit, a French engineer, developed BCA in the 1800s. In the 1900s, people became concerned with the efficient allocation of government resources, and by the 1930s, the United States was using BCA in many areas, such as water projects constructed by the U.S. Army Corps of Engineers. Additional concern led the Office of Management and Budget (OMB) to issue Circular A-94 in 1972, requiring the use of BCA for federally funded projects for agencies within OMB's jurisdiction, such as FEMA. Since then, the law giving the President authorization to provide funding for hazard mitigation, the Robert T. Stafford Act, mandates hazard mitigation projects be cost-effective. To determine “cost-effectiveness”, FEMA uses BCA. Although BCA and cost-effectiveness analysis are different, we use the term, “cost-effective,” to mean the benefits of a project are at least as great as the costs. Additionally, the code of federal regulations (CFR) that governs the HMGP, the 44CFR, requires projects be cost-effective. There are two reasons for conducting BCA, according to OMB Circular A-94: cost-saving and market failure. As FEMA pays local governments for the repair of their buildings and roads after a disaster, money spent to reduce or eliminate this damage may save FEMA money. A study by the Multihazard Mitigation Council (MMC), titled An Independent Study to Assess the Future Savings From Mitigation Activities, indicates that every dollar FEMA spends on mitigation saves $4.00 in future spending.",4
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.16,"Getting Off Track: How Government Actions and Interventions Caused, Prolonged, and Worsened the Financial Crisis",July 2009,Jerry H Tempelman,,,Male,Unknown,Unknown,Male,,2
44,3,Business Economics,25 August 2009,https://link.springer.com/article/10.1057/be.2009.19,Capital Ideas Evolving,July 2009,Robert A McLean,,,Male,Unknown,Unknown,Male,,
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.34,From the Editor,October 2009,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"This year's Mennis Award competitors produced an unusually robust crop of papers (a result of pent-up supply?). This issue leads off with the Mennis and NABE award winners. James A. Wilcox won the Mennis Award—for the second time. The NABE Award was a virtual dead heat between a paper by Dan Hamilton, Rani Isaac, and Kirk Lesh and another by Gad Levanon. Both were selected for NABE Awards and will appear in the January issue. Several other papers that were submitted were also considered to be worthy of publication and are likely to appear in Business Economics in subsequent issues. Although much ink has been shed over the role of lowered underwriting standards in contributing to the ongoing housing crisis, there has been little hard data or analysis on the degree to which underwriting standards have changed. James A. Wilcox's paper develops a measure of the tightness of underwriting standards and investigates how they have changed and how they have impacted housing markets and aggregate growth. Oil price shocks have been an important influence in the U.S. economy, but how important remains an important question. Neal Ghosh, Chris Varvares, and James Morley investigate this issue after careful definition of what is in fact a “shock.” Their empirical analysis advances our knowledge on how to address this issue and provides important results. Lawrence Seidman and Kenneth Lewis ask whether temporary fiscal stimulus causes too much federal debt. Using the Fair macroeconometric model, they conduct simulations that indicate that a robust fiscal stimulus would effectively mitigate the current recession. Moreover, they find that debt as a percentage of GDP is only slightly greater with the fiscal stimulus than it would be without it. Given that the current recession was driven by a crisis in residential real estate, an important question for the future is the shape of this market. The analysis presented in James E. McNulty's paper suggests that the existing stock of housing far exceeds the number of households seeking to occupy it—largely due to high levels of construction in the mid-2000s—and that it will take several years for the excess to be worked off. To help avoid recurrence, Mc Nulty suggests that future monetary policy should routinely take long-run housing markets into consideration. Continuing on the theme of monetary policy, Jerry H. Tempelman examines how the Federal Reserve conducts monetary policy under noncrisis circumstances and compares this with its actions in response to the current financial crisis, particularly in late 2008. In this issue's Focus on Statistics, Robert P. Parker describes the sources and methods of the statistics on health care expenditure and insurance coverage that shape the current debate over health care reform. The article covers current practices, differences in definitions and coverage of different reporting agencies, strengths and weaknesses, and plans for future improvement. In Economics at Work, Kate MacArthur describes her work in the Baton Rouge Area Chamber, where she applies a broad range of economic and related skills in targeted marketing efforts to attract new firms to the Baton Rouge area. This issue has two book reviews. The first, by Jesse S. Hixson, reviews Thomas Sowell's The Housing Boom and Bust, in which the author ascribes the ongoing crisis to misguided governmental policies. Hixson finds it to be an excellent study and commends it. The second review, by John C. Goodman, is The End of Prosperity: How Higher Taxes Will Doom the Economy, by Arthur B. Laffer, Stephen Moore, and Peter J. Tanous. Goodman finds this to be an enjoyable and rigorous account of the disincentives posed by higher taxes at a time when this aspect of tax policy has been de-emphasized. He concludes that whether or not one agrees with this book's policy prescriptions, it is well worth reading. Finally, several months ago, business and financial economics lost one of its giants. Peter L. Bernstein is remembered by Thomas W. Synnott for his contributions to understanding risk and investment management.",
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.22,In Memoriam: Peter L. Bernstein,October 2009,Thomas W Synnott,,,Male,Unknown,Unknown,Male,,
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.26,"Underwriting, Mortgage Lending, and House Prices: 1996–2008",October 2009,James A Wilcox,,,Male,Unknown,Unknown,Male,"Over the past decade, house prices and mortgages rose enormously, peaked, and then began their declines. Figure 1 plots two quarterly data series for 1996–2008: real house prices (RHP) and mortgage balances relative to potential nominal GDP (MORTPOT). Both series rose steeply, nearly doubling by 2006, before declining thereafter. (Appendix II describes the data series more precisely and provides their sources. All data series are national aggregates and seasonally adjusted as appropriate.) Real House Price (RHP) and Residential Mortgage Loans Per Potential Gross Domestic Product (MORTPOT) Indexed: 1996:1=100Sources: Freddie Mac, Bureau of Economic Analysis, Federal Reserve, CBO quarterly data, 1996–2008. Similarly, Figure 2 plots data for the four-quarter growth rate of nominal house prices (GNHP) and data for the percent difference between actual and potential real GDP.Footnote 1 Figure 2 shows that house prices not only rose considerably in the late 1990s, but that they accelerated thereafter, rising faster and faster through 2006, after which they decelerated and then, starting in 2007, the real and nominal levels of house prices declined. Figure 2 also shows that incomes (relative to potential GDP) also rose considerably during the late 1990s, but from 2001 onward, hovered just below potential GDP. Given the quite rapid advance of potential real GDP after 2000, actual incomes rose considerably too. But, Figure 2 also intimates that it would be difficult to attribute the strong and rising growth rate of house prices to accelerating income growth. Growth in Nominal House Prices (GNHP) and Output Gap (GAP) Sources: Freddie Mac, Bureau of Economic Analysis, CBO percent, quarterly data, 1996–2008. At various times, various analysts include different aspects of lending under the rubric of “underwriting.” For concreteness here, we take underwriting standards to consist of all noninterest-rate terms and conditions that affect decisions about mortgage applications. Thus, we consider, for example, a lender's choices about minimum Fair Issac Corporation (FICO) scores,Footnote 2 documentation requirements, the maximum loan-to-value (LTV) ratio, and applicants’ debt-to-income (DTI) ratio. This is consonant with the Federal Reserve's survey question, which asks banks about their “credit standards for approving applications from individuals for mortgage loans to purchase homes … . ” (See Appendix I). There are many ways that lenders can ease or tighten underwriting. Lenders might lower the minimum FICO score or down payment that they would consider. In addition to altering quantitative standards, lenders might also alter the nature of a standard. For example, during the housing boom of the mid-2000s, sellers (often builders) came to provide “gifts” of down payments to buyers to help them qualify for Federal Housing Administration (FHA) mortgages. From a very small share around 2000, by 2005–06, the shares of FHA loans that included down payment gifts from nonprofits (which in effect were seller-funded) rose to nearly one-half of FHA mortgage originations.Footnote 3 Thus, there are myriad ways that lenders can ease or tighten underwriting. Consider some of the better-known data series that we might use to better understand the time series of aggregate (residential mortgage) underwriting standards. Figures 3 and 4 plot average values of some variables for which lenders often have quantitative standards, say maximum LTV, based on data for LTV at the time that mortgages were originated. Figure 3a plots the average LTV based on data from the Federal Housing Finance Agency. The average LTV might have suggested that underwriting had been tightening, as evidenced by LTVs falling (and therefore down payments rising), from the mid-1990s through the mid-2000s. Analogously, the higher LTVs in 2006–08 might have been a signal of more lax underwriting then. (a) Loan to Value (LTV) Ratio for All Conventional Single-Family NonFarm Mortgage Loans; (b) Percent of Conventional Single-Family NonFarm Mortgage Loans with Loan-to-Value (LTV) Ratio Greater or Equal to 90 Percent Source: Federal Housing Finance Agency, quarterly data, 1996–2008. (a) Average Combined Loan to Value at Origination; (b) Loan Documentation at Origination Source: Sherlund [2008], monthly data, 2000–2007. Figure 3b shows the share of all mortgage originations that had LTVs greater than 90 percent (or equivalently, had down payments of 10 percent or less). The data in Figure 3b could be seen as support for the pattern of underwriting tightening followed by laxity. The series shows a rather steady descent from the mid-1990s until 2006, when it had fallen to about half its average value recorded over the full decade of the 1990s (not shown). The share then leapt, rising during the financial crisis to about twice the low levels recorded in the mid-2000s. A priori, one might have thought that the share would have tracked overall underwriting tightenings: if underwriting tightened, minimum down payments likely would rise, thereby reducing the share of borrowers who made down payments of 10 percent or less. But, by virtually all accounts, the opposite was true: underwriting eased during the mid-2000s and then tightened sharply when the financial crisis struck beginning in 2007. How then did average LTVs move opposite to underwriting laxity? The answer, as we now understand it, is that, at least in part, second mortgages originated at closing (piggybacks) and other mechanisms allowed more borrowers to have first mortgages that had 80 percent or lower LTVs, thereby reducing the series in both Figures 3a and b. To further upset the conventional correlation between underwriting and its indicators, Sherlund [2008] shows that, at least in the securitized portion of the subprime mortgage market, average FICO scores rose quite steadily over the 1997–2007 period. However, some data series do conform more closely to underwriting having eased in the 2000s before tightening significantly during the financial crisis. For example, again based on securitized subprime mortgages, Sherlund [2008] shows that average ratios of DTI and of LTV rose, and the share of adjustable-rate mortgages (ARMs) rose. Figure 4, taken from Sherlund [2008], show that the combined first-plus-second mortgage-LTV (CLTV) rose and the share of originations that had full documentation declined throughout the 2000s, until the financial crisis began. And, the share of “low quality” mortgages, defined as those with low documentation and LTVs of at least 95 percent, rose markedly after 2002, before plummeting in 2007. Thus, the data in Figure 4 suggests evermore lax underwriting until 2007. Therefore, although some commonly used data series seemed to signal tightening of underwriting standards, other series were simultaneously signaling laxity during the mid-2000s and tightening thereafter. Thus, there is plenty of reason to suspect that the usual proxy variables for underwriting in the aggregate are unlikely to suffice for analyzing recent events in housing markets. Federal banking regulators regularly conduct surveys to ask more directly about banks’ underwriting standards. The Federal Reserve asks banks themselves to report whether they have tightened underwriting; the OCC asks its own employees about whether the banks that they have directly examined have tightened underwriting standards.Footnote 4 We use data series on the net percentage of banks each quarter that were reported to the Federal Reserve and by the OCC as having tightened underwriting.Footnote 5 Since we are more interested in the aggregate level of underwriting tightness than the number of banks that tightened each period, Figure 5 displays the cumulative sum of net tightenings of underwriting since 1996:Q1 (when the series takes a starting value of zero). The two series have been highly correlated, but there were also some notable differences. The cumulated Federal Reserve series, SUMUWFED, in Figure 5 implies no net change in underwriting during the late 1990s or even from 2002 through the end of 2006. By contrast, the cumulated OCC series, SUMUWOCC, implies that underwriting eased considerably before the 2001 recession. Especially notable, given the widespread sense that underwriting had broadly and significantly eased from 2004 onward, SUMUWOCC exhibits a large and steep decline until 2007. Thus, the OCC data paint a quite different picture of banks’ underwriting standards. We cannot, of course, be sure which series more accurately portrays actual underwriting practices—presumably each series has some virtues. But, we can see that different series, even those that presumably are meant to measure quite similar phenomena in similar samples, can carry quite different information. Cumulative Net Percentage Tightening of Residential Mortgage Underwriting Standards Percentage of banks tightening minus percentage loosening, indexed 1996:1=0.Source: Surveys of banks by the Federal Reserve (SUMUWFED) and the Office of the Comptroller of the Currency (SUMUWOCC), quarterly data, 1996–2008. OCC source data pertain to Q1; remaining quarters are linearly interpolated. Other series are also likely to add information. They may cover different lenders or measure different aspects of underwriting. For example, the Federal Reserve and the OCC conducted surveys of commercial banks. Over this sample period, banks’ share of mortgage originations and holdings fell significantly. That decline may be partly attributable to other lenders’ having lower and lowered underwriting standards relative to those of the much more heavily regulated and examined commercial banks. Other variables might well allow for such developments. Thus, we seek a manageable list of other variables that might affect aggregate underwriting and/or might reflect changes in underwriting. One less direct, but potentially useful, indicator of underwriting might be based on (nonmortgage) interest rate spreads. The spread that we used as a proxy variable for spreads on risky bonds was the difference (in percentage points) between yields on high-yield corporate (“junk”) bonds and U.S. Treasury yields. This spread is one indicator of the amount of, and return per unit of, credit risk. Figure 6 plots SPREAD, the yield spread on high-yield corporate bonds. SPREAD declined until the Asian financial crisis of the late 1990s and then generally rose around the 2001 recession and thereafter. The spread then fell precipitously into 2005 and was at about record lows rose until the financial crisis that began in 2007. Thus, credit markets seemed to judge that there were were relatively low default probabilities and/or low rewards per unit of credit risk. The Yield Spread between Junk Bonds and U.S. Treasurys (spread) and Cumulative Net Tightening (SUMUWOCC) Sources: Economy.com, OCC quarterly data, 1996–2008. The results of Demyanyk and Van Hemert [forthcoming] can be used to estimate changes in underwriting for some of the years in the mid-2000s. Their estimates are based on a very large sample of mortgages that were originated in the 2000s by banks or by nonbanks. Their estimates control for the effects of a lengthy list of factors on delinquency rates: borrowers’ FICO score, down payments, house price growth, and so on. Given the controls, we interpret the remaining changes in default rates as reflecting the tightness of prior underwriting standards: the higher the ensuing delinquency rates (importantly, given their long list of controls), the more lax were underwriting standards.Footnote 6 The mnemonic for this variable is XSDEL. Finally, we used an indicator based on the relation between the prevalence of ARMs and the interest rates on adjustable- and on fixed-rate mortgages (ARMs and FRMs). Historically, and not surprisingly, the ARM share of mortgage originations has reliably risen as FRM rates rose relative to those on ARMs. During this period, it appears that underwriting changes were perhaps concentrated among subprime and similar (for example, Alt-A) borrowers. These borrowers disproportionately took on ARMs, which temporarily sometimes had fixed-rate-based payments and/or permitted negative amortization. Such “pay option ARMs” have become infamous. They also had become more numerous during the mid-2000s. Applications for these and other mortgages, as suggested by Figure 4b, were also subject to easing documentation requirements. Thus, through the mid-2000s, more and more borrowers were being approved for mortgages with essentially easier underwriting standards. To allow for these developments, we constructed a data series, ARMRESID, which was the residual from a regression (over a longer, 1987–2008 sample period) of the market share of ARMs on a constant term, the nominal interest rate on FRMs, and the nominal interest rate on ARMs. The residuals from that regression indicate the otherwise-unexplained ARM share. We interpret the large positive values for ARMRESID over the 2003–06 period as indicative of generally eased underwriting standards. These market developments may well be peculiar to this sample period. Thus, one would not want to presume that this indicator would be valid for other situations. But, for this period, it may well have captured an important part of the underwriting conditions that prevailed. Other series are likely to convey additional relevant information about underwriting. But, we deliberately chose to exclude many of them. For example, numbers and volumes of mortgages, housing starts, residential construction expenditures, and house prices are likely to be useful indicators of mortgage underwriting. But, because our goal is to construct an indicator that we can then use to help account for movements in those and other variables, we chose not to include them in the construction of our indicator of underwriting. We have argued that we have five variables that serve as indicators of various aspects of bank and nonbank underwriting standards. Each of the five variables had some strengths and some weaknesses as indicators of aggregate underwriting standards. (If any one variable had been plausibly regarded as a “sufficient variable,” we would have just used that variable.) Because they each are related to overall underwriting, they tend to be somewhat correlated; the average simple correlation coefficient between them was 0.55; the multicollinearity of this group of five variables was naturally considerably higher than that. Because each variable pertained to underwriting, using the five indicators separately would render interpretation somewhat problematic. For all of these reasons, we applied the method of PC to our five indicator variables to derive a single, composite indicator of underwriting.Footnote 7 The resulting first PC is the single data series that most closely tracks the five variables used in the PC analysis: the Federal Reserve and the OCC underwriting data, the risky bond yield spread, the Demyanyk-Van Hemert “excess” default rates, and the “excess” ARM share variable. In that way, the PC method assimilates some of the information from each of the five series into a single indicator variable. Use of the PC method in economics has often been hindered by the inability to attach persuasive structural interpretations to the results. In the case at hand, however, using input variables that are reasonably connected to underwriting increases our confidence that the first PC is a satisfactory candidate as an indicator of aggregate underwriting. Our confidence is buttressed by the resulting equation for the first PC of the five chosen indicator variables. To the first PC, we assigned the mnemonic “UWPC”: As we might expect from an indicator of underwriting tightness, UWPC rose both with the Federal Reserve and with the OCC measures of underwriting tightness. UWPC also rose with increases in the bond-yield credit spread, SPREAD. On the other hand, UWPC fell, and thus indicated underwriting easing, as “excess” Demyanyk-Van Hemert-adjusted delinquency rates (XSDEL) rose and as the “excess” share of ARMs (ARMRESID) rose. Thus, UWPC seems consistently to rise and fall with underwriting tightness and laxity. By construction, UWPC is not perfectly correlated with any of the individual series but rather tends to reflect the common part of the movements that is present in each of the series. Nonetheless, the correlation with each of the series was quite high; the average of the five correlations with UWPC was 0.65, ranging from about 0.4 with ARMRESID to about 0.8 with SPREAD. To illustrate the differences in the time paths of some of the variables used to construct UWPC, Figure 6 plots SPREAD and the OCC-based cumulative tightening variable, SUMUWOCC. In general, SPREAD suggested episodes of tightening and loosening considerably before SUMUWOCC did. They both, however, pointed toward underwriting tightening starting with the 2007 financial crisis, an episode that everyone recognized. Figure 7 shows that UWPC hovered near its average value (zero) from 1996 until 2000. UWPC then rose modestly into 2002. UWPC then declined significantly and quite steadily until hitting its lowest value in early 2007. In that respect, UWPC suggests that underwriting eased significantly from 2002 through 2006. As a result, UWPC may contribute significantly to explaining the housing boom of the mid-2000s. Underwriting Standards Estimated by First Principal Component (UWPC) of Spread, ARMRESID, SUMUWFED, SUMUWOCC, and XSDEL Sources: Federal Housing Finance Agency, economy.com, Federal Reserve, OCC, Demyanyk and Van Hemert [Forthcoming] quarterly data, 1996–2008. The onset of the financial crisis in 2007 then saw UWPC rise very sharply, by more than double the prior decline, indicating extreme underwriting tightness. Again, the size and speed of the rebound of UWPC should not be too surprising in light of the extent to which the credit markets shut down in late 2008, which was reflected in SPREAD and in the upward jolts to net increase percentages recorded in the Federal Reserve and OCC surveys. In that regard, too, UWPC appears to have generally tracked the tightening of underwriting standards during the financial crisis.",9
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.24,Does Fiscal Stimulus Cause Too Much Debt?,October 2009,Laurence S Seidman,Kenneth A Lewis,,Female,Male,Unknown,Mix,,
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.27,"The Long-run Demand for Housing, the Housing Glut, and the Implications for the Financial Crisis",October 2009,James E Mcnulty,,,Male,Unknown,Unknown,Male,"The techniques of estimating the demand for housing used in this paper were first developed at the Housing and Home Finance Agency, a predecessor of the U.S. Department of Housing and Urban Development [Rapkin, Winnick, and Blank 1953]. A later publication, FHA Techniques of Housing Market Analysis [1970] was used to train analysts to make projections of demand for individual metropolitan areas. These local market reports are still prepared by the agency. To estimate demand for new housing, these studies suggest that three components must be estimated separately: (a) household formations, (b) net removals from the housing stock, and (c) the change in the number of units needed to maintain a normal vacancy rate as the housing stock increases. The first component is the largest and most important. As the Census Bureau defines a household as a group of people occupying separate living quarters, one additional household formation equals one unit of housing demand. Harvard University's Joint Center for Housing Studies [Belsky, Drew, and McCue 2007] estimates the long-run demand for housing in the United States for the period 2005–14 using this approach. This projection is based on their own estimates of household formations and detailed analysis of net removals and increased vacancies. They prepare three estimates of “sustainable demand”; their most conservative is 1.95 million units per year, including mobile homes. My estimates for the recent past are lower than their estimate of future demand, even accounting for mobile home sales. Including mobile home shipments is conceptually appealing and theoretically correct, as mobile home sales are part of the supply of new housing. Nonetheless, I focus on developing a measure of demand that can be compared directly to housing starts, because it is conventional for business economists, as well as forecasters working for government agencies, to focus on that variable. Put simply, I construct my analysis in the language that forecasters are accustomed to using. The Census Bureau does report shipments of manufactured housing separately, but these do not receive much attention in the media or elsewhere. Also, the factors affecting mobile home shipments are somewhat different than for conventional housing starts. Many macroeconomic forecasters report housing starts in their analyses without considering mobile homes sales or shipments. In addition, while housing starts vary from year to year, mobile home shipments fluctuate to an even greater degree, and the two often do not move in the same direction. For example, Figure 1 shows that in 2004 and 2005, when housing starts reached record levels, mobile home shipments had fallen to about 35 percent of their peak 1998 levels. The excess monetary stimulus discussed by Taylor [2008] apparently did not have much impact on mobile home shipments. Housing Starts and Mobile Home Shipments As noted, my analysis is ex post; it asks, what was the underlying demand for housing over the recent past? I focus specifically on the 7-year period 1997–2003, and its aftermath, a period that includes the monetary stimulus analyzed by Taylor [2008; 2009]. The approach of the Joint Center Study is ex ante; they project demand for new housing for the period 2005 through 2014. In addition, my study differs from theirs in that I consider these issues in the context of monetary policy. In addition, I evaluate the ratio of residential construction spending to GDP. I show that this is an important variable that corroborates my finding that substantial overbuilding was evident in the data, if one had been trained to look for it. Because the Joint Center estimate of housing demand for the period when the two estimates overlap is higher than mine, their estimate of the housing glut is lower. Their study, published in November 2007, estimates the surplus to be 500,000–750,000 units. My estimates, based on more recent data, are more than double these levels. Moscovitch [1990] and McNulty [1995] show how overbuilding can create serious instability in total economic activity. (Note the quotation at the beginning of this paper.) Moscovitch shows how the deterioration in New England's economy in the early 1990s was masked for a time by a wave of speculative overbuilding. The strength in the construction sector kept employment growth high, even as the region's economic base of manufacturing firms was moving to other areas. When construction slowed, the decline in the regional economy was much more severe than it would have been otherwise. McNulty [1995] incorporates Moscovitch's insights into a formal econometric test of economic base theory. Economic base theory considers a region's “export” sector, those industries that produce goods for sale to other regions, to be the driving force behind regional economic growth. The analysis in McNulty [1995] is consistent with the notion that construction is not a basic sector in the long run. McNulty [1995, pp. 38, 49] notes, “If the lags in the transmission process underlying the economic base multiplier had been understood in the banking and real estate communities, and incorporated into loan-evaluation policies and feasibility studies, the real estate debacle that eventually resulted in one of the costliest bank failures in U.S. history, the Bank of New England, could have been mitigated significantly. Similar comments apply to many other bank failures in New England and elsewhere, which also resulted from excessive real estate lending in a deteriorating economic environment.” “The causes of overbuilding are rooted in two lags—the lag in the transmission mechanism between basic and non-basic economic activity and the lag between approval and construction of real estate projects…. While overbuilding is generally attributed to naive over optimism by ‘greedy’ real estate developers and lenders, and thus subject to correction, overbuilding may actually be inherent in real estate markets.” Monetary policy, of course, has a direct effect on the housing market. It thus follows that the central bank can aggravate economic instability by over-stimulating that market. Thus, if overbuilding is endemic, it follows that a central bank needs to have an estimate of the long-run demand for housing, and to adjust monetary policy accordingly when construction activity is believed to be excessive. In addition, overbuilding causes bank failures, clearly a concern of any central bank. In the long run, builders cannot produce more housing than the market demands. This is why excess housing construction aggravates economic instability. Between January 2007 and January 2009, for example, over one third of the decline in U.S. nonfarm employment was in construction. Eventually, a decline in construction employment will exacerbate the overall downward spiral of employment and income. This happened during the current financial crisis, just as it did in New England in the early 1990s.",6
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.23,The Financial Crisis and the Implementation of Monetary Policy,October 2009,Jerry H Tempelman,,,Male,Unknown,Unknown,Male,"In September 2008, however, this playbook began to see some significant changes. For much of the week of September 15, which was the day that investment bank Lehman Brothers filed for bankruptcy, pressures in the federal funds market caused the actual daily average federal funds rate to be above the FOMC's then 2 percent target rate. In response, the Desk injected large amounts of reserves, conducting open market operations of which some supplied as much as $50 billion to the system. These amounts were far greater than the usual size of the Desk's operations, and more than twice the size of the previous high since the crisis began in August 2007.Footnote 1 Shortly afterwards, the Federal Reserve began ramping up its targeted credit support programs, in the process more than doubling the size of its balance sheet from approximately $1 trillion to $2 trillion by early November. To keep the federal funds rate at or near the target level, ordinarily the Desk would drain some of this excess liquidity from the system. Indeed, from September 23 to mid-December, the Desk frequently executed open market operations that had the effect of draining reserves. But the size of these operations was outdone by the size of Desk operations that simultaneously injected reserves into the system. An important difference between the two kinds of operations was the type of security that collateralized them. During the fourth quarter, open market operations that injected reserves into the system were collateralized exclusively by either mortgage-backed securities or, in one instance, federal agency securities, but never U.S. Treasury securities. On the other hand, open market operations that drained reserves from the system were collateralized exclusively by U.S. Treasury securities, which are generally deemed to be of superior credit quality to federal agency securities and mortgage-backed securities. The consistency of this pattern in the Desk's open market operations suggests that these operations were intended at least as much to change the composition of the Federal Reserve's balance sheet (fewer Treasury securities, more mortgage-backed securities) as its size. Due to these open market operations that injected reserves while federal funds were trading below the FOMC's target rate, an expectation set in among market participants that the Federal Reserve would not try to boost the federal funds rate when it drifted below the target rate. Thus, the large injections of liquidity from the various credit support programs, combined with the expectation on the part of market participants that the Federal Reserve would not actively try to raise the actual federal funds rate toward the stated target rate when trading below it, meant that the daily average federal funds rate was below the FOMC's target rate nearly every day from mid-September through mid-December, when the target was officially lowered to a range of 0–0.25 percent (Figure 1). Federal Funds Target Rate vs. Actual Daily Average Rate (Percent)Source: Federal Reserve Bank of New York.",1
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.33,The Effects of Oil Price Shocks on Output,October 2009,Neal Ghosh,Chris Varvares,James Morley,Male,,Male,Mix,,
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.32,Key Statistics for Health Care Reform,October 2009,Robert P Parker,,,Male,Unknown,Unknown,Male,"The NHEAs, prepared by CMS and its predecessor agencies, provide what are often deemed to be the official estimates of total health care expenditures in the United States. Annual estimates of these accounts have been prepared since the mid-1960s. The most recent estimates are for 2007 and were released in January 2009. Projections, now covering 11 years, have been prepared since the mid-1990s, the most recent of which are for 2008–2018 and also were released in January 2009. The NHEAs are designed to provide a framework for the analysis of economic activity within the health sector and reflect concepts, definitions, and classifications that CMS has, in conjunction with NHEA users, determined are critical to the financing and provision of U.S. health care. This framework allows for measuring changes in the structure of this sector—particularly changes in the amount and cost of health expenditures—and in sources that finance these expenditures. Additionally, the NHEA can serve as a database for researchers to analyze economic factors that cause changes in the health sector. The key source data used to prepare the estimates of the NHEAs are from a variety of statistical sources, both government and private, and administrative records. Most important among the government surveys are quinquennial economic and government censuses and related annual surveys (Census Bureau); the Medical Expenditures Panel Survey (AHRQ); National income and product and input-output accounts (Bureau of Economic Analysis (BEA)); Consumer Expenditures Survey, Consumer Price Index, Employment Compensation Survey, Current Employment Statistics (Bureau of Labor Statistics); and surveys of research and development (National Science Foundation). Private sources used for the NHEA include the American Hospital Association Annual Survey, National Association of Blue Cross and Blue Shield Plans, and IMS Health Inc. Administrative record sources include the Medical Provider Analysis and Review database (CMS), State Medicaid agency reports to state and local government (CMS), and the Budget of the United States. The annual estimates of these health accounts consist of three major presentations: Estimates of U.S. health care spending are provided by type of service or supplies purchased; by the sources of funding for the spending; and for spending (excluding investment) by sponsor. Supplemental estimates cover expenditures by age group, state of residence, and state of provider, but the most recent of these estimates are for 2004. As shown in Table 1, total health care expenditures, which reflect the amount spent in the United States to purchase health care goods and services, are defined to consist of two components:
 health care services and supplies; investment. Health services and supplies consists of personal health care expenditures, which in turn encompasses:
 spending by persons for hospital care; professional services, primarily physician and clinical services; nursing home and home health care and retail sales of medical products (primarily prescription drugs); government administration (of health programs) and net cost of private health insurance; government public health activities (such as disease prevention programs and epidemiological surveillance). Investment consists of research of nonprofit organizations and government entities and of investment in structures and equipment by medical sector businesses, except for retail trade. Table 1 also shows the estimates of national health expenditures for 2007, not only by type of expenditure but also by source of funds and, for health services and supplies, by type of provider. The sources of public funds are specific public programs (such as Medicare and Medicaid), but the sources of private funds are private insurance, out-of-pocket payments, and philanthropic funds that directly pay health care bills.Footnote 1 The most recent NHEA estimates (Table 1) show that spending in 2007 for hospital care and physician and clinical care services accounted for slightly over one-half of all spending, with spending on prescription drugs accounting for another 10 percent. Looking at spending by source of funds, private funds accounted for about 54 percent of all expenditures. As a share of GDP in 2007, national health expenditures were 16.2 percent of GDP. Compared with 2006, expenditures increased by 6.1 percent and the percent of GDP increased from 16.0 percent.Footnote 2 As previously noted, CMS annually prepares 11-year projections of the NHEA spending categories by source of funds and by type of service. CMS uses an econometric model based on relationships between major macroeconomic variables and private health expenditures, as well as between major variables of the health care sector. The macroeconomic and demographic outlook are derived from the 2007 Medicare Board of Trustees Report, and the projections of Medicare and Medicaid spending produced by the CMS Office of the Actuary are exogenous inputs into the model.Footnote 3 According to the most recent projections, CMS expects national health expenditures to have increased 6.1 percent in 2008, the same as in 2007, and average 6.2 percent per year over the projection period (2008–2018). Expenditures as a percent of GDP is projected to reach 16.6 percent in 2008 and 20.3 percent by 2018. Part of the increase in this percent is associated with a greater slowdown (or a decline) in GDP in 2008 and 2009. A frequently asked question about the NHEAs is the relationship with the NIPAs prepared by the BEA. In general the two accounts are statistically consistent—some NIPA estimates are incorporated in the NHEAs. However, the NHEAs provide a more comprehensive picture of the health care sector, covering not only expenditures for medical goods and services, but also the sources of funds that finance these expenditures. The NHEAs and NIPAs also differ for some definitional and statistical reasons. Among the definitional reasons, there are differences in the valuation of services performed by nonprofit organizations, in the treatment of non-operating income, and in the treatment of secondary products. Statistical differences result from the timing of incorporation of newly available source data and some differences in source data. Among the major differences is the treatment of the activities of nonprofit organizations. In the NHEAs, the value is revenue, whereas the NIPAs use current operating expenditures and depreciation. (Both accounts value health care provided by governments as current operating expenditures and depreciation.) In addition, the two accounts differ on the coverage of services. NHEA expenditures consist of the total revenue for hospital services, including contract research, cafeteria sales, merchandise sales, and other services. In the NIPAs, on the other hand, hospital expenditures are almost entirely for the production of medical services and exclude secondary income.Footnote 4 In the future, both CMS and BEA are planning to improve their accounts. In January 2010, CMS is scheduled to release revised NHEA estimates that will reflect a major updating of source data and revised definitions and concepts. Major revisions to the NHEAs generally occur every five years when data from new (2007) quinquennial economic and government censuses are available and to incorporate revised data from the latest five-year updating by BEA of the NIPAs and benchmark input-output accounts. In addition, CMS is reviewing the definitions and concepts underlying the NHEAs. For 2010, it plans to review the treatment of health care and aid in daily living in long-term care facilities, government expenditures that subsidize nonpublic benefits, and imports and exports of health care—that is, expenditures for health care in the United States by residents of other countries and expenditures abroad by U.S. citizens. CMS is also reviewing definitions of health care services provided within industrial plants, health care provided within schools, and public medical transportation.Footnote 5 At BEA, work also has been progressing on improving the core estimates of health care expenditures [Aizcorbe and others 2008]. BEA has been working with CMS to complete a comprehensive reconciliation of the expenditure estimates in the NHEAs and the NIPAs. BEA also is working to develop disease-based estimates of health care expenditures to enable analysts to more accurately evaluate the economic impact of medical treatments. BEA also intends to improve the deflators used to decompose changes in expenditures into changes in price and changes in quantity. BEA will first develop disease-based price indices for use in deflation in a satellite health account, but it will not attempt to account for potential changes in the quality of treatments until there is clear consensus among experts on how to measure such changes. Finally, BEA is studying the results of a recently released workshop held by the Committee on National Statistics of the Academies of Science [2008].",
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.25,Creative Marketing Solutions for Lead Generation in a Regional Development Agency,October 2009,Kate MacArthur,,,Female,Unknown,Unknown,Female,"Chamber of Commerce and Economic Development Organizations (EDO) marketing materials generally conjure up thoughts of brochures, postcards, magazine ads, etc. that tout the benefits of locating a business in a particular area. In 2006, we developed a strategic marketing plan that supported a lead generation approach. Focusing on the operations side of business economics, our department went beyond the typical recruitment and marketing strategies to generate project leads. After a target industry assessment yielded the region's five major industry targets:headquarters/back office, life sciences, construction materials manufacturing, warehouse and distribution, and digital media/film (due to a number of factors, digital media was done as a separate marketing campaign)—I coordinated with local experts in each of the industry sectors to create a comprehensive analysis framed around location-based competitive advantages. Each assessment was developed for our recruitment and expansion team in presentation form both printed and digitally. The assessments yielded a:  regional economic analysis, logistics infrastructure, competitive market analysis, relevant available sites, state & local incentives, and leading quality of life indicators. The competitive market analysis was the key piece of information that varied from book to book. This section covered topics specific to each industry, such as operating costs, employment costs, available labor, tax climate, and key industry trend analysis. Furthermore each section was displayed to comparable data in competing regional economies. The presentation book data was then transformed into target marketing mail pieces, designed to peak the interest of key industry decision makers and site-selection consultants. The data and analysis that supported our regional economy's ability to compete within each industry sector was completed, however the most difficult task became transforming the data into a lead generating marketing piece. Using the data as a driver for the marketing pitch each mail piece had a unique value proposition. For life sciences, the mail-out was a plastic petri dish filled with informative coasters about the life sciences industry in Baton Rouge (pharmaceuticals to food manufacturing). A small shipping crate, similar to a Rubik's cube, folded out to highlight the regional benefits of warehouse and distribution locations, with an available site blueprint map in the packaging or sent by the target companies in those industry sectors. Another Rubik's cube was used to tout the region's attractiveness for construction materials manufacturing. Finally, an extra-large pop-up card was created to grab the attention of companies with back office/ headquarters needs. When opened, a young woman in a cubicle emerged out of the card signifying the ample labor pool while the pictures on the card signified all of the region's quality-of-life amenities that employees enjoy outside of the office. In order to identify the target audience, we solicited a third party analytics firm to evaluate all firms nationwide in each of our target industries and rank them by those most likely to consolidate or expand. This was accomplished by using variables such as revenue growth, CEO changes, mergers, and so on. By applying this criteria and filtering process, we were able to generate a list of the top-300 companies in each industry, based on their proclivity to expand in the near future. There were preliminary calls conducted to verify that the recipient was either a site selector or President/ CEO. The intended reaction was to catch their attention through the piece's innovation and creativity while utilizing the coasters or taking the Rubik's home for their children to play with. Going for the “cool” factor was determined to give the material content more than just a glance. Five days after receipt of the materials, an outside firm completed follow-up calls to each recipient to verify receipt of the piece and gauge interest in follow-up information (the presentation book) or a phone call from the business development team in Baton Rouge. Based on these calls, the firm was categorized as a qualified prospect, suspect, or no opportunity. I monitored the results of the campaign through return on investment calculations in the form of solid leads and new projects generated.",
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.30,The Housing Boom and Bust,October 2009,Jesse S Hixson,,,Male,Unknown,Unknown,Male,,
44,4,Business Economics,29 October 2009,https://link.springer.com/article/10.1057/be.2009.31,The End of Prosperity: How Higher Taxes Will Doom the Economy,October 2009,John C Goodman,,,Male,Unknown,Unknown,Male,,
45,1,Business Economics,11 January 2010,https://link.springer.com/article/10.1057/be.2009.40,From the Editor,January 2010,Robert Crow,,,Male,Unknown,Unknown,Male,,
45,1,Business Economics,11 January 2010,https://link.springer.com/article/10.1057/be.2009.41,Principles for Economic Recovery and Renewal,January 2010,Lawrence H Summers,,,Male,Unknown,Unknown,Male,"First, confidence is essential. Franklin Roosevelt caught a very substantial truth, even if he exaggerated slightly, when he said that the only thing we had to fear was fear itself. Americans may have had other things to fear, but panic and fear figured as significant obstacles to recovery. We have seen repeatedly that confidence can be a self-fulfilling prophecy. Indeed, I would go so far as to assert that there is a Heisenberg element in economics, where the measurement and observation of the performance of an economy has a direct effect on how that economy performs. Businesses, consumers, and investors need to feel both that recovery can be sustained and that the economy is returning to a long-run sustainable path. That is indispensable to maximizing our growth potential. No actions to combat short-run output gaps must be allowed to call into question our national commitment to sound money, noninflationary growth, and sustainable devolution of government debt. Equally, policy measures to spur growth or achieve other objectives should, wherever possible, go with rather than against the grain of the market. Think, for example, of reliance on pricing rather than on heavy-handed regulation to discourage the excessive production of goods that pollute. Wherever possible, we should seek to resolve policy debates as rapidly as possible so as to minimize uncertainty. I first met Ben Bernanke in the late 1970s, when he presented one of the key papers from his thesis at a seminar at Harvard. It was, as you would expect, a rather elaborate and quantitative analysis; but it also made a very fundamental point. If you were considering buying a new boiler, and you knew the price of energy was going to be high, you would buy one kind of boiler. If you knew the price of energy was going to be low, you would buy another kind of boiler. If you did not know what the price of energy was going to be, but you thought you would know a year from now, you would not buy any boiler at all. In exactly that way, it is illustrative that the reduction of uncertainty through the resolution of disputes is central to maintaining confidence. I cannot overstate the importance of confidence. Although broad comfort with the long-run path and reduction in uncertainty are not sufficient to establish a return to rapid growth, I would suggest that they are absolutely necessary.",2
45,1,Business Economics,11 January 2010,https://link.springer.com/article/10.1057/be.2009.28,Using Aggregate Time Series Variables to Forecast Notices of Default,January 2010,Dan Hamilton,Rani Isaac,Kirk Lesh,Male,Female,Male,Mix,,
45,1,Business Economics,11 January 2010,https://link.springer.com/article/10.1057/be.2009.29,Evaluating and Comparing Leading and Coincident Economic Indicators,January 2010,Gad Levanon,,,Male,Unknown,Unknown,Male,"Recessions are the most negative periods of economic activity. An optimal leading indicator provides a recession signal only before or during a recession and should not signal a recession during a period when there is not a recession, or in other words, provide a false signal. In addition, they should not fail to provide a signal when a recession does happen—a missed signal. Similarly, a coincident indicator should provide a recession signal only during a recession. In the Markov switching method, time-series data are divided into those that are low regime and high regime. The method provides the probability of being in the low regime. For certain indicators, the low regime aligns closely with periods of recessions. Therefore, this method has been extensively used for estimating recession probabilities and dating recessions. However, for many indicators the regimes do not align with periods of recessions, and therefore the probability of being in a low regime is not equal to the recession probability. For these indicators, the Markov switching method does not extract recession probabilities. In addition to regimes not aligning with recessions for many indicators, regimes are not defined in the same way across indicators. Therefore, in one indicator a low regime could mean periods of low growth and recessions. In another indicator, a low regime could mean a severe recession. For example, in Figure 1 we see the regime probabilities for two variables: investment in equipment and software and new orders for consumer goods and materials. This figure suggests that the two variables differ significantly in the periods when they signal recession. But if low regimes mean different severity in the weakness of economic activity, then the comparison between the recession signals of these two variables is not valid. The bottom line is that—in its current form—the Markov switching method cannot extract valid recession signals from many indicators and cannot be used for comparing recession signals across indicators. However, the Markov switching method in its current form is useful for comparing recession signals for an individual indicator across time. Examples of Regime ProbabilitiesSource: Bureau of Economic Analysis and Census. Shaded areas indicate periods of recession. The lines plot the Markov switching regime probabilities. I offer the following solution: rather than using the regime probabilities from the Markov switching method, I suggest first converting the regime probabilities into percentiles. Then, a threshold for defining a recession signal will be determined. The X percent highest regime probabilities will be considered recession signals. I assume that in this way, recession signals across different indicators are comparable. In Figure 2, I show the percentiles of the regime probabilities for the same two indicators as in Figure 1. I see that the top percentiles for each indicator usually occur around recessions for both indicators and make the comparison more reasonable. Examples of Percentiles of Regime ProbabilitiesSource: Bureau of Economic Analysis and Census. Shaded areas indicate periods of recession. The lines plot the percentiles of the Markov switching regime probabilities. The vertical axis is in logs. An additional step is needed to decide what the threshold should be for defining a recession signal. In this article, I set the threshold so that the percent of quarters that are defined as recession signals is equal to the percent of recession periods out of the entire sample, as defined by the NBER. If 15 percent of the months in the sample are determined by the NBER to be recession months, then 15 percent of the quarters with the highest regime probabilities will be recession signals. Since the seminal work of Hamilton [1989], a large body of literature has applied regime switching to various empirical settings. The basic idea has been that the parameters of an econometric model are not constant over time. Allowing them to switch between several regimes would improve a model's fit and forecasting ability. A by-product of this method has been regime-switching probabilities, or filtered probabilities. The filtered probabilities are the probabilities that a given indicator is in a low-mean regime. I will now briefly describe the Markov regime-switching model that I propose to use in this article. Suppose the indicator y has two regimes, “high growth”(S=1) and “low growth” (S=2), which follow a stationary Markov chain. The regimes are unobservable. The dynamics of the indicator y when S=1 are characterized by: And when S=2 by: where x is an additional indicator, and where σ is the standard deviation of the error term that could also switch across regimes. In terms of parameters, the transition matrix that governs the evolution of regimes is 
illustration
 Here, p is the probability that the economy will be in expansion tomorrow, given that today the economy is in an expansion; 1−p is the probability that the economy will be in recession tomorrow, given that today the economy is in an expansion. By the same token, q is the probability that the economy will be in recession tomorrow, given that today the economy is in a recession; 1−q is the probability that the economy will be in expansion tomorrow, given that today the economy is in a recession. The conditional density of y in each of the two regimes is given by: The likelihood value in time t is given by: where Prr(1, t) is the probability of being in Regime 1 at quarter t based on the data up to and including quarter t−1, and Prr(2, t) is the corresponding probability of being in Regime 2. In this method, I estimate simultaneously the parameters for each regime and the recession probability in every period. Moving toward a recession, the low-mean regime's equation fit of the incoming data improves, but the high-mean regime's equation becomes less accurate. In the setting of the log likelihood function, this means that the density from the recession equation becomes larger and the estimated recession probability becomes larger. The difficulty in the conventional maximum likelihood estimation of this kind of model is that all the possible values of the regimes need to be integrated out. The Expectation-Maximization (EM) algorithm greatly simplifies the computational burden of the estimation, and in this article a specific algorithm was developed for each specification. Assuming that θ is a vector of the model's unknown parameters, the EM algorithm is an iterative procedure that consists of the following “expectation” and “maximization” steps at the k-th step iteration:
 Given the parameter estimates (θk−1) obtained from the (k−1)-th iteration, expectations of the regimes are formed. Conditional on the expectation of the regimes, the likelihood function is maximized with respect to the parameters of the model, resulting in θk, which are the assumed parameters for the next iteration. In my implementation, I will iterate the above two steps, until θk converges, that is, when θk and θk−1 are close enough. The filtered probability of a specific date is conditional on the data up to that date, but is computed using parameters that were estimated using the entire sample. As in any type of estimation, the results are sensitive to the specification chosen and to the sample period. There are several specifications that could be used for estimating filtered probabilities: Auto Regressive (AR)(0), AR from higher order, and additional regressors. In addition, I can allow for the variance of the error term to vary across states or not. There is no unique specification that is used across the literature for identifying turning points using Markov switching methods. Chauvet and Hamilton [2005] and Diebold and Rudebusch [1996] use an AR(0) specification with constant variance across regimes. Chauvet and Piger [2002] and Hamilton [2005a] use an AR(1) specification when only the constant is allowed to switch across regimes. Hamilton [2005b] uses AR(2) in which only the constant is allowed to switch across regimes. Very few of the articles on this topic allow for the variance to switch across regimes. One such article is Kontolemis [2001]. In deciding which specification to use it is important to keep in mind that at this stage the goal is to estimate useful filtered probabilities, as opposed to forecasting the dependent variable. For that purpose it is less important to get the best possible fit, or highest value of the likelihood function. It is more important that the features that separate between the two states are indeed what separate between recessions and expansions. More than anything else, this is the mean growth rate. It is not surprising, then, that for most indicators I find that the specification with AR(0) with constant standard deviation produces the best results. I compared between an AR(0) and AR(1), and in most cases the probabilities from the AR(0) specification were clearly closer in timing to the business cycle chronology. In this specification the only difference between the two regimes is the mean growth rate. This is also the specification with the smallest number of parameters, and that reduces the problem of achieving meaningful convergence. The results in the Markov switching method could be very sensitive to the initial guesses of the parameters in the likelihood function, especially if the initial guesses are very far from the “true values.” This problem is less severe in the predominant specification used in this section. In this specification, only five parameters are estimated: two transition probabilities (P and Q), two means in the two regimes, and the standard deviation of the error term. My choices of the initial guesses were functions of the mean and variance of the dependent variable, as opposed to numbers. That way when the magnitude of the dependent variable changes, the initial guesses change as well. The initial guesses were: μ1=0.1 × (mean (y)) μ2=1.5 × (mean (y)) σ=Var(y) An important part of this exercise is deciding whether the indicator to use should be the level of the indicator or the changes in the indicator. When using nonstationary indicators like GDP or employment, it is clear that in Markov switching estimation one should use changes. But in the case of many indicators, the levels are stationary. In these cases, both levels and changes are used.",13
45,1,Business Economics,11 January 2010,https://link.springer.com/article/10.1057/be.2009.39,Recent Controversies over CPI Methodology,January 2010,John S Greenlees,Robert McClelland,,Male,Male,Unknown,Male,"In January 1999, the BLS adopted a geometric mean formula in the calculation of most CPI basic indices. The purpose was to reflect the demonstrated ability of consumers to shift away from products whose prices had increased relative to other products in the same basic CPI component—for example, away from apples whose prices had increased more than, or decreased less than, other apples in Chicago.Footnote 2 The geometric mean is widely used by statistical agencies around the world. It is one of the two formulas that are recommended for use in CPIs by the International Monetary Fund Statistics Department [2003, p. 20] and that are approved by the Statistical Office of the European Communities [Eurostat 2001, p. 59] for use in the Harmonized Indexes of Consumer Prices (HICP).Footnote 3 As of 2008, the geometric mean was used by 20 of 30 countries as a primary formula for computing the elementary indices in their HICPs.Footnote 4 Most economists are familiar both with the concept of consumer substitution and with the proposition that the Laspeyres-type arithmetic mean price index formula that was used throughout the CPI prior to 1999 tends to overstate changes in the cost of living. Most are also familiar with the definition of a geometric mean. As a price index formula, the geometric mean allows for a specific degree of consumer response to relative price change by giving greater weight to price drops, and less weight to price increases, than does an arithmetic mean. Its model of consumer behavior is the Cobb-Douglas model, which is characterized by unitary elasticities of substitution (the proportional change in purchased quantities equals the proportional change in price) and constant shares of total expenditure devoted to each good.Footnote 5 The Laspeyres formula, by contrast, models Leontief-type behavior in which there is no substitution. Notice that a unitary elasticity of substitution is a conservative assumption in many circumstances. For example, one could easily imagine that if orange juice went on sale for half price at one store, the quantity purchased could much more than double. Extremely high consumer responses to sale prices have been found in studies using supermarket scanner data [Feenstra and Shapiro 2003]. Those who object to the 1999 geometric mean decision often erroneously assert that reflecting substitution behavior in the CPI amounts to tracking a declining standard of living. Perhaps misled by the word substitution, many seem to believe that when the price of one good rises, the CPI simply substitutes a less desirable good, such as hamburger for steak, so as to show no price change in the index. Put another way, the critics assert that the BLS builds into the CPI the idea that when a good becomes more expensive consumers will turn to a less desirable substitute and simply accept a lower standard of living. That is not the case; the CPI aims in all cases to measure changes in the price of a constant standard of living. Embedded in the critics’ arguments have been several explicit or apparent misunderstandings of the use of the geometric mean. First, to take the most common example, neither the headline CPI-U nor the CPI-W, the index most widely used in wage and benefit escalation, allows for substitution between steak and hamburger, which are in different CPI item categories.Footnote 6 Instead, the BLS formula implicitly assumes substitution only among the goods and services within an item-area component of the index (such as substitution among grades of hamburger in Boston or varieties of apples in Chicago). Second, as noted above, the CPI's assumption about substitution is that consumers shift their purchases toward items whose prices are rising less (or falling more), not necessarily toward less desirable goods. For example, within the Chicago beef steaks category, the CPI implicitly assumes that Chicago consumers on average would shift up from flank steak toward filet mignon if flank steak prices rose there by a greater amount (or fell by less) than filet mignon prices. On the other hand, if a rising cost of beef caused filet mignon and flank steak prices both to increase by 10 percent, the geometric mean formula would not assume any substitution toward or away from flank steak. The quantitative impact of the CPI's use of the geometric mean formula also has been grossly overestimated by some. On this, we have rigorous empirical evidence because the BLS has continued to calculate Laspeyres indices for all CPI basic indices on an experimental basis for comparison to the official index. These experimental indices show that the geometric mean led to an overall decrease in CPI growth of only about 0.28 percentage point per year over the period from December 1999 to December 2004 compared with the Laspeyres index [Johnson, Reed, and Stewart 2006, p. 13]. This is close to the original BLS prediction that the annual impact would be approximately 0.20 percentage point [Dalton, Greenlees, and Stewart 1998].",1
45,1,Business Economics,11 January 2010,https://link.springer.com/article/10.1057/be.2009.38,Who Benefits from Job Creation at County Level? An Analysis of Leakage and Spillover of New Employment Opportunities in Virginia,January 2010,Xiaobing Shuai,,,Unknown,Unknown,Unknown,Unknown,,
45,1,Business Economics,11 January 2010,https://link.springer.com/article/10.1057/be.2009.42,The Causes and Consequences of Sectoral Reallocation: Evidence from the Early 21st Century,January 2010,Andrew Figura,William Wascher,,Male,Male,Unknown,Male,"To identify the industries from which labor was reallocated during the first half of this decade, we examine how employment patterns in the recent downturn differed across sectors.Footnote 4 In particular, for each industry we compute the difference between the average percent change in employment between 2000 and 2003 and the average percent change in employment from 1990 to 2000; the 1990s period is assumed to be a rough indicator of trend growth in each industry prior to the 2001 recession.Footnote 5 We then use three criteria and some judgment to define downsizing industries. First, an industry must have experienced a decline in employment from 2000 to 2003. Second, the industry must have experienced a large decline in its average percent change in employment between 1990–2000 and 2000–03, relative to other industries. Thus, downwardly trending industries do not automatically qualify as downsizing in our definition because the employment losses during 2000–03 must be much larger than those experienced previously. Lastly, the employment losses during 2000–03 must be permanent; that is the industry must not have subsequently recovered the employment lost during these years. Table 1 ranks industries according to the second criteria (the third column; worst to best). Of the 29 industries for which the relative decline in employment growth was below the median (warehousing), 16 experienced employment contractions that were not reversed in the following two years. We amended this list of industries to include computer systems design and related services because that industry was near the borderline of qualification on the last criterion (job growth over 2003–05 reversed a relatively small part of the losses incurred over 2000–03), and it was the industry with the largest decline in employment growth between 1990–2000 and 2000–03.Footnote 6 We also included chemical manufacturing because it satisfied the first and third criteria and was close to satisfying the second. In contrast, we excluded the apparel and leather manufacturing industry from our list because its post-2000 declines looked little different from its trend over the late 1990s. The 17 remaining industries, which we characterize as downsizing industries, are highlighted in bold type in the table; time-series plots of employment for each of these industries are also shown in Figure 1. Employment in Downsizing Industries (Thousands) These downsizing industries accounted for 13.5 percent of employment in 2000 and experienced net job losses totaling 2,478,000 between 2000 and 2003—more than the decline in total nonfarm payroll employment over this period. Downsizing industries also accounted for 37 percent of the change in employment growth from 1990–2000 to 2000–03. Nevertheless, three industries—administrative support, retail trade, and construction—that made large contributions to the change in employment growth from 1990–2000 to 2000–03 were not downsizing industries according to our criteria because all experienced reasonably strong employment growth in 2004 and 2005. This suggests that although downsizing was an important part of the 2000–03 deceleration in employment, temporary or cyclical employment losses also played a role. Although downsizing industries are similar in terms of their persistent decline in employment growth between 2000 and 2003, they differ considerably in some other respects. For example, the dispersion across industries in the average rates of employment growth during the 1990s is quite wide. A number of industries classified as downsizing in the early 2000s enjoyed employment gains in the 1990s at a pace well above the average rate for the private sector as a whole; indeed, computer systems design and related services was one of the fastest growing industries in the 1990s, and information and data processing services, broadcasting and telecommunications, and motion pictures and sound recording all posted robust gains as well. In contrast, other industries—including paper products, chemical products, electrical equipment, primary metals, and textile mills—experienced below-average growth rates or outright employment declines over the 1990s. Finally, employment growth in some industries—including motor vehicle and parts production, publishing, and plastics and rubber products—was close to the average growth rate over the 1990s for the private sector as a whole. By economic sector, downsizing industries can be reclassified into four broad groups: high-tech (both services and manufacturing); the information and entertainment sectors; nondurable and durable manufacturing excluding high-tech; and airlines and other miscellaneous industries. Each of these broad sectors was associated anecdotally with what seem to be idiosyncratic explanations for the persistent declines in employment growth seen in the first half of the 2000s. With regard to the high-tech sector, for example, soaring investment in high-tech capital (computers, communications equipment, and software) in the second half of the 1990s provided substantial impetus to computer electronics manufacturing, computer systems design, and information and data processing services. However, the economic environment for these industries changed abruptly with the onset of the 2001 recession, when business investment in high-tech equipment and software dropped sharply and remained depressed through early 2004. As businesses reduced spending on IT capital, they also cut back on the purchases of IT services. In addition, some observers have speculated that demand for IT services shifted from U.S. to foreign suppliers as improvements in international telecommunications reduced the price of imports of high-tech services.Footnote 7 The late 1990s also witnessed an explosion in investment by telecommunications companies. The promise of the internet spurred many companies to invest in fiber optic cable and switching equipment in anticipation of huge increases in the demand for information transmitted electronically. However, the demand for information services did not grow as fast as anticipated, leaving some parts of the industry suffering from overcapacity at the end of the decade. Other developments in the communications industry included a rapid shift to wireless and away from wired communications services, as well as changes in market regulation stemming from the 1996 Telecommunications Act. Developments in information technology also buffeted the broadcasting and sound recording industries, as the promise of new products and more efficient channels of distribution (online music sales, digital recording, and pay-per-click internet advertising) threatened to debase the value of existing products and distribution networks. In contrast, downsizing in the manufacturing sector tended to be characterized as an intensification of the downward trend that originated in the late 1970s. Common explanations include the shift of manufacturing activity abroad and rapid technological progress.Footnote 8 Finally, the September 11, 2001 terrorist attacks drastically reduced demand for air transportation services, at least temporarily. However, in the longer run it is difficult to separate this effect from other factors. In particular, the post-2000 period also saw an acceleration in the transfer of market share from so-called legacy airlines to lower-cost start-ups, with uncertain effects on the level of employment in the industry. In sum, anecdotal evidence suggests that a broad array of factors may have caused employment in downsizing industries to exhibit a greater and more persistent deceleration than in other industries. With these explanations in mind, in the next section we assess the extent to which the declines in employment growth in downsizing industries were driven primarily by supply or demand factors. In particular, we use a very simple framework to generate predictions about how the correlations between movements in employment, output, wages, prices, and productivity should be related to changes in supply or demand conditions in an industry. We then compare these predicted correlations to those observed in the data.",4
45,1,Business Economics,11 January 2010,https://link.springer.com/article/10.1057/be.2009.36,Reflections on 25 Years Following the U.S. Economy,January 2010,Mark Vitner,,,Male,Unknown,Unknown,Male,"One of my favorite rules is “economics is just common sense made difficult.” Too often, economists make things more difficult than they need to be. The most important concept to grasp when analyzing the economy is what motivates individuals and businesses to buy goods and services and what motivates them to produce and provide them. Forecasting the economy then simply devolves into determining if new policies or events would cause individuals to buy more goods and services or less, invest in more plant and equipment, hire more workers, or work more or less. Of all the things John Godfrey taught, the one I always find most useful is that “it is important to distinguish between what you think the Fed will do and what you think they should do.” As economists, we tend to follow the Federal Reserve very closely and while we may have a good understanding of how monetary policy works, our views on what the Federal Reserve should do are irrelevant. It is far more important to have an understanding of what you believe the Federal Reserve, under its current leadership, will actually do. Another key concept deals with the business cycle. “Recessions are caused by the build up of imbalances and some sort of event or policy change that causes investors, consumers, businesses and regulators to become more risk averse.” Once you understand where recessions come from, you can begin to assess the risk of falling into one. The significant events leading up to the most recent recession were the housing boom, which resulted in an enormous oversupply of housing, and the sharp run-up in housing prices. The event that put us on a path to recession was the unprecedented drop in home prices that began back in 2006. Falling home prices led to growing financial problems and bankruptcies at mortgage lenders and government-sponsored enterprises that eventually brought down huge investment banks and financial institutions. “Imbalances can build up far longer than seems logical.”Footnote 3 During a boom, all sorts of justifications for the elevated level of economic activity will seem logical. We saw this at the height of the tech bubble and the height of the housing bubble, which is one reason we ended up with such a tremendous oversupply of fiber optic cable and single-family homes. “Persistent inflation is always a monetary phenomenon.”Footnote 4 Measured price increases can sometimes pop up because of supply disruptions, spikes in key commodity prices, or bad weather. A persistent rise in inflation, however, will only take hold if the money supply has increased dramatically for a sustained period of time. “Rising food and energy prices by themselves are deflationary if they are not accommodated by a looser monetary policy.” If consumers are spending more of their income for necessities, they have less to spend on everything else. “Conditions do not have to be perfect in order for the economy to grow,” and that is a good thing because conditions seldom are perfect. The economy almost always faces a seemingly endless string of challenges: the budget deficit is too big, taxes are too high, regulation is too burdensome, and consumers have too much debt. Yet, while these conditions prevailed for most of the prior 25 years, the economy grew solidly throughout most of this period. “There is a tendency for forecasters to focus more attention on what is wrong with the economy than what is right.” Bad news almost always gets more attention than good news, and this is no different with economics. A danger, however, is that focusing too much attention on the negatives might cause you to miss out on valuable opportunities. “The natural tendency for the U.S. economy is to grow.”Footnote 5 Each year the United States adds close to three million new residents, which means we add the equivalent of France to our population every 20 years. In addition, trend productivity growth is somewhere around 2 percent. When you add in Americans’ strong desire to live better than each preceding generation, there is an enormous natural tendency for the economy to grow. Over the past 25 years, “the greatest forecasting mistake economists have made is to underestimate economic growth.” “A trend will continue until it stops.”Footnote 6 This famous line attributed to Herb Stein [1998] really cuts to the heart of economic analysis. Anything that grows faster than the underlying fundamentals will ultimately stop. If homebuilders build houses at a faster rate than the growth in the number of households that can afford to buy them, then ultimately building activity will stop rising. Likewise, if federal government spending rises faster than the Treasury's ability to raise taxes or borrow money, then spending growth will eventually stop. “You can learn an awful lot about the economy by simply observing.”Footnote 7 Some of the best economic indicators I have seen in recent years have been things that I have observed with my own eyes and then verified with the data. If the airports seem more crowded, take a hard look at airline revenue passenger miles, which growth tends to coincide with real GDP. A pick-up or deceleration in airline revenue passenger miles may tip you off to a shift in the economy's underlying momentum. “Never be overly eager to change your forecast.” Obviously you have to change it when the facts change, but always remember that economic data are revised frequently and often by substantial margins. Some of the worst mistakes I have made have been to give up on a forecast too soon. “Do not be afraid of making mistakes.” You will make them. It is part of the job. I learned when I was a goalkeeper on our high school soccer team not to get so upset about the other team scoring a goal that you constantly second-guess every move you made. You can do the same thing in economics. The critical questions you need to ask are: Is your forecast well thought out? Have you researched and tested you conclusions? What are the holes in your argument and what are the risks associated with them? Are you making conservative or aggressive assumptions? If you have done all your homework, stated your position clearly and identified the risks, then you are likely to be wrong far less than you are right. Most important, inform your clients and prepare them for contingencies. “Rapid growth nearly always sows its own seeds of destruction.” Booms generally lead to busts because they lead to overproduction or overinvestment in the sector that is booming. Along those lines, “booms generally lead to unforeseen problems.” When activity is booming—sloppy credit underwriting, inefficient operations and outright fraud are hard to see. This is the basis behind one of Warren Buffet's [2002] favorite sayings “You only find out who is swimming naked when the tide goes out.” Just think of the Bernie Madoff scandal, which did not become evident until last fall's huge financial market sell-off. “Capital will always flow to the highest available risk-adjusted rate of return.” Every investment and business endeavor involves evaluating the risks in investing in that business and the return on that investment. The greater the risks, the higher the return has to be in order to attract any given amount of capital. Anything that heightens risks in the economy tends to restrain investment and business activity in general. “The economy does not simply grow and contract, it is constantly evolving.” This is a concept made famous by Joseph Schumpeter [1975] that has become known as creative destruction. The basic concept is that there are always new industries and growth sectors evolving in the economy and there are always industries and sectors that are declining. Unfortunately, the declining sectors are typically easier to see than those that are growing. “Soft landings are extremely hard to pull off.” The relatively long business cycles of the past few decades have witnessed several attempts by the Federal Reserve to bring the economy in for a soft landing. The Federal Reserve's objective is to slow the economy just enough to head off inflation pressures without causing the unemployment rate to increase. The Federal Reserve's record is spotty, to say the least. The problem with soft landings is slower growth leaves the economy vulnerable to external shocks. The Federal Reserve nearly engineered a soft landing back in 1990, but then Saddam Hussein invaded Kuwait and oil prices skyrocketed, sending the economy into recession. We were also headed for a soft landing back in 2001, but the 9/11 terrorist attacks put an end to that. “Changes in political leadership matter” because the forces behind these changes have an immense amount of resources invested in their efforts. Whenever possible try to “view the economy through the eyes of a business owner, consumer, and policymaker.” Take in how each would view the current environment and what each would view as risks and opportunities. “Always look for consistencies and inconsistencies” in the economic data and your forecast. Inconsistencies demand considerable attention and may point to mistakes and vulnerabilities in the data and your forecast. “Write your reports and give presentations as if you were explaining economic concepts to your mother.” This will help ensure that you respect your audience and do not talk over their heads. “Listen to those who have opposing views.” At a minimum they will provide a good stress test to your own view, and they might be right. I have found that I learn much more from reading reports and books written by folks that I disagree with than those that I agree with. “Do not outrun your headlights.” Know your limitations. The economy is like a giant puzzle you will never finish piecing together. Do not try to do everything. Accept help when it is offered and concentrate on the things you know and do best.",
45,1,Business Economics,11 January 2010,https://link.springer.com/article/10.1057/be.2009.35,Making the Grade: The Economic Evolution of American School Districts,January 2010,W Steven Barnett,,,Unknown,Unknown,Unknown,Unknown,,
45,1,Business Economics,11 January 2010,https://link.springer.com/article/10.1057/be.2009.37,Mostly Harmless Econometrics: An Empiricist's Companion,January 2010,Jan Kmenta,,,Male,Unknown,Unknown,Male,,14
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.10,From the Editor,April 2010,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,1
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.2,A Tale of Two Crises: One We Missed and One We Can’t Afford To,April 2010,Chris Varvares,,,,Unknown,Unknown,Mix,,
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.11,The Impact of the Housing Market Boom and Bust on Consumption Spending,April 2010,Jeremy A Leonard,,,Male,Unknown,Unknown,Male,"Of the major components of economic activity (consumption, investment, government spending, and imports and exports), consumption is one of the least volatile. Since 1950, inflation-adjusted consumption has grown by an average of 3.5 percent annually, with a standard deviation of about half of that average. At the other extreme, inflation-adjusted fixed investment has grown at a slightly faster annual rate on average—3.9 percent—but the standard deviation around that average is 7.5, almost twice the mean. This should not be surprising, because most households seek to avoid short-term fluctuations in their material standard of living to the extent they can, and thus use savings and borrowing to smooth consumption over time. In addition, certain policies (notably unemployment insurance and progressive income tax rates) act as “automatic stabilizers,” mitigating aggregate income declines during recessions and restraining growth in booms. Over the past 50 years, there has been a secular shift (in relative terms) in consumption toward services and away from manufactured goods (Figure 3). Consumption of services has risen from just over 30 percent of disposable income in 1952 to nearly 60 percent today. This has been mainly at the expense of nondurable goods (of which the largest subcategory by far is food and beverages, which accounts for nearly half of the total). Durable goods have not displayed the secular decline of nondurables, but are much more volatile than the other two broad subcomponents, particularly during cyclical downturns. During the 2008–09 recession, consumption of durables has plummeted to a post-World War II low of 9.2 percent of disposable income. Personal Consumption Expenditures by Broad Type, 1952:Q1–2009:Q3 Source: U.S. Department of Commerce, Bureau of Economic Analysis. A closer look at the major recessions of the past shows the degree to which they disproportionately affect durable goods consumption. Table 1 shows that consumption of services does not typically decline in recessions—its growth merely slows. At the other extreme, durable-goods consumption fell by an average in excess of 10 percent during the three deep post-World War II recessions. In the ensuing recoveries, however, these sharp declines were typically followed by much more rapid growth compared with other consumption classes.Footnote 1",6
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.3,Inflexible Wages and Prices? Evidence in the Current Recession,April 2010,William C Dunkelberg,Jonathan A Scott,Michael J Chow,Male,Male,Male,Male,"There are two ways to adjust labor costs: (1) reduce employment (hours) and (2) reduce compensation for those employed by the firm. Table 1 shows the adjustments made in private-sector employment for each recession since 1973 from peak to trough (based on quarters, not months). In simple terms, the adjustment in private-sector employment in the current recession period (especially on a quarterly basis) has been substantially larger than in any other recession (and the bottom for employment, as of December 2009, has yet to be reached). The average reduction in private employment per quarter during the recession period has been much larger than in any past recession since 1973. Figure 1 shows the reported average change in employment per NFIB firm since 1976:4, when the data were first collected. Owners were asked if they had increased or decreased the total number of people working for them and if so, the number of workers affected. The average change is for all firms, including those that did not change employment. It is clear that the reductions in employment in the fourth quarter of 2008 and in 2009 were the largest in the 32-year history of this data series. The adjustment in labor costs (head count) was rapid and deep. Change in Employment Per NFIB Firm. Figure 2 shows the incidence of compensation cuts (the seasonally adjusted netFootnote 3 percent of firms reporting higher employee compensation), clearly sensitive to the business cycle. When the economy weakens, the incidence of compensation reductions rises. Unfortunately, no compensation data are available for the 1980–82 period. Interestingly, the incidence of compensation reductions is much higher in the milder 2001 recession period than in the 1990–91 period (more flexibility?). Figure 3 shows that the incidence of reported reductions soared in the first quarter of 2009, rising to levels more than double any in the history of the NFIB survey. This suggests that there might be more downward flexibility in nominal compensation today than in the past.Footnote 4 The percent of owners reporting increases in worker compensation also dropped to record low levels. This process may have been made easier by the behavior of inflation in this period, which favorably impacted real wages. Failing to raise compensation in an inflationary environment would represent a reduction in real compensation. However, the CPI inflation rate was negligible in 2008, when the incidence of compensation increases reached a survey low, so the real reductions in compensation were muted by falling prices. Net Percentage of NFIB firms Raising Worker Compensation (seasonally adjusted). Net Percentage of NFIB Firms Changing Compensation (not seasonally adjusted). The NFIB data suggest that labor costs (employment and compensation) have adjusted to the economic “shock” of the subprime crisis much more quickly than in response to past shocks such as the 2000–01 stock market crash. This would generally suggest less need for government intervention to aid the economy, as prices and wages are adjusting more quickly than in past recessions. Of relevance, of course, is the degree to which the adjustment falls on “wages” rather than employment (for example, everyone takes a 10 percent “cut” rather than a 10 percent reduction in employment).",1
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.5,Victorian Financial Crises and their Implications for the Future,April 2010,Kenneth N Kuttner,,,Male,Unknown,Unknown,Male,"A distinguishing feature of these four episodes is that they all occurred after the passage of the Bank Act of 1844. Sometimes referred to as “Peel's Act” for its author, Prime Minister Robert Peel, the Act was a victory for the “currency school” (the antecedents of modern monetarists). The essential element of the Act was to split the Bank of England into two quasi-independent institutions: the Issue Department and the Banking Department. The Issue Department had sole responsibility for the issuance of Bank of England notes, which, except for a limited fiduciary issue, were fully backed by specie. Limiting the notes in circulation thus accomplished the currency school's objective of preventing the over-issue of currency. The Banking Department spin-off was established as a commercial bank like any other, with shareholders and a profit motive. Like conventional banks, it took deposits, made loans, and discounted bills of exchange.Footnote 3 Its only official function was as the manager of the government's accounts. However, it was endowed with an unusually large reserve of notes and specie to enable it to manage the internal and external hard currency “drains,” and thus help stabilize the value of the pound. At the end of 1869, the Bank held £23.8 million in coin and notes, representing 26 percent of total assets, a far higher proportion than any other London bank. Although it was not officially empowered to do so, the Bank of England found itself called upon to use its reserves to stabilize the financial system during panics, effectively functioning as the lender of last resort. It performed this by discounting private securities such as bills of exchange, lending against other assets, and occasionally making loans to specific institutions. The Bank assumed this role uneasily at first, and it was initially indecisive in its response to crises [Kindleberger, 1989, p. 180]. In Lombard Street, Walter Bagehot [1873] recognized the critical importance of providing liquidity during times of crisis and urged the Bank to embrace this role with less hesitation than it had shown in the past. His insight was that liquidity is a public good: an ample inventory of currency and specie served as a cushion against runs and panics, but since it diverted resources from interest-earning assets, profit-maximizing banks would have little incentive to hold sufficient cash. This, in Bagehot's view, made the Bank of England essential to a smoothly functioning financial system. The problem facing the Bank of England was that the Bank Act constrained the Banking Department's ability to serve as the lender of last resort. Lacking the ability to issue its own currency, the Banking Department would not be able to discount privately issued securities once its currency and specie reserves were exhausted. The Bank Act consequently came under heavy criticism from some quarters. A parliamentary report issued after the 1847 crisis stated: “the recent panic was materially aggravated by the operation of that statute … and the restriction imposed on the means of accommodation” [Francis 1888, p. 266]. Writing in 1873, Bagehot took pains not to criticize the Bank Act, but other contemporaries, such as Chubb [1872], criticized the Act for the straitjacket it imposed on the Bank during financial crises. The Bank was nonetheless able to function as a lender of last resort, but only by virtue of acts of Parliament suspending the Bank Act in 1847, 1857 and 1866. By allowing for the temporary issuance of new notes not backed by specie, the suspension allowed the Bank to discount a greater volume of bills than it otherwise would have been able to.",5
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.4,Stress Testing Economic Data,April 2010,Alan B Krueger,,,Male,Unknown,Unknown,Male,"In thinking about the kinds of data that policymakers employ, I find the analogy of a car to be helpful. First, the car has a dashboard whose indicators—speedometer, thermometer, fuel gauge—all contribute to telling the driver about the car's current state and performance. To stretch the analogy to the macroeconomy, the dials and gauges on the dashboard represent statistics like payroll employment, GDP or the CPI—data that are directly relevant for steering the economy in the near term. A dashboard is a common analogy, but not everything that is relevant for a car's performance is summarized on its dashboard. That is why, every 10,000 miles or so, we bring our car into the garage for a check-up—a look under the hood—in order to ensure the car's longer-term performance. In the case of an economy, such a check-up draws on statistics that paint a broader picture of the economy's and society's underlying health. For example, GDP can tell us whether the economy is contracting, but it tells us less about whether the composition of demand is sustainable over the longer term, and it tells us nothing about whether income is equitably distributed. GDP also does not tell us whether resources are being used wisely to maximize the well-being of society. For example, pollution and other negative externalities are not subtracted from GDP, and leisure time is not valued in GDP. Hence, when thinking about the longer-term health of the economy, we also turn to statistics like the poverty rate, or the state of consumers’ finances, or the state of the environment, or how people spend and experience their time. Clearly, then, both types of data are useful for setting policy: For example, the first sort of data will often be used to provide guideposts for stabilization policy, while the second sort might help inform policies to raise the quality of life in the longer-term. In addition, both types of statistics are useful for forward-looking analyses to the extent that they can reveal underlying imbalances in the economy, or help derive forecasting models, or help identify causal relationships. My main theme today is that limitations of statistics of both varieties, the dashboard and 10,000-mile check-up, were revealed in the current crisis. We can learn from the economic meltdown of 2008 how to do a better job producing the statistics that can help steer the economy during a crisis and reduce the odds of other crises from occurring. Today, I will focus more on the high-frequency dashboard-type indicators, but both types of statistics need attention.",
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.7,Three Simple Techniques to Analyze a Complex Economic Phenomenon: The Case of Profits,April 2010,John Silvia,Azhar Iqbal,,Male,,Unknown,Mix,,
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.1,"Indoor Climate Control: The Global Demand for Heating, Ventilating, and Air-Conditioning Equipment",April 2010,Michael A Deneen,Andrew C Gross,Jennifer L Mapes,Male,Male,Female,Mix,,
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.6,Aiming Beyond the Box: Bringing out Economists’ Transferable Skills,April 2010,Susan Krug Friedman,,,Female,Unknown,Unknown,Female,"Job descriptions (including our own) can give one a good idea of the skills currently used in a particular niche. The Occupational Outlook Handbook (OOH), published by the U.S. Bureau of Labor Statistics (BLS), offers more perspectives and profiles a range of occupational categories (for example, “Management,” “Professional,” “Service,” “Sales”). “Economists” are listed under the “Professional” job heading. The section describes the activities typically done by economists, including data collection, analysis, and forecasting, and talks about requirements, such as being focused on details and having strong written and oral communication skills:
 They may conduct research, collect and analyze data, monitor economic trends, or develop forecasts. Economists research a wide variety of issues including energy costs, inflation, interest rates, exchange rates, business cycles, taxes, and employment levels, among others…. Presenting economic and statistical concepts in a clear and meaningful way is particularly important for economists whose research is intended for managers and others who do not have a background in economics. [BLS 2008–09, “Nature of the Work”] In addition, the economists’ profiles on the NABE website (under “Careers”) provide intriguing examples of these skills in action in a range of jobs, from “traditional corporate economist,” to “consulting,” to “law and economics,” among others. Similarly, job explorers can create their own list of examples that illustrate their key skills, whether from their previous work or from other endeavors. The OOH also discusses “Related Occupations,” such as budget analysts, insurance underwriters, statisticians, management analysts, and market researchers. Other possibilities discussed in the OOH include teaching at the university or secondary school level.",
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.8,"Gillian Tett, Fool's Gold: How the Bold Dream of a Small Tribe at J.P. Morgan Was Corrupted by Wall Street Greed and Unleashed a Catastrophe",April 2010,Malcolm C Harris Sr.,,,Male,Unknown,Unknown,Male,,1
45,2,Business Economics,08 April 2010,https://link.springer.com/article/10.1057/be.2010.9,"Charles Gasparino, The Sellout: How Three Decades of Wall Street Greed and Government Mismanagement Destroyed the Global Financial System",April 2010,Cliff Tan,,,Male,Unknown,Unknown,Male,,
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.24,From the Editor,July 2010,Robert Thomas Crow Editor,,,Male,Unknown,Unknown,Male,,
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.12,Is the Global Economy Headed for a Lost Decade? A European Perspective,July 2010,Jürgen Stark,,,Male,Unknown,Unknown,Male,"It is undeniable that in the 15 years leading up to the crisis, the world economy achieved exceptionally high growth, combined with low levels of inflation and financial market volatility. Between the early 1990s and 2007, global economic growth averaged 3.7 percent per annum, but the volume of world trade expanded at an average pace of 7.1 percent. Global growth before the crisis was driven by several factors. First, globalization led to increased trade openness and a larger global labour supply. Key in this process was the integration of emerging Asia and the former COMECON countries into the world economy during the 1990s, which led to a significant expansion in the global workforce. The larger labour supply reduced production costs at the aggregate level and increased the comparative advantage of Asian economies in particular. A number of countries benefited from this environment by pursuing export-led development strategies. In the case of emerging Asia, these strategies were supported by managed exchange rate policies that directly or indirectly targeted the U.S. dollar. As a result, emerging Asian economies ran persistent current account surpluses and accumulated vast foreign exchange reserves. Another feature was the debt-fuelled consumption booms experienced in a number of advanced economies. These booms were underpinned by positive wealth effects stemming from the appreciation of housing and financial assets. In addition, favourable credit conditions allowed consumption to be financed through a substantial rise in household indebtedness. The resulting shortfall in national savings meant that the large trade surpluses of emerging Asia were mirrored by deficits in the United States and certain other advanced economies. Finally, growth was supported by a benign financial environment and a prolonged period of “easy credit”. This phenomenon was related to a general process of deregulation and inappropriate regulation of the financial innovations that had taken place since the early 1980s. The securitisation of assets is one example of such innovations, which allowed the financial sector to offer credit at a lower cost than it had previously. Favourable financing conditions were further amplified by accommodative macroeconomic policies, abundant global liquidity, and subdued financial market volatility. Nonetheless, the financial crisis has forcefully demonstrated that the constellation of the recent past was fragile. With hindsight, it can now be seen that some of the factors driving growth in the past also sowed the seeds for the crisis. So why did the global growth model prove to be unsustainable? One striking feature of the high global growth rates was the reliance on large and unsustainable global imbalances. In principle, current account imbalances can be desirable, if they channel funds across the world to their most productive use. But in the years before the crisis imbalances were a symptom of economic distortions: in some countries asset price bubbles developed and household debt levels rose beyond sustainable levels. Eventually, the rise in the household debt burden resulted in an acceleration of defaults on mortgage and consumer loans, which undermined the stability of the financial system. In other countries—for example, in emerging Asia—which held the value of their currencies at artificially low levels to support their export-oriented growth strategies, the vast accumulation of foreign exchange reserves had potentially high opportunity costs. These managed exchange rate regimes may also have contributed to hampering necessary domestic adjustments and distorting the allocation of resources toward export-oriented industries. As regards the financial system, there was a general underpricing of risk, reflecting the apparently benign macroeconomic environment. This was exacerbated by the development of increasingly complex financial products, which made it difficult for investors to assess the quality of the underlying assets. A deterioration of credit standards—because of ill-designed compensation schemes for loan managers—went hand in hand with growth in the leverage employed by financial institutions, which increasingly relied on short-term funding. Admittedly, the crisis was not only the result of market failures, but also of policy and supervisory failures. The institutional framework failed to keep pace with financial innovation. At the same time, insufficient coordination at the global level allowed financial institutions to engage in regulatory arbitrage.",1
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.17,Labor Markets and Monetary Policy,July 2010,Charles L Evans,,,Male,Unknown,Unknown,Male,"The usual starting point for thinking about this issue is Okun's famous “law” relating gross domestic product (GDP) growth to the change in the unemployment rate. The usual estimates of Okun's law imply that the unemployment rate should be at least a percentage point lower than the 9.7 percent we actually saw last Friday. However, this calculation assumes that the association between economic activity and the unemployment rate does not vary across the business cycle. In fact, many employment indicators tend to deteriorate faster during recessions than they improve during expansions. A simple statistical model that uses the historical relationship between GDP growth and unemployment estimated only during recessions can actually account for the sharp rise in the unemployment rate. Figure 1 compares the actual unemployment numbers (light line) to their predicted values from models estimated using GDP data only from recessions (the heavy line). The two solid lines are just about spot on. But the dashed line—the prediction from a standard model that does not distinguish between behavior in expansions and recessions—is not capable of capturing the current numbers. Similar “recession-only” models can also explain the rise in broader measures of unemployment and “underemployment” like the U.S. Bureau of Labor Statistics’ U-6 rate, as well as the declines in payroll employment [Aaronson, Brave, and Schechter 2009]. Actual vs. predicted unemployment rate Based on these exercises, I think that it is reasonable to conclude that the unemployment rate and employment growth have evolved about as we would expect, given the severity of this recession.",2
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.14,Preparing for a Smooth (Eventual) Exit,July 2010,Brian P Sack,,,Male,Unknown,Unknown,Male,"When discussing the U.S. Federal Reserve's exit strategy, it is important to separate liquidity facilities from the stance of monetary policy. Although the exit from the accommodative monetary policy stance has yet to begin, the exit from liquidity facilities is nearly complete. Let me begin with some comments on recent developments regarding the liquidity facilities, and then I will move on to monetary policy. As is well known, the U.S. Federal Reserve launched a number of liquidity facilities to provide short-term funding to the financial markets during the crisis, in order to meet the extraordinary demand for liquidity at that time. Here I am referring to those facilities that provided funding at maturities of up to three months to particular sets of firms, such as the primary dealers, money market mutual funds, commercial paper issuers, and depository institutions.Footnote 2 Just today, we conducted the last operation associated with those facilities, meaning that all of the short-term liquidity facilities that were introduced during the crisis have now effectively been retired. The only special liquidity program that remains active is the Term Asset-Backed Securities Loan Facility, which I consider to differ from the short-term liquidity programs because it provides funding for up to five years. With the wind-down of these short-term liquidity facilities, it is a good time to look back and assess their performance. The bottom line here is simple: These programs were an unquestionable success. We have witnessed a remarkable improvement in the functioning of short-term credit markets and an impressive recovery in the stability of large financial firms. Although a whole range of government actions contributed to this recovery, giving financial institutions greater confidence about their access to funding, and that of their counterparties, was most likely a crucial step toward achieving stability. Moreover, the exit from these facilities has been quite smooth. At their peak, these facilities provided more than $1.5 trillion of credit to the economy. Today, the remaining balance across them is around $20 billion. It is impressive that the U.S. Federal Reserve was able to remove itself from such a large amount of credit extension without creating any significant problems for financial markets or institutions. That success largely reflects the effective design of those programs, as most were structured to provide credit under terms that would be less and less appealing as markets renormalized. This design worked incredibly well, as activity in most of the facilities gradually declined to near zero, allowing the U.S. Federal Reserve to simply turn them off with no market disruption. The success of these facilities should be judged by the outcomes they produced for financial market functioning, and not by the financial returns they generated on the U.S. Federal Reserve's books. However, there are several reasons why the U.S. Federal Reserve might be expected to profit from this type of lending under most circumstances. First, the U.S. Federal Reserve is providing funds in response to an extreme move in the price of liquidity—that is, it is in effect buying a cheap asset. Second, the programs themselves, if successful at returning market functioning, would help the performance of the U.S. Federal Reserve's loans to be sound. And third, the lending under these facilities has to be adequately secured.",
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.13,"E-coli, Repo Madness, and the Financial Crisis",July 2010,Gary Gorton,,,Male,Unknown,Unknown,Male,"This question, though the most basic and fundamental of all, seems very difficult for most people to answer. They can point to the effects of the crisis, namely the failures of some large firms and the rescues of others. People can point to the amounts of money invested by the government in keeping some firms running. But they can’t explain what actually happened, what caused these firms to get into trouble. Where and how were losses actually realized? What actually happened? The remainder of this short note will address these questions. I start with an overview. There was a banking panic, starting on August 9, 2007. In a banking panic, depositors rush en masse to their banks and demand their money back. The banking system cannot possibly honor these demands because they have lent the money out or they are holding long-term bonds. To honor the demands of depositors, banks must sell assets. But only the U.S. Federal Reserve is large enough to be a significant buyer of assets.Footnote 1 Banking means creating short-term trading or transaction securities backed by longer term assets. Demand deposits are the leading example of such securities. The fundamental business of banking creates a vulnerability to panic because the banks’ trading securities are short term and need not be renewed; depositors can withdraw their money, while assets (primarily loans) are longer term. However, panic can be prevented with intelligent policies. What happened in August 2007 involved a different form of bank liability, one unfamiliar to regulators and academics, who were not aware of the size or vulnerability of the new bank liabilities. In fact, the bank liabilities that we will focus on are actually very old, but have not been quantitatively important historically. The liabilities of interest are sale and repurchase agreements, or “repos.” Before the crisis, trillions of dollars were traded in the repo market. The market was very liquid, like the market where goods are exchanged for checks (demand deposits). Repos and demand deposits are both forms of money. There have always been difficulties creating private money (like demand deposits), and this time around was no different. The panic in 2007 was not observed by anyone other than those trading or otherwise involved in the capital markets because it was not like the previous panics in American history (like the Panic of 1907, or those of 1837, 1857, 1873 and so on) in that it was not a mass run on banks by individual depositors. Rather, it was a run by firms and institutional investors on financial firms. The fact that the run was not observed by regulators, politicians, the media, or ordinary Americans has made the events particularly hard to understand. It has opened the door to spurious, superficial, and politically expedient “explanations” and demagoguery. As the economy transforms with growth, banking also changes. But, at a deep level, the basic form of bank liability has the same structure, whether it is private bank notes (issued before the Civil War), demand deposits, or sale and repurchase agreements. Bank liabilities are designed to be safe; they are short term, redeemable, and backed by collateral. However, they have always been vulnerable to mass withdrawals, or panics. This time the panic was in the repo market. But, before we come to that, we need to think about how banking has changed. Americans frequently experienced banking panics from colonial days until legislation establishing deposit insurance was passed in 1933, effective in 1934. Government deposit insurance finally ended the panics that were because of runs on demand deposits, which allow you to keep money safely at a bank and redeem it for currency any time you want. The idea that you can redeem your deposits anytime you want is one of the essential features of making bank debt safe. Also, bank debt is backed by sufficient collateral in the form of bank assets. Before the Civil War, the dominant form of money was privately issued bank notes; there was no government currency issued. That is, individual banks issued their own currencies. During the Free Banking Era, 1837–63, these currencies had to be backed by state bonds deposited with the authorities of whatever state the bank was chartered in. Bank notes were also redeemable on demand, and there were banking panics because sometimes the collateral (the state bonds) was of questionable value. This problem of collateral will reappear in 2007. During the Free Banking Era, banking slowly changed, first in the cities and then nationally over the decades after the Civil War. The change was that demand deposits came to be a very important form of bank money. During the Civil War, the government took over the money business in that national bank notes (“greenbacks”) were backed by U.S. Treasury bonds, and there were no longer private bank notes. But banking panics continued because demand deposits were vulnerable to panics. Economists and regulators did not figure this out for decades. In fact, when panics because of runs on demand deposits were ended it was not owing to the insight of economists, politicians, or regulators. Deposit insurance was not proposed by President Roosevelt; in fact, he opposed it. Bankers opposed it. Economists decried the moral hazard that would result from such a policy. Instead, deposit insurance was a populist demand to have the dominant medium of exchange protected. It is not an exaggeration to say that the quiet period in banking from 1934 to 2007, because of deposit insurance, was basically an accident of history. Times change. Now, banking has changed again. In the last 25 years or so, there has been another significant change: a change in the form and quantity of bank liabilities that has resulted in a panic. This change involves the combination of securitization and the repo market. At root, this change has to do with the traditional banking system becoming unprofitable in the 1980s. During that decade, traditional banks lost market share to money market mutual funds (which replaced demand deposits) and junk bonds (which took market share from lending), to name the two most important changes. Keeping passive cash flows on the balance sheet from loans, when the credit decision was already made, became unprofitable. This led to securitization, which is the process by which such cash flows are sold. I discuss securitization below.",9
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.16,Inflation and Default Dynamics,July 2010,Parul Jain,Leo Kamp,,Female,Male,Unknown,Mix,,
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.15,Can Industry Consolidation Lead to Greater Efficiencies? Evidence from the U.S. Defense Industry,July 2010,Nayantara Hensel,,,Unknown,Unknown,Unknown,Unknown,,
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.18,Compensation of Economists—Measuring the Market Value of NABE Member Characteristics,July 2010,Elizabeth Bernstein,Christopher Swann,,Female,Male,Unknown,Mix,,
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.23,Do Immigrant Nurses in Canada See a Wage Penalty? An Empirical Study,July 2010,Karen J Buhr,,,Female,Unknown,Unknown,Female,"This study uses data from the confidential master files of the 2001 Canadian Census on Individuals and will examine those whose major field of study was nursing. Using a basic human capital model of wage determination, this paper examines the wages of those with a Canadian degree and those with a foreign nursing degree. Using Census data, inferences can be made regarding where an individual likely attained her education.Footnote 3 It will be assumed that individuals who are born in Canada will have been educated in Canada. There may be a few cases of people who were born in Canada and attained their nursing education outside Canada, but it is anticipated that this will be a very small number. It will also be assumed that people who immigrate to Canada before the age of 19 will have received their nursing education in Canada, and these individuals will be treated as “domestically educated”. Individuals who immigrated to Canada after the age of 30 and hold at least one degree will be assumed to have received their nursing education in a country other than Canada and will be considered to be “internationally educated”. Those who immigrated to Canada between the ages of 19 and 30 and have at least one degree are considered to have a mixed education since it cannot be as easily inferred where they received their education. These individuals may have received some of their education in Canada and some in their country of origin, so it is not possible to infer where their education was received. Since their source of education cannot be determined, it will be interesting to examine whether these nurses more closely resemble the Canadian or the internationally educated immigrant groups. For the purposes of this study immigrants will be grouped into five general groups of origin: the United States, Australia and Western and Northern Europe (which will form the reference group when those not born in Canada are examined); Eastern and Southern Europe; Central and South America; Africa and the Caribbean; and Asia. Within this sample of foreign-educated nurses, 66 percent list themselves as a visible minority and just over half (51 percent) list languages other than English or French as the language used at home. It is plausible that nurses who are fluent in English or French may be more likely to be promoted and therefore will have higher earnings, because individuals holding supervisory positions in particular may be called on to write reports or performance evaluations. Racial discrimination in hiring could possibly result in people who are of United States, Western or Northern European, or Australian descent being more likely to be promoted and therefore to earn higher wages and to have better outcomes than other visible minority groups. Coefficients on visible minority status, which remain statistically significant after controlling for language, education and other explanatory variables, can be interpreted as a measure of the extent of discrimination in the labour market. Within a unionized environment, which has set pay scales for earnings, one should not expect to see a significant variation in these variables. Any variation that is seen can be partially driven by the locations where these nurses are employed. Those employed at nursing homes will see earnings lower than those in hospitals. Similarly, there will be some variation in pay seen among different nurses working within a hospital. There is expected to be a difference in pay seen by those who are working in various jobs within the same institution. Other factors can also underlie differences in pay seen by various individuals. Potential experience is another variable that can be underestimated since it is derived from someone's age and years of schooling. There is a potential that this value can be overestimated for individuals who may not have been able to enter the nursing labour market as quickly or easily as others. Canadian experience, not foreign experience, is recognized under union agreements so the structure of wage determination in the nursing profession would lead us to expect a lower return to potential experience for foreign born or educated individuals. It is anticipated that individuals who are educated in the United States, Australia, and Western and Northern Europe will see wages closer to that which is seen in a nurse who is educated in Canada. This result is expected since immigrants from these regions have more similarities to Canadian nurses in terms of language and/or race. Thirty-eight percent of nurses who have emigrated from the United States, Western and Northern Europe, and Australia speak languages other than English or French as the language used at home, and only 2 percent report being a visible minority. Educational quality is deemed to be closely related to that which is seen in Canada, and this should allow for easier credential recognition for immigrants from these regions. Until recently in Canada, nurses have had two main paths to human capital acquisition; these are the baccalaureate nurse (the BN designation) and community college-based diploma nurse (the RN designation). The education of nurses in the United States, Australia, and Western and Northern Europe is similar to that seen in Canada. Nurses in the United States have three basic paths for human capital accumulation; these include the two-year associate degree program, the three-year diploma program (equivalent to the Canadian RN designation), and the four-year baccalaureate program (equivalent to the Canadian BN designation). The nursing training programs vary across Europe, but mostly they involve either a diploma or baccalaureate nurse-training program. Individuals from these regions are also similar in terms of broad racial categories as perceived by employees and employers. These countries are also similar in language, with English and French being commonly used languages. Finally, there are historical and cultural ties between Canada and these countries that are also expected to have an influence on perceptions of employees and employers. It is anticipated that nurses who are educated in Africa and the Caribbean will see somewhat lower wages compared with Canadian educated nurses. In these cases it is expected that language will not be a factor in cases where English or French are commonly spoken. However, self-identified visible minority status will have a negative effect on the wages that will be earned by nurses from these areas of the world. Only 28 percent of nurses who have emigrated from Africa and the Caribbean speak languages other than English or French as their home language, and 92 percent of these immigrants report being a visible minority. Nurses who are educated in Central and South America and in Asia are also expected to face wage discrimination, since language and self-identified visible minority status may both have potentially negative effects on the earnings of these individuals. Ninety-two percent of nurses who have emigrated from these regions speak languages other than English or French as their home language, and 97 percent report being a visible minority. Nurses educated in the Philippines currently must have a baccalaureate certification, but in the past they also had the option of taking a diploma program. Nurses in China can train though an associate degree program, a diploma program, or a baccalaureate program The most common framework in economic literature for the study of wage determination is the human capital model. The human capital earnings function will form the basis for this study. The most commonly used empirical estimation for the human capital model is based on the functional form of the Mincer [1974] earnings equation: where w
i
 is a measure of hourly wages for an individual i, S
i
 represents the measure of the individual i's schooling or educational attainment, e
i
 is a measure of potential experience. This is entered as a quadratic term to capture the concavity of the typical earnings profile; X
i
 is a vector of other variables such as marital status, presence of children, province of residence, and native language each of which are assumed to affect earnings, and u
i
 is a disturbance term which is assumed to be independent of X
i
 and S
i
. A straightforward method utilizing the human capital framework can be used to examine the relative earnings of Canadian-born and educated nurses compared with nurses who have been trained in countries other than Canada. To answer the question about relative earnings of nurses in Canada, those who are trained as nurses and who are actively working as registered nurses will be included in the sample.Footnote 4 This allows me to focus explicitly on a very specific group of individuals to determine if there is any variation in the relative earnings of people with differing educational credentials who are working in the same profession. Much of the literature related to immigrant labour market outcomes show that immigrants face wage penalties compared with domestic workers. This study will test if this general result holds for individuals with nursing credentials. The simplest method will look at two specifications. The first will include four dummy variables: one for the reference group of Canadian-born and educated nurses, one for the foreign born and educated nurses, a third for those who have a mixed education, and a final group for those who are foreign born but Canadian educated. This will be done within a pooled regression equation that includes all individuals in the data set who trained as nurses and who are working as nurses in Canada. The second specification will include a series of dummy variables for the various country groups of origin (those from the United States, Western and Northern Europe, and Australia; those from Eastern and Southern Europe; those from Central and South America; those from the Caribbean and Africa; and those from Asia), with those born in Canada being the reference group. The level of wage advantage or disadvantage can then be measured by the coefficients on these dummy variables. These specifications will allow for an easy identification of a wage penalty associated with having a foreign nursing credential and for the identification of which specific regions of origin may be facing wage penalties compared with Canadians. If the hypothesis that foreign educated nurses suffer wage penalties is supported, one expects to find a statistically significant and negative coefficient on that independent variable. The dependent variable for this study is natural logarithm of hourly wages. In the 2001 Census on Individuals, employment income refers to total income received by persons 15 years of age and over during calendar year 2000 as wages and salaries. Since the standard Mincer equation uses the log of hourly wages as the dependent variable, the total income reported in the Census is divided by the average number of weeks worked and the average number of hours per week worked for each individual to derive hourly wage.Footnote 5 An individual's age and potential experience are also characteristics that are, in general, expected to be positively related to wages. The standard Mincer equation using each person's age minus the years of schooling they have minus six is used to obtain an individual's potential work experience. Potential experience squared is also included in the regressions to capture the concavity of the typical earnings profile. One problem with this measure of potential experience is that it may overestimate the actual experience for people who have taken time out of the labour force for various reasons and thus lead to a bias in the results. However, because of the nature of the sample being used, this cannot be eliminated easily. For the foreign-educated nurses, their experience is composed of two aspects: their foreign and their Canadian work experience. Numerous existing studies including, for example, Reitz [2001], Frenette and Morissette [2003], Aydemir and Skuterud [2005], and Alboim, Finnie, and Meng [2005], show that an immigrant's foreign experience is penalized and not fully recognized once the immigrant begins working in Canada. This study will examine if this result holds for the specific occupation of nursing. Finally, numerous individuals are omitted from the sample. Nurses who reported zero income or zero hours of work in the reference year are removed from the sample. Nurses who immigrated to Canada in 2000 or 2001 are also removed from the sample since there is a potential for these people to have arrived and begun work, but not have worked a full year yet. This number of people is quite small.",5
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.19,"Nariman Behravesh, Spin-Free Economics—A No-Nonsense, Nonpartisan Guide to Today's Global Economic Debates",July 2010,Anthony Clark Director,,,Male,Unknown,Unknown,Male,,
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.20,"Scott Patterson, The Quants: How a New Breed of Math Whizzes Conquered Wall Street and Nearly Destroyed It",July 2010,Jesse S Hixson Senior Fellow,,,Male,Unknown,Unknown,Male,,
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.21,"James Barth, Tong Li, Wenling Lu, Triphon Phumiwasana, and Glenn Yago, The Rise and Fall of the U.S. Mortgage and Credit Markets: A Comprehensive Analysis of the Market Meltdown",July 2010,Michael Lea Director,,,Male,Unknown,Unknown,Male,,
45,3,Business Economics,12 August 2010,https://link.springer.com/article/10.1057/be.2010.22,"Harold L. Vogel, Financial Market Bubbles and Crashes",July 2010,Jerry H Tempelman,,,Male,Unknown,Unknown,Male,,
45,4,Business Economics,26 November 2010,https://link.springer.com/article/10.1057/be.2010.34,From the Editor,October 2010,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
45,4,Business Economics,26 November 2010,https://link.springer.com/article/10.1057/be.2010.31,Estimated Macroeconomic Effects of a Chinese Yuan Appreciation,October 2010,Ray C Fair,,,,Unknown,Unknown,Mix,,
45,4,Business Economics,26 November 2010,https://link.springer.com/article/10.1057/be.2010.32,A Temporary Federal Discount Program to Stimulate Consumer Spending,October 2010,Kenneth Lewis,Laurence Seidman,,Male,Female,Unknown,Mix,,
45,4,Business Economics,26 November 2010,https://link.springer.com/article/10.1057/be.2010.29,Will the U.S. Auto Market Come Back?,October 2010,Ted H Chu,Yingzi Su,,Male,Unknown,Unknown,Male,"Although economic pain is widespread, this recession is really concentrated in two sectors—housing and automobile. Table 1 shows that, measured in 2005 constant dollars, U.S. real GDP declined from 13.4 trillion to 12.9 trillion from the start of the recession till 2009:Q2, a drop of 3.7 percent. During the same period, automobile production was down from $403 billion to $223 billion and residential investment dropped from $525 billion to $344 billion. In other words, the slump in these two sectors contributed roughly 74 percent of total GDP contraction in this recession! It is worth noting that the service sector actually grew one percent during this period. From its peak at 2005:Q4, real residential investment fell 56 percent—from $783 billion to $345 billion (measured in 2005 dollars). As a share of nominal GDP, residential investment fell from the peak of 6.25 percent in 2005:Q4 to 2.45 percent in 2009:Q2. Figure 1 shows that housing starts fell to the lowest level in at least half a century. Single-unit starts fell 80 percent from the record high of 1.8 million in November 2005 to the record low of 0.36 million in January 2009. Meanwhile, new single-family home sales fell 78 percent from the peak of 1.4 million in July 2005 to 0.33 million in January 2009. This housing bubble has been even larger and the bursting even greater than during the Great Depression. From 1915 to 1925, U.S. home prices rose 52 percent, followed by a 30 percent decline through 1933. This time, housing prices rose a stunning 137 percent increase from 1996 to 2006, as measured by S&P/Case-Schiller home price index. So far, it has decreased 30 percent as of 2009:Q2. The Collapse of Housing MarketsSource: Census, Grebler, Blank and Winnick/U.S. Department of Commerce; S&P/Case-Shiller. There is a broad consensus that household wealth, in the form of home equity, supported household spending after the technology stock bubble burst in 2001; but the magnitude of the impact is a subject of debate. A 2001–02 survey by Federal Reserve researchers Canner and others [2002] found that cash-out refinances were used for home improvement (35 percent), consumer and other debt repayment (26 percent), consumer expenditures (16 percent), and investment (21 percent). Greenspan and Kennedy [2007] provided solid historical data on the disbursement of equity extraction that showed parallel rises of home value, home equity withdrawals, and propensity of consumption. Benjamin and others [2004] estimated that every one dollar increase in home equity led to roughly eight cents of additional consumption, with the impact four times larger than that of other financial assets, while an econometric analysis by Macroeconomic Advisors found no empirical evidence that home equity withdrawals led to higher personal consumption. Now there is renewed interest in the adverse effect from the burst of the housing market bubble. Mian and Sufi [2009] claim that money extracted from home equity was not used to purchase new real estate or to pay down high interest credit card debts; rather it was used for real outlays—consumption or home improvement. They also found home equity-based borrowing was stronger for younger households, households with low credit scores, and households with high initial credit card utilization rates. These households also saw sharper reductions in auto loans from 2006 to 2008, which offered one explanation for the plummet of automobile sales during the downturn. The auto market rode the housing market up and down. It has always been true that durable goods are the biggest casualty of every recession. Among durable goods, a new car is the ultimate deferrable purchase. Since most American households already own one or more vehicles, it is easier to hold onto the “old clunker” a bit longer or turn to the used vehicle market. Yet the collapse of automobile sales dropped well below the previous recession troughs as the market was hit by three “rogue waves”: high gasoline prices, the credit crunch, and job losses. Spending on new vehicles as a share of disposable personal income sank to 2.7 percent in 2009:Q2, comparing to the troughs of 4.1 percent in the 1991 recession and 3.7 percent in 1980–82 recession. The historical average is 4.9 percent. Figure 2 shows that the lowest total vehicle sales (including medium and heavy duty trucks) in this cycle so far was 9.3 million in 2009 (as measured by the Seasonally Adjusted Annual Rate), the worst since December 1981, when nine million new vehicles were bought. But the U.S. population is 30 percent larger today: 307 million in 2009 vs. 230 million in 1981. On a population-adjusted basis, 2009 auto sales of around 10.5 million (partly boosted by the government-sponsored Cash-for-Clunkers program) will mark the lowest sales since 1958. The peak to trough decline of 40 percent is the worst since 1941 [Hughes-Cromwick 2009]. The Collapse of U.S. Vehicle Sales Source: Ward's Yearbook, Bureau of Economic Analysis, Census Bureau, General Motors. In stark contrast to the utter collapse of the housing and auto industries, consumer spending on durable goods excluding autos declined a modest 6.5 percent during the recession, nondurable goods spending decreased nearly 3 percent, and service sector spending increased 0.3 percent. Even though the Bush and Obama administrations spent tens of billions dollars rescuing automakers and parts suppliers, unemployment in the Mid-West region rose well above the national average, along with states hit hard by the housing crisis. Understanding just what happened with the economy and the structural drivers behind the dynamics will help us get a better grasp of the drivers for the recovery as well as the risks ahead. One view is that since the auto sector has dropped so low, the pent-up demand will make it play a key role in the recovery.",
45,4,Business Economics,26 November 2010,https://link.springer.com/article/10.1057/be.2010.25,Fueling the Credit Crisis: Who Uses Consumer Credit and What Drives Debt Burden?,October 2010,Diane K Schooley,Debra Drecnik Worden,,Female,Female,Unknown,Female,"Saving and borrowing link household consumption and income. Households save when income exceeds spending and borrow (dissave) when spending exceeds income. Many theories of household saving/borrowing motives and behavior that link consumption and income have been developed over the past 70 years. This section reviews several optimization models, where saving/borrowing behavior optimizes a household's utility over time. Optimization models include variations of the permanent income hypothesis (PIH) model, attributed to Friedman [1957] and variations of the Modigliani and Brumberg [1954] life-cycle (LC) model. For PIH/LC models, the objective of saving or borrowing is to smooth consumption over predictable fluctuations in income in order to achieve constant marginal utility of consumption over time. Although generally similar, the models differ in their assumptions about length of planning period. This period is infinite in PIH but finite over the life of the household for LC. Over the life cycle of a typical household, variation in income is greater than the variation in spending or consumption. Income is low in the early stages of the life cycle, increases to a maximum before retirement, and then decreases during retirement. According to LC, in order to maintain a constant level of marginal utility, households will borrow during the early stages of life, save in the middle stages, and spend savings during retirement. Expected income plays a key role in these models, where permanent income has much more influence on consumption than current or temporary income. Households expecting a permanent increase in income will reduce their savings and/or borrow against that higher income, realizing an increased level of consumption that they expect to sustain over time. Households anticipating a permanent decrease in income will decrease consumption—saving more and/or paying down debt. Temporary changes in income—such as might result from fluctuations in the economy—have no sustained impact on consumption. However, a temporary increase in income yields more saving and/or debt repayment; a temporary decrease yields less saving and/or more debt. A household's savings and borrowing decisions are influenced by its preference for future vs. present consumption. A decision to borrow implies that current consumption is preferred to future consumption; a decision to save implies the opposite. If interest rates are expected to increase, the associated increase in prices, and the implied higher discount rate, reduces the present value of future consumption. This should generate a stronger preference for current consumption and encourage borrowing. Baek and Hong [2004] outline several limitations of the PIH/LC models and the lack of empirical evidence supporting them. The basic models do not recognize marital status nor allow for the presence of children in the household, determining life cycle stage solely by age of the household head [Browning Deaton and Irish 1985]. The basic PIH/LC models also do not provide for liquidity constraints. Although households may desire to borrow in order to smooth consumption, they may not qualify for adequate amounts of credit needed to smooth consumption [Deaton 1992]. Finally, the basic PIH/LC models do not recognize a precautionary motive of saving [Carroll and Summers 1991; Deaton 1991]. Prudence can explain why households in early life cycle stages may not borrow as much as the PIH/LC model would predict and why households in later life cycle stages may not draw down assets as quickly as would be predicted. The PIH/LC models predict that households will borrow during a recession in order to maintain a level of consumption. When income declines, the savings rate should be low or at least decline. In other words, these models predict that borrowing increases and saving decreases during a recession. However, although the personal savings rate—defined by the Bureau of Economic Analysis as personal savings as a percent of personal disposable income—declined throughout 2007 and early 2008 to a low of 1.2 percent, it increased substantially as the recession deepened, reaching a high of 5.4 percent in the second quarter of 2009. In this regard, household behavior may better be explained by the buffer-stock model, discussed below. According to the buffer-stock savings model [Deaton 1991; Carroll 1992], consumers hold assets to protect consumption against unpredictable fluctuations in income. Households can be both impatient—borrowing to finance current consumption if income is known with certainty—and prudent, by holding precautionary balances. A tradeoff of impatience and prudence ensures a target level of wealth held as a sufficient buffer against income fluctuations. Holding a target level of wealth to weather fluctuations in incomes implies a precautionary motive for saving. Unlike in the buffer-stock model, uncertainty in future incomes plays no role in the PIH/LC models. The possibility of unemployment leads to uncertainty in household income and so affects current consumption and saving. The more uncertain household income, the higher the buffer stock of wealth required; saving increases relative to consumption. This precautionary motive can explain the increases in personal saving rates in 2008 and 2009. With employment and income uncertainty rising during the recession, households save more in order to increase their buffer stock. And, as the economy entered its slow recovery, consumer spending increased cautiously and the personal savings rate began to decline, reaching 3.5 percent in the first quarter of 2010. The PIH/LC models hold that stage of life cycle, expected interest rates, and expected changes in permanent income affect saving and borrowing as individuals seek to smooth consumption over their lifetimes. According to the buffer-stock model, uncertainty of future income and employment drive the size of buffer-stock assets, and thereby saving and borrowing, needed to smooth consumption. Both models assume rational tradeoffs, but neither address attitudes toward credit. The easy credit climate (until recently) and the seemingly unlimited appetite for consumption may influence households to consume now regardless of consequences later. Unwary consumers may underoptimize utility over their lifetimes by overconsuming today and limiting future consumption. Contrary to theoretical assumptions, they may not be looking beyond today. The analysis that follows examines the impact of credit attitudes on debt use, as well as factors proposed by the models.",16
45,4,Business Economics,26 November 2010,https://link.springer.com/article/10.1057/be.2010.28,How Biased are Current Measures of Asian Output Growth and Price Change?,October 2010,Brooks B Robinson,,,Male,Unknown,Unknown,Male,"Foreign investment decisions are, in part, based on general economic growth and price trends in target economies. Therefore, it is important that national economic statistics exclude patterns of bias. When biases are inherent in economic statistics, investors may forecast faulty estimates of growth and price change, incorporate errors into their estimates of expected returns, and make poor investment decisions. For many decades, national accountants have known that Fisher-Ideal chain-type measures of output growth and price change are less biased than measures based on Laspeyres formulations. Nevertheless, many nations around the world continue to produce national economic statistics using the latter formulations. Figure 1 shows annual average biases in measures of output and price change for four Asia-Pacific economies.Footnote 5 Figure 2 reflects the cumulative bias in measures of output growth and price change for the same economies over the period under study—noting that the cumulative biases in both price change and growth for India are under 0.1 percent, largely because of offsetting positive and negative biases, as discussed below. This paper explores the biases in these estimates of output growth and price change, attempts to explain the biases, and offers additional insights about them. A key contribution of this paper is that it helps reduce uncertainty concerning the magnitude of the bias that may be inherent in Laspeyres measures of real output growth and price change. Bias in Annual Output Growth and Price Change Cumulative Bias in Output Growth and Price Change",
45,4,Business Economics,26 November 2010,https://link.springer.com/article/10.1057/be.2010.30,"Refractory Materials: The Global Market, The Global Industry",October 2010,Michael A Deneen,Andrew C Gross,,Male,Male,Unknown,Male,"Refractory technology advanced along with ceramic and metal processing technology over the centuries. More recently, dramatic improvements occurred in regard to refractory use in steelmaking. In 1970, refractory consumption was about 30 to 50 pounds per ton of steel produced; by 2007, the range was 10 to 20 pounds and under 10 pounds in the most efficient plants. Other technologies have also progressed in the production of nonferrous metals, glass, and cement. These changes in processing efficiency and in refractory performance are expected to continue, with weight or volume consumption declining per unit of output. The decrease in refractory consumption is offset in part by higher prices, especially in highly industrialized nations. For example, in Germany, the average price of refractory materials moved from $710 per ton in 2002 to $840 in 2007 and is projected to reach $940 in 2012. In Japan, the corresponding figure for 2007 was put at $960 per ton. At the global level, price changes come slowly, however, from $600 in both 2002 and in 2007 to an estimated $630 in 2012. The explanation lies in China's dominant role where prices of refractory materials are truly low, about $450 per ton on average in 2007, with a projected increase to $495 in 2017. China has a vast supply of requisite materials and low labor costs; energy prices have also been kept low until now. The country is the dominant player in this field. Refractory technology should continue to advance, with emphasis on design, custom solutions, decreased energy use, reduced maintenance, and increasingly sophisticated mixes of forms and materials. Companies are striving to offer an ideal combination of properties desired by end users; these range from high melting points to low thermal expansion, from strength and hardness to low permeability, from heat conduction to resistance to abrasion, deformation, and erosion. End users expect longer life spans and improved durability from refractories and are willing to collaborate with makers in such areas as purity of raw materials, consistency in particle size, and customized new installations. Refractory production facilities are located close to mineral deposits because of the expense involved in transporting heavy, low-value materials. Makers also seek to locate plants close to major centers of demand because of high transport costs and need for on-time delivery to sites where construction takes place. China is the largest single producer, accounting for about 40 percent of global output in 2007. Other leading countries are the United States, Japan, Russia, Germany, and Brazil. Developing/industrializing nations will assume a greater role in the future. China will also continue to dominate as a leading net exporter, with a value at $840 million in 2007, along with nations of Western Europe whose value was put at $660 million in the same year.",4
45,4,Business Economics,26 November 2010,https://link.springer.com/article/10.1057/be.2010.27,"Stephen Roach, Stephen Roach on the Next Asia: Opportunities and Challenges for a New Globalization",October 2010,Cliff Tan,,,Male,Unknown,Unknown,Male,,
45,4,Business Economics,26 November 2010,https://link.springer.com/article/10.1057/be.2010.26,"Gary L. Reback, Free the Market! Why Only Government Can Keep the Marketplace Competitive",October 2010,Thomas A Hemphill,,,Male,Unknown,Unknown,Male,,
45,4,Business Economics,26 November 2010,https://link.springer.com/article/10.1057/be.2010.33,"Henry M. Paulson, On the Brink: Inside the Race to Stop the Collapse of the Global Financial System",October 2010,Cliff Tan,,,Male,Unknown,Unknown,Male,,
46,1,Business Economics,31 January 2011,https://link.springer.com/article/10.1057/be.2010.44,From the Editor,January 2011,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
46,1,Business Economics,31 January 2011,https://link.springer.com/article/10.1057/be.2010.35,Macroprudential Supervision and Monetary Policy in the Post-crisis World,January 2011,Janet L Yellen,,,Female,Unknown,Unknown,Female,"It is now clear that our system of regulation and supervision was fatally flawed. Despite volumes of research on financial market metrics and weighty position papers on financial stability, the fact is that we simply didn’t understand some of the most dangerous systemic threats. Looking back, I believe the regulatory community was lulled into complacency by a combination of a Panglossian worldview and benign experience. The notion that financial markets should be as free as possible from regulatory fetters had evolved into the conviction that those markets could, to a very considerable extent, police themselves. Meanwhile, things went along so well for so long that the common belief came to be that nothing could go disastrously wrong. Over a period of decades, the financial system was tested repeatedly—the Latin American debt crisis, the savings and loan crisis, the Asian crisis, the failure of Long-Term Capital Management, and the stock market crashes of the late 1980s and early 2000s. With each crisis, policymakers rolled up their sleeves and beat back the systemic threat. The levees held. Despite these financial market ups and downs, economies in the United States and other parts of the world performed very well. We appeared to have entered a new era of stability. We even gave it a name: the Great Moderation. We were left with the mirage of a system that we thought was invulnerable to shock, a financial Maginot Line that we believed couldn’t be breached. We now know that this sense of invincibility was mere hubris. To understand what went wrong, I refer you to Hyman Minsky's [1992] path-breaking work on speculative financial booms and busts. As Minsky showed, success can lead to excess, and excess to ruin. The Great Moderation saw a progressive growth of credit and extension of risk, which came to a head in the mortgage market. Credit flowed freely and cheaply, and households and financial institutions alike took on greater risk, borrowing to the hilt. House prices soared off the charts. Financial innovators found increasingly exotic ways of packaging loans for investors. Securities became so opaque that few understood their risks. Originators sold mortgages to investors whose due diligence consisted of glancing at ratings. The financial system became increasingly complex, interconnected, and hyperleveraged. When housing prices plunged, the value of real-estate-related assets on the balance sheets of financial institutions, both in the traditional and shadow banking sectors, collapsed. Not only were many of those institutions highly leveraged, they had also relied heavily on short-term borrowing to acquire those assets. The combination of leverage and short maturity funding left them perilously vulnerable to runs. The result was panic and market breakdowns on an unimaginable scale, bringing us frighteningly close to a meltdown of the global financial system and a second Great Depression. Thankfully, that depression never occurred. Governments and central banks around the world took extraordinary action to prevent complete collapse. But what we did get was bad enough—the deepest and most prolonged recession in generations. And the recovery has been agonizingly slow, held back, in part, by the ongoing efforts of overleveraged households and financial institutions to repair their balance sheets. These events demonstrate both the natural tendency of the financial system to cycle through booms and busts and the potential, absent adequate supervision and regulation, for breakdowns in the financial system to threaten the global economy. Financial market participants can be bipolar—prone to fads, manias, myopia, panics, and depression; driven by short-term gain; and easily caught up in the madness of crowds. One's belief about the beliefs of others is critical, like Keynes's example of a beauty contest [Allen and others 2006]. In addition, the structure of compensation and incentives in the financial sector created strong motives for excessive risk taking, especially during boom years. Methods of modern risk management may have intensified the cycle because of their reliance on metrics such as value at risk that are highly sensitive to recent performance, especially volatility. In good times, volatility declined, and value at risk along with it. This pattern generated a procyclical willingness to take on risk and leverage, amplifying and propagating the boom and bust cycle. The vicious cycle of a collapse of confidence, asset fire sales, evaporation of liquidity, and a deleveraging free fall was the mirror image of the manic mortgage market that preceded it. To avoid a repeat of these events, it is essential that we change the landscape of supervision and regulation. The landmark legislation recently passed by the Congress, the Dodd-Frank Wall Street Reform and Consumer Protection Act (Dodd-Frank Act), is designed to do just that. It requires regulators to contain risks to the financial system before they erupt into crises. And it creates a structure to collect data, identify emerging threats to financial stability, and formulate policies to contain these risks. Within this structure, the Federal Reserve will play an important role. We will supervise all systemically important institutions and, jointly with other regulators in the new Financial Stability Oversight Council, establish stricter prudential standards for such firms. We will help ensure the safety of financial market utilities that are critical to our payment, clearing, and settlement systems. And we will actively support the council's mission to identify and address emerging risks to financial stability. At the same time, though, we must take care to preserve incentives for innovation and reasonable risk taking. We must remain prudent, while avoiding an overly strict approach that unduly impedes financial intermediation and stifles capital formation. Vigilance to threats of systemic financial risk must also inform the conduct of monetary policy. We have seen that the eruption of a financial crisis can have severe economic consequences, compromising the ability of a central bank to attain its primary macroeconomic objectives. Monetary policymakers should also be aware that the decisions they make in pursuit of price stability and full employment could, in some circumstances, affect the development of systemic risk. For example, if compensation incentives in the financial sector are misaligned, low interest rates might heighten the ability and desire of financial market participants to reach for yield and take on risk. Our goal should be to deploy an enhanced arsenal of regulatory tools to address systemic risk, making the financial system far more robust. That way, monetary policy can concentrate on its long-standing goals of price stability and maximum employment. Supervision and regulation must serve as the first and main line of defense in addressing systemic risk. We have at our disposal a tool kit of regulatory instruments that are well adapted for this purpose. Monetary policy cannot be a primary instrument for systemic risk management. First, it has its own macroeconomic goals on which it must maintain a sharp focus. Second, it is too blunt an instrument for dealing with systemic risk. All the same, I cannot unequivocally rule out the possibility that situations could emerge in which monetary policy should play some role in reining in risk taking behavior. I will return to this point later. In the remainder of my remarks, I will discuss key issues that must be resolved before an effective policy regime for the containment of systemic risk can be established. First, we must understand the sources of systemic risk and design surveillance practices that enable us to detect threats to financial stability early on. Second, we must develop a tool kit of supervisory policy instruments—so-called macroprudential policies—and guidelines on how and when to deploy them. And third, we must strive to avoid situations in which macroprudential and monetary policies are working at cross-purposes, given that macroprudential policies affect macroeconomic performance and that monetary policy may affect risk taking incentives. All of these issues raise complex questions of design and implementation. Interesting parallels may be drawn, however, between the design issues we face in macroprudential policy and those we have grappled with in conducting monetary policy. These parallels include the appropriate roles of rules vs. discretion, the need for policies that are robust in addressing uncertainty, and the assignment problem, all of which I will discuss later. The fertile field of monetary policy research may therefore offer lessons that can fruitfully be applied to managing systemic risk.",36
46,1,Business Economics,31 January 2011,https://link.springer.com/article/10.1057/be.2010.36,The Federal Reserve's Mandate: Long Run,January 2011,Thomas M Hoenig,,,Male,Unknown,Unknown,Male,"Currently, a major and necessary rebalancing is taking place within our economy. This includes the deleveraging of consumers, businesses, and financial institutions, and it's during a time that state and local governments are struggling with budgets and mounting debt loads. In this context, a modest recovery with positive overall data trends should be seen as highly encouraging. Following a bounce back from restocking earlier this year, the economy has slowed but it has not faltered. GDP growth has averaged about a 2½ percent annual pace since the first of the year. Industrial production is showing growth of almost 6 percent, and high-tech more than double that. The consumer continues to buy goods, with personal income growing at more than a 3 percent rate, personal consumption expenditures at about 3 percent, and retail sales at more than 4 percent. And the U.S. economy has added more than 850,000 net new private sector jobs since the first of the year. While modest, these are positive trends for the U.S. economy. The issue is, of course, that while private jobs are being added within the economy, it is not enough to bring unemployment down to where we all would like to see it. Unemployment remains stubbornly high at 9.6 percent. With such numbers, there is, understandably, a desire and considerable pressure for the Federal Reserve to “do something, anything” to get the economy back to full employment. And for many, including many economists, this means having the Federal Reserve maintain its zero interest rate policy or further still, engage in a second round of quantitative easing—now called QE2. Some are even suggesting these actions are necessary for the Federal Reserve to comply with its statutory mandate.",
46,1,Business Economics,31 January 2011,https://link.springer.com/article/10.1057/be.2010.37,The “New Economic Reality”: How New and How Real?,January 2011,Lynn Reaser,,,,Unknown,Unknown,Mix,,
46,1,Business Economics,31 January 2011,https://link.springer.com/article/10.1057/be.2010.38,"The Deeper the Recession, the Stronger the Recovery: Is It Really That Simple?",January 2011,Azhar Iqbal,Mark Vitner,,,Male,Unknown,Mix,,
46,1,Business Economics,31 January 2011,https://link.springer.com/article/10.1057/be.2010.40,The Consumer-Driven Economy at a Crossroads,January 2011,Robert P Yerex,,,Male,Unknown,Unknown,Male,"Economists have generally sought to describe the act of consumption by individuals in a free-market economy in terms of a theoretical framework based on rational and efficient behavior by consumers to maximize the value of their acquisition transactions. One such theoretical framework is presented by Milton Friedman [1957]. This framework is that consumers act rationally to maximize their utility, based on a set of a priori expectations. These, and similar, paradigms have come under considerable scrutiny over the past decade, especially in light of the recent financial crisis. Rotheli [2007] takes an econometric approach to examining the issues of consumer rationality, expectations, and decision making in the face of uncertainty. Although the often complex models generate useful results when applied to situations of limited scope, they are of little use in explaining large-scale changes in consumer behavior. Recently, there has been a good deal of discussion debunking the notion that markets, as a manifestation of the actions of millions of rational actors, are themselves rational and self-correcting. One of the more comprehensive of these critiques is that put forward by John Cassidy [2009]. The emerging field of behavioral economics seeks to expand on the idea, first stated by Hayek [1988], that there are two forms of rationality. Smith [2008] expands upon the differences between constructivist and ecological rationality with an understanding that much, if not most, of the decisions taken by individuals are related to a biological rather than logical form of rationality. Moving further away from theories based on the economic rationality to describe consumer behavior, Sassatelli [2007] examines the social and cultural forces that influence consumer actions, essentially positing the notion that we, as individuals in a free-market economy are programmed to consume, and not to be rational about it. Further still from mathematical models as an approach to understanding and predicting consumption, Haim Ofek [2001] looks at the very roots of economic exchange from the perspective of human evolution, while Geerat Vermeij [2004] examines biology and nature itself in terms of economics.",4
46,1,Business Economics,31 January 2011,https://link.springer.com/article/10.1057/be.2010.41,Start-Ups and External Equity: The Role of Entrepreneurial Experience,January 2011,Peter A Zaleski,,,Male,Unknown,Unknown,Male,"Building on the financial growth cycle model of Berger and Udell [1998], start-ups seeking external equity need to clear information hurdles to convey their profitability to outside investors. There may be proxies for information that investors consider when investing in a new start-up, given that the firm's business plan lines up with the interests and knowledge of the investor. Perhaps, the best simple proxy that investors use may be the experience level of the founding entrepreneur. The experience level of the founding entrepreneur of a business start-up may be classified into one of three categories. In group 1, there are start-ups founded by brand new entrepreneurs. These entrepreneurs have no prior experience running a business. In group 2, there are start-ups founded by entrepreneurs who have previous experience as entrepreneurs but not in the specific industry as the new start-up. Finally, in group 3, there are start-ups founded by entrepreneurs who have previous experience in the same industry as the new start-up. Outside investors value the experience of the entrepreneur for two reasons. First, as Cooper and others [1994] find, industry specific know-how on the part of the entrepreneur is positively correlated with both survival and growth. Second, entrepreneurial experience in general lends credibility to any information the entrepreneur might provide. They also find that entrepreneurs tend to be overly optimistic about the future profitability of their new start-ups. In this regard, experience on the part of the entrepreneur serves two possible purposes. First, because of prior experience, entrepreneurs may temper their forecasts of the future. Second, and perhaps more importantly, whether entrepreneurs temper their forecasts or not, outside investors may find the numbers more believable if they are forecasts from somebody with experience. Aside from differences in entrepreneurial experience, there are a variety of key characteristics that an investor considers when deciding whether to invest in a new start-up. Many of these characteristics are analyzed by Cassar [2004] to determine which ones impact a firm's capital financing. In this section, we consider what those characteristics may be. This paper hypothesizes that not only do entrepreneurs differ in these characteristics across experience categories, but equity investors might weigh these characteristics differently across experience categories. These characteristics include both demographic characteristics and firm characteristics. Services can represent a value-added feature for those companies that sell products and create brand loyalty over time if the service acts as a complement to the firm's core competence. Alternatively, if the service acts as a distraction from the firm's core competence, then it may hamper future profitability. Given the two different impacts that providing an additional service might have, no a priori hypothesis is made regarding the impact of offering additional services on the likelihood of obtaining external equity financing. A competitive advantage is described as something unique or distinctive a business provides that gives it an advantage compared with its competitors. It is important to note that in the KFS, the competitive advantage is self-reported by the entrepreneurs and may be biased as a result. Absent any self-reporting bias, it is hypothesized that entrepreneurs with a competitive advantage are more likely to obtain external equity financing than those entrepreneurs that do not have a competitive advantage. Patents, trademarks, and copyrights are legal means by which a firm retains monopoly rights to its product. These ensure that, should the firm be successful, the future profits will not be subject to erosion as outside competitors enter the market and drive down price and profit. It is hypothesized that entrepreneurs with legal barriers to entry are more likely to obtain external equity financing than those who do not have such barriers. Having a government contract or being a business-to-business provider may signal to outside investors greater stability and future certainty than a business firm that relies on consumer demand for its sales. Also, depending on the product, government sales could be a great base on which to build private sales. This signal may be appealing for outside investors. Alternatively, with greater risk comes greater rewards. Outside investors may find the stability of a government contract or business-to-business sales less enticing. As such, no a priori hypothesis in made regarding the impact of these variables. Educational achievement sends a signal to potential equity investors. Lack of a high school diploma may be viewed negatively by potential investors. Further, it is assumed that the impact of education is not linear. More education may be preferred to less, but not indefinitely. A college education is viewed more favorably than a high school education; and in certain areas, a doctorate or professional degree is required. Thus, it is hypothesized that entrepreneurs with higher levels of education are more likely to obtain external equity financing than those with less. Several demographic variables are collected by the KFS. These include ethnic group and gender. No a priori hypothesis is made regarding the impact of these. These variables are included in the analysis to determine if they do have an impact. Empirical analysis by Robb [2002] and by Coleman and Robb [2009] suggests that women and minorities receive significantly less external financing than white males. With respect to women, this may be because of the fact that while women-owned ventures are just as likely to survive as male-owned ventures, they are less likely to grow [Cooper and others 1994].",9
46,1,Business Economics,31 January 2011,https://link.springer.com/article/10.1057/be.2010.39,Mesirow Financial,January 2011,Adolfo Laurenti,,,Male,Unknown,Unknown,Male,"With hindsight, the opportunity to join Mesirow Financial has been fortuitous, the result of two major consolidations in the Chicago banking industry. My new boss, Diane Swonk, joined Mesirow Financial after Bank One merged with J. Morgan Chase; a few years later, Bank of America took over LaSalle Bank/ABN Amro, where I was working, forcing me to look for new opportunities. I had known Diane for several years, dating back to the fateful NABE Annual Meeting breakfast on the morning of September 11, 2001 at the Marriott Twin Towers hotel in New York. Diane was attending as a former NABE president, while I participated as a new student member. After the traumatic events of that day, we remained good friends, but I also developed a tremendous respect for Diane's accomplishments and her unique, idiosyncratic style of economics, in which the joys, pains and worries of real people are always at the forefront. Hard data became the connector between our daily lives, our dreams and our economic realities. I was thrilled at the opportunity to join Mesirow Financial, but little did I know how crucial that choice would be during the peak of the financial crisis. A mid-sized financial company headquartered in Chicago, but with offices all over the United States and in London, Mesirow Financial appealed to me for two reasons: the much diversified nature of its businesses and its corporate culture. The best way to define what Mesirow does is probably by describing what we are not: we are not a commercial bank in that we do not make loans or take deposits. Otherwise, all banking functions are covered: from traditional investment banking to fund-of-funds and private equity, from wealth management to currency trading, plus public finance, broker-dealer services, insurance brokerage, real estate, and turnaround consulting. All in all, 1,200 people working in four divisions (Investment Management, Global Markets, Insurance Services and Consulting), operating in four markets (institutions and public sector entities, corporations, individuals and families, and broker/dealers and investment advisors). To me, the variety of this business is very attractive: as an economist, I enjoy the variety of challenges that comes from such a diversified environment. One challenge is to keep the pulse and pace of global financial markets, but another is the opportunity to sit with small and mid-sized manufacturing firms around the Midwest to discuss their daily efforts to navigate an economic landscape that is constantly changing, including becoming more and more globalized. As I had already learned at LaSalle, there is no longer such a thing as a “local” small business: nowadays, even for small operations, the reality is very international, decision-making is complex and economic risk is high. For an economist, this provides the perfect opportunity to do exciting, strategic work. But even more appealing than its collection of businesses, it was the culture of Mesirow Financial that boosted my confidence and excitement. Our company is extremely entrepreneurial, with a flattened hierarchy and a very short chain of command. The company is independent and privately owned by about 300 of our managers. As a result, the working environment is dynamic but cooperative and retains the laid-back attitude of a family-owned business. This approach served us well before the crisis. Mesirow Financial resisted calls to get involved with the instruments of creative mortgage finance and rejected suggestions to boost profitability with leverage during the roaring 2000s. As our Chief Executive Officer Jim Tyree still quips, “by 2005 I was considered a dinosaur.” It turned out that Mesirow made the right choices at the right time: by the time liquidity evaporated and the house of cards collapsed, the company was not only in a relatively healthy condition, but ideally positioned to take advantage of the disruptions taking place in financial markets. There are business opportunities in chaos, when uncertainty and panic paralyze most people. Mesirow moved aggressively in this environment, and secured several top-notch professionals displaced by the upheaval, at a moment when most companies, especially in the financial sector, were more concerned with shedding jobs than creating opportunities. These factors contributed to inject confidence and excitement during rather demanding times.",1
46,1,Business Economics,31 January 2011,https://link.springer.com/article/10.1057/be.2010.42,"Laurence Kotlikoff, Jimmy Stewart is Dead: Ending the World's Ongoing Financial Plague with Limited Purpose Banking",January 2011,John C Goodman,,,Male,Unknown,Unknown,Male,,1
46,1,Business Economics,31 January 2011,https://link.springer.com/article/10.1057/be.2010.43,"Chip Heath and Dan Heath, Switch",January 2011,Gerald L Musgrave,,,Male,Unknown,Unknown,Male,,
46,2,Business Economics,20 May 2011,https://link.springer.com/article/10.1057/be.2011.9,From the Editor,April 2011,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"I am pleased to announce that Charles Steindel has joined the Business Economics Editorial Board. Charles is a former Director of NABE and is now with the New Jersey Department of the Treasury. For many years, he was with the Federal Reserve Bank of New York.",1
46,2,Business Economics,20 May 2011,https://link.springer.com/article/10.1057/be.2011.2,Will It Take a Crisis to Fix Fiscal Policy?,April 2011,Rudolph G Penner,,,Male,Unknown,Unknown,Male,"Some believe that the numbers just cited are far too pessimistic. They argue that the nation faced severe budget problems before and that we have done something about it. In 1990, when the deficit was rising at an alarming rate and blasting through the targets set by the Gramm-Rudman-Hollings law, the elder President Bush got together with a Democratic Congress and they fashioned a major, bi-partisan budget deal that very significantly reduced the deficit from where it would have been otherwise. Unfortunately, the deficit continued to rise rapidly because of the lingering effects of the 1990 recession; and that induced President Clinton to negotiate another deal with Congress, almost as large as Bush's. Republicans refused to go along and the deal enacted in 1993 was supported only by Democrats. Exogenous events also contributed to deficit reduction. The Cold War ended, and defense spending was cut substantially. The tech boom on Wall Street helped bring in a flood of revenues—mostly from the very, very rich—and a budget surplus emerged in 1998 that caught everyone by surprise. It was the first surplus since 1969. Optimists argue that there is no good reason that we cannot combine good policy and a bit of good luck yet again and cure our fiscal woes.",1
46,2,Business Economics,20 May 2011,https://link.springer.com/article/10.1057/be.2011.3,"The Margin, Currency, and the Price of Oil",April 2011,Philip K Verleger Jr.,,,Male,Unknown,Unknown,Male,"Microeconomics and microeconomists focus intensely on marginal costs and consumer demand. Some might argue that microeconomists spend inordinate time on these subjects. There is, however, good reason for their interest. Price levels are set when marginal costs and marginal revenues (determined by the price elasticity of demand) balance. The analysis of marginal costs and marginal revenues is relatively simple in a world with one product, one group of consumers, and one group of producers, all operating in a single location. The problem becomes much more complex when there are several products, more than one input, a widely dispersed set of producers, and an equally widely dispersed set of consumers. In such circumstances, the analysis and prediction of prices becomes more difficult, sometimes impossible. Perhaps no market presents a greater challenge in this respect than the petroleum market. Consumers are spread across the world, and their consumption patterns change. For many years, consumption expanded most rapidly in the United States. Then Europe took the lead following World War II. The United States forged ahead again in the last decades of the twentieth century. Today, China and other Asia are the most rapidly growing markets. Oil prices are not set by crude oil demand but by demand for petroleum products. Consumers do not buy crude. They buy gasoline or jet fuel or diesel fuel or distillate or residual fuel oil. There is no single marginal market for these fuels. The United States is clearly the marginal market for gasoline. The United States burns 43 percent of the gasoline produced globally. Europe is the marginal market for distillate fuel today. The marginal market for jet fuel is probably in Asia. The link between the various product markets and crude oil prices depends critically on the location of refining centers, their capacity to process the various crudes available to meet local demand for particular products, and the quality of crude oils available in local markets. Generally, the product in shortest supply in the market most dependent on imports will effectively set prices globally. As noted, the United States has been the marginal market in the past. In the United States, gasoline was the marginal product; and crudes offering the highest gasoline yields, principally light crudes, became the marginal input. Over the last decade, though, diesel fuel has emerged as the marginal product and Europe as the marginal market. Gasoline is in surplus globally as use drops in the United States because of recession and greater conservation. Diesel use, on the other hand, keeps rising globally. Diesel consumption has increased most significantly in Europe, where incentives have prompted motorists to change over to diesel-powered vehicles. The European dieselization policy has been motivated by the greater efficiency of diesel engines. It is widely acknowledged that diesels get more miles per gallon. The shift to diesel has been assisted by the development of better, cleaner, and more efficient diesel engines for cars, primarily by European and Japanese auto manufacturers. Governments in Europe have encouraged dieselization by imposing taxes on engine displacement and by taxing gasoline at a higher rate than diesel fuel, and consumers have responded. In many countries there, diesel-powered cars outsell gasoline-powered vehicles by a four or five to one ratio. Distillates, particularly diesel fuel, now account for an increasing share of European petroleum consumption. The rub here is that European oil refineries, particularly the French ones, were not designed to produce a high distillate yield. Instead, like most world refineries, the European facilities were built to distill crude or to distill and crack crude, meaning they could best produce high gasoline yields. These refiners also have produced significant amounts of residual fuel oil because most lack coking facilities. As a result, they have not been able to adjust to the shift to distillate fuel oil. This situation required the European oil industry to start exporting gasoline as it imported more distillate. At the same time, its refining margins declined because global demand for European gasoline was, with some exceptions, shrinking. The resulting drop in profits contributed to refining plant closures in Europe, which boosted the region's dependence on imported distillate even more. Refiners that continue to operate in Europe have attempted to boost returns by focusing on North Sea crude oils. These crudes are generally sweet and light, meaning they offer distillate yields of around 50 percent.Footnote 1 By increasing their use of North Sea crudes, these refiners can maximize gasoil and ULSD production while cutting gasoline output. European refiners will likely maintain this practice. On October 1, the European Commission ruled that refiners operating there would have to purchase carbon emission allowances beginning in January 2013. According to Bloomberg reporters Mathew Carr and Ewa Krukowska [2010], the industry will likely pay as much €4.4 billion in 2013 if refiners receive no free allowances. The authors also reported that refiners would pay roughly €1 billion if they received free allowances totaling 105 million tons. Costs would increase through 2020 because the EU plans to reduce the free allowances given and increase the allowances sold. The imposition of such fees on the European refining industry cannot help but force more facilities to close or operate at lower rates. The Bloomberg authors suggested that operating costs could rise as much as 13 percent. However, this estimate seems low. A carbon fee of €4.4 billion would translate into a charge of $1.25 per barrel. This amounts to one-third of the complex refining margins reported in Argus Global Markets and two-thirds of simple refining margins reported in the same publication. This implies that many operators will close or drastically scale back operations. At least one firm has noticed the potential change. PBF Investments, led by Thomas O’Malley, has purchased two U.S. East Coast refineries from Valero and announced plans to refurbish them [Mira 2010, p. 1]. O’Malley clearly understands Europe's burgeoning distillate requirements. He also no doubt recognizes that Europe's increasingly stringent environmental rules will force more refining offshore. The East Coast refineries that PBF purchased on the Delaware River and in New Jersey are about as close as one can get to Europe. Europe's push to boost diesel use, the configuration of its refineries, and the likely imposition of carbon tariffs will probably make Europe the marginal market for petroleum products for at least the next decade. This will have a number of implications for markets:
 Spot prices of European distillate products (ULSD, gasoil, and jet fuel) should trade at a premium to prices in other markets, especially the United States. Increasing volumes of distillate products should move to Europe from the United States and other regions. North Sea crude oils should trade at a premium to similar crudes in other parts of the world. Futures and swap markets for crude oil and petroleum products delivered in Europe should become more important. Correlations in fluctuations between the dollar/euro exchange rate and petroleum product prices should increase. In the sections below, we discuss each of these issues and offer documentation. While it goes without saying that the oil market is always changing, this shift is more significant than most.",12
46,2,Business Economics,20 May 2011,https://link.springer.com/article/10.1057/be.2011.4,The Economy Today and a Policy Framework for Today and Tomorrow,April 2011,Dennis P Lockhart,,,Male,Unknown,Unknown,Male,"My assessment of the state of the economy and the outlook is pretty mainstream. In terms of gross domestic product (GDP) growth, the economy is demonstrating moderate strength, and the pace of growth is accelerating somewhat. The private spending components of GDP are trending positively. Personal consumption overall is growing briskly, even while the savings rate continues at a healthy level and households continue to deleverage. Retail sales excluding autos are growing at a solid pace, just a little below total retail sales including autos. And, importantly, consumer confidence appears to be gaining strength. Industrial activity has been strong in recent months. Industrial purchasing managers reported accelerating activity in February. The proportion of managers reporting improved orders is at its highest level in more than six years. Anecdotal accounts of manufacturers in the Southeast confirm this picture. Business investment on equipment and software, a bright spot for most of last year, slowed in the fourth quarter. But January orders for core capital goods were consistent with forecasts for another year of solid growth in business spending. Exports, which expanded strongly in the fourth quarter, should also be a significant contributor to final demand in 2011. The housing sector remains a soft part of an otherwise encouraging picture. House sales are still recovering and house inventories, although down from the peak relative to sales, remain elevated. Prices of homes were still falling at year end and may still be seeking a bottom. Residential construction picked up a little in January but remains very weak. In sizing up the likelihood of sustained growth, it's useful, I think, to compare early 2011 with early 2010. It is true that GDP growth was slower at the end of 2010 than at the end of 2009 and will likely be somewhat slower in the first quarter of this year than it was at the beginning of last year. Despite that, I have more confidence in the fundamental strength of the economy than I did a year ago. A year ago, the handoff between public-sector stimulus and private demand did not occur as smoothly as anticipated, and some of the growth in the early part of last year was clearly borrowed from later in the year. In addition, the GDP statistics at the end of 2009 and beginning of 2010 were dominated by changes in inventories. In contrast, as 2010 ended, changes in inventories arithmetically exerted a significant drag on GDP growth. Unlike the beginning of last year, recent GDP growth has been dominated by strong growth in private final demand. The stronger growth in consumption and investment by households and businesses along with the stronger demand for exports give me more confidence in the sustainability of economic activity than I had at this time last year.",
46,2,Business Economics,20 May 2011,https://link.springer.com/article/10.1057/be.2011.7,New Orders of Durable Goods Don't Forecast Shipments,April 2011,Daniel Bachman,,,Male,Unknown,Unknown,Male,"Victor Zarnowitz [1973] carried out the most comprehensive examination of the relationship between new orders and shipments. Zarnowitz demonstrated that there was a relationship between the timing of local maximums and minimums for new orders and shipments. He also estimated a number of different regression models, resulting in estimates of the approximate length of time it takes orders to be translated into shipments (as well as production, which he treated separately) both in the aggregate and by detailed industry. Zarnowitz spent a great deal of time demonstrating that the relationship between new orders and shipments varied among different industries. This is a key point, because it suggests that an emphasis on the aggregate series—as is typical in current media accounts—might be misleading. He noted that aggregate series might not provide clear indications of the true relationship because these series mix industries that produce to order with those that produce to stock. He admitted that these problems might even exist for detailed industry categories, as some specific subindustries and even factories might produce to order while other factories and subindustries do not, or do not have the same lags. Zarnowitz also made the very important point, often overlooked, that the relationship between orders and shipments is very likely not stable over time. However, he did not formally test the relationship between orders and shipments: his discussion rather assumes that the relationship exists, at least for industries that he determined are “produce to order” industries. Today's economist will note two things lacking in Zarnowitz's analysis. First, modern econometrics suggests different approaches to testing and measuring the relationship. This is not so much a question of simply applying more recent (and complicated) econometrics, but also casting the question in the language for forecasting, rather than focusing on coefficient significance. Second, Zarnowitz used data for periods from the late 1940s to the mid-1960s. For much of his research, the only Census data available covered the period 1953–1965, although he used other sources to extend his analysis. Today, Census publishes data from 1958 through the present. Since 1973, few economists have questioned the basic premise that NODG is a useful indicator of future activity. John Carlson [1973] observed that regressions between production and new orders did not yield any consistent lead or lag relationships between new orders and shipments for all manufactured goods. He stated “the only consistent pattern, and it was very strong, was of coincident movements between the two series.” Although published at the same time as Zarnowitz's research, few analysts seem to have taken note of the implications of this observation. In a much more recent paper, Christopher Jones and Salale Tuzel [2009] estimated a relationship between the ratio of new orders to shipments and stock prices. In examining the new order series, they find that “the growth rate of new orders, a series that is frequently cited in the press as an indicator of the business cycle trend … has little predictive ability beyond a horizon of just a few months.” This suggests that the change in new orders might be useful for predicting at least short-term movements in shipments. However, Jones and Tuzel do not pursue this particular issue, because they are more interested in the constructed ratio than with the relationship between the two series. Standard economic commentary on the Advance Durable Goods release does not acknowledge that aggregating the durable goods industries involves combining industries that produce to order with industries that produce to stock, that the timing of the relationship between orders and shipments might be unstable, or that NODG might suffer from significant data collection problems. The standard commentary assumes that NODG is the most important piece of information in the Advance Durable Goods release and interprets that information as providing information about future economic activity. That is the hypothesis that I will test in this paper.",8
46,2,Business Economics,20 May 2011,https://link.springer.com/article/10.1057/be.2011.8,Forecasting Recession and Slow-Down Probabilities with Markov Switching Probabilities as Right-Hand-Side Variables,April 2011,Gad Levanon,,,Male,Unknown,Unknown,Male,"In the selection of indicators for forecasting current and future recession probabilities, I chose to continue with The National Bureau of Economic Research (NBER)-Conference Board tradition of hand-picking a relatively small number of indicators, rather than using methods that use hundreds of indicators without an individual screening process for each of them. This approach increases the chances that the indicators chosen are good ones. It also makes it easier to track what indicators are responsible for the changes in the forecast. The hand-picking method for selecting indicators is partly based on judgment. This could be viewed as a disadvantage, as objective methods are not subject to human biases or tinkering. Here are the criteria I used for selecting the indicators: Part of the purpose of this paper was to use some of the relatively new indicators that were not used in other studies that forecast recessions. However, I wanted each indicator to have a time series that was long enough that I could evaluate its performance in several recessions. The shortest indicators I am using started in 1989, so they cover three recessions. I did not estimate the models using years prior to 1978 because prior to 1978 the number of indicators declined significantly. The model in this paper is based on monthly data. Quarterly indicators are not included. There are many good quarterly indicators, so in future research I will try to incorporate quarterly indicators to supplement the monthly model presented here. The main purpose of the indicators is to help in estimating current recession probabilities or to forecast future recession probabilities. The individual prediction ability of the indicators is an important factor in the selection process. For this purpose, I compared the model fit using simple probit models where the dependent variable was a dummy variable receiving the value of one in the current month for coincident indicators and three months ahead for leading indicators. I attempted to diversify the indicators across sectors, and I tried to avoid choosing more than one indicator from the same survey. For example, while there are many ISM indicators, I chose only the production component, for the current month estimation, and only the new orders component for forecasting future recessions.",2
46,2,Business Economics,20 May 2011,https://link.springer.com/article/10.1057/be.2011.6,"Regional Competitiveness: Labor-Management Relations, Workplace Practices, and Workforce Quality",April 2011,Jack Kleinhenz,Russ Smith,,Male,Male,Unknown,Male,"The literature on organized labor reaches back several decades and deals with both social and economic issues. Although many topics have been investigated, two sets of issues generally occupy center court in contemporary research:
 What is the microeconomic impact of unions on wages, firm performance, labor market efficiency, and welfare? From a broader perspective, are unions beneficial to the economy, firms, or workers? These two sets of issues are initially captured by Freeman and Medoff [1984]. According to Bruce Kaufman [2005, p. 555], this publication “is a widely acclaimed work on trade unions and the most influential book on the subject written in several decades.” Freeman and Medoff's research was a bold task – it presented theoretical arguments that countered acceptable economic principles of competition and markets. They conduct an empirical investigation of two propositions. The first is that, assuming a perfectly competitive marketplace, unions are able to raise wages above competitive levels resulting in fewer workers hired in the union sector and thus a loss of economic efficiency. This negative view is called the “monopoly face of unions.” The second proposition is called the “voice/response face” of unionization, which posits that the production worker (as a body) has knowledge that can benefit workplace conditions, productivity, and shareholders. Freeman and Medoff accept that labor market monopolies (unions) “distort an otherwise competitive wage structure” but allow for correction of subpar performance through organizational adjustment. That said, they argue that the “voice/response face outweighs the monopoly face” (going against the prevailing thinking) and contributes economic and social benefits. The two faces theory is widely cited and is often the launch pad for academic research that concentrates on one or several of the issues that Freeman and Medoff examined. Given Freeman and Medoff's prominence in the literature, our review examines research that has been undertaken since this path-breaking study was written. We use many of the issues they investigated to structure our literature review and to develop a framework for the implications from our review of the literature. With our review completed, we organized the topics around seven central themes: productivity, wage premium, job security, training, innovation, communication, and the union's role in economic development. In the following sections, we present general findings in, followed by discussion of the literature on which they are based.",1
46,2,Business Economics,20 May 2011,https://link.springer.com/article/10.1057/be.2011.1,"Glenn Hubbard and Peter Navarro, Seeds of Destruction: Why the Path to Economic Ruin Runs through Washington, and How to Reclaim American Prosperity",April 2011,Douglas J Lamdin,,,Male,Unknown,Unknown,Male,,
46,2,Business Economics,20 May 2011,https://link.springer.com/article/10.1057/be.2011.5,"Charles Wheelan, Naked Economics: Undressing the Dismal Science",April 2011,Gerald L Musgrave,,,Male,Unknown,Unknown,Male,,
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.20,From the Editor,September 2011,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.11,The Prospects for the U.S. Economy and Its Major Issues: A View from the Council of Economic Advisers,September 2011,Austan Goolsbee,,,Unknown,Unknown,Unknown,Unknown,,
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.19,Four Observations about the Federal Budget,September 2011,Douglas W Elmendorf,,,Male,Unknown,Unknown,Male,"The Congressional Budget Office (CBO) estimates that, if current laws remain unchanged, the budget deficit this year will be $1.4 trillion, or 9.3 percent of gross domestic product (GDP). That would follow deficits of 10.0 percent and 8.9 percent of GDP, representing the three largest deficits since 1945. As a result, debt held by the public will jump from 40 percent of GDP at the end of fiscal year 2008 to nearly 70 percent at the end of this fiscal year. Over the next few years—assuming that current laws are unchanged, as we do in our baseline projections—budget deficits as a share of GDP would drop markedly. Deficits would average almost 3½ percent of GDP from 2012 through 2021, totaling $6.7 trillion over the decade. As a result, debt held by the public would reach 76 percent of GDP in 2021. However, that projection understates the budget deficits that would occur if many policies currently in place were continued, rather than allowed to expire as scheduled under current law. For example, suppose instead that three major aspects of current policy were continued during the coming decade:
 First, the higher 2011 exemption amount for the alternative minimum tax was extended and, along with the alternative minimum tax brackets, was indexed for inflation; Second, that the other major provisions in December's tax legislation that affected individual income taxes and estate and gift taxes were extended, rather than allowed to expire in January 2013; and Third, that Medicare's payment rates for physicians’ services were held constant, rather than dropping sharply as scheduled under current law. Figure 1 shows that if those policies were extended permanently, deficits from 2012 through 2021 would average nearly 6 percent of GDP and would cumulate to nearly $12 trillion. Figure 2 shows that debt held by the public in 2021 would rise to almost 100 percent of GDP, the highest level since 1946. Deficits Under CBO’s Baseline or with a Continuation of Certain Policies, Compared with Past Deficits and Surpluses Federal Debt Held by the Public Under CBO’s Baseline or with a Continuation of Certain Policies, Compared with Past Debt It bears emphasis that under current policies federal fiscal policy is heading into territory that is unfamiliar to us and to other developed countries as well. Figure 3 shows that if we continue on the path to debt held by the public becoming nearly as large as GDP, our debt-to-GDP ratio will exceed that of most developed countries during the past several decades. To be sure, debt in other countries is also rising now, but it is unclear that we should take much comfort from other countries’ fiscal challenges. Instead, moving into unfamiliar territory creates significant risks. Debt Burden Across Countries in 2009Source: For the United States, debt held by the public net of financial assets. For other countries, general government debt net of financial liabilities as reported by the Organisation for Economic Co-operation and Development.",
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.18,World War II to 2011: Changes and Challenges in the Global Economy,September 2011,Robert Hormats,Ariel M Ratner,,Male,Male,Unknown,Male,"Today, I was at a celebration that Secretary Clinton and First Lady Michelle Obama held for the 100th anniversary of World Women's Day. I mention this because it was very impressive to see women leaders from all over the world there, and all of them had interesting things to say. Prior to this, I was talking to Secretary Clinton about World Women's Day, and I told her that I was going to give a speech to NABE. We then discussed the emerging role of women in transforming economics—a point that I have also addressed in previous speeches. When I was at Goldman Sachs, the firm published a report that quoted the Chinese saying that women hold up half the world. It is certainly true that women make up at least half the world—and in some countries a lot more than that. Think back on the American economy over the last hundred years and look at the major changes. A hundred years ago, the economic role of women was confined to very few areas. One of the most significant changes in American law and society that has enhanced opportunity and productivity over this century is the change in the economic status of women. There has been a growing and more significant role for women in the American economy, albeit that this development has not been perfect and still has a way to go. Much of the contribution of women remains unpaid or underpaid, despite its vital role in America's success. When you look at developing countries—particularly the Arab world, but many other parts of the world as well—one of the reasons some countries are not living up to their full potential—and this was described by a number of the women honorees at today's anniversary celebration—is that they do not give women the chance to participate fully, or even at all, in their economy. As a result, they are depriving themselves of half of their human capital, of half of their economic potential. It is increasingly clear, when you look at economies around the world and determine how likely they are to achieve progress over the next 5, 10, 15 years, that if they do not allow women the opportunity to participate fully in their economies, those economies are going to underperform dramatically. For strong economic performance, it is necessary to give women the education, opportunity, and the rights they need, as well as other reforms. One of many lessons from what is going on in North Africa and other parts of the Arab world is that liberation has to include liberation of women. It has to include women's rights and women's opportunities. If these countries are really going to be democracies and economies in which participation is meaningful, that participation has to include men and women, equally and fully. As far as American foreign policy is concerned, ensuring equal opportunities and upward mobility for women is very important for Secretary Clinton. It is also very important for me, and I think it is going to be increasingly important for all economies that want to grow in the future—in the Arab world in particular. Perhaps that will be one of the lessons from the people in Liberation Square in Egypt. They were not only men. Women were just as participatory, just as enthusiastic, and just as expectant that their rights will be provided to them as the men were. So, changes in opportunities for women are something we ought to keep a very close eye on because they will, to a large degree, determine the political and economic future of many countries.",1
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.13,Can the United States Transform Itself Into an Exporting Juggernaut?,September 2011,C Fred Bergsten,,,Unknown,Unknown,Unknown,Unknown,,
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.14,Will the Health Care Cost Curve be Bent? Where We Stand at the Start of 2011,September 2011,Charles S Roehrig,,,Male,Unknown,Unknown,Male,"The monthly health spending data I present was created by the Center for Sustainable Health Spending (CSHS) at Altarum Institute. Official estimates of national health expenditures are only available annually, and the estimates for 2009 came out in January 2011. So these monthly estimates that are current through January 2011 greatly improve the timeliness of our ability to track health spending. CSHS also produces regular monthly briefs on health employment and prices. The monthly GDP series is from Macroeconomic Advisors. Figure 1 shows the 12 month (year-over-year) growth in nominal health spending. The data span January 1990 through January 2011. The rate of growth has declined over this time period from 11 percent at the start to 6 percent during the mid-1990s to its current level of 4 percent. While this decline was interrupted between 2000 and 2004, the overall pattern suggests that perhaps the cost curve is already bending. 12 Month Growth Rates in Health Spending and GDPSource: Altarum monthly health spending estimates and Macroeconomic Advisors monthly GDP estimates. Note also how health spending growth seems largely unaffected by recessions, thus providing a stabilizing influence on the economy. Since the start of the most recent recession, health employment has grown by 6 percent while non-health employment is still down 7 percent [Altarum 2011]. The good news, then, is that the growth in health spending has been at historically low rates in recent months, and health spending can be credited with a positive impact on the economy during the recent recession. However, the growth in health spending remains problematic. The problem is that, over time, this pattern results in an increase in the share of GDP accounted for by health spending. As shown in Figure 2, in 1990 the health share was 12 percent; and as of January 2011 it was nearly 18 percent. This curve has a rather interesting pattern. But there are no obvious signs of bending. Since the year 2000, annual health spending growth has exceeded that of GDP by 2.6 percentage points. This same annual growth rate differential holds for the past five years and, roughly, for the past 40 years. Health Spending Share of GDPSource: Altarum monthly health spending estimates and Macroeconomic Advisors monthly GDP estimates. Why is this increased share a problem? One major problem is that about half of health spending is financed by the government. This is largely unavoidable because those at highest risk for health care needs are often the least able to afford insurance. As the share of GDP going to health spending increases, so does the share of GDP going to government spending. And this conflicts with current efforts at deficit reduction and with other budget priorities. Health spending increases are also a major problem for employers whose health insurance offerings represent a larger and larger portion of total compensation and who have commitments to fund health insurance for retirees. Families who are not covered by employer-sponsored insurance or by Medicare or Medicaid are increasingly unable to afford insurance due to increasing health costs. Figure 3 illustrates the broad problem for federal spending. Between 1970 and 2006, the increase in federal spending on Social Security, Medicare, and Medicaid from 4 percent to 8 percent of GDP was offset by a decrease in defense spending from 8 percent to 4 percent. Clearly, the next 4 percentage point increase in federal entitlement spending cannot be offset by cuts in defense. In fact, since 2006, entitlement spending as a share of GDP has grown by nearly 2 percentage points, and defense spending by 1 percentage point. In the coming decades, a continuation of historical growth rates in the health spending share of GDP would push federal spending beyond what most believe to be sustainable levels. Federal Entitlement and Defense Spending as a Percent of GDPSource: White House Office of Management and Budget. I conclude by commenting on the term “bend the health care cost curve”. There is an important distinction between bending the curve downward and shifting the curve downward. Many suggestions for saving health care dollars, such as reducing administrative waste and unnecessary services, represent a downward shift in the cost curve but do not address the underlying growth rate. While it is important to take advantage of these opportunities to shift the curve downward, it must be recognized that such steps only postpone the problems associated with health spending growth. The distinction between bending and shifting the healthcare cost curve is illustrated in Figure 4. While no one really knows the tolerable upper limit in the health spending share of GDP, in this illustration let us suppose that it is 25 percent.Footnote 1 Under the baseline projection in which health spending continues along its historical path with respect to GDP, the health spending share of GDP reaches 25 percent in 2027 as shown by the solid line. The dotted line represents a 15 percent reduction in health spending in 2014 (perhaps through eliminating a massive amount of administrative waste) but then a return to historical growth. This line reaches a 25 percent share of GDP in 2033. Thus, it postpones the problem for six years. The dashed line represents a bent health care cost curve that plateaus just under 25 percent of GDP. Thus, it represents a more permanent solution rather than a simple postponement. This is why health economists are searching for solutions to bending the cost curve rather than just shifting it [Sommers 2010]. Bending Versus Shifting of the Health Care Cost Curve In summary, this overview of the problem with health spending growth has made the following points:
 The historical rate of growth in the health spending share of GDP is unsustainable for a variety of reasons, including the impact on federal and state budgets, employers, and families. Many proposals to reduce health spending represent a downward shift in the cost curve but do not change the underlying rate of increase. These proposals are important, but they postpone rather than solve the problems associated with current health spending growth rates. Thus, the nation is seeking ways to bend the health care cost curve so that its long term growth rate is more in line with that of GDP.",3
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.12,Bending and Stretching the Health Care Cost Curve,September 2011,Gail R Wilensky,,,,Unknown,Unknown,Mix,,
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.10,Ford Motor Company's Global Electrification Strategy,September 2011,Ellen Hughes-Cromwick,,,Female,Unknown,Unknown,Female,,
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.17,Economic Challenges in the Clean Energy Supply Chain: The Market for Rare Earth Minerals and Other Critical Inputs,September 2011,Nayantara D Hensel,,,Unknown,Unknown,Unknown,Unknown,,
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.16,"Edward Glaeser, Triumph of the City: How Our Greatest Invention Makes Us Richer, Smarter, Greener, Healthier, and Happier",September 2011,John C Goodman,,,Male,Unknown,Unknown,Male,,4
46,3,Business Economics,08 September 2011,https://link.springer.com/article/10.1057/be.2011.15,"Tim Harford, Adapt: Why Success Always Starts with Failure",September 2011,Gerald L Musgrave,,,Male,Unknown,Unknown,Male,,
46,4,Business Economics,14 November 2011,https://link.springer.com/article/10.1057/be.2011.28,From the Editor,October 2011,Robert Crow,,,Male,Unknown,Unknown,Male,"Barbara Yates’ membership on the Business Economics Editorial Board precedes my own accession as Editor in 1999. In all of these years, she has been a reliable and thoughtful contributor, particularly as a referee for many articles. Her contributions will be missed. I wish her the best of fortune in pursuing a fulfilling retirement.",1
46,4,Business Economics,14 November 2011,https://link.springer.com/article/10.1057/be.2011.24,Nightmare on Kaiserstrasse,October 2011,Kenneth Rogoff,,,Male,Unknown,Unknown,Male,"Why do sovereign debt crises so often follow a wave of global banking crises? The reason is twofold. First, the aftermath of banking crises typically involves a large run-up in public debt (debt owed by the government). Reinhart and I find that for post-World War II deep financial crises, the average run-up in debt is 86 percent after three years, adjusted for inflation.Footnote 2 The run-up in debts follows from the deep and prolonged recessions that typically accompany banking crises (the main cause), but also from the cost of bank bailouts and, in some cases, fiscal stimulus. To save the economy, the government opens the deficit floodgates. But unfortunately, this sometimes involves extending itself beyond the county's sustainable borrowing capacity. A second important reason why sovereign defaults rise after a financial crisis is simply that the crisis shakes financial markets, typically leading to a loss of liquidity, a rise in risk premiums, and a generalized fall in asset values. Even in normal times, risky debtors are more likely to face problems when big shocks hit, even if they are not at the epicenter of the shocks. In many ways, the Eurozone crisis was an accident waiting to happen, but one that is more a result of the financial crisis than a primary initial cause of it. I have stated that after a deep financial crisis, some countries end up finding themselves in unsustainable debt situations. Of course, the truth is that there is no bright red line above which a country suddenly cannot pay its debt. Neither economic theory nor empirics provide such a bright red line. The theory of sovereign default, despite a large number of interesting and inventive papers, is still at a very early stage.Footnote 3 While theory lays out some useful general principles (countries that are very open to trade and with very highly integrated financial markets are less likely to default), most of our practical understanding of sovereign default still rests on empirical benchmarks. Reinhart and Rogoff [2009a, Ch. 2] argue that a country's default history is an important factor in determining how much debt it can handle without running into trouble. Emerging market economies with a history of problems may run into debt default difficulties at ratios of external debt to GDP as low as 30–40 percent, whereas the rare economies that have pristine or almost pristine records of repaying sovereign external debt can run up debt levels more than double that without running into trouble. (Even after many decades old defaults can affect a country's debt thresholds, with a long half-life.) Indeed, benchmark debt levels are often a better guide to vulnerability than either interest rate risk premiums or ratings agency assessments. A number of studies find that interest rate risk premiums are very poor guides to assessing default risk, at least not very far in advance. Interest rates on sovereign debt typically rise significantly only within a month or two, or at most a year, of a default event. This reflects both the knife edge stability of confidence and expectation as well as the problem of “hidden debts,” which often come jumping out of the woodwork in a crisis [Reinhart and Rogoff 2009a, Ch 2; 2011]. Ratings agencies, for many reasons, are under pressure to provide relatively smooth changes in assessments, which often in turn make ratings a lagging indicator of problems (and recovery). The fact that relatively distant defaults are still correlated with a country's general financial fragility reflects a broader point about “graduation” to advanced economy status (again, a concept developed in my work with Reinhart). In general, it takes many decades, indeed often centuries, for a country to make the transition from emerging market to an advanced economy [Qian and others 2011]. No class of economy has yet durably graduated from inflation crises, but advanced economies do not suffer nearly as much from defaults on their external debt. In some ways, the Eurozone experiment was an attempt to see if the process of “graduation” could be accelerated by integrating emerging markets such as Greece and Portugal into the euro system (not to mention Spain, Ireland and Italy). Rather than accept the rule of thumb that emerging markets take generations to develop financially, economically and socially into advanced economies, European leaders pressed on with monetary unification as a leading edge of political unification. Skeptics, including it must be said many American economists, were told that they simply did not understand the conviction and commitment of Europeans to the single currency project (which by implication also meant the power and influence of the European elite, since polls typically showed a far more mixed view of the euro among the masses, particularly in Germany). The result of the euro's introduction is now familiar enough. Thanks to their new status in international markets as advanced economies, the periphery European countries were able to reach external and public debt limits on the outer envelope of anything ever observed for emerging markets. Portugal's public debt of 90 percent of GDP may seem tame next to Greece at 140 percent, but both numbers are massive by emerging market standards. Of course, with Greece, Portugal, and Ireland combined representing only 6 percent of Eurozone GDP, it is possible that Europe will be able to cover its losses much the way West Germany carried East Germany after unification. But countries such as Italy and Belgium have massive debts over 100 percent of GDP, and Spain's debt might be as high when contingent claims from municipalities and the banking sector are accounted for, particularly if real estate prices continue to fall. Although European leaders have argued that the euro could never unravel—and they may yet prove right—it is interesting to at least think through the thought experiment of how it might happen, catastrophic as that might be.",2
46,4,Business Economics,14 November 2011,https://link.springer.com/article/10.1057/be.2011.27,U.S. Workers Delaying Retirement: Who and Why and Implications for Businesses,October 2011,Gad Levanon,Ben Cheng,,Male,Male,Unknown,Male,"This section relies on previous work by Levanon and others [2011]. Many Americans have delayed retirement—or wished to—after witnessing their household net worth fall and retirement preparedness waver. However, older workers who lost their jobs and faced difficulty in finding new ones in the weak labor market may have retired earlier than desired. When looking at aggregate retirement numbers, it is difficult to disentangle these factors. Yet for businesses, the increase in workers consciously planning to delay retirement is perhaps the more important factor, as strategic workforce planning hinges on the retirement decisions of the existing workforce. In this study, we define workers who retired in the past year as full-time workers who identified themselves as retired one year after they were initially surveyed. “Retired” in this context means no longer in the labor force in any capacity. Furthermore, to partly isolate those who decided to retire on their own from those who retired because they could not find work, we refined the definition by excluding individuals who were more likely to have been laid off. In particular, we excluded participants who were unemployed, working part-time for economic reasons, or stated a desire for a job despite identifying themselves as retired during the three months prior to the final interview.Footnote 2
",7
46,4,Business Economics,14 November 2011,https://link.springer.com/article/10.1057/be.2011.23,The Small Business Sector in Recent Recoveries,October 2011,Michael J Chow,William C Dunkelberg,,Male,Male,Unknown,Male,"The Great Recession is notable for the severity of its labor market dislocations compared with other postwar recessions. In the worst stages of the recession, the national unemployment rate peaked at a level above 10 percent, a threshold surpassed only once since WWII, during the 1980–82 recession. The recent fall in the unemployment rate to 9 percent, which by itself signals improving labor market conditions, belies continued difficulties faced by the American workforce. Part of the fall in the unemployment rate is due to workers exiting the workforce: the labor force participation rate recently fell to 63.9 percent, its lowest reading since 1984. Additionally, both the share of long-term unemployed and the duration of long-term unemployment are at post-WWII highs. Small firms were especially hit hard by the recession and reacted by cutting costs, including those for labor. The NFIB data set documents changes in the level of employment, hiring plans, and vacancies at small businesses on a quarterly basis (monthly since 1986), providing information on labor market trends at the firm level. The reductions in small business employment during the fourth quarter of 2008 and in 2009 were the largest ever recorded in the history of the NFIB data series.Footnote 10 Firms, reacting to lower-than-expected sales and falling profits, sought to return to profitability by reducing spending on factors of production. The data also suggest that these spending cuts may also have been made with a view toward rebalancing supply in anticipation of a prolonged decrease in customer demand. Related to the dramatic reduction in employment was a “fire sale” to raise cash and eliminate a huge excess inventory produced by the sudden increase in the saving rate in 2008:Q4. This produced an unprecedented surge in price cutting that many felt signaled the start of “deflation.” However, once the excess inventories were eliminated, price cutting was reversed: in 2011, prices started rising sharply. Figure 2 shows time paths for the average quarterly change in small firm employment for the past four recovery periods starting at the business cycle trough. The net change in employment per firm hit an all-time low of −1.02 workers per firm in 2009:Q2, coinciding with the most recent trough. The previous recession low was −0.56 workers per firm in 1982:Q3, about half as severe. Although the economy stopped contracting in the second quarter of 2009, changes in small firm employment remained negative throughout 2009 and most of 2010. The streak of negative quarterly job growth paused briefly in 2010:Q4 when the net change in employment was zero, a measure indicating that firms experienced neither net gains nor net losses in employment for the period. Although employment change turned slightly positive in 2011:Q2 (reported employment change in the preceding months), any suggestions that job creation in the small business sector had finally turned the corner were short-lived, as employment change was once again negative (−0.15 worker per firm) in 2011:Q3 (the July survey reports job change in 2011:Q2). Average Change in Employment (Per firm) (Recovery from NBER trough) Unemployment is a lagging indicator, and continued job losses among small firms five quarters after a new business cycle begins are not unusual. Looking at the past four recessions, only in 1983:Q3 were net employment changes positive five quarters following a cycle trough. What is noteworthy about job losses in the present recovery is their severity. In the second half of 2009, small firms shed jobs at a faster rate than during the initial stages of any of the 1982, 1991, or 2001 economic recoveries. Net employment losses at small firms averaged −0.81 workers per firm in 2009:Q3 and did not fall below −0.5 workers per firm until four quarters later. In contrast, quarterly net employment change averaged −0.17 at the start (cycle trough) of the other three recoveries and never fell below −0.27 during any of the three recovery paths. As can be seen in Figure 3, most of the volatility in job growth came from changes in layoffs. New hires demonstrated less volatility, but both indicators were clearly more extreme in this recession than in any prior downturn. Job reductions have returned to historically “normal” levels, but reports of increases remain historically low. Percent of Owners Increasing or Reducing Employment A second noteworthy observation is that while the rate of job creation today is roughly the same as it was at this stage in previous recoveries, a considerable amount of time elapsed before the current time path converged to the level of its peers. Four quarters passed dating from the 2009:Q2 trough before job creation “caught up” with statistics from previous recoveries. Counting backward from 2011:Q1, net employment changes per firm were at or below zero for 11 consecutive quarters. This is the second longest such streak on record. Only during the 1980–82 period was there a longer period of sustained small business job loss (14 quarters). Thus, in terms of both the scale of job separations and the duration of sustained negative job growth, the impact of this recession on small business employment has been particularly acute, consistent with expectations of economic recovery following a financial crisis. Despite recent improvements in labor market indicators, the NFIB data argue against the return of robust job growth at small firms in the near term. Supporting this view are two measures of labor demand that are trending at noticeably lower levels than they did following previous recessions. The first indicator, net job creation plans as shown in Figure 4, measures expected future demand for labor by small firms. This recovery period witnessed the first occasion in history that this indicator remained negative for four quarters following an employment trough. Net job creation plans returned to positive territory in 2010:Q3 (where it has since remained) when more owners began planning to hire workers than shed them. However, the trajectory of planned hiring has remained below where it was in previous recoveries. Job Creation Plans (Net percent planning to increase employment) (Recovery from NBER trough) This does not augur well for the economy's quick return to a state of full employment. For at least two and a half decades (the period of time for which data exist), the small business sector has been the source of most new jobs created in the United States. Lacking evidence of major structural change that might alter this dynamic, if conditions remain static, one may expect a prolonged and gradual reduction in the rate of unemployment. More rapid improvements in employment will only be achieved by policy that delivers more customers and offers a more positive view of the future. The second NFIB labor demand measure is the reported number of unfilled job openings at small firms (Figure 5).Footnote 11 In general, a larger number of unfilled openings indicates tighter labor markets. As with job creation plans, the number of unfilled job openings during this recovery has consistently lagged the number of unfilled openings reported in previous recoveries. Since 2009:Q2, the net percentage of firms reporting positions they were unable to fill rose gradually from 9 to 14 percent in 2011:Q2 before dipping to 12 percent in the most recent quarter. Although this measure is performing better than during the depths of the recession, it is still below any other previous recovery. Unfilled Job Openings (Recovery from NBER trough)",9
46,4,Business Economics,14 November 2011,https://link.springer.com/article/10.1057/be.2011.21,An Economic Perspective on Patent Licensing Structure and Provisions,October 2011,Thomas R Varner,,,Male,Unknown,Unknown,Male,"The data set used for this study consists of 1,458 patent licenses and patent assignment agreements included as exhibits in filings to the SEC. The data set consists of 474 bare patent licenses, 820 patent-plus-know-how licenses, and 164 patent assignment agreements. I excluded from the data set agreements that include a grant for patent rights but whose focus was not specifically on a patent grant. Among the agreements I excluded were university research agreements, consulting and employment agreements, merger and acquisition agreements, product re-branding or re-marketing agreements, and joint venture agreements. These other agreements, although certainly worthy of economic study in their own right, often include provisions that address issues and contingencies not directly related to patent technology licensing, which is the focus of this paper. I also excluded from the data set patent settlement agreements resulting from litigation since such agreements often arise from different circumstances than nonlitigated patent licenses. Patent licenses arising out of patent infringement litigation were identified by searching for parties to the agreements in an electronic database of U.S. federal court records.Footnote 2 All of the patent licenses in the data set were obtained by searching company filings submitted to the SEC including registration statements, annual and quarterly reports, proxy statements, and current reports. The SEC requires that registrants disclose in their filings “material definitive agreements not made in the ordinary course of business.”Footnote 3 While all of the agreements in the data set were submitted by filers to the SEC, some of the agreements predate the filer's initial registration by months or years, and thus may involve parties in which neither was an SEC registrant at the time of the agreement. I have reviewed numerous portfolios of firms’ technology licenses as part of consulting engagements (performed under nondisclosure agreements) and testifying engagements (performed under protective orders of a court). My experience in reviewing these portfolios suggests that the data set used for my analysis in this paper may not be representative of this larger set of undisclosed agreements. Thus one should not necessarily infer that agreements in this data set are representative of all patent license agreements, and findings based on this analysis may not be relevant to the universe of undisclosed agreements that do not meet the SEC's threshold for material definitive agreements. Many of the agreements filed with the SEC are available only in redacted form, that is, they may be marked by the filer as “Confidential Treatment Requested.” In some industries, such as the pharmaceutical industry, over 75 percent of the technology agreements filed with the SEC are granted confidential treatment. Notwithstanding the confidential treatment granted by the SEC, many of these agreements can be obtained in unredacted form through FOIA requests to the SEC after the grant of confidential treatment has expired. Included in this study's data set are approximately 350 patent licenses in unredacted form previously granted confidential treatment and obtained through FOIA requests to the SEC.Footnote 4 Once the patent licenses and patent assignment agreements were collected, I summarized certain information from each agreement. This information included: (1) the type of agreement (bare patent, patent-plus-know-how, or patent assignment agreement); (2) caption information (names of parties and date of agreement); (3) the type of licensor and licensee (commercial or for-profit business, individual, educational, nonprofit, or government entity); (4) the type of technology being licensed and the nature of the licensed products; (5) exclusivity provisions; (6) the amount and nature of fixed fees, maintenance fees, and milestone payments; (7) the maximum and minimum running royalty rates; (8) sublicensing royalty rates; (9) minimum annual royalties; (10) the presence of equity consideration; and (11) provisions related to royalty reductions. For purposes of this paper, I define a “fee” as any fixed financial payment and a “royalty” as a payment contingent on the level of sales. A royalty may be expressed as a percentage of sales (more commonly defined by net sales as opposed to gross sales), a percentage of profits, or dollars per unit of licensed product sold. I searched for technology license agreements that included a reference to “patents” among all SEC filers over the period 1994 through 2010 and broadly grouped the agreements into high-tech (computer software, computer hardware, electronic components, instrumentation, and telecommunication industries), life sciences (medical device and pharmaceutical industries), and other industries. Of the 1,458 agreements in the data set, approximately half are dated 1999 or earlier. The agreements are skewed toward earlier years for two reasons. First, while an agreement may be submitted to the SEC shortly after its effective date, an agreement also may be submitted as part of a registration statement (SEC Form S-1), in which case the agreement may predate the filing by several years. Second, the data set includes previously redacted agreements in which a grant of confidential treatment has expired—a period that is often five or more years after the agreement is first filed with the SEC. Thus, although approximately 25 percent of the data set is composed of previously redacted agreements, the data set includes a higher proportion of these agreements in earlier years.",8
46,4,Business Economics,14 November 2011,https://link.springer.com/article/10.1057/be.2011.29,Rising Crude Oil Prices: The Link to Environmental Regulations,October 2011,Philip K Verleger Jr,,,Male,Unknown,Unknown,Male,"The simple, stylized model is characterized by two simple components that describe product demand and crude oil qualities. I postulate simple log-linear product demand curves for clean and dirty products: where demand is characterized by the letter D, price by the letter P, and the subscripts “C” and “D” denote clean and dirty. Price elasticities are represented by “b” with the subscript “1” referring to clean product price elasticity and “2” referring to the dirty product price elasticity. The model also postulates two crude oils, denoted as L for light sweet crude and H for heavy sour crude oil. I assume that each crude produces a percentage of clean product C and dirty product D. The percentage of clean product produced by light sweet crude is denoted by a
L
 and the percentage of clean product produced by heavy sour crude by a
H
. The percentage of dirty product produced is given by (1−a
L
) and (1−a
H
). The model is based on the following assumptions:  a
L
>a
H
. The supply of each type of crude is fixed at any moment in time, and the supply of light sweet crude is determined in a competitive market for a sufficiently long period. This comports with the actual market's observed behavior, where the time required to boost production is seen as long and where the prevailing price is well above the level where producers would curtail output. Producers set the heavy sour crude supply one month ahead by establishing a fixed differential to the light sweet crude price. This differential is adjusted over time to maintain the desired output level. This assumption is discussed further below. Product prices are determined in a competitive market. This assumption comports with the Federal Trade Commission's findings in various merger investigations, which have found that refining is generally competitive. It also comports with observed industry behavior, where integrated companies—having seen the profits previously earned through their control over crude supply vanish—chose to exit refining in many world regions. Buyers for competitive refiners bid for a crude based on the values they can obtain for the products produced from the crude. The product prices are derived by solving equation (1) for price under the assumption that crude supply is fixed and known in advance for any period. Letting S
LS
 and S
HS
 represent the supply of light sweet and heavy sour crude, the supply and demand of clean and dirty products are given by Equations (1) and (2) provide a complete description of the market under the assumption that product prices are set competitively, supply is fixed for the period, no part of crude or product production is removed from the market to be stored in stocks, and no crude or product is withdrawn from stocks. These equations also provide a means of characterizing the market response to changes in environmental regulations, changes in producer actions, and changes in refining characteristics. The product prices predicted from the model are used to establish crude prices. Given the assumption that refining markets are competitive, the price paid for light sweet crude, P
LS
, is given by while the price paid for heavy sour crude, P
HS
, is given by Here we assume that the sweet crude oil market is competitive while the heavy sour market is dominated by OPEC, a group that attempts to operate as a cartel. Like any cartel, OPEC might limit production or engage in more sophisticated management approaches. Below, I explain that OPEC does the latter. Below, I also defend the assumption that the light sweet crude market is competitive or almost competitive. This description of the crude market is at variance with many studies. Smith, for example, has published several articles on how OPEC countries seek to maximize their income. More recently, Fattouh [2011] and his colleagues at the Oxford Institute of Energy wrote a long (and, in my view, irrelevant) article on oil pricing that by definition assumes oil price determination starts at the crude market and works down to products.Footnote 1 Neither Smith nor Fattouh nor other academics writing on the subject ever seem to contemplate the economic process by which oil is bought or sold. The one exception may be my own work [Verleger 1982]. As noted above, the assumption here is that refining is a highly competitive business in most regions, an assumption affirmed by various FTC studies and by the actions of the multinational companies that have sold or closed refineries in many regions. (Table 1 presents a partial list of refinery sales and closures by large multinationals over the last decade.) For refineries to survive today, their traders must refrain from bidding more for crude oil than they can get from the products they sell in the competitive product market. The aggregate bids of all traders in the competitive refining business establish the prices paid and reported for crude oils. Oil-exporting countries influence oil prices by adjusting volumes of the various crude oil types put on the market. For more than 20 years, they have accomplished this by setting price differentials for their crudes against market-specific benchmark crudes. For example, Saudi Aramco, the marketing arm for Saudi Arabia, will announce the differential to be paid by buyers of each type of Saudi crude delivered to a specific market. The differential can be represented abstractly as Refiners, operating in a competitive market, indicate the amount of oil they wish to purchase each month from Saudi Arabia and other oil-exporting countries after the differentials are made public. Producing countries then tailor their output to meet the resulting nominations. On April 5, 2011, for example, Saudi Aramco informed European buyers they would pay $9.75 per barrel less than Brent “B-Wave” (the volume-weighted average price of Brent futures traded on the Intercontinental Exchange or ICE) for 45 to 54 days after a heavy crude cargo purchased from the Saudis was loaded on ship. Brent is a light sweet crude traded in a competitive market. In this instance, traders at refineries would use the expected prices of products to predict the anticipated price of light sweet crude and, using a variant of equations (4) and (5), calculate the value they expected for the differential between light sweet crude and Arab Heavy. They would then base their nominations for the amount of heavy sour crude on this assessment. The hypothesis seems to be confirmed when OPEC output levels are compared with price spreads announced by Saudi Arabia. Figure 1 shows the price spreads announced for Arab Heavy sold to Western European buyers from 2002 through 2011. These discounts have ranged from $1.88 to $13.80 per barrel. Discount to Brent B-Wave Offered to Buyers of Arab Heavy for Delivery to Europe, February 2002 to May 2011 Figure 2 presents the discount as a percentage of the outright price. Figure 2 reveals that, over the last year, the Arab Heavy discount has ranged between 5 and 7 percent, suggesting a Saudi policy to hold a specific line regarding price. Looking back, one can observe that Saudi Arabia aggressively raised the discount in 2008 in an attempt to cool the market and support the global economy after the Lehman Brothers bankruptcy. They boosted discounts first to 10 percent and then 22 percent in December 2008. Saudi Discount to B-Wave as a Percentage of the Outright Price, February 2002 to May 2011Source: PKVerleger LLC.",3
46,4,Business Economics,14 November 2011,https://link.springer.com/article/10.1057/be.2011.22,Electric Power Transmission and Distribution Equipment,October 2011,David A Petina,Michael Murphy,Andrew C Gross,Male,Male,Male,Male,"Modern economies run on electricity; about one-third of the world energy is used for electric power. In the United States, electricity is integral and indispensable for homes, factories, and all types of commercial establishments. Any interruption in the steady supply of electric power is unwanted and can result in much economic loss. The nation's electric grid is a complex network of generation plants, transmission lines, and distribution systems. The North American grid consists of three major networks (Eastern, Western, and Texas) with 200,000 miles of high voltage transmission lines (110 kv to 765 kv) and million miles of distribution lines. The federal government is currently pushing policies to upgrade the nation's aging electrical network. In mid-June 2011, Secretary of Energy Steven Chu called for a speed-up in the development of the next-generation electrical network [Vastag 2011]. This so-called smart grid would deliver power more efficiently, coordinate traditional and renewable sources, reduce consumption, and alleviate the number and length of outages. T&D equipment should then be able to respond immediately to demand, yet maintain an instant response system to connectivity and other operating variables. At present, the nation's current electric grid is so creaky, according to Chu, that “Edison would feel at home with most of today's power system.” Per U.S. Department of Energy (DoE) figures, there were 349 blackouts in 2005–2009 vs. 149 in 2000–2004. According to the article just cited, the new policies immediately drew skepticism, in part because the Electric Power Research Institute (EPRI), an independent institute funded by the electric utility industry, estimates the cost of implementation to be in the $338 to $476 billion range. Nonetheless, the federal government is determined to push the electric power companies to invest in new technologies, provide loans to upgrade the transmission lines in rural areas, and fund “smart grid” R&D activities from a DoE research hub. According to Dr. Chu, the technology underpinning the current U.S. electrical grid lags behind that of other countries, including China and Ireland. Their systems have more efficient high-voltage transmission lines, up-to-date distribution equipment, and better integration of power generation from traditional and renewable sources. If the administration's policies are implemented, they would provide a major boost to T&D investment. However, the fate of enabling legislation is highly uncertain. The supply and demand situation for electric power T&D equipment is clearly affected by both cyclical and secular macroeconomic variables. The most notable influence was the recent recession and ongoing slow recovery, causing slower spending patterns by households, business, and governments. The overall weakening in the economy has forced business firms to shelve major expansion plans through 2013. A significant share of T&D equipment demand is related to fixed investment activity. Both electric utilities and other industrial-commercial users are slowing down their purchases of switchgear, transformers, and other types of equipment.",2
46,4,Business Economics,14 November 2011,https://link.springer.com/article/10.1057/be.2011.25,"Kajal Lahiri, Transportation Indicators and Business Cycles",October 2011,John E Silvia,,,Male,Unknown,Unknown,Male,,
46,4,Business Economics,14 November 2011,https://link.springer.com/article/10.1057/be.2011.26,"Timothy J. Bartik, Investing in Kids: Early Childhood Programs and Local Economic Development",October 2011,W Steven Barnett,,,Unknown,Unknown,Unknown,Unknown,,
47,1,Business Economics,15 February 2012,https://link.springer.com/article/10.1057/be.2011.38,From the Editor,January 2012,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
47,1,Business Economics,15 February 2012,https://link.springer.com/article/10.1057/be.2011.30,What Can the Developed World Learn from the Latin American Debt and Mexican Peso Crisis?,January 2012,Guillermo Ortiz,,,Male,Unknown,Unknown,Male,"To offset the effects of the crisis, authorities in the epicenter implemented both monetary and fiscal countercyclical policies. Without these measures, the real and financial consequences would probably have been much worse. Despite such efforts, the stimulus adopted in advanced economies has not proved as effective as in previous recoveries. In fact, Figure 1 shows that GDP in the six largest industrialized countries is still below precrisis peaks. This illustrates both the severity of the recession and the slow recovery process in the developed world. Also, labor markets have not responded as expected. Gross Domestic Product (Index 1Q08=100).Source: U.S. Bureau of Economic Analysis and Eurostat. The reasons for this lie mostly in the very rapid growth of household and financial sector leverage in the major developed economies prior to the crisis, the unprecedented large credit and/or property bubbles, and the subsequent adjustment and de-leveraging process in which we are still immersed. Countercyclical policies implemented were not without a cost. In supporting stressed economies, government finances deteriorated at an alarmingly fast pace in developed countries. Policymakers had to sustain banking systems and compensate for private-sector de-leveraging. As a consequence, public debt and fiscal deficits as a percentage of GDP dramatically increased. For instance, in 2007 Ireland's public debt was 25.0 percent of GDP and is expected to surpass 110.0 percent of GDP in 2011. For the United Kingdom, debt in 2011 is projected to practically double the 43.9 percent of GDP reached in 2007. For the United States and Spain, debt-to-GDP ratios are expected to increase by about 30 percentage points from 2007 to 2011.Footnote 1 According to the BIS, if governments do not make substantial fiscal policy changes, deficits worldwide will soar by 2020. Ireland's deficit could reach 13 percent of GDP; that of Japan, Spain, the United Kingdom, and the United States, 8 percent; and that of Australia, Germany, Greece, the Netherlands, and Portugal, between 3 and 7 percent. But even more worrisome is the impact on indebtedness. Without significant policy changes, the debt-to-GDP ratio would go above 300 percent in Japan, 200 percent in the United Kingdom, and 150 percent in Belgium, France, Ireland, Greece, Italy, and the United States [Cecchetti, Mohanty, and Zampolli 2010]. This is clearly an unsustainable path. As alarming as these numbers appear, the need for fiscal consolidation does not hold the same level of urgency across the developed world. For instance, the United States is not facing an immediate fiscal crisis. The U.S. downgrade by Standard and Poor's led to a flight into treasuries rather than away from them. The United States is facing an employment and growth crisis and a problem of debt sustainability over the medium term. In truth, at least conceptually, it is much easier to visualize fiscal consolidation in the United States than in most other developed economies that need to set government debt on a sustainable course. This is not least because U.S. government spending (measured by total spending as a percent of GDP) is smaller than in the rest of the G10 countries.Footnote 2 The real challenge for the United States is to stimulate economic growth and job creation in the short term while at the same time credibly addressing the issue of fiscal sustainability in the medium term. Let's turn to the situation in Europe. In the weeks prior to presenting this paper (September 11, 2011), capital markets plummeted while uncertainty continued to grow, as shown in Figure 2. Yet the European fiscal crisis has been a serious source of concern for almost two years now. The initial causes of the fiscal crisis were narrowly focused on the issue of funding pressures for some countries, particularly Greece. However, contagion to other countries in the periphery spread swiftly, and more recently began to contaminate even core countries due to links between the sovereign debt crisis and the banking sector of the region. Actions taken by European politicians and the European Central Bank (ECB) seem to belong more to the realm of political economy than the crisis-solving manual. In other words, the European political system has turned a manageable economic situation into a nearly unmanageable political problem. Capital Markets1 (Index 01-Jul-2011=100)1/MSCI Indexes.Source: Bloomberg. A little bit of history is useful to understand how these countries have found themselves in such a dilemma. In 1992, the Maastricht Treaty set the rules for all member-countries to achieve Economic and Monetary Union (EMU): low inflation, low interest rates, and controlled public spending. The Stability and Growth Pact (SGP) agreed on in 1997 specified that the same rules should apply once the euro was launched. However, in 2003 some countries, even among the largest economies in the Eurozone such as France and Germany, broke the rules. Instead of taking strong action against them, in 2005, the European Council agreed to give the SGP more flexibility. Furthermore, even these new rules were challenged in 2007. So the loosening of the fiscal anchor was already taking place before the global financial crisis occurred. It is worth keeping in mind that the architecture of the Exchange Rate Mechanism (ERM) did not include the framework for the proper functioning of the monetary union. Those missing pieces are crucial for answering key questions in the current context. For instance, apart from no fiscal coordination, there is no defined mechanism for banks’ recapitalization, no debt restructuring procedure, or clear debt reduction mechanism. Nor is there even an exit route from the euro for countries that cannot or will not adjust. A comprehensive set of rules defining how to deal with overly indebted countries without risking the overall health of the system is vital for the European monetary union. In order to preserve the union, European leaders need to establish an effective political mechanism for policy coordination and fiscal decision-making. In the absence of these rules, the introduction of the euro merely substituted foreign exchange risk for credit risk for some of its weakest members. This imperative has been borne out in each and every step of the crisis. The timeline of events of the European crisis can be split into three distinct phases: first, denial; second, recognition; and finally, implementation and action.
 Denial: During the early stages of the crisis, policymakers were divided over whether Greece needed IMF support. Many believed that Greece was a European problem that had to be handled within the existing European framework, and that default or even debt restructuring in the euro area was not an option.  Apparently, authorities were trying to avoid any possibility of restructuring, as its potential consequences for the euro system were (and probably still are) unknown. They also aimed to ring-fence other countries in the periphery. This, in the end, proved impossible. Recognition: It was a long and arduous process for European leaders to pass from anger and disbelief to finally recognize the seriousness of the problem. Gradually, they have realized that weak public finances in several countries undermined the stability of the Union and that the sovereign debt crisis threatened to spread to the banking system. However, the political dynamics in different blocs of countries are still far from converging.  Taxpayers in Germany and other northern countries are reluctant to finance potential resource transfers to the periphery, while citizens in southern countries are increasingly unhappy with austerity measures. Implementation: Today, European authorities are finally facing the reality of the crisis, including the institutional need for a much more coordinated fiscal policy. Proposals have ranged from the strengthening of the existing mechanism with stricter sanctions for deviations (Maastricht and the SGP), to the establishment of a European finance ministry, to the issuance of a Eurobond. The basic notion that the preservation of the Eurozone requires much closer fiscal integration and a substantial surrendering of sovereignty is slowly sinking in. This may be the ultimate solution, but it will certainly take time, and meanwhile the debt crisis must be dealt with. Three things, in my view, are clear from this process:
 In the case of Greece, the problem was initially characterized as one of liquidity rather than solvency both by the IMF and the European authorities. This made sense in order to buy time, ring-fence other countries (especially Spain), and avoid widespread contagion. The strategy failed mostly due to the widespread perception that the authorities have consistently been “behind the curve.” Policy implementation has been reactive to market stress, and the dynamics between the political and economic dimensions of the problem seems to have further deteriorated. Recession fears in Europe have risen, exacerbated by widespread austerity measures and the perception of an increasingly complex political decision-making process. It is becoming clear that the final problem is one of loss absorption—who pays for the losses accumulated so far and those that will be added until the problem is finally restored. The longer it takes to restore market confidence that a solution is in place, the larger the losses and the spillover effects to the rest of the world. Although the European leadership has rhetorically emphasized the issues of growth and competitiveness, in practice the programs put together so far have mostly focused on austerity measures and fiscal consolidation. Achieving competitiveness and growth in the context of a fixed exchange rate system requires more drastic structural measures and, in most cases, a painful “internal devaluation” process. The flip side to this is that austerity with no foreign exchange devaluation is almost always doomed to fail. This last point is particularly important and can be illustrated with the Latin American debt crises of the 1980s.",
47,1,Business Economics,15 February 2012,https://link.springer.com/article/10.1057/be.2011.35,“Real-Feel” Inflation: Quantitative Estimation of Inflation Perceptions,January 2012,Michael J Ashton,,,Male,Unknown,Unknown,Male,"A complaint that noneconomists frequently raise about the CPI is that the practice of adjusting price changes for changes in the quality of the surveyed item—in particular when the method used is hedonic adjustment—doesn’t mesh with the real-life experience of inflation. A consumer might complain, “I paid $150 for a digital camera three years ago; I just paid $200 for a new one – there is surely no deflation in digital photography!” The fact that the new digital camera has 14.1 megapixels of resolution while the old one had 5.1 megapixels is often considered significant by the consumer, but it doesn’t get mentally accounted for as a price decrease, even though the consumer is getting much more for the money. Another gripe concerns the adjustments made to correct for substitution bias. Consumers will tend to change their consumption patterns as relative prices change, even if the absolute level of prices does not change. For example, if a consumer who consumes 50 percent chicken and 50 percent beef sees the price of beef fall 10 percent while the price of chicken rises 10 percent, he is likely to consume less chicken and more beef even though the price index consisting of equally weighted chicken and beef has not changed.Footnote 6 This would be an example of an “upper-level” substitution of items that are significantly different. A more subtle, and less controversial, adjustment is made for “lower-level” substitution of very similar items, such as two different brands of orange juice.Footnote 7 Because the CPI is intended to track the cost of a static standard of living,Footnote 8 real-life consumers who tend to drift gradually to higher standards of living will experience price changes associated with quality improvements and opportunistic substitution, as well as those due to inflation as we understand it. All three types of price increase will tend to be recorded mentally as inflation, even though from the standpoint of the index two of them are not.Footnote 9 Our goal here, however, is not to improve on the CPI but to converge on the consumer's perception of inflation. It is conceptually fairly easy to deduct the adjustments made for substitution and hedonics. Johnson, Reed, and Stewart [2006] demonstrate how to use other BLS indices to estimate the effects of lower- and upper-level substitution. To estimate the lower-level substitution effect, the authors compared the regular CPI—which includes a geometric-means adjustment—to the CPI-U-XL, an experimental index that used the pre-1999 method for computing lower-level substitution effects.Footnote 10 To estimate the upper-level substitution effect, the authors compared the regular CPI to the chained CPI (C-CPI-U). The BLS began producing the C-CPI-U in 2002; it uses actual consumer behavior, rather than a model of it, to determine expenditure weights. Therefore, the difference between the CPI-U-XL and the C-CPI-U represents a reasonable estimate of the total of the substitution effects.Footnote 11 For our purposes we can ignore the upper-level substitution effect, because the “official” CPI (CPI-U) does not incorporate the inflation-lowering adjustment made in the C-CPI-U. The CPI-U-XL is not readily available, but it can be obtained on request from the BLS. For the year ended September 2010, the difference between the CPI-U-XL and the CPI-U was 0.247 percent. The effect has been reasonably stable, ranging between 0.2 and 0.4 percent over the last decade or so. Johnson, Reed, and Stewart's [2006] estimate for the 1999-2004 period was 0.28 percent, and Greenlees and McClelland [2008] noted that unpublished results for the longer period from December 1998 to December 2007 resulted in a 0.27 percent effect. I will use that estimate as a constant, in the absence of a convenient way to access the CPI-U-XL. Adjusting for the misperception of quality adjustments is more difficult, partly because this one concept affects many different parts of the CPI in different ways and because different adjustments are made depending on the type of quality change. Johnson, Reed, and Stewart [2006] state, “the net effect of hedonics from 1999 onward… on the All Items index is estimated to be less than 1-hundredth of 1 percent per year, specifically +0.005 percent.” This result, however, obtains from the fact that hedonic adjustment lowers the CPI for a number of categories whose weights total to around 0.7 percent of the index, but increases the CPI somewhat for Rent, Owners’ Equivalent Rent (OER), and Apparel, in addition to several smaller categories. Rent, OER, and Apparel have a combined weight of around 31.5 percent of the CPI. So, the large downward adjustments to the lightweight categories are finely balanced by very small upward adjustments to the heavyweight categories. It seems reasonable to suppose that consumers would object to thinking of the quality improvements in computers as deflation, while not being particularly opposed to the idea that their homes tend to deteriorate over time. As a coarse adjustment, I would suggest simply removing the quality adjustments that tend to depress the index.Footnote 12 However, as a practical matter, the unadjusted CPI for the various categories is not available outside the BLS. Moreover, as discussed above, the effect of the quality adjustments on the overall index is quite small. Johnson, Reed, and Stewart [2006] estimate the aggregate impact of quality adjustment as around +0.005 percent. This implies that all of the categories where quality adjustments tend to lower CPI must sum to around −0.095 percent, so as a fair approximation I will use an adjustment for quality, ADJQual=+0.1 percent. Thus, the corrections for misperceived quality and substitution would be:  Note that, because I am subtracting the adjusted inflation rate for the category from the unadjusted rate, quality adjustments that tend to depress measured inflation rates will be positive in the MAX operator—we are extracting the adjustment to add back to the rate, but only for adjustments that pull down the rate. Given present BLS methodologies, the main effect here is to add back the OER age bias adjustment, which is the single most significant quality adjustment in that direction (by a wide margin).",3
47,1,Business Economics,15 February 2012,https://link.springer.com/article/10.1057/be.2011.36,"How Does the FOMC Learn About Economic Revolutions? Evidence from the New Economy Era, 1994–2001",January 2012,Richard G Anderson,Kevin L Kliesen,,Male,Male,Unknown,Male,"During the last decade or so, a consensus has arisen among economists that trend growth rates for potential real output and labor productivity increased around 1995. Figure 1 shows that the acceleration in productivity, as measured over different horizons, appeared to occur in the mid-1990s. Because the 1990s acceleration of labor productivity was unforeseen and challenged extant views, its recognition was delayed. Typical is the 1996 Economic Report of the President, prepared during 1995. In the report, the Council of Economic Advisers projected that labor productivity in the private nonfarm business sector would increase at an average annual rate of 1.2 percent from the third quarter of 1995 to the end of 2002. This estimate extrapolated recent experience: from 1973 to 1995 productivity had grown at an average annual rate of 1.4 percent. Initial productivity measurements published during 1995 and 1996 were consistent with the Council's forecast, and did not signal an increase in productivity growth. Growth of Labor Productivity The incoming data during 1995 and 1996 clashed with widespread anecdotal firm-level evidence that spending on information and communication technology (ICT) equipment was increasing productivity. To some analysts, the productivity acceleration was no more than a cyclical response to more robust economic activity. But, at the same time, large investments in ICT equipment could not be ignored.Footnote 4 Analysts quickly identified decreases in semiconductor prices, and the prices of business equipment built with them, as the primary cause of the productivity acceleration. But, prices also had fallen in the past—would rapid price decreases be sustained? Further, some analysts asked if the increased investment in ICT was largely a change in the type of producers’ durable equipment being purchased, rather than a genuine capital deepening. A typical after-the-fact assessment is Jorgenson, Ho and Stiroh [2002]: “… the story begins with an increase in total factor productivity growth in the IT-producing sectors (computer hardware, software, and telecommunications), which led to falling relative prices and induced capital deepening in IT equipment.” By 2001, the Council of Economic Advisors had increased its projection of the annual growth of structural labor productivity to 2.3 percent per year. Indeed, the 2001 recession dispelled doubt regarding the staying power of the productivity acceleration. During the recession, productivity growth defied historical experience by increasing rather than decreasing.Footnote 5 Other forecasters, including many included in Blue Chip Economic Indicators and the Federal Reserve Bank of Philadelphia's Survey of Professional Forecasters (SPF), were even more optimistic.Footnote 6 As seen in Figure 2, private-sector forecasters surveyed for the SPF since 1992 did not increase their estimate of structural productivity growth until 2000, when they raised their estimate from 1.5 percent to around 2.5 percent. Long-Term Expectations for Output and Productivity GrowthSource: Federal Reserve Bank of Philadelphia",
47,1,Business Economics,15 February 2012,https://link.springer.com/article/10.1057/be.2011.37,State Minimum Wage Differences: Economic Factors or Political Inclinations?,January 2012,William F Ford,Travis Minor,Mark F Owens,Male,Male,Male,Male,"States tend to increase their minimum wage, above the federally mandated level, as time passes without a federal increase. This is illustrated by Figure 1, which shows the number of states with minimum wage levels above the federal level each year. In this paper, we investigate the primary factors driving these changes. Although there could be multiple reasons to justify a minimum wage increase, the arguments put forth to the voting public by politicians typically fall into two broad categories. Number of States with Higher than Federal Minimum Wages by Year One stance is that the purchasing power provided by the current minimum wage is not able to compete with the minimal cost of living and therefore reduces all workers earning minimum wages to below the poverty level. The Center for Policy Alternatives [2007] maintains that the federal minimum wage is not effective because many workers do not have sufficient earnings to cover the cost of basic needs.Footnote 5 Or, they argue that the federal wage floor is too low to be binding for many employers and is thus ineffective. Concerns that full-time workers with families are earning the minimum wage are still near or below the poverty line, as well as normative beliefs about the skewed nature of the U.S. income distribution, drive such movements for higher minimum wages.Footnote 6 Some groups argue that minimum wage levels should be directly tied to cost of living measures, effectively creating a “living wage” that will help workers across all industries. For example, President Barack Obama stated at the Take Back America Conference in 2007, “Let's finally make the minimum wage a living wage. Let's tie it to the cost of living so we don’t have to wait another 10 years to see it rise.” (See Ballot Initiative Strategy Center [2006] and The Center for American Progress [2007], for a description; and Sander and Williams [2005] for an assessment of living wages.) The other position deals primarily with concerns for fairness to American workers. Numerous politicians have maintained that it is unfair to pay workers the minimum wage rate as it stood before the passage of the Fair Minimum Wage Act of 2007. A report from the Office of Senator Edward M. Kennedy [2006] states “No one who works hard for a living should live in poverty, and the federal government owes all minimum wage workers a well-earned raise.” Additionally, Bill Clinton and Al Gore, in their book Putting People First [1992], state “It's time to honor and reward people who work hard and play by the rules… No one who works full time and has children should be poor anymore.” In both of these arguments there is little to no mention of what the minimum wage affords people, rather the language usage such as “owes” and “rewards” implies that minimum wages should be raised as a matter of fairness or political ideology. We attempt to examine both of these factors separately in the determination of state-level minimum wages, to see whether either is more dominant in the passage of legislation. While previous papers have estimated the impact of political leanings on minimum wage laws, few have tried to disentangle the basic need and fairness effects. Levin-Waldman [1998] concludes that the minimum wage is not only an economically motivated law, but is also highly influenced by politics. Waltman and Pittman [2002] also estimate the effects of wealth, politics, and public ideology on the adoption of state-level minimum wages. They argue that minimum wages are mainly symbolic since they typically have a small effect on the economy and are determined primarily by public beliefs rather than wealth or political influences. Their measure of political influence ranks a state on a Likert scale of 0 to 5. Instead, we will utilize a percentage scale derived from Congressional voting records that is a combination of the Liberal Quotient scores tabulated for each state by Americans for Democratic Action (ADA). The ADA records all votes by both U.S. Senate and U.S. House members. The individual proposals up for vote are categorized as liberal or conservative. The ADA sums the number of times each member voted for the liberal side of the proposal and divides this by the total number of votes cast to create a score for each member on the percentage of the times they voted liberal. State averages for both the House and Senate were calculated, and these were combined to give a single score for each state. ADA records these scores as the Liberal Quotient (LQ) of a state. LQ is recorded on a scale from zero to one, with one being the most liberal score a state can receive. This variable captures the political leanings of a state at the federal level and is exogenous since these votes cannot directly influence an individual state government's minimum wage policy.Footnote 7 We argue that actual votes are a superior measure because they allow for a more accurate description of a state's political climate than a discrete, categorical variable obtained from survey data. The sample size utilized in this paper also greatly exceeds the one utilized by Waltman and Pittman [2002] and therefore allows for more robust results. Our measure of political leanings is similar to the measure utilized by Kau and Rubin [1978], who conclude that this factor does influence the probability of voting for federal minimum wage legislation. We differentiate from Kay and Rubin in two important ways. First, we estimate the effect of the cost of living and political leaning on state level minimum wages. This distinction provides us with more variation in our panel of states than a previously analyzed time series of voting on federal minimum wages. Second, and potentially more importantly, we are directly concerned with the estimate of the cost of living on minimum wages; whereas Kau and Rubin included this measure primarily as a control variable. They utilize average hourly earnings to account for the cost of living, which is probably endogenous with respect to the federal minimum wage. Instead, we account for the cost of living by using an interstate housing price index, which we argue is more exogenous than the average wage level. Finally, our data are for the most recent minimum wage cycles. Other existing literature on minimum wage legislation is primarily concerned with the effects these laws have on economic efficiency and their distributional consequences. Although we take all consequences of minimum wage laws as given, it is interesting to note the findings in this branch of the literature. Card and Krueger [1994, 2000] examine a natural experiment, with variations in minimum wage laws across states. They observe no negative consequences from an increase in the minimum wage level, with no loss in employment or any significant increase in prices, even though standard economic theory predicts that a binding minimum wage will create unemployment and potentially raise prices.Footnote 8 However, studies since then have looked not only at prices and employment effects but also at numerous other economic variables that may be adversely affected (see for example Burkhauser and others [1996] and Neumark and others [2005]). Neumark and Wascher [2003] estimate that exposure to binding minimum wages may lower school enrollment, thus having a negative impact on labor force skill acquisition. Chaplin and others [2003] also find that teenagers’ school enrollment declines in the presence of a binding minimum wage. More recently, Neumark and Nizalova [2007] examine the longer-run implications of a minimum wage, and they estimate that prolonged exposure to the minimum wage as a teenager has detrimental effects later in life, which includes less labor force participation and lower long-term wages.Footnote 9 Neumark and Nizalova [2007] also demonstrate that these decisions have important long-term implications for workers. The motivation behind the minimum wage change likely does not matter to the workers, who simply respond to the incentives presented to them. However, the disincentives to human capital formation that are introduced may be more substantial in cases where cost of living differentials are not the primary reason for changing the state law. Thus, we might expect lower human capital acquisition in areas covered by the legislation when it is driven primarily by political concerns.Footnote 10 As such, a state minimum wage increase could have a different economic impact, depending on existing conditions in the labor market. Increases in the national minimum wage are more likely to be binding in low-income and low cost of living areas, and less likely to be binding in high-income and high cost of living areas. This, in turn, may lead to different schooling and long-term employment outcomes in different locations.",6
47,1,Business Economics,15 February 2012,https://link.springer.com/article/10.1057/be.2011.32,Constructing Global Production Activity Indices: The Chemical Industry,January 2012,Saswati Mahapatra,Thomas K Swift,,Unknown,Male,Unknown,Male,"The chemical industry is complex, providing more than 70,000 products. It is this complexity and multitude of products that presents challenging measurement problems. Although many of these products find limited use, there are some 8,000 high-production-volume chemical products manufactured in the United States alone. Each of these features different end-use market dynamics and can often be produced by a variety of distinct production processes and feedstocks (or raw materials). For example, Figure 1 provides a condensed version of the product chain for ethylene, a bellwether building block. The figure is a summary even for ethylene, as it has literally thousands of derivative products. Simplified Ethylene Flow Chart Source: American Chemistry Council. Another feature of the chemical industry is its globalization, a phenomenon that originated in the nineteenth century when American, German, Belgian, and British companies led the way as they exploited new technologies. Until the 1970s, globalization was based largely on trade rather than investment. In recent decades, however, trade gave way to investment as the main driver of globalization in this highly competitive and mature industry [Freeman 1999]. Currently, the chemical industry in the United States is the second largest in the world, after China, accounting for 18 percent of the global industry's $4.12 trillion output. This is down from 27 percent in 1985 and 33 percent in 1965. Clearly, industry growth is occurring in other nations, most notably in China, the resource-rich nations of the Middle East, and other emerging nations. There is a need for data to measure industry activity not only in the United States but also globally.",
47,1,Business Economics,15 February 2012,https://link.springer.com/article/10.1057/be.2011.31,Economists in a World of Shaky Recovery,January 2012,Richard Koss,,,Male,Unknown,Unknown,Male,,
47,1,Business Economics,15 February 2012,https://link.springer.com/article/10.1057/be.2011.33,"Jeffrey G. Williamson, Trade and Poverty: When the Third World Fell Behind",January 2012,Jan Kmenta,,,Male,Unknown,Unknown,Male,,
47,1,Business Economics,15 February 2012,https://link.springer.com/article/10.1057/be.2011.34,"William J. Holstein, The Next American Economy: Blueprint for a Real Recovery",January 2012,Thomas A Hemphill,,,Male,Unknown,Unknown,Male,,
47,2,Business Economics,27 April 2012,https://link.springer.com/article/10.1057/be.2012.9,From the Editor,April 2012,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
47,2,Business Economics,27 April 2012,https://link.springer.com/article/10.1057/be.2012.5,On the Importance of Education,April 2012,Richard L Wobbekind,,,Male,Unknown,Unknown,Male,"We are all familiar with the standard growth model. In an increasingly competitive world, we are always seeking ways to increase productivity. We know the key ingredients for growth are increases/improvements in:
 Physical capital … capital deepening or increased capital per worker Human capital … increased levels of education and skill formation Technological progress … Innovation, creation and diffusion of new ideas, and entrepreneurship While education is only mentioned specifically under human capital, it plays a significant role in product processes and product innovation, as well. Clearly, a strong and complex interrelation exists among education, capital intensity, technological advances/innovations, and efficiency gains, which makes it difficult to assess the precise productivity contributions of each factor. That said, academic research has credited education with up to one-third of the productivity growth in the United States from the 1950s to the 1990s [Jones 2002].",3
47,2,Business Economics,27 April 2012,https://link.springer.com/article/10.1057/be.2012.7,What Small Countries Can Teach the World,April 2012,Jeffrey Frankel,,,Male,Unknown,Unknown,Male,"For some countries that gained their independence in the twentieth century, the Soviet Union became the country to emulate. Toward the end of the century, of course, the socialist model lost its appeal. It became evident to all that Russia and the other members of the Soviet bloc had failed miserably. At the same time, east Asian economies had ridden capitalism to prosperity. Although the issue of market economics vs. socialism had been settled by 1990, there remained competing models of capitalism. At the time, many thought the lesson of the 1980s had been that Japan's variant of capitalism was the best model and that other countries around the world should and would follow it. The model was said to include such institutions as: strategic trade policy, administrative guidance, relationship banking, life-time employment, collusive industrial groupings (keiretsu), and corporate governance that seeks to maximize the capital stock and long-term market share rather than short-term profits.Footnote 1 As it turns out, there is indeed such a thing as accumulating too much capital. The Japanese model quickly lost its luster in the 1990s as the speculative bubbles of the late-1980s burst and the economy sunk into two decades of stagnation. From this, many drew the lesson that the U.S. variant of capitalism had been the best model all along. The touted institutions included American-style corporate governance: securities markets, rating agencies, accounting standards, generous compensation for CEOs (tied, for example, to options), and pursuit of profitability and share prices rather than sheer size. The United States began the 1990s with military triumph in Kuwait and ended it with the longest economic expansion in its history. Other countries should and would follow the American model. The American model in turn lost its attractiveness in the decade of the 2000s. Its reputation for competence and integrity took some heavy blows, including the Enron-type accounting scandals of 2001, falling incomes among blue collar workers, the subprime mortgage crisis and ensuing recession, massive budget deficits, the occupation of Iraq and associated failures, and disasters in the Gulf of Mexico. Where should countries look for a model, now? Some will respond, “China.” It is undeniable that the rate of growth sustained by China over the last three decades is a miracle of history. But I find it difficult to think of many Chinese institutions that I would recommend that other countries try to copy [Williamson 2012]. Even today, China's status as the world's second largest economy owes more to the size of its population than to its GDP per capita, which has still to rise above the median level in global rankings. We are accustomed to looking to large countries for innovations that push out the frontier of governance. But some smaller countries, and countries on the periphery, have experimented with policies and institutions that could usefully be adopted by others. Small countries tend to be trade-dependent, and open to new ideas. Countries that are small, newly independent, remote, or emerging from a devastating war often find it easier politically to institute radical reforms than do the United States or other large, established countries. Not all the experiments will succeed. But some will. Those innovations that succeed may be worthy of emulation by others. Let us begin with examples from countries that are small in size, but have relatively high per capita incomes.",7
47,2,Business Economics,27 April 2012,https://link.springer.com/article/10.1057/be.2012.6,Disentangling Beta and Value Premium Using Macroeconomic Risk Factors,April 2012,William Espe,Pradosh Simlai,,Male,Unknown,Unknown,Male,"Our data consist of portfolios that include all nonfinancial NYSE, AMEX, and NASDAQ firms in the monthly Center for Research in Security Prices (CRSP) tapes, for the period of January 1972 through December 2008. We obtained the accounting information from the COMPUSTAT database and do not use firms with negative BE. We matched the accounting data for all fiscal year ends in calendar year t−1 with the returns for July of year t to June of year t+1. This ensures that the accounting data are known before we examine individual stock returns. The six-month gap between fiscal year end and the return helps us to avoid the look-ahead bias [Banz and Breen 1986]. Our BE/ME ratio is the ratio of COMPUSTAT book value for fiscal year end in calendar year t−1 to CRSP market value at the end of December of year t−1. We defined size as the natural logarithm of CRSP market value for June of year t. To be included in the sample, a firm must have a CRSP stock price for December of year t−1 and June of year t and book equity data for year t−1. Following the standard procedure of empirical research, we do not include firms until they have appeared on COMPUSTAT for two years. This helps us to avoid the survival bias [Banz and Breen 1986]. For the portfolio returns based on BE/ME, we use the BE/ME breakpoints for year t, which are the NYSE BE/ME quintiles at the end of June of year t. The stocks in the portfolios are value-weighted. Our dependent variable is based on the return of each of the 10 BE/ME and 25 double-sorted portfolios,Footnote 5 which are the intersections of five size portfolios and five BE/ME portfolios, from January 1972 through December 2008. By construction, in our one-dimensional sorts, a value (high) portfolio consists of the top 10 percent of stocks ranked by BE/ME ratio, and a growth (low) portfolio contains stocks in the bottom 10 percent. As a result, the high-low represents the return of a long value and short growth portfolio, commonly known as the zero-net-investment (spread) portfolio. We use the procedure described in Fama and French [1993] to construct mimicking risk factors. The mimicking risk factors in returns relating to size and BE/ME are based on the intersection of two sizes and three BE/ME groups. Basically, the risk factor in returns mimicking size (SMB—small minus big)Footnote 6 is a zero-net-investment (spread) portfolio that is long in small-firm stocks and short in large-firm stocks. Also, the risk factor in returns mimicking BE/ME (high-minus-low (HML)) is a zero-net-investment (spread) portfolio that is long in high BE/ME stocks and short in low BE/ME stocks. The values of the risk factors related to size and BE/ME equity are obtained from Kenneth French.Footnote 7 Finally, for the market proxy, we use the return of CRSP's value-weighted index on all NYSE, AMEX, and NASDAQ stocks.",1
47,2,Business Economics,27 April 2012,https://link.springer.com/article/10.1057/be.2012.3,The Long Wave Revisited,April 2012,Thomas W Synnott 3rd,,,Male,Unknown,Unknown,Male,"If the center line of Figure 1 is taken as the long-run average rate of inflation, the “third-phase” represents a period not only of slowing inflation but of low inflation or actual deflation. (This was particularly acute in Japan.) One might expect in this environment that corporate profitability would be under intense pressure. This was certainly true of the 1876–90 period, when virtually the entire American railroad industry became bankrupt and was reorganized by J.P. Morgan. Another severely affected industry was cotton textiles, as new technologies and high costs in New England led to a shift of the industry to the South. Of course, declining agricultural prices and wholesale prices generally put great pressure on farmers and small businesses, leading to continuing political struggles in the latter part of the 19th century. Table 1 lists some characteristics of the Third Phase periods (1876–90, 1932–46 and 1988–2002) drawing heavily on the 1876–90 experience as well as what we knew, in 1994, about our current period. As anticipated in my 1995 paper, the Third Phase (1988–2002) proved exceptionally difficult. The savings and loan crisis in 1989, the Mexican peso crisis in 1995, the Asian financial crisis in 1997–98, the Long-Term Capital Management problem in 1998 (Lowenstein 2000), and the dot.com boom and bust in 1998–2002 roiled world financial markets and led the Federal Reserve (and the Bank of Japan) to pursue aggressively easy monetary policies. In the United States this monetary reflation led to a surge in house prices, a boom in housing construction, and a huge bad-credit expansion. The conclusions of my 1995 paper (pages 15–16) mainly came true—particularly that U.S. economic growth was “likely to lag behind other centers—especially those in Southeast Asia.” This period also saw “increased political turmoil in the industrialized world as restive electorates strive to find political solutions to high unemployment.” However, despite significant monetary stimulus, overall inflation in the United States and other industrialized countries remained low, and interest rates were much lower than expected. First, the federal government did not shift spending toward long-term investment. Rather the U.S. involvement in two costly and protracted wars has resulted in a crowding-out of productivity-enhancing investments in infrastructure. Second, the U.S. economy, following the collapse of the housing boom and the crisis in the financial system, has yet to get onto a sustainable growth path.",
47,2,Business Economics,27 April 2012,https://link.springer.com/article/10.1057/be.2012.4,A U.S. Manufacturing Strategy for the 21st Century: What Policies Yield National Sector Competitiveness?,April 2012,Thomas A Hemphill,Mark J Perry,,Male,Male,Unknown,Male,"In this section, we will review the U.S. manufacturing industry back to 1947, and specifically address the issue of whether, or to what extent, America's industrial sector has stagnated or declined and to what extent it has thrived and expanded. We start by analyzing real U.S. manufacturing output from 1947 to 2010 using GDP-by-Industry data from the Bureau of Economic Analysis [2011]. Figure 1 displays manufacturing value-added in inflation-adjusted 2010 dollars from 1947 to 2010, along with a trend line that reflects the growth rate of U.S. real manufacturing output of about 2 percent annually since 1947. U.S. manufacturing has been highly cyclical, and output has fallen sharply during recessionary periods, especially during the two recessions in 1980 and 1981–82 and during the last two recessions in 2001 and 2007–09. However, the manufacturing sector has always recovered from contractionary periods, and the most recent recovery from the 2007–09 recession is no different. Although not shown on the graph, the manufacturing sector is actually leading the current U.S. economic recovery, both in employment gains and output gains. The key point here is that the U.S. manufacturing sector has continued to expand over time on an absolute basis; and except for business cycle downturns, it has trended upward consistently in the postwar period. For example, the U.S.$1.775 trillion constant-dollar peak value of manufacturing in 2006 and 2007—before the recession—was double the size of the manufacturing sector in 1963. Real Manufacturing Value Added, 1947–2010 To put the size of the U.S. manufacturing sector in perspective, the approximately U.S.$1.72 trillion of manufacturing output in 2010 would have placed America's manufacturing sector, if it were a separate country, as the world's ninth largest economy, just behind eighth-ranked Italy and larger than the entire national economies of India, Canada, Russia, or Spain [United Nations 2011]. Therefore, the relative size of the U.S. manufacturing sector cannot be discounted in terms of its influence and importance for the United States and world economies overall, and there has really been no overall or systematic long-run decline in U.S. manufacturing in an absolute sense. In fact, the United States has led the world in manufacturing output for more than 100 years. Based on the most recent data available from the United Nations, the United States produced just slightly less manufacturing output in 2010 than first-place China (measured in current U.S. dollars), almost twice as much as third-place Japan, three times as much as fourth-place Germany, and roughly 10 times as much output as Mexico or Russia [United Nations 2011]. To further appreciate how much manufacturing output is produced in the United States, consider that the United States produced almost as much total manufacturing output in 2010 as the manufacturing sectors of Germany, Italy, Brazil, South Korea, France, and the United Kingdom combined. It should also be noted that in addition to being the world's largest manufacturer for more than a century, the United States has also produced a relatively stable share of world manufacturing output over the last 30 years. For example, the United States produced about 18.24 percent of world manufacturing output in 2010, compared with its 21 percent share of world output in 1980. And until 2008, the United States produced more than 20 percent of world manufacturing output in every year. Only in the period 2008–10 has the U.S. share of world manufacturing dipped below 20 percent [United Nations 2011]. Although China produced slightly more manufacturing output than the United States in 2010, it is estimated that China employs more than 100 million workers in its manufacturing [Bureau of Labor Statistics 2009], compared with only 11.5 million workers in U.S. manufacturing, and thus on an output-per-worker basis, the United States is still far ahead of China. It will be many decades or longer before China comes close to matching the U.S. manufacturing sector on an output-per-worker basis. The strongest evidence of a decline in the U.S. manufacturing sector, and the one area that has captured most of the media attention and thereby influenced public opinion, is the reality of manufacturing job losses over the last three decades. Figure 2 graphically displays the historical relationship between annual U.S. manufacturing output in 2010 dollars and the average annual number of manufacturing payroll employees in the United States from 1947 to 2010. Manufacturing Output vs. Employment, 1947–2010 In the 32-year period between 1947 and 1979, there was a clear and positive upward trend in both manufacturing output and employment. During this period, manufacturing employment expanded by about 1 percent per year on average, which increased total manufacturing employment by five million more workers during this period to the historical record-high level of 19.5 million by 1979. Importantly for our discussion here, real manufacturing output in the United States increased at an annual rate of 3.2 percent during this period, or three times faster than employment, which meant manufacturing output on a per-worker basis approximately doubled during this period, from about U.S.$35,000 per worker (in 2010 dollars) in 1947 to more than U.S.$70,000 per worker by 1979—an annual rate of about 2.5 percent—as shown in Figure 3. Real Manufacturing Output per Worker, 1947 to 2010 Productivity for U.S. manufacturing workers then accelerated to a 3.3 percent average annual growth after 1980, and by 2010 the manufacturing output per worker doubled again, from about U.S.$70,000 in 1981 to almost U.S.$149,000 by 2010. This ongoing increase in worker productivity, owing to technological advances like robotics and other forms of automated assembly, like the computer numerical control innovations introduced in the late 1970s, has allowed the United States to continue to expand manufacturing output year after year (except for recessions) but with fewer and fewer manufacturing workers since 1980. By 2010, the accelerating manufacturing productivity gains combined with the recession were responsible for the number of manufacturing workers falling to only 11.5 million, the lowest manufacturing employment level since 1941, and 8 million workers below the 1979 peak of 19.5 million. Despite the decline in manufacturing employment levels over the last 30 years, the United States has been able to produce increasingly higher levels of output in almost every year and maintain its status as the world's largest manufacturer through 2009. However, it is the decline in manufacturing employment that leads to the perception that American manufacturing is in a perpetual state of decline, without taking into account that both the manufacturing sector's output and productivity continue to show strong signs of growth and expansion. An important issue related to the dramatic increases in manufacturing-worker productivity is the decrease in the manufacturing sector's value as a share of GDP. From a postwar high of 28.3 percent in 1953, manufacturing's share of U.S. GDP has gradually decreased over time, falling to an historic low of 11.2 percent in 2009 before increasing to 11.7 percent in 2010. The 0.5 percent increase in manufacturing's share of GDP in 2010 was the largest annual increase since a 0.6 percent increase back in 1976. At the same time, the share of private services-producing industries increased from 47.3 percent of GDP in 1953 to 68.5 percent in 2010. Over the same period, manufacturing jobs as a percent of total U.S. payroll employment fell from 32 percent to less than 9 percent, whereas service sector jobs increased from 61 percent to 86.3 percent of total payroll employment. Manufacturing's decreasing shares of total output and total employment have also supported the public's perception that America's manufacturing sector is in a state of decline. However, there are two key points to be made here about the declining share of manufacturing in the total U.S. economy. First, the downward trend in manufacturing's share of GDP and employment, and upward trend in the service sector's share of GDP and employment, is not unique to the United States but is a global phenomenon. Based on international data from the United Nations [2011], global manufacturing output as a share of world GDP fell from about 27 percent in 1970 to 16.25 percent in 2010, while in the United States it fell from 24.3 percent to 12.9 percent (Note that the United Nations [2011] data on manufacturing are slightly different from the Bureau of Economic Analysis [2011] data). Therefore, we can conclude that at the same time that some manufacturing output for low-cost, low-value-added production has shifted from the United States to China and elsewhere, it is also the case that manufacturing's share of global output is declining. When we hear complaints that “nothing is made here anymore,” it is not always the case that another country is now manufacturing output that used to be produced in the United States. However, it is also increasingly the case that the United States and most other countries are no longer producing as much manufacturing output relative to the overall size of the national and global economies. This global shift could be reflecting the transition from a traditional manufacturing-intensive Machine Age economic model to more of a services-intensive Information Age economic model. Thus, even if the United States was a closed economy with no low-cost imported manufactured goods, it is very likely that manufacturing's share of national income and employment would have followed a downward trend as the U.S. economy evolved into a modern services-based economy. Second, the significant and ongoing efficiency and productivity gains in the manufacturing sector of the U.S. economy over time have contributed to a declining manufacturing output/GDP ratio by lowering the real prices of manufactured products relative to other products and services. According to Chicago Federal Reserve economist William Strauss [2010], the productivity growth of the manufacturing sector of 3.3 percent annually between 1980 and 2009 considerably exceeded the 2 percent growth in productivity for the economy as a whole. As a result of disproportionately greater productivity gains in the manufacturing sector relative to the service sector, the prices of manufactured goods relative to services became increasingly more affordable for consumers and contributed to the manufacturing sector's declining share of total output. According to Strauss [2010]:  As one example, inflation (as measured by the Consumer Price Index) averaged 3.7 percent between 1980 and 2009, while at the same time the rise in prices for new vehicles averaged 1.7 percent. So while the number (and quality) of manufactured goods had been rising over time, their relative value compared with the output of other sectors did not keep pace. This allowed manufactured goods to be less costly to consumers and led to the manufacturing sector's declining share of GDP. This pattern of increased productivity leading to lower prices, employment, and a declining share of GDP, is analogous to the trend that has been taking place in the agriculture sector of the U.S. economy for over 200 years. In the early 1800s, more than 80 percent of both U.S. employment and output were directly tied to a relatively inefficient (by today's standards), labor-intensive agriculture sector. Food products were very expensive and consumed a large part of a typical household's income. Over time, advances in technology revolutionized farming, resulting in the same trends we observe today in manufacturing: huge increases in farm worker productivity, reduced farm employment both in absolute numbers and as a share of total employment, significantly lower prices, and a reduced share of food expenditures in both household income and GDP. Accompanying those trends has been a continually increasing size of U.S. agricultural output over time in an absolute sense, as technological advances allow us to produce more farm output over time with fewer workers year after year. In the same way that it would be unreasonable and inaccurate to talk about the “decline or demise” of farming in the United States by focusing only on the declines in farm employment and agricultural output as a share of GDP, without acknowledging the increasing overall size of agricultural output, it is also incomplete to think of the U.S. manufacturing as being in a state of decline. It is important to note that the manufacturing sector made significant gains in both employment and output in both 2010 and 2011, as noted above. For example, following 12 straight years of employment declines, U.S. manufacturing employment increased by 109,000 jobs in 2010—the first annual increase in manufacturing employment since 1997—and increased by another 225,000 jobs in 2011. The last time that U.S. manufacturing companies added jobs for two consecutive years was in 1996–1997. According to the manufacturing component of the Federal Reserve System's [2011] monthly measure of industrial production, the U.S. manufacturing sector grew by 4 percent in 2011, or more than two times the 1.7 percent rate of real GDP growth rate for the economy overall last year. Many other measures of national and regional manufacturing output—Institute for Supply Management manufacturing, Empire State Manufacturing Index, Richmond Federal Reserve Manufacturing Index, motor vehicle sales, new orders for manufactured durable goods, outbound loaded export containers, and record-high after-tax manufacturing profits in 2011—all indicate a strong recovery for U.S. manufacturing that gained momentum throughout 2011. Far from being an industry in decline, U.S. manufacturing has shown strength and resiliency as it emerges from the Great Recession at the forefront of the U.S. economic recovery. It is now recognized again as one of America's most vital and vibrant industries, with an increasingly bright future. Numerous media reports have documented what is being called a “manufacturing renaissance.” Part of the current and future strength of American manufacturing could be explained by the global shift in manufacturing that has leveraged the relative cost advantages of manufacturing low-value goods in developing countries like China, while advanced economies like the United States have increasingly specialized in producing higher-end, higher-skilled manufacturing in areas like aerospace, pharmaceuticals and medicine, industrial machinery, medical and scientific equipment and supplies, computers, software and semiconductors, and oil and natural-gas equipment. The next question is how to formulate and implement a strategic, national manufacturing strategy that will promote America's high-value added manufacturing and help the United States maintain its century-old position of global leadership in the 21st-century world economy.",1
47,2,Business Economics,27 April 2012,https://link.springer.com/article/10.1057/be.2012.2,Interpreting the Performance of Business Economists During the Great Recession,April 2012,Kathryn Lundquist,Herman O Stekler,,Female,Male,Unknown,Mix,,
47,2,Business Economics,27 April 2012,https://link.springer.com/article/10.1057/be.2012.1,"Paula Szuchman and Jenny Anderson, Spousonomics: Using Economics to Master Love, Marriage and Dirty Dishes",April 2012,Gerald Musgrave,,,Male,Unknown,Unknown,Male,,
47,2,Business Economics,27 April 2012,https://link.springer.com/article/10.1057/be.2012.8,"John E. Silvia, Dynamic Economic Decision Making",April 2012,George A Fulton,Daniil Manaenkov,,Male,Male,Unknown,Male,,
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.22,From the Editor,July 2012,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"I would like to welcome Kevin Swift as co-editor of Focus on Industries and Markets. Andrew Gross has done yeoman service for many years and will continue, sharing the burden with Kevin.",
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.20,Choices for Federal Spending and Taxes,July 2012,Douglas W Elmendorf,,,Male,Unknown,Unknown,Male,,1
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.14,Perpetuating Puffery: An Analysis of the Composition of OMB's Reported Benefits of Regulation,July 2012,Susan E Dudley,,,Female,Unknown,Unknown,Female,"Measuring the effects of regulations is notoriously difficult. Regulations are designed to achieve social goals, as is the direct government spending supported by taxes; but the costs of regulations are felt through higher prices paid for goods and services and opportunities foregone. There is no mechanism like the fiscal budget for keeping track of the spending by individuals and businesses on regulatory compliance, nor of the benefits that regulation brings. Perhaps because the issuance of new regulations is not constrained by a budget, presidents and Congress have tried to rely on other tools to ensure regulations make society better off. For over three decades, presidents have required agencies to conduct ex ante regulatory impact analysis (RIA) of proposed new major regulations to determine whether their net effects will be positive. Executive Order (EO) 12866, issued by President Clinton in 1993 and still in effect today, reflects the regulatory philosophy that he and subsequent presidents have endorsed:
 (a) The Regulatory Philosophy. Federal agencies should promulgate only such regulations as are required by law, are necessary to interpret the law, or are made necessary by compelling public need, such as material failures of private markets to protect or improve the health and safety of the public, the environment, or the well-being of the American people. In deciding whether and how to regulate, agencies should assess all costs and benefits of available regulatory alternatives, including the alternative of not regulating. Costs and benefits shall be understood to include both quantifiable measures (to the fullest extent that these can be usefully estimated) and qualitative measures of costs and benefits that are difficult to quantify, but nevertheless essential to consider. Further, in choosing among alternative regulatory approaches, agencies should select those approaches that maximize net benefits (including potential economic, environmental, public health and safety, and other advantages; distributive impacts; and equity), unless a statute requires another regulatory approach. [E.O. 12866, Sec.1(a)] Presidents have assigned the Office of Information and Regulatory Affairs (OIRA) in the U.S. OMB authority to ensure that regulations meet E.O. 12866 requirements. Pursuant to the order, executive branch agencies submit all significant regulations to OIRA for review. Agencies must prepare for OIRA and the public an RIA for proposed and final regulations that are deemed to be “economically significant,” defined as the subset of significant regulations that are expected to have “an annual effect on the economy of $100 million or more or adversely affect in a material way the economy, a sector of the economy, productivity, competition, jobs, the environment, public health or safety, or State, local, or tribal governments or communities” [EO 12866, Sec. 3(f)(1)]. In 2003, OIRA issued Circular A-4 to guide agencies in conducting RIAs, and in 2011 President Obama reinforced E.O. 12866 and elaborated on its analytical requirements in E.O. 13563. In a recent primer, OIRA described the purpose of the RIA as follows:
 Executive Orders 13563 and 12866 require agencies to provide to the public and to OMB a careful and transparent analysis of the anticipated consequences of economically significant regulatory actions. This analysis includes an assessment and (to the extent feasible) a quantification and monetization of benefits and costs anticipated to result from the proposed action and from alternative regulatory actions. Executive Order 13563 specifically requires agencies “to use the best available techniques to quantify anticipated present and future benefits and costs as accurately as possible.”  The purpose of the RIA is to inform agency decisions in advance of regulatory actions and to ensure that regulatory choices are made after appropriate consideration of the likely consequences. To the extent permitted by law, agencies should proceed only on the basis of a reasoned determination that the benefits justify the costs (recognizing that some benefits and costs are difficult to quantify). Regulatory analysis also has an important democratic function; it promotes accountability and transparency and is a central part of open government. [OMB 2011] Pursuant to these executive orders, OIRA reviews over 500 significant regulations each year, of which between 40 and 75 are economically significant regulations undergoing a final review before they become law.",8
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.17,Tough Love: The True Nature of the Eurozone Crisis,July 2012,Holger Schmieding,,,Male,Unknown,Unknown,Male,"The Eurozone crisis is often portrayed as a debt crisis. Like many popular misconceptions, this portrayal contains one or two kernels of truth: tiny Greece and two even smaller countries (Portugal and Ireland) are struggling with serious public debt crises. In addition, the outlook for public finances plays a major role in the Eurozone discussions. But the “debt crisis” label misses the major story. As Table 1 shows, the United States carries a greater public debt burden than the Eurozone, and the United Kingdom comes close to the Eurozone ratio of gross public debt to GDP. In fact, the United Kingdom looks set to surpass the Eurozone's debt ratio by 2013 at the latest. The point comes out even more clearly if we look at the running fiscal deficit rather than the accumulated stock of gross public debt. In 2011, the fiscal deficit of general government in the Eurozone (4.1 percent of GDP) was far below that of the United Kingdom (8.3 percent) and the United States (around 10 percent). Relative to the United States and the United Kingdom, the Eurozone needs only comparatively minor adjustments to put its public finances on a sustainable path. A calculation by the International Monetary Fund [2012] underscores this point. Figure 1 shows that in order to reduce general government gross debt to 60 percent of GDP by 2030 and cope with the burden of an aging population, Japan would have to tighten its fiscal policy by a cumulative 18.9 percent of its GDP until 2020, the United States by 17.6 percent, and the United Kingdom by 11.3 percent. This exceeds the tightening need in Germany (2.5 percent), Italy (3.6 percent), Ireland (5.3 percent), France (6.6 percent), and Spain (10 percent) by a wide margin. Even Greece (10.7 percent) has less hard fiscal work left to do than the United States, the United Kingdom, and Japan on these IMF calculations. Tightening Need, Adjusted for Age-Related Spending Source: IMF Fiscal Monitor April 2012. This leads us to the fiscal paradox of Europe: although the Eurozone as a whole has much less need to tighten its fiscal policy than all other major regions of the Western world, most Eurozone countries are now delivering their still-required fiscal repair in a very frontloaded manner. Take the most striking example: According to the IMF Fiscal Monitor [2012], Italy is implementing almost the entire fiscal repair required in the long term in one single year, namely 2012. Of a cumulative need to correct its fiscal stance by 3.4 percent of its GDP until 2020 to reach the benchmark (60 percent debt-to-GDP ratio by 2030), Italy has imposed spending cuts and tax hikes worth 2.9 percent of its GDP in 2012. At the same time, those countries with the biggest need to adjust (Japan and the United States) have barely started the effort. To understand the underlying dynamics in Europe in responding to the debt and deficit situation, we first need to discuss some basic features of the euro crisis and the Eurozone. The Greek crisis erupted in late 2009 when, shortly after a national election, the new Greek government revealed that the old government had de facto lied about the extent of the fiscal deficit. As the Greek fiscal deficit for 2009 was revised up and up from a preelection forecast of “around 7 percent” of GDP until it surpassed 15 percent of GDP, global investors refused to buy Greek sovereign bonds, forcing Greece to ask for help from the other 16 Eurozone member countries and the International Monetary Fund in April 2010. Although Ireland and Portugal had to follow suit in late 2010 and early 2011, the problems of these three peripheral countries did not hold back the overall Eurozone economy by much. Taken together, Greece, Ireland, and Portugal account for a mere 6 percent of Eurozone GDP. Despite serious problems in these very small countries, the overall Eurozone was the only economic region in the Western world that enjoyed trend growth in the first half of 2011. But in mid-2011, the Eurozone economy suddenly decoupled from the rest of the world. Figure 2 shows the sudden divergence in the Purchasing Managers Index for manufacturing for the Eurozone and a weighted average for the United States and China, the other two global heavyweights. Having avoided any serious contagion from the debt crisis in some small peripheral economies to the Eurozone as a whole until then, contagion suddenly spread like wildfire in the summer and autumn of 2011. Whereas the U.S. economy regained some momentum in late 2011 and early 2012 and China enjoyed a soft landing, the Eurozone contracted in 2011:Q4 and stagnated in 2012:Q1. The key issue in the Eurozone crisis has never been the fate of Greece or some other small highly indebted country. The key issue has always been contagion control. Greece, Ireland, and Portugal have genuine debt crises. The Eurozone as a whole does not. The Eurozone economy only hit the rocks when contagion control failed in July 2011. PMI: the United States and China vs. Eurozone Source: Markit. To some extent, the frontloaded and severe austerity in parts of the Eurozone contributes to the gap between the Eurozone and other major economies. But it is not the main factor. As a first step to understand the true nature of the Eurozone crisis, we need to clearly distinguish between two different aspects of a financial crisis.
 The underlying problem, say an asset bubble that has turned to bust or an over-indebted sovereign that has lost access to markets. The financial panic that can be triggered by such an underlying problem. The distinction is similar to the two risks which arise if somebody cries “fire” in a crowded theatre. There may actually be a dangerous fire. But in a panic rush to escape from a fire, whether real or imagined, people may also trample each other to death. The second risk exists whether or not the alarm later turns out to have been a real or a false one. The U.S. experience illustrates the distinction between these two aspects of a financial crisis. From mid-2007 onward, the United States had a serious fundamental problem. One of the worst real estate bubbles ever, the Greenspan bubble, was going bust. This was bound to cause major strains in the U.S. financial system with losses of several hundred billion dollars. When Bear Stearns became insolvent in March 2008, the issue was handled well. The U.S. economy stayed in a rather mild recession, and the rest of the world barely wobbled. But when the United States grossly mismanaged the insolvency of Lehman Brothers in September 2008, de facto shutting down major parts of the global wholesale financial markets instead of winding down the firm in an orderly manner, the ensuing financial tremors pushed the Western world into its worst recession in 80 years. That German GDP nosedived by 6.2 percent within the two quarters after Lehman had nothing to do with direct or indirect exposure to U.S. real estate. It was the result of a panic that the U.S. authorities had allowed to erupt and spread. Only when the U.S. Federal Reserve signalled that it would do virtually everything to stop the rot, including almost unlimited purchases of U.S. securities, did the panic end. One month after the Federal Reserve leaked to the press in early March 2009 that it was discussing “quantitative easing”, the German economy went straight back from mega-recession to a rapid pace of growth. No central bank can correct underlying fiscal problems or structural deficiencies in a labour market. But a central bank can prevent or arrest the financial panic that may erupt if such an underlying problem is mishandled. In the United States, the United Kingdom, and Japan, the central banks are ready to buy assets in abundance in order to calm markets, lower sovereign borrowing costs, and/or pump in extra liquidity into the dysfunctional parts of the financial system. They respond eagerly to any real or perceived risk of a panic. In the Eurozone, the ECB holds back.",6
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.15,Economic Sanctions Against Iran: Is the Third Decade a Charm?,July 2012,Jeffrey J Schott,,,Male,Unknown,Unknown,Male,,4
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.21,Musings on the Macroeconomic Effects of Oil Price Increases,July 2012,Chris Varvares,,,,Unknown,Unknown,Mix,,
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.11,Impact of a Middle East Oil Export Disruption,July 2012,Philip K Verleger Jr,,,Male,Unknown,Unknown,Male,"Sanctions on Iran should not by themselves have any significant impact on oil prices because Iran plays a much diminished part today relative to its past position. As can be seen from Table 1, Iran may have net crude exports of approximately two million barrels per day. In addition, Iran probably exports around 500,000 barrels per day of natural gas liquids. The oil volumes that sanctions may prevent Iran from exporting can be easily replaced, assuming other oil-exporting countries boost production to replace the loss and/or consuming countries release an offsetting amount of oil from strategic stocks. (This is an important assumption that I discuss further below.) OPEC surplus capacity today stands at roughly four million barrels per day, according to Energy Intelligence Group estimates. As can be seen from Figure 1, this surplus has declined recently but still exceeds total Iranian exports by a wide margin. Monthly OPEC Surplus Capacity, 1999–2011 Source: PKVerleger LLC. The available oil will also satisfy the needs of global refiners. In the past, product and crude oil prices have risen significantly when light sweet crude supplies were disrupted. The price doubling in 2008 was caused, for example, by the cutoff of 500–700 thousand barrels per day of Nigerian sweet crude output. Prices rose because other producers could not match the quality of these volumes. The 2010 price increase can also be traced in part to lost sweet crude supplies from Libya. The loss of Iranian crude does not create such problems. Iran produces Middle East sour crude with no unique characteristics. It can be replaced. The lost Iranian exports must, however, be offset to avoid a price increase. In the past, some oil-exporting countries have cut production opportunistically when global markets were tight. Obviously, every producer enjoys increased market power when world crude output is near capacity. Given the very low short-term price elasticities of demand, any nation exporting more than 1.1 million barrels per day can increase revenue by cutting production. This means OPEC members Angola, Iraq, Nigeria, Kuwait, United Arab Emirates, and Venezuela all could push prices higher just by offering less oil to buyers. Historically, several of these nations apparently exercised their newfound power, cutting sales just enough to add further upward pressure to prices. Their oil ministry officials seem to understand that consuming countries are clueless regarding this ploy. These officials also recognize that their counterparts in consuming nations count barrels rather than watch prices. This focus on quantity means consuming countries will remain ignorant of any exporter shenanigans for weeks if not months, thus enabling the latter nations to boost revenue. Oil buyer representatives, particularly major oil company executives, could tip off the consumers to such producer actions. Many oil firms have a conflict, though, because they also benefit from higher prices. Absent such market manipulations, the sanctions that force Iran to cut exports should have no impact on crude prices.",
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.18,Medical Malpractice Liability and Physician Migration,July 2012,John J Perry,Christopher Clark,,Male,Male,Unknown,Male,"There are many anecdotes about severe medical malpractice climates causing doctors to move their practice or exit the profession [Tibbles 2004; Lynch 2008]. These stories typically suggest that such legal climates lead to excessive judgments against physicians, which provide incentives for lawyers and patients to file frivolous lawsuits. These supposedly excessive judgments and the increased threat of facing legal action put a physician's personal and business assets at risk and cause medical malpractice insurance premiums to increase, potentially reducing the supply of physicians and thus access to medical care. Beyond anecdotes, there has been an active research literature concerning the effect of medical malpractice climates on the supply of physicians. Generally speaking, the empirical literature can be divided into two categories: studies that analyze the microlevel effect on physician labor supply and studies that use aggregate data to analyze the macrolevel impact on physician labor supply. The smaller part of the literature examines microlevel effects, or the impact on individual physician behavior of medical malpractice climates. Helland and Showalter [2009] examine the impact of malpractice liability risk on hours worked by individual physicians. Using estimated measures of liability risk for each state, they find an elasticity of hours worked of −0.285. Thus, a 10 percent increase in liability exposure is estimated to reduce hours worked by 2.85 percent. Older physicians are estimated to have a much larger elasticity (−1.224). Dranove and Gron [2005], using hospital inpatient data from the Florida State Center for Health Statistics, compare the activity levels of obstetricians and neurosurgeons, two medical specialties commonly thought to be most impacted by medical malpractice climates, in Florida between 1997 and 2000 to the period 2000 to 2003. They find evidence that neurosurgeons significantly altered their practices in response to medical malpractice liability pressures but found no such result for obstetricians. Reyes [2010], using a 2003 survey of 1,476 obstetrician-gynecologists, finds some evidence that malpractice liability can lead OB/GYNs to specialize in either obstetrics or gynecology rather than practice in both specialty areas. Polsky and others [2010] examined the effect of malpractice premiums on practicing obstetricians using a longitudinal data set spanning 1998–2004. They found that rising malpractice premiums appear to be associated with both a reduction in entry into the field and an increased exit rate from the field. Most work has taken a more macrolevel approach, such as examining state-level populations of physicians. Using American Medical Association (AMA) data on the number of physicians per capita from 1980 to 1998 by state, Klick and Stratmann [2005] estimate the impact of specific medical malpractice reforms. Employing an instrumental variables approach, they find that only caps on noneconomic damages impact physicians per capita—states with caps on noneconomic damages are estimated to have about 37 percent more physicians per capita than states without this reform. Klick and Stratmann [2007] use AMA state-level data on physician populations to estimate the impact of medical malpractice liability reform on high-risk specialty physicians per capita. To identify the effect of liability reforms, they use state fixed effects and physicians in low risk specialties as a control group and find that noneconomic damage caps increase physician populations and that the increase is concentrated on the highest-risk specialties. Kessler and others [2005] find similar results examining the impact of medical malpractice reform on state-level data on physician populations. Following the framework of Kessler and McClellan [1996], they classify medical liability reforms into two broad categories “direct” (reforms that directly affect how much a defendant will have to pay in the event of a judgment) and “indirect” (limitations on whom/when a plaintiff can sue). They find that between 1985 and 2001, states that implemented “direct” malpractice reforms had greater growth in physician populations per capita than states that did not. Encinosa and Hellinger [2005], using county-level physician data from the U.S Department of Health and Human Services’ Area Resource Files from 1985 to 2000, find that noneconomic damage caps lead to an increase in physician populations by about 2.2 percent. They also find that the effect of the caps is larger for rural counties. While most of the literature finds evidence that medical liability reforms tend to have a positive effect on physician populations, there are exceptions. Matsa [2007], using county-level data from Area Resource Files, finds that damage caps do not impact physician populations generally. However, Matsa [2007] does find evidence that caps on damages increases the supply of rural physicians. One additional study, Baicker and Chandra [2005a, 2005b] uses data from 1993 and 2001 to examine per capita physician levels. They find that medical malpractice costs, which they suggest are the mechanism in which medical liability reforms would impact physician supply, do not impact physician populations. However, like Matsa [2007], Baicker and Chandra [2005a, 2005b] find that there is some evidence that malpractice costs reduce the supply of physicians in rural counties. Our paper sits at the nexus of the two literatures by investigating microlevel effects that could explain macrolevel findings. We borrow from the empirical approach of Klick and Stratmann [2007] and use state fixed effects and two control groups to identify the effect of specific medical malpractice reforms, but our unit of observation is the individual physician.",2
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.16,Toward a More Comprehensive Measure of Labor Underutilization: The Alabama Case,July 2012,Samuel N Addy,Michaël Bonnal,Cristina Lira,Male,Male,Female,Mix,,
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.13,"Michael Grabell, Money Well Spent? The Truth Behind the Trillion-Dollar Stimulus, the Biggest Economic Recovery Plan in History",July 2012,Ken Simonson,,,Male,Unknown,Unknown,Male,,
47,3,Business Economics,14 August 2012,https://link.springer.com/article/10.1057/be.2012.12,"Eamonn Butler, Milton Friedman: A Concise Guide to the Ideas and Influence of the Free-Market Economist",July 2012,John C Goodman,,,Male,Unknown,Unknown,Male,,
47,4,Business Economics,12 November 2012,https://link.springer.com/article/10.1057/be.2012.30,From the Editor,November 2012,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
47,4,Business Economics,12 November 2012,https://link.springer.com/article/10.1057/be.2012.26,The Myths and Facts of Patent Troll and Excessive Payment: Have Nonpracticing Entities (NPEs) Been Overcompensated?,November 2012,Jiaqing “Jack” Lu,,,Unknown,Unknown,Unknown,Unknown,,
47,4,Business Economics,12 November 2012,https://link.springer.com/article/10.1057/be.2012.27,A Comparison of Consensus and BVAR Macroeconomic Forecasts,November 2012,John Silvia,Azhar Iqbal,,Male,,Unknown,Mix,,
47,4,Business Economics,12 November 2012,https://link.springer.com/article/10.1057/be.2012.28,Predictability of Financial Crises: Lessons from Sweden for Other Countries,November 2012,Hubert Fromlet,,,Male,Unknown,Unknown,Male,"From the mid-1970s, when the consequences of the first big modern oil crisis were felt, until the mid-1990s, Sweden was characterized by significant deficits in national savings. During these two decades, there were only a few years in which the current account was roughly in equilibrium [Lindström and Lundberg 1993]. Several devaluations took place during the latter part of the 1970s and during the 1980s, accompanied by persistent inflation problems.Footnote 1 A major fall in the value of the Swedish krona also occurred after the transition of the krona to a floating exchange rate regime in late 1992. In the 1980s, the Swedish economy was in horrible structural shape, caused by high government expenditure, enormous wage increases, and inflation; but it existed also in terms of poor regulations, weak competition, high subsidies, an inefficient structure of the labor market, wrong or lacking incentives on all levels, ineffective financial markets, and a harsh tax system. For example, marginal taxes for ordinary wage earners were as high as 60–65 percent. On the other hand, interest payments for private households were deductible in these high ranges. In combination with high inflation, this constellation meant, over many years, negative real interest rates for credits to consumers and homeowners. This situation favored the creation of enormous private debt—a situation that was not remedied until the implementation of a major income tax reform in 1990 and 1991 [Lybeck 1992]. From this point onward, interest rate deductibility in the tax bill has been reduced to 30 percent. Initially, this change was received as an income shock by most Swedish households, but households gradually have become used to it. Although the tax reform was badly needed, its initial shock came at the same time as the banking crisis was getting progressively worse. There was no real co-ordination of changes in economic policy by the public authorities involved. In the 1980s, there was no real comprehensive financial market: daily trading of government and mortgage paper did not exist to any mentionable degree. At this time, reality focused on regulated credit and bond markets. By law, banks had to invest an important percentage of their deposits in government bonds and in the bonds of the mortgage institutes and banks. Insurance companies had strict investment rules that favorable bonds, too. Consequently, the government and the housing sector were in a favorable non-tradable investment position. Credit ceilings for the banks were considered to be a “normal” regulation. However, banks got around this by selling parts of their excess new loans to insurance companies overnight—just before the end of the month when the new credits were measured—and buying these loans back as soon as the new month had started. Thus, the credit ceilings were formally met—one of the most serious loopholes in the Swedish credit market. Therefore, nobody was surprised when the credit ceilings imposed by the Riksbank (the Swedish central bank) were finally scrapped on November 21, 1985. From this day onward, a fast credit expansion started, which ended with the bursting real estate bubble in 1990 and 1991. Another phenomenon contributed to the origins of the Swedish financial crisis of the early 1990s. Until 1989, Swedes and Swedish institutions were virtually banned from investing in foreign stocks, bonds, and real estate. There were some minor exceptions, but these were not of any economic significance. Suddenly, these cross-border activities were made possible. This happened in a period of an ongoing fast credit expansion. This could not be regarded as an optimal timing. Large numbers of Swedish real estate investors went directly to London, Paris, Amsterdam, Frankfurt, and elsewhere in order to purchase commercial real estate. This happened without any experience in foreign real estate markets: herd mentality boomed. In the early 1990s, I actually heard many comments by colleagues in London, Frankfurt, and elsewhere that the Swedes were paying “any price” to enter local commercial real estate markets in continental Europe and Britain. Obviously, some kind of irrational exuberance could be noted. Thus, the main characteristics of the new financial structure in the year 1990 were:
 the credit ceiling for the Swedish banks had been abolished a couple of years before; high liquidity ratios for compulsory purchase of government and mortgage bonds had been scrapped; deductibility of interest payments for the debt of private households was reduced substantially; free cross-border financial and real estate investments for Swedish investors and foreigners were created (and the other way around). A big question at the time was how the Swedes would handle this abruptly won freedom of cross-border capital flows. However, nobody investigated the topic seriously. The relief after the newly found liberty to invest abroad was so great that it was difficult to discuss, for instance, the important issue of the sequencing of the different financial deregulations and other structural reforms [Agénor 2004]. Deregulation was treated like the “deus ex machina” of Greek mythology—something strongly desired suddenly showed up. Consequently, there was little time or room for second thoughts and criticism. The whole banking community, the Swedish government, and the Riksbank had been acutely aware of the fact that the old regulations for banks, insurance companies, and cross-border investors were full of loopholes. Also, other advanced economies had already deregulated their financial markets substantially. So, why could anything go wrong in the Swedish deregulation process? There was a psychological hype going on in the second half of the 1980s that nobody cared about. New structural conditions on deregulated financial markets that lead to euphoria may end in overconfidence, control illusion, and financial bubbles. This is shown by the Swedish examples and should be considered in countries where financial markets still are to be deregulated. Psychological studies and observations of the players on all financial markets—mature and developing—should be important to business economists all over the world. In growth terms, the early 1990s Swedish financial crisis was very costly. GDP fell 3 years in a row between 1991 and 1993, altogether by 5 percent. Consumers were squeezed by the government's austerity packages. The unemployment rate rose from 1.6 to 8.2 percent in the first years of the 1990s. On the other hand, two ruling governments during the crisis—one nonsocialist and one socialist—succeeded in making the public sector much more effective. Furthermore, subsidies were reduced substantially. These two latter achievements are often underestimated in the public debate, even though they contributed substantially to the large improvement of the public financial situation. But Sweden was lucky as well. At approximately the same time as the beginning of the Swedish financial crisis, the United States started its positive performance in the 1990s, and Europe was positively affected by the German unification boom. This all happened when the Swedish krona started to float down to much weaker levels than noted during the period of the unilateral link to the European Currency Unit (ECU). Swedish export companies were stimulated enormously by the sizable depreciation of the krona, which meant a remarkable recovery on the stock exchange as well. Despite continuously weak domestic demand in private consumption and construction and very serious unemployment problems, confidence gradually increased in Swedish corporate rooms and private homes. Altogether, the recovery of the Swedish economy was painful. It took several years after the turning point to bring unemployment down substantially. Unfortunately, youth unemployment has remained high (about 20 percent), among the highest rates in the whole EU area. This may well be the main failure in economic policy during the past two decades, despite all the positive developments in inflation, government finance, the current account, competitiveness, the pension system, entrepreneurial spirit, and innovation capacity. The sequencing of economic policy measures affecting financial markets may be crucial. Sometimes flexibility is needed even in reforms. Too many major structural changes to the financial markets within a short period of time may have counteracting effects. There is not so much to write about this event in Sweden. Stockowners were hit—but hardly Swedish banks. The Stockholm Stock Exchange went down by more than 70 percent in the following 3 years as a consequence of the bursting global and Swedish IT bubble. However, one big question was never really investigated: Why did so many financial executives, asset managers, economists, and journalists have such short memories considering that substantial financial exuberance had taken place in Sweden only 10 short years earlier? In early 2000, quite a number of listed IT companies had price earning rations exceeding several hundred—more or less without major concern on the part of financial analysts. One reason for the relative silence may have been that many analysts or financial advisors had been fired during the early 1990s crisis. This meant that a large number of recently employed analysts—without experience from the past crisis—were not able to interpret the exuberance of IT stocks appropriately and more experienced financial officers may have been too ambitious in selling stocks. In addition, Sweden was part of the global IT overconfidence. Previous exuberance in the financial markets is not always remembered very well, even if the latest bursting bubble occurred more recently. This phenomenon is striking. It remains to be seen whether new and better conclusions from history can be made in the future once global financial markets have been brought back on more solid ground in the aftermath of the current global crisis. The 2008–09 crisis was very avoidable in Sweden. During the 1990s, some Swedish banks made strategically logical and far-reaching investments in the Estonian, Latvian, and Lithuanian banking systems. One could say that the Baltic banking system was dominated by two Swedish banks, Swedbank and SEB. Already in the 1990s, however, some banking problems showed up in the Baltic states, although they did not cause any major macrofinancial and macroeconomic problems at the time. In the early 2000s, the Baltic financial sectors seemed to be healthy. The two Swedish banks made a lot of money, but in the Baltic states a very unhealthy credit boom began and accelerated while macroeconomic fundamentals simultaneously worsened—without intervention by the supervisory and fiscal authorities. The path of this decline is shown in Table 1. Very importantly, monetary policy was no longer a tool to cool down the ongoing credit boom. The reason for this was Estonia's, Latvia's and Lithuania's joining the European Exchange Rate Mechanism 2, with more or less fixed rates against the euro. This currency regime meant that monetary policy had to target the exchange rate rather than inflation, credit growth, and—consequently—domestic financial stability. This is further evidence that emerging countries preferably should apply floating exchange rates. Fixed exchange rates in a credit boom tend to worsen insufficient competitiveness and current account imbalances.",3
47,4,Business Economics,12 November 2012,https://link.springer.com/article/10.1057/be.2012.25,Keynesian Fiscal Stimulus: What Have We Learned from the Great Recession?,November 2012,Laurence S Seidman,,,Female,Unknown,Unknown,Female,"Keynesian policies for fiscal stimulus encompass the use of temporary tax cuts, or increases in government spending (transfers or purchases) to increase aggregate demand during a recession, when aggregate demand is too weak to generate normal production and employment. Paul Samuelson's first edition of his introductory economics textbook, perhaps the most influential exposition of Keynesian macroeconomics to a broad audience, has this passage [Samuelson 1948, pp. 413–14]:
 In addition to public-works expenditures and welfare expenditure, countercyclical compensatory fiscal policy can also rely on cyclically timed tax policies … Even without Congress changing any laws, it turns out that governmental tax collections fall off when national income falls off … This is a powerful factor stabilizing the whole economy and moderating the business cycle … Even this is not all. Congress can also change tax rates …Those who believe in countercyclical compensatory fiscal policy argue that the time to reduce tax rates is in de pression, when over-all purchasing power is too low. In the early 1960s, the Kennedy Administration experienced an internal debate between two Keynesian advisers, Walter Heller and John Kenneth Galbraith. They agreed that a tax cut and/or a government spending increase would raise aggregate demand and generate more output and employment when unemployment was high. For various reasons, Heller argued for a tax cut and Galbraith for a spending increase. Today, anti-Keynesians claim the Kennedy tax cut worked because it induced more supply of labor and capital, not because it stimulated aggregate demand. But Heller's argument for the tax cut was Keynesian: it would lift aggregate demand. Presidential candidate Ronald Reagan in 1980 and George W. Bush in 2000 each proposed a permanent “across-the-board” tax rate cut with the purpose of reducing the size of government, providing supply-side incentives, and cutting income taxes in proportion to household income. Each of the tax cuts, however, had the Keynesian effect of raising aggregate demand during a recession—Reagan's in 1982, Bush's in 2002—which helped mitigate the severity of each recession. It should be noted that all tax cuts or cash transfers to households are “Keynesian” in that they raise aggregate demand, but some are more effective than others because some households have a higher propensity to spend. The debate over fiscal stimulus, therefore, should not be framed as “government spending vs. tax cuts.” Both are Keynesian and both are fiscal stimulus. A fiscal stimulus package can be designed to appeal to either political liberals or conservatives. A liberal fiscal stimulus package would consist of domestic spending and tax cut components that favor middle and low-income households while a conservative fiscal stimulus package would consist of tax cuts that are at least proportional to or even favor high income households by reducing marginal rates. Attempts to reconcile the two polar views have been advocated by the National Commission of Fiscal Responsibility (Simpson-Bowles Commission) [National Commission 2010] and the Debt Reduction Task Force [2010], cochaired by Pete Dominici and Alice Rivlin. Advocates of fiscal stimulus emphasize that it must be reinforced by monetary stimulus. Inherent in fiscal stimulus are federal budget deficits. Keynesians call upon the Federal Reserve to buy Treasury bonds from the public beyond what is necessary to finance the deficit through its open market operations so that the fiscal expansion is reinforced by a monetary expansion that keeps interest rates from rising. The magnitude of the combined fiscal/monetary stimulus is intended to be set large enough to raise aggregate demand back to normal. Opponents of fiscal stimulus often confuse the debate by asking what would happen if fiscal stimulus were not supported by monetary stimulus. This intellectually interesting question has nothing to do with the actual policy recommendation of advocates of fiscal stimulus, who always prescribe simultaneous monetary stimulus as well. The real debate is over whether Keynesian policies should be adopted along with monetary stimulus in a severe recession. Many economists oppose adopting any temporary fiscal stimulus, asserting either that the economy will automatically recover from a recession, that monetary stimulus alone is sufficient to engineer a recovery, or that uncertainty about when stimulative policies will have an effect might actually be counterproductive—taking hold only when a recovery is under way and causing boom conditions with an inevitable and painful bust. Advocates of fiscal stimulus emphasize that it can and should be gradually phased out as the economy recovers from recession. They aver that confidence rises with the increase of actual output and employment in the economy (Keynes’ “animal spirits”). Thus, the increase in aggregate demand becomes self-reinforcing so that the fiscal stimulus can be gradually phased out. During a severe recession, advocates of fiscal stimulus place a higher priority on combating the recession than on balancing the federal budget or preventing an increase in federal debt. Fiscal stimulus involves a temporary increase in government borrowing and government debt. Keynes’ practical policy message was that in a severe recession, stimulating aggregate demand is more important than preventing a rise in government debt. It follows that advocates of fiscal stimulus should be supporters of achieving a low ratio of federal debt to GDP during prosperity.",3
47,4,Business Economics,12 November 2012,https://link.springer.com/article/10.1057/be.2012.29,The Global Engineering Consultancy Market,November 2012,Andrew C Gross,,,Male,Unknown,Unknown,Male,"Among the engineering disciplines, civil engineering is the oldest—as ancient as humanity: even primitive shelters involved design and the understanding of the materials from which they were constructed and the environment in which they had to function. Next came the design of harbors and fortresses; followed by the design of bridges, canals, roads, and railways.Footnote 2 Other engineering fields developed in the wake of scientific discoveries and the determination to adapt them to human use. Recently, electrical and computer engineering have taken center stage, while bioengineering and biological engineering are examples of the cutting edge of engineering consultancy. Although engineering in general is ancient, formal education in the field is relatively new. In North America, engineering programs can be traced back to mid-nineteenth century. Today, there are thousands of courses offered across this continent as well as in all other regions. Table 1 shows recent trends in bachelor's degrees in engineering awarded in the United States, China, and India. Engineering education is rigorous, but debates continue about what a “proper” engineering curriculum should be at the undergraduate and graduate level. Associations, government agencies, and educator groups differ in their emphasis on the role of science, design, and craftsmanship courses, with no clear resolution in sight. There is still concern with enrollment, retention, and accreditation. Graduates of accredited engineering programs are sought after by engineering consultancies as well as other knowledge-intensive businesses. Many firms promise a parallel path of advancement where those staying with the technical route can earn as much as those who move into managerial ranks. However, many firms pay lip service to this credo, and so we find engineers making a transition into management in search of better remuneration and career advancement, but seeking to avoid obsolescence. Our research shows high levels of job satisfaction by engineers in North America, regardless of the path chosen. Large and small engineering consultancy firms have gone global, establishing offices abroad; but trade is flourishing too. For example, the “headcount” for the largest international design firms by country are shown in Table 2, and we can see a definite shift from the United States toward Australia–Asia. The history of the consultancy aspect of engineering has a parallel in management consulting. Both fields date their origin to the mid-nineteenth century in England. The building of railways and bridges combined engineering and managerial skills. Project engineers then began to work in specific areas as consultants. After World War II, large engineering consultancies in many engineering fields were established in the United States, with a major cluster in California.",4
47,4,Business Economics,12 November 2012,https://link.springer.com/article/10.1057/be.2012.10,The Biases that Limit Our Thinking about the Economic Outlook and Policy,November 2012,John E Silvia,,,Male,Unknown,Unknown,Male,"In the current recovery, the pace of growth is subpar compared with prior economic recoveries. This brings us to our first policy problem and the first bias that limits our perceived available choices. In his book, Into Thin Air, Jon Krakauer [1997] offers us an example of mountain climbers who seek to reach the top of Mount Everest. After a considerable expense of training, equipment, and time, the climbers reached the last camp before the approach to the summit. However, the approach to the summit was more difficult than anticipated, and a number of climbers who started the final ascent after the recommended last departure time subsequently died. This real-world example brings up the problem of the sunk cost bias—the tendency to escalate commitment to a course of action while ignoring the mounting costs of the action relative to the anticipated benefit. In military terms, this is the story of many of the battles of the Somme, Ypres, and Verdun in World War I. In economic policy today, the challenge is for us to judge whether another fiscal stimulus program, another quantitative easing, another housing program, or another financial regulation would be just enough to reach the top of the mountain, for example full employment, reestablish the strength of housing a la 2005–07 or greater financial stability than what we have achieved so far. Or could further policy moves be too much? Would we be violating some sort of economic rules or balancing act in the economy where the continued pursuit of these policies be counterproductive, such as bringing on too much federal debt that could not be repaid over time—a mini Greece perhaps? The basic question is whether the marginal cost of expanded policy actions would outweigh the marginal benefits of such actions—not how much has been done already. Of course, for the private sector, this is drummed into economists in their first encounter with the theory of the firm.",
47,4,Business Economics,12 November 2012,https://link.springer.com/article/10.1057/be.2012.23,"Michael S. Hyatt, Platform: Get Noticed in a Noisy World",November 2012,W Steven Barnett,Jen Fitzgerald,,Unknown,Female,Unknown,Female,,
47,4,Business Economics,12 November 2012,https://link.springer.com/article/10.1057/be.2012.24,"John B. Taylor, First Principles: Five Keys to Restoring America's Prosperity",November 2012,Jesse S Hixson,,,Male,Unknown,Unknown,Male,,1
48,1,Business Economics,18 February 2013,https://link.springer.com/article/10.1057/be.2012.40,From the Editor,February 2013,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"I would like to welcome Rajeev Dhawan to the Editorial Board. Rajeev is the director of the Economic Forecasting Center at Georgia State University's Robinson College of Business. In addition to being an experienced and accomplished forecaster, Rajeev has been a leader in NABE and is interested in a number of areas pertinent to business economics.",
48,1,Business Economics,18 February 2013,https://link.springer.com/article/10.1057/be.2012.37,In Search of System Understanding and Control,February 2013,Gene Huang,,,,Unknown,Unknown,Mix,,
48,1,Business Economics,18 February 2013,https://link.springer.com/article/10.1057/be.2012.36,Real-Time Forecasting Revisited: Letting the Data Decide,February 2013,Jackson Kitchen,John Kitchen,,Male,Male,Unknown,Male,"The research and literature for “real time” forecasting and “nowcasting” have been expanding rapidly over the past decade.Footnote 1 Stock and Watson [2006] surveyed the “theoretical and empirical research on methods for forecasting economic time series variables with many predictors,” describing how that “provides the opportunity to exploit a much richer base of information than is conventionally used for time series forecasting.” Kitchen and Monaco [2003] described an early version of a real-time forecasting system “adopted at the U.S. Treasury to use the broad variety of incoming data to construct ‘real-time’ estimates of quarterly real GDP growth.” Evans [2005] used a comprehensive analysis to calculate daily real-time estimates by “modeling the growth in GDP as the quarterly aggregate of an unobserved daily process for real economy-wide activity” with “model parameters … estimated by (quasi) maximum likelihood using the Kalman filter algorithm.” Giannone, Reichlin, and Small [2008] presented an estimating methodology for producing current-quarter forecasts by adapting a common factors model, combining “the idea of ‘bridging’ monthly information with the nowcast of quarterly GDP with the idea of using a large number of data releases within a single statistical framework.” In particular, they clearly addressed the challenge from the evolving nature of the incoming data of the current quarter:
 In real time, some data series have observations through the current period, whereas for others the most recent observations may be available only for a month or quarter earlier. Consequently, the underlying data sets are unbalanced. Appropriately dealing with this “jagged edge” feature of the data is key for producing a nowcast that, by exploiting information in the most recent releases, has a chance to compete with judgmental forecasts. [Gian none, Reichlin, and Small 2008, p. 666]Footnote 2 Their approach allows for the nowcast to be conditioned on a large number of variables. A variety of studies have examined real-time forecasting for euro area GDP and activity [Golinelli and Parigi, 2008; Giannone, Reichlin, and Simonelli 2009; Runstler and others 2009; Bulligan, Golinelli, and Parigi 2010; Angelini and others 2011; Drechsel and Maurin, 2011].Footnote 3 The system that we present and use for the analysis of this paper is a descendent of the earlier model system described in Kitchen and Monaco [2003]. The Kitchen and Monaco system estimated the indicator-specific historical relationships between monthly indicators and real GDP growth (while properly accounting for indicators’ intraquarter data availability), produced current-quarter indicator-specific forecasts from the estimated relationships, and then combined the individual indicator forecasts by weighting according to the strength of the indicators’ historical relationships to real GDP growth. The software code for the system we use in this system, while maintaining much of the general methodology, was a complete rewriting relative to the prior system in order to incorporate more estimation flexibility and an inherent decision process within the system for choosing the best estimations by specifications and sample horizons. The prior methodology was primarily a constrained “estimation system” that ran the various estimating equations for largely ex ante user-specified and predetermined specifications and sample periods, which then generated the system estimate from those estimations. The current system is more of an “estimation and decision system” in that it iteratively runs through various alternative specifications and samples for the estimating equations, and within that process it makes the decisions for the “best” specifications and samples to use in the final estimation.Footnote 4 The choices in the system are based on the minimum Schwarz criterion to determine the best specification for a given sample size, then using the minimum absolute error for the prediction for real GDP growth for a recent period of time to determine the best sample (and for the best specification for that sample). Hence, this approach is much more in the spirit of “letting the data decide,” allowing for the estimation process to be determined in an ongoing and evolving real-time analysis, largely independent of subjective ex ante user-specified relationships. A significant part of the software code needed to implement both the prior system and the current system involves addressing the “jagged edge” problem of the availability of data in the current quarter, and assuring the proper estimation given that problem. The methodology and system operation are described in more detail below.",1
48,1,Business Economics,18 February 2013,https://link.springer.com/article/10.1057/be.2012.38,Is Productivity Growth Too Strong For Our Own Good?,February 2013,Mark Vitner,Azhar Iqbal,,Male,,Unknown,Mix,,
48,1,Business Economics,18 February 2013,https://link.springer.com/article/10.1057/be.2012.35,Connecting U.S. Health Expenditures with the Health Sector Workforce,February 2013,Ani Turner,Paul Hughes-Cromwick,,Female,Male,Unknown,Mix,,
48,1,Business Economics,18 February 2013,https://link.springer.com/article/10.1057/be.2012.34,Using Event Studies to Assess the Impact of Unexpected Events,February 2013,James V Koch,Robert N Fenili,,Male,Male,Unknown,Male,"Event studies rely upon a factual-counterfactual model that takes the firm's actual stock price return after a specific event and compares it with a counterfactual share price return that assumes the event never took place. The difference between the actual return and the counterfactual return is an “abnormal return.” The abnormal return is directly related to the change in a firm's market capitalization. Daily abnormal returns (“DARs”) are computed for each post-event trading day in the analysis. Cumulative abnormal returns (“CARs”) are the accumulation of DARs over a specific, limited number of days. Campbell and others [1997, p. 149] suggest that the first published event study concerned the impact of announced stock splits on firms’ share returns [Dolley 1933]. However, the event study methodology did not attain popularity or sophistication until the 1960s [Ball and Brown 1968; Fama and others 1969]. An important impetus for the Ball and Brown and Fama research was a desire to test variants of the efficient market hypothesis. Since then, event studies typically have focused upon the impact of unanticipated announcements and events on share returns and have assumed that equity markets are relatively efficient. In short, “event study methodology has… become the standard method of measuring security price reaction to some announcement or event” [Binder 1998, p. 111]. Many early studies examined the impact on share returns of announcements of stock splits, actual and projected earnings, and mergers and acquisitions. Soon after, other fields, including economics, law, management, and marketing, embraced event study methodology. There are hundreds of such studies and they span topics such as product tampering, product failures and recalls, regulatory changes, natural disasters, fraudulent acts, executive turnover, and executive compensation. Bowman [1983], Armitage [1995], MacKinlay [1997], McWilliams and Siegel [1997], Binder [1998], and Johnston [2007] have cataloged many of these studies and also have examined some of the theoretical and empirical issues that arise when one performs such studies. It is often important that decision makers be able to assess the impact of unanticipated events. Thus, an organization's Board of Directors, in determining the compensation of its CEO and other top executives, is better situated if it is able to separate the performance impact of unanticipated events (either good or bad) from what would have been the case, but for the unexpected event. Event studies can be a useful analytical tool in such cases. McWilliams and Siegel [1997, p. 626] note that event studies have become an increasingly popular tool because they obviate the need to analyze potentially deceptive accounting-based measures of profits. Kane [2004] is one of many who have examined how accounting-based profit measures unfortunately can generate inaccurate perceptions of company prosperity—disinformation about the true status of a firm.",4
48,1,Business Economics,18 February 2013,https://link.springer.com/article/10.1057/be.2012.39,Relative Intragenerational Economic Mobility: A Cross-Period Analysis from 1968 to 2009,February 2013,John E Silvia,Tim Quinlan,Joseph Seydl,Male,Male,Male,Male,"According to the previously mentioned report from the CBO, the share of income earned by the top 1 percent of households soared 278 percent between 1979 and 2007, while income growth for the bottom 20 percent of households was limited to just 18 percent during the same period, as shown in Figure 1. Previous authors have assessed uneven distribution of income in various contexts including: redistributive taxation policies [Alesina and Rodrik 1994] and the impact of income inequality on economic growth [Persson and Tabellini 1994]. In addition, it has been demonstrated that macroeconomic adjustment to aggregate shocks differs between countries and can be attributed, at least in part, to differences in wealth and income distribution [Galor and Zeira 1993]. Real Growth in After-Tax Income from ‘79 to ‘07 (percent increase by income bracket) Source: The Panel Study of Income Dynamics. The CBO study essentially divides the population into quintiles and adds a sixth category by carving the first percentile out of the top quintile. Each quintile experienced growth in income, but the higher the income group, the greater the income growth experienced. In other words, since 1979, the rich have kept getting richer. When looked at as share of market income, the data clearly indicate that the income growth experienced at the top percentile has come at the expense of the other groups’ income share. In 1979, the top 1 percent claimed 11 percent of the total market income. By 2007, the piece of pie going to the top 1 percent nearly doubled to 21 percent. Consequently, the share earned by each of the other five income groups was smaller in 2007 than what it had once been in 1979, as shown in Figure 2. Shares of Market Income from ‘79 to ‘07 (percent, income bracket's share of total market income) Source: The Panel Study of Income Dynamics. We are all doing better on average, the study tells us, with inflation-adjusted mean household market income up 57 percent since 1979. But this is one of those times when the difference between mean (average) and median (the data point in the middle) really stands out. The extreme growth at the top skews the mean higher. In this case, the change in median income really tells the story; inflation-adjusted household income growth here was just 19 percent from 1979 to 2007, as shown in Figure 3. Growth in Mean and Median Market Income (percent, cumulative growth, real household income) Source: The Congressional Budget Office. To get a sense of a more contemporary assessment of income disparity, we also looked at data from the Internal Revenue Service and the Tax Policy Center, a nonpartisan joint venture between the Urban Institute and the Brookings Institution. This is a time when a picture (Figure 4) is worth a thousand words. Household Income by Percentile, 2011 (in millions of USD, annual income, married filing jointly, estimated) Source: The Tax Policy Center. Working together, these groups have preliminary results for 2011 household income, and the data are consistent with the income inequality depicted in the CBO report. Median annual income for married couples filing jointly comes in at approximately $75,000. Roughly one-third of households will make six figures in 2011, but less than 2 percent of households at the top will make more than $500,000. The income distribution then goes parabolic: incomes north of $1 million are reserved for the top 1 percent, and incomes north of $3 million are reserved for the top 0.1 percent.",1
48,1,Business Economics,18 February 2013,https://link.springer.com/article/10.1057/be.2012.31,Finance Contributing to the Good Society,February 2013,Robert J Shiller,,,Male,Unknown,Unknown,Male,"In 2010, the U.K. nonprofit, Social Finance, developed a proposal to deal with recidivism rates at the Peterborough prison that had defied solution. After prisoners who had committed minor crimes were released from there, they often committed another crime and landed right back in the same position. For various bureaucratic reasons, the U.K. penal system seemed unable to offer ex-prisoners the kind of help they needed to rejoin constructive activities in society and was unable to deal with high re-incarceration rates at that prison. There was no government bureau specifically responsible for this problem, which seemed intractable. Social Finance acted like a good investment banker. It approached the U.K. Ministry of Finance, with an idea: Social Finance would put together a group of organizations to solve the problem, if paid to do so, for nothing more than a commitment from the U.K. government to pay them in six years according to a formula if the re-incarceration rates actually fell. Thus the government would issue a “social impact bond.” The U.K. government was persuaded to agree because the lower re-incarceration rates would save it money in the future, resulting in no net cost to the government. Social Finance then persuaded 17 investors—15 in the United Kingdom and 2 in the United States—to contribute 5 million pounds at risk. This group consists of other foundations with a philanthropic bent but also happy to make a return on their investment. If they succeed, Social Finance will develop a reputation not unlike that of a venture capital firm and may be able to put through many more such investments. For-profit investors as well as ethical investors may be attracted to this. There is now a U.S. branch in Boston, Social Finance U.S., which is on track to solve similar problems in the United States.",3
48,1,Business Economics,18 February 2013,https://link.springer.com/article/10.1057/be.2012.32,"John C. Goodman, Priceless: Curing the Healthcare Crisis",February 2013,Linda Gorman,,,Female,Unknown,Unknown,Female,,2
48,1,Business Economics,18 February 2013,https://link.springer.com/article/10.1057/be.2012.33,"Nate Silver, The Signal and the Noise: Why So Many Predictions Fail—But Some Don’t",February 2013,Ken Simonson,,,Male,Unknown,Unknown,Male,,
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.10,From the Editor,April 2013,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.11,A Dialog with George Soros,April 2013,George Soros,Anatole Kaletsky,,Male,Male,Unknown,Male,,
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.7,What's Wrong With the Federal Reserve: What Would Restore Independence?,April 2013,Allan H Meltzer,,,Male,Unknown,Unknown,Male,"One possible defense of the limits on independence might be that the Federal Reserve's policies were more successful as a result. Selgin, Lastrapes, and White [2012] cast doubt on that conclusion. Their comparison suffers from differences in the quality and content of data over two distinctly different periods, under very different political regimes. It seems better to conclude that a largely discretionary policy has not brought clear evidence of superior performance. My own study of Federal Reserve history found that, in its (almost) 100 years, the Federal Reserve has rarely achieved sustained periods of relatively stable growth and low inflation. The two periods I identified were both years in which the Federal Reserve more or less followed a specific rule. In 1923–28, the Federal Reserve followed a weak type of gold standard. From about 1985–2003, the Federal Reserve closely followed John Taylor's rule [Taylor 1993]. In other nonwar years, the Federal Reserve caused the Great Depression and did very little during the subsequent slow recovery, 1929–41. Its main action contributed to the serious 1937–38 recession. During the Great Inflation, 1967–79, it produced a series of cycles that usually ended with higher inflation followed by recession and increased unemployment. This is not a distinguished record. Regulatory policy does not improve the record. The Federal Reserve watched while banks reduced equity capital after the government approved deposit insurance. Before the most recent crisis, the Federal Reserve permitted large banks to circumvent capital regulations that would have restricted their portfolios of risky mortgages. And it sent examiners into all large banks to observe portfolio decisions, but it failed to prevent any purchases. Earlier, the Federal Reserve discussed the problems created by interest rate ceilings on bank deposits, but it never chose to remove them. As a result, a gigantic nonbank industry rose. In the 1920s, the Federal Reserve succumbed to bank pressure by permitting national banks to invest in mortgages. And it took more than one banking crisis to rid the United States of many local or regional banks that failed in large numbers when the local industry went into recession and could not repay its borrowing.",4
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.1,Federal Reserve Independence in the Aftermath of the Financial Crisis: Should We Be Worried?,April 2013,Donald Kohn,,,Male,Unknown,Unknown,Male,"Naturally, understandably, and appropriately, these circumstances have increased the scrutiny of central banks, including the Federal Reserve, raising questions about the goals, governance and accountability of these institutions. This panel has been asked whether we should be worried that this scrutiny will result in an erosion of independence. To foreshadow my answer: the extreme steps taken by central banks in response to the crisis should not and need not lead to a loss of monetary policy independence, but we need to be vigilant because the number and magnitude of risk factors have increased. When discussing central bank independence, it is important to draw two key distinctions about what we mean. The first distinction concerns functions performed by the central bank. With regard to independence, our main focus has been on the setting of monetary policy, not regulatory policy. In this regard, the Federal Reserve has always lived with a bifurcated regime. The regulatory functions of the Federal Reserve have involved a very high degree of cooperation and coordination with other agencies. Major decisions are arrived at jointly by several agencies. And those decisions are subject to examination and oversight by the General Accountability Office of the Congress. The cooperative character of bank regulation is made necessary by the balkanized U.S. regulatory system, with many different agencies having a hand in regulation and supervision of the financial system. It also reflects the nature of the actions taken. Regulation is necessary to offset the moral hazard created by the safety net available to financial institutions and to deal with externalities. But it involves elements of credit allocation, it constrains private decisions, and it affects the relative positions of individual firms. And it can have consequences for the public purse if regulation and supervision are not effective enough at constraining risk. Some degree of independence from political pressure is helpful in carrying out these tasks, but they may not lend themselves to the same arm's length relationship to elected representatives as does monetary policy. The conduct of monetary policy has enjoyed considerably more, but still limited, independence. Within monetary policy, it is useful to distinguish goal from instrument independence. Goals for policy are and should be set in the democratic process by elected representatives. But independence is critical in the setting of the instruments to achieve these goals. Central banks should be held accountable for outcomes, not inputs. Instrument independence is necessary to overcome the short-term perspective of politicians, who are more interested in boosting growth for the next election and less focused on the longer-term inflationary consequences of such actions. Across time and countries there is plenty of evidence that less independence is correlated with higher inflation. Even instrument independence is not absolute. Instrument settings will always be subject to political pressure and discussion. Moreover, some control is exercised through the appointments process, which for the Chairman occurs every four years. Nonetheless, an independent central bank—one that has been insulated from these pressures—does not need to follow the politicians’ instructions; it should resist where those desires are inconsistent with its own views of how to achieve the objectives it has been given.",4
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.8,The Road Ahead: The Graying of America and its Implications for Finance and the Economy,April 2013,Roger W Ferguson Jr,,,Male,Unknown,Unknown,Male,"So to begin: where are we today? These remain challenging times—with a host of economic and political uncertainties. Our nation's economic recovery remains slow and uneven, and there remain some risks in the global picture. Our high unemployment rate continues, although its gradual decline has certainly been welcome news. Moreover, it is unknown whether dueling visions of fiscal policy can be reconciled. The Federal Reserve's announcement of QE3 in September 2012—and its plan to maintain low interest rates through mid-2015—remains controversial, with some hailing it and others dismissing it. In one of NABE's own surveys of economists, conducted before QE3 was announced, nearly 60 percent did not think a third round of quantitative easing was warranted, with the respondents split on whether the first two programs had made a positive impact on the economy. Amid all the uncertainty, it is no surprise that many companies are taking a wait-and-see approach with regard to hiring and investment, despite having record levels of cash. On the positive side, some sectors of the economy—especially the housing market—have shown steady improvement. And despite sluggish U.S. and global growth prospects, equity markets are doing well. So there is room for some cautious optimism about our prospects going forward.",2
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.4,Forecasting the Downturn of the Great Recession,April 2013,Herman O Stekler,Raj M Talwar,,Male,,Unknown,Mix,,
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.6,Looking Ahead: Opportunities and Challenges for U.S. Manufacturers,April 2013,Chad Moutray,Kevin Swift,,Male,Male,Unknown,Male,"This section outlines three major challenges that manufacturers will face in the coming years. These are (1) strengthening the overall economy, (2) taxes and regulation, and (3) attracting and retaining a quality workforce. When Moutray [2009] discussed the challenges facing small businesses four years ago, the U.S. economy was mired in a steep economic downturn. The “Great Recession,” as it is often referred, officially began and ended on December 2007 and June 2009, respectively, according to the National Bureau of Economic Research [2010]. Manufacturers—along with their brethren in the construction sector—took the brunt of it. Industrial production for manufacturers declined 20.8 percent. By the end of 2009, the sector had also lost 2.3 million workers, or 16.6 percent of its total workforce, as shown in Figure 1. For durable goods manufacturers, the damage was even worse, losing nearly 20 percent of their employment. To put it all in perspective, manufacturing and construction firms accounted for 26.3 percent and 21.2 percent of all of the nonfarm payroll losses in 2008 and 2009, respectively, or almost half of the 8.7 million workers who lost their jobs in those two years. Manufacturing Employment Growth (December 2007=100)Source: U.S. Bureau of Labor Statistics. Since then, manufacturing has rebounded. While industrial production remains 4.5 percent below its prerecession levels, it was up 20.6 percent at the end of 2012 since mid-2009. The largest gains occurred in the durable goods sectors, which increased nearly 27 percent since the beginning of 2010. Nondurable goods production grew 4.5 percent during that time frame. Sectors such as motor vehicles, machinery, plastics and rubber products, computer and electronic products, aerospace, metals, electrical equipment, and petroleum and coal products were among the strongest gainers in the economic recovery. Incidentally, these were also the sectors with the greatest increases in employment growth (Table 1). Much has been made of the “renaissance” in manufacturing. After two decades of declining employment, the recent net job gains are highly welcome. Yet, this revival goes beyond just having more workers. While mostly anecdotal in nature, we continue to hear more about manufacturers who have expanded production in the United States from elsewhere. Some of the factors for this reshoring include rising costs in China and elsewhere [Boston Consulting Group 2011], improvements in labor productivity and quality in the United States, increased transportation costs, and a general re-evaluation of the supply chain.Footnote 1 Of course, there are numerous factors involved in deciding where to locate one's production process, and there are some structural impediments that might make the United States less attractive to some (having the highest corporate tax rate in the world, for example) [Manufacturing Institute/MAPI 2011]. Yet, this is occurring frequently enough to say that the United States has turned a corner on the topic of location, with more manufacturers thinking of U.S. locations as viable either for reshoring or for new operations [PricewaterhouseCoopers 2012]. These facts aside, it is also clear that there is a lot of room for improvement. Manufacturing employment today is still well below the levels seen in December 2007. Figure 1 illustrates jobs progress since the beginning of the recession, with manufacturers shedding a larger percentage of its workforce than the larger nonfarm economy. While nonfarm employment remains roughly 2.9 percent below the prerecession level, the number of manufacturing employees is 12.8 percent lower than before. Virtually all of the net job gains in the past couple of years have come from the durable goods sector, which also experienced the steepest drop during the recession.Footnote 2 Lately, growth in manufacturing has stalled, with businesses and consumers anxious about slowing global sales and concerns about the future direction of the economy. On the international front, several of the top markets for U.S. manufactured goods exports struggled in 2012, even though by year's end, there were definitely improvements outside of Europe and Japan.Footnote 3 With slowing economic growth overseas, we have seen export orders ease substantially. In the first nine months of 2012, manufactured goods exports rose 6.6 percent, well below the 15.9 percent increase experienced in the same time period in 2011. On the one hand, the fact that exports have remained positive might be a surprise, given the number of headwinds in the market, including a recession in Europe. Yet, it is clear that international trading activity has ticked down of late because of slower growth, hurting manufacturers’ ability to expand their businesses. As noted earlier, manufacturers were extremely concerned about the fiscal cliff and budget sequestration at the end of 2012, with many of them pulling back on hiring and capital investment due to their anxieties. Real GDP grew only 0.4 percent, largely on fiscal cliff frustrations and slowdowns related to possible budget sequestration. Political frustrations and concern about the impact of the “cliff” on the economy was the top concern, cited by over 84 percent of respondents, in the NAM/IndustryWeek Survey of Manufacturers released in December [Moutray 2012]. In the end, the fiscal cliff was partially averted in a Congressional deal enacted on January 2, 2013.Footnote 4 Yet, this legislation did not address the long-term fiscal challenges for the country. As a result, there are still seeds of doubt left to address, with businesses closely following how policymaker actions to address our debt and deficit might impact them. Moving forward, though, manufacturers are eager to build on recent gains, with the slowness in the second half of 2012 being the exception to what has been a decent three years since the recession. The sector is poised to continue to expand, taking advantage of new opportunities and its recent momentum. But this will hinge on the adoption of progrowth policies that will allow businesses to flourish. Once business leaders feel that the economic environment will be on a firmer footing, we believe that manufacturers will respond with more investments and increased hiring. Some of the dollars that have been on the sidelines waiting for a strengthened economy should flow back into the marketplace. While we are generally bullish on manufacturing prospects in the coming years, there are some things that policymakers can do to improve the global competiveness of the U.S. manufacturing industry. At the top of this list is the perceived business environment. According to the Manufacturing Institute [2011], U.S. manufacturers face a 20 percent structural disadvantage on nonlabor costs compared with competitors in our largest trading partners. A large part of this discrepancy can be explained by our high corporate tax rates. As of April 1, 2012, U.S. corporations pay the highest rates among all developed nations. Politicians from both parties are advocating for corporate tax reform, and there is a sense of urgency for the need to make the United States more competitive globally. Reform proposals under discussion generally lower the corporate tax rate and broaden the tax base by reducing or eliminating some (or all) tax deductions and credits. Businesses generally support reducing the corporate tax rate from 35 to 25 percent or lower, with the Obama administration pushing for a rate around 28 percent and lower for manufacturers [The White House 2012]. Manufacturers are also supportive of shifting to a territorial tax system as part of corporate tax reform, which would allow businesses to bring their earnings made elsewhere back into the United States without an additional tax on profits. While corporate tax reform is a worthy and necessary goal, we cannot forget that a lot of our small and medium-sized manufacturers are organized as pass-through entities like Subchapter S corporations and pay taxes at the individual personal—not corporate—rate. Based on the most recent IRS data (2008), almost two-thirds of all tax returns from manufacturing businesses were from pass-through entities, including S corporations, partnerships, and LLCs.Footnote 5 These entities also accounted for over 20 percent of all of net income that year. For that reason, many business leaders are pushing for comprehensive tax reform—and not just corporate tax reform—as a worthy goal to both broaden the base and improve the competitiveness of our tax structure. U.S. businesses also worry about the overall regulatory burden, which is perceived to be excessive and increasing. In the most recent NAM/IndustryWeek Survey of Manufacturers, three-fourths of the respondents said that an unfavorable business climate—including the tax and regulatory environment—was one of their top challenges [Moutray 2012]. One of the more widely cited studies of federal regulations found that the burden was disproportionately high for manufacturers [Crain and Crain 2010]. This is especially true because manufacturing is such a dynamic process, involving the transformation of raw materials into finished products. It creates more environmental and safety regulatory issues than other businesses. Moreover, the burden is heaviest on small manufacturers because there are economies of scale in mitigating the cost of compliance. The notion that we need to do more to reduce regulatory burdens is not one that is unique with manufacturing. President Obama signed executive orders and the Office of Management and Budget (OMB) has issued memoranda on the principles of sound rulemaking, considering the cumulative effects of regulations, strengthening the retrospective review process, and promoting international regulatory cooperation [Federal Register 2011]. With that said, it is important to keep in mind that these efforts might not be enough to counteract overly burdensome regulations that federal agencies will be promulgating in the coming months, many of which stem from the Wall Street Reform and Consumer Protection Act (Dodd-Frank) and the Affordable Health Care for America Act (ObamaCare). In addition, there are a host of consumer protection, environmental, labor, and occupational rules that will be working their way through the process. While other issues tend to garner most of the headlines, the ability to attract and retain a quality workforce is one that we hear about from nearly every manufacturer. An internal National Association of Manufacturers (NAM) survey of small and medium-sized manufacturers conducted in October 2012 found that over 62 percent of them had positions at that time that they were unable to fill because of a shortage of qualified candidates. That is an astounding statistic, and one that is perhaps surprising given the still-elevated unemployment rates that we continue to experience. A study by Deloitte [2011] estimated that the “skills gap” was equal to roughly 600,000 workers. A fair share of this challenge is due to the changing dynamic in manufacturing that emphasizes a higher-skilled workforce. To be sure, the United States still maintains a comparative advantage overall within manufacturing, and productivity improvements help keep us competitive with our global trading partners. U.S. manufacturing today is a high-income, high-skill endeavor. Technological know-how is an essential ingredient in production. As manufacturers have become more frustrated with the ability to access a skilled workforce, industry-recognized training and certification programs are increasingly necessary to meet this demand. Reflecting the skills required for many of these jobs, manufacturing employees averaged over $77,000 in annual compensation in 2011, or roughly $20,000 more than the average for all private sector workers. At the same time, manufacturers are currently benefiting from investments made in innovation and technology in years past. Those investments help to propel productivity, keep manufacturing costs down, increase process efficiencies, and allow U.S. companies to become more competitive globally. To stay ahead, though, we must stay focused on new innovations that keep us at the forefront of technology. This means expanding our efforts with respect to investments in both human and physical capital. Many of our leading trading partners—particularly in Asia—are devoting tremendous resources to innovate, and our students’ knowledge in science, technology, engineering and math (STEM) skills lags behind where it should be. We risk our long-term competitive advantage if we do not make a strong commitment to these areas, ceding ground to nations that are willing to make these investments. In addition, as manufacturing faces retirements, the pipeline of new talent able to replace the aging workforce is shrinking. While the public perception of manufacturing ranks high with all Americans, including today's students, there are stigmas with going into a “blue collar” occupation that confronts those looking for new talent. Among today's students and young job-seekers, manufacturing ranks toward the bottom of the list of career choices. According to one study, for instance, only 17 percent of surveyed 18- to 24-year olds were encouraged to pursue a career in manufacturing [Deloitte 2012]. To combat this challenge, the Manufacturing Institute—which is an NAM affiliate—announced the goal of credentialing 500,000 workers with skills certifications aligned to manufacturers’ hiring needs. President Obama highlighted The Manufacturing Institute's NAM-Endorsed Skills Certification System in June 2011 and announced key steps toward building the educated and skilled workforce U.S. manufacturers need to successfully compete in the twenty-first-century economy [The White House 2011]. The NAM-Endorsed Manufacturing Skills Certification System is a group of stackable credentials applicable to all sectors of the manufacturing economy. These nationally portable, industry-recognized credentials validate the skills and competencies needed to be productive and successful in entry-level positions in any manufacturing environment, and they can be learned and earned in secondary and postsecondary education. The result is a professional and technical manufacturing workforce with valuable industry credentials, making companies more innovative, more competitive, and more marketable. The Manufacturing Skills Certification System, which is already available in 31 states and continues to expand, can help manufacturers achieve the trained workforce they demand.",3
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.9,The Transition from a Major Commercial Bank to a Privately Held Financial Services Firm,April 2013,Diane Swonk,,,Female,Unknown,Unknown,Female,"Banking was not exactly fun during my career, with merger layered upon merger, increasing complexity and escalating risks. That said, as chance would have it, I got out just in time. A mutual friend introduced me to Jim Tyree, the CEO of Mesirow Financial in 2004. He was a larger-than-life figure in Chicago, with a reputation for being visionary, ethical, and generous. Jim was a life-long diabetic; as a result of that condition, he did more for medical research and education than anyone I ever met. He was also a consummate South Sider, who loved sports, particularly the White Sox, and Chicago food. Jim, however, was better known than the company he was leading, building on the legacy of founder Norman Mesirow. That was something Jim wanted to change. He did not believe in advertising for advertising's sake. He also did not like public relations or spin. Mesirow Financial was a firm born of the depression, in 1937, when the emphasis was on reestablishing public trust in Wall Street. When I came to the firm in 2004, it still relied heavily for new business on word-of-mouth and the credibility it earned with clients. Much of its growth, however, was coming from abroad, where word-of-mouth did not carry the same meaning to potential clients. Jim and I played phone tag for what seemed like weeks before we finally caught up with each other. It was late on a Friday night before I could return his call; he asked if I could meet him for lunch on the following Monday. At that point, I was in talks with another prominent bank and still knew too little about Mesirow Financial. I spent the weekend learning all that I could and drew up a plan listing what I could do for the firm. Monday at lunch, I laid out my strategy to raise the brand equity of Mesirow Financial by leveraging the credibility I had built in the banking industry. Financial service firms differentiate themselves by knowing more than their competitors; one way to illustrate that is to be seen as an expert by the media. I had received enough regular media coverage over the years to fill that bill. In the late 1990s, the Atlanta Federal Reserve Bank did an analysis of the top forecasters in the country, and I made the cut. The only sad part was that to be considered among the top echelon, you only had to be right about 50 percent of the time. It is difficult to have too much hubris in a profession where the probability of being “good” is about the same as a coin toss. I had built up an extensive network of economists and policymakers from my years in banking, with the National Association for Business Economics (NABE) and as a member of myriad groups to which I belonged. I led organizations, working with the top echelons in the business community, and developed contacts in nearly every industry and major country. The combination of widely sourced contacts meant that I could enter the new job at Mesirow Financial with a very small team: myself, my editor, and my former mentor and first boss, Jim Annable, on retainer as a consultant. Annable had taught me that the only way to protect our intellectual property, the proprietary model we had designed, was to move it outside the bank with a consulting relationship. Jim Tyree understood this and agreed to pay the consulting fees for Annable, to keep our team intact. He made me a handshake offer of a job I accepted and started soon after. I later learned from Richard Price, one of Jim's closest friends and board members at Mesirow Financial, that the Board had been reluctant to hire me; they had no idea what a chief economist could do for the firm. Jim made one of many executive decisions and hired me anyway. Richard later told me that hiring me was one of the better decisions Jim ever made. That was one of the best compliments I ever received for my work and a harbinger of what it would be like to work in an environment where compensation was aligned closely with the fate of the company. Mesirow Financial is owned by about 300 of its 1130 employees. They have bought in with their own money, a policy that kept incentives aligned. Jim refused to take the company public because he said it would distort the company's culture and incentive structure. Four months into my tenure at the firm, Jim called me into his office. He told me that he knew what I could do to raise the visibility of the firm externally. What had surprised him was what that recognition meant for internal morale. The press I generated for Mesirow Financial was just the start of a broader strategy: to open doors for the firm and have our line producers leverage my expertise and sources to grow their business. This would mean offering more off-the-record briefings with top clients and prospects, as well as working with key business lines to help them manage economic risk. It took some time for producers to learn how to use me best. Too often, people ask the wrong questions of economics and economists. It is our job to frame the debate and actively define our positions. The hardest issue I find is speeches. Some people see economists as entertainers to headline an event, rather than strategists. Delivering speeches was important when I was establishing my career and in aligning me with Mesirow Financial when I started; it is even something that people have been willing to pay me to do. The risk is that it also tends to render my work a public good and therefore of less value to the firm. I have found that the key is to provide the public with a sense of what I can do as an economist—via publications, speeches, press, blogs, twitter, and so on—without giving up the best information for free. I also have to consider the time that speeches demand. If I accepted all that I am asked to do, I would not have time to do anything else. I have always said that the external things I do are effectively the icing on the cake; I still have to bake the cake (do the research). The speeches serve another purpose—leverage to add value to the firm. We trade my service for in-kind sponsorships, signage, attendance, and tables for key events.",
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.5,Evidence on the Growing Profit Disconnect between the Public's Views of Corporate America,April 2013,Charles F Beauchamp,William F Ford,Douglas M Tatum,Male,Male,Male,Male,"The significance of corporate profits to the value of the firm and the capitalist economic system is well established [Stewart 1991]. The importance of a broad understanding of profits as the entrepreneurs’ reward for effort and risk taking in creating and managing business enterprises, at least among college graduates, is essential to assuring the health of the firm in a capitalistic system. Yet, Berner [2002] contends that even business analysts undervalue the importance of profit by not concentrating enough of their focus on it. This lack of profit focus, even by business professionals, suggests additional undervaluing of the importance of profits among college students. From a marketing and operational standpoint, corporate management sets prices that are equal to their customers’ perceived value of goods and services demanded. Contained within these prices is the profit earned by the firm [Lamb and others 2011]. This profit represents a reward to the firm for providing value to the customer. Reaching the correct price is imperative because setting prices too high would leave customers dissatisfied, thus reducing sales volume and firm profits. However, firms can maximize profits without maximizing prices. Firms accomplish this by reducing costs and increasing customer satisfaction [Lamb and others 2011]. In fact, prior research establishes a significant positive relationship between customer satisfaction and firm profits [Rust and others 2002]. This link between customer satisfaction and firm profits should logically support a positive public opinion of firm profits, yet the reverse appears true. Again, a lack of knowledge regarding the true nature of profits may help explain this discrepancy. Existing research establishes that a large deficit in economic and financial literacy exists among the American public [Lusardi and Mitchell 2007; Braunstein and Welch 2002]. The majority of these studies concentrate on the knowledge deficit's effect on household income and consumer decisions. Few, if any, concentrate on the literacy level's impact on opinions regarding corporate finance functions and their effects. Thus, simply not considering the nature of profits as a source of value could contribute to the reported negative opinions toward firm profits. If the percentage of Americans that harbor these negative opinions were to increase further, then the public's trust in Corporate America might also decline further. The result of such a decline, in turn, would be an erosion of our free enterprise-oriented economic and political systems [Cook and Schilke 2010]. Therefore, it is imperative to establish whether a growing knowledge deficit exists, not just among the general public, but even among college students. We provide preliminary evidence of that, below.",
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.2,"June E. O’Neill and Dave M. O’Neill, The Declining Importance of Race and Gender in the Labor Market",April 2013,John C Goodman,,,Male,Unknown,Unknown,Male,,
48,2,Business Economics,17 May 2013,https://link.springer.com/article/10.1057/be.2013.3,"Robert Sirico, Defending the Free Market: The Moral Case for a Free Economy",April 2013,Merrill Matthews,,,Male,Unknown,Unknown,Male,,
48,3,Business Economics,13 September 2013,https://link.springer.com/article/10.1057/be.2013.18,From the Editor,July 2013,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
48,3,Business Economics,13 September 2013,https://link.springer.com/article/10.1057/be.2013.13,Commentary and Dialog with Paul Volcker,July 2013,Paul Volcker,Steve Liesman,Kenneth D Simonson,Male,Male,Male,Male,"I have two other things on my mind, at least one of which may strike you as a bit unusual. It is, however, a much neglected subject despite the fact that it has been a major underlying ingredient in the financial and economic crisis we have. This is the international monetary system. Of course, it’s hard to call it a system. A system concerns itself with some interrelated parts in a mechanism that are working together to produce some stability and progress: that’s hardly a description of the international monetary system—what many people have said is an international nonsystem. I believe that it is a nonsystem—and while I’m not going to elaborate on it right now—I want to call your attention to one characteristic of that nonsystem. In the first seven or eight years of this century, its biggest characteristic was a big current account surplus on the part of China and a huge accumulation of U.S. dollars by China and others. Of course, looking at the other side, United States has been running the biggest current account deficits that it had ever run, in my memory anyway—the biggest economy in the world running current account deficits reaching 5 or 6 percent of GDP and foreign purchasers of Treasury debt financing most of its large federal deficit. These purchasers have been happy to lend at very low interest rates in a seemingly prosperous world as a by-product of their current account surpluses. Now, would anybody really design a system that would permit the Chinese to run cumulative current account surpluses of trillions of dollars over a few years? One in which the United States can run deficits of that size for a period of time without suspecting that something is going to happen in a serious way to upset the system? It is all very nice to be able to finance external deficits very easily while it happens. After all, the world is giving the United States a lot of rope for the exorbitant privilege of the dollar being the reserve currency. But at what point does your rope get to the point where it strangles you? We should try to avoid that. I’m not sure that we are at that point yet; but I am concerned about the lack of control in this system, which is replicated within the euro zone where deficits of some member countries could go along more or less indefinitely. I worry that finally, finally, something happens that interrupts the flow and the United States finds itself with a degree of international indebtedness that is not easy to manage.",1
48,3,Business Economics,13 September 2013,https://link.springer.com/article/10.1057/be.2013.15,The Effectiveness of Central Bank Independence vs. Policy Rules,July 2013,John B Taylor,,,Male,Unknown,Unknown,Male,"To measure changes over time in macroeconomic performance, I focus here on the size of the fluctuations in real output and inflation. A simple framework for evaluating the effect of monetary policy on such fluctuations is the tradeoff between variance of inflation and the variance of output that was developed in the years preceding the Great Moderation [Taylor 1979; 1980]. This is the framework that Ben Bernanke used in his assessment of monetary policy and performance in his paper, “The Great Moderation,” first presented in 2004. The framework has also been used by other central bankers, including Mervyn King [1999; 2012] and Lars Svensson [2012]. While the tradeoff between the levels of inflation and output (or unemployment) is very short lived, the tradeoff between the fluctuations of these two variables is longer lasting and appropriate for comparing economic performance for more than two or three years. This framework naturally takes research beyond the question of why the financial crisis occurred and puts it in a broader context of why the downturn was large, why the recovery was so slow and—depending on the future—why the next downturn is likely to be large or small. We are considering fluctuations over longer periods of time. Figure 1 replicates the tradeoff diagram as it appears in Bernanke [2004]. On the horizontal axis is the variance of inflation; on the vertical axis is the variance of real output (deviations from potential GDP). Points more to the north or to the east represent more macroeconomic instability and thus poorer economic performance. Chart from Bernanke [2004] “The Great Moderation” The curve represents a tradeoff in the sense that along the curve monetary policy can achieve smaller inflation fluctuations only by generating larger output fluctuations. Points to the left or below this tradeoff curve are infeasible for a given structure of the economy. Points to the right and above are inefficient, in the sense that a better monetary policy would be on the curve. The position and shape of the curve depend on the underlying structure of the economy and the size of the exogenous shocks to which it is subject. An economy with less rigid wage and price setting has a tradeoff curve closer to the origin than an economy with more rigid wages and prices. An economy with larger external shocks has a tradeoff curve further away from the origin than an economy with smaller shocks. Tradeoff curves can be derived quantitatively from a wide range of estimated or calibrated macroeconomic models, including dynamic stochastic general equilibrium models and New Keynesian models of the type collected in Volker Wieland’s and others [2012] monetary model database. Of course, the curve will differ somewhat from model to model because the economic structures of the models differ. The position of the economy on a given curve depends on how much emphasis the monetary authority places on inflation fluctuations vs. output fluctuations. For example, a higher weight on inflation in the central bank’s objective function implies a position on the curve more to the upper left. Using Figure 1 and these ideas, Bernanke [2004] examined the reasons for the Great Moderation. The momentous movement from the instability of the 1970s toward the Great Moderation can be represented in the diagram by a movement from point A to point B. Alternative causes of such a movement can be illustrated using the curve. If the cause is smaller shocks or an improved economic structure—such as more flexible or more forward-looking wage and price setting—one can represent this as a shift of the curve from TC1 to TC2. If the cause is a better monetary policy—such as a move from go-stop policies in the 1970s to more predictable rule-like policies in the 1980s and 1990s—then the move is toward a given TC curve. In that case, one could say that the tradeoff curve was always at TC2 and policy moved from the inefficient point A to the more efficient point B. Of course, in reality, both shifts in the curve and movements toward the curve might be at work. Arguments have been made on both sides of this debate about the causes of the Great Moderation, and many empirical papers have been written, from the Stock and Watson [2002] research with time-series models to the Cecchetti, Flores-Lagunes, and Krause [2006] research with structural models. Complicating the empirical work is a fundamental interrelation between the alternative causes: an improvement in monetary policy might lead to a change in the structure of the economy if, for example, wage and price decision-making becomes less rigid as a result of the change to a more predictable policy, as pointed out in Taylor [1980]. Considering all these arguments, Bernanke [2004] concluded that a move toward a more efficient monetary policy was a significant cause of the Great Moderation. I completely agree with that assessment—and for similar reasons—as stated in Taylor [1998]. Moreover, it is likely that the change in policy generated an improved economic structure as represented by some leftward shift of the tradeoff curve. However, the Great Moderation has ended, and it is time to move on to study the causes of this equally momentous change. In Table 1, I show the actual variability of the key variables. I report the variance as well as the standard deviation, which was the variability metric I originally focused on in Taylor [1979], where I drew the tradeoff curve in standard deviation space. The variability measures in Table 1 are computed for the three time periods indicated. They represent the periods before, during, and after the Great Moderation. The variance and the standard deviation of inflation are measured by the quarterly percentage change (at an annual rate) in the GDP price index. The variance and the standard deviation of output are measured from the GDP gap, or the percentage deviation of real GDP from the Congressional Budget Office’s estimate of potential GDP. For the third period, I measure output variability by the fluctuations of the output gap around zero rather than about its mean, which is −4.63 per cent during that period. Note that the period since the end of the Great Moderation is only five years in length and shorter than the other periods. The recovery from the 2007–2009 recession does not appear to be over, and thus the change in the standard deviation may exaggerate the deterioration of performance in a post-Great Moderation regime. It is very difficult to identify an emerging historical period in real time (and of course we hope the economy will go back to Great Moderation conditions soon). By way of comparison, I first wrote about the post-1984 secular decline in volatility in Taylor [1998]—14 years after it began. By that time we had the strong recovery from the recession in the early 1980s, the small recession of the early 1990s, and the start of a long expansion in the 1990s. Nevertheless, there is already plenty to study about this post-Great Moderation period even though we will certainly learn more as time goes on. To represent this change I have updated, in Figure 2, the variance tradeoff diagram used by Bernanke [2004] by adding a point C and an arrow from point B to point C. Chart from Bernanke [2004], Updated to Post-2006 Observe that the line from point B to point C does not simply retrace in reverse the path from point A to point B. The movement from the 1970s toward the Great Moderation is much as in Bernanke’s [2004] generic sketch. But the movement away from the Great Moderation, thus far, is much different. It is a nearly perfectly vertical move upward in the diagram. Virtually all of the deterioration in performance is reflected in a major increase in output volatility due to the Great Recession and the very slow recovery. Inflation performance has remained steady, though that could change in the future. The end of the Great Moderation raises many of the same questions that have been raised about the Great Moderation itself. Was the end due to a change in the structure of the economy traced, for example, to less aversion to risk as argued by King [2012]? In this case the tradeoff would have shifted back away from the origin. Or was there a change in monetary policy as I have argued in Taylor [2012], in which case the tradeoff curve did not simply move exogenously, but rather policy took the economy to point C as shown in Figure 2. That virtually all of the deterioration in macroeconomic performance has been on the output dimension, not the inflation dimension, is an important fact that helps identify the reasons for the shift.",26
48,3,Business Economics,13 September 2013,https://link.springer.com/article/10.1057/be.2013.12,Financial Crises and Central Bank Independence,July 2013,Alan S Blinder,,,Male,Unknown,Unknown,Male,"Lots of things change in a crisis. One of them is the time horizon. When the issue becomes how to keep your financial system functioning until the weekend, the hangman’s noose concentrates the mind wonderfully—and not on the long run. The central bank’s vaunted long time horizon evaporates, temporarily replaced by the same time horizon the treasury has—which is short. That makes cooperation more natural. Second, and related, the principal objective of the central bank changes. It is no longer fighting inflation, which may be unnecessary or even counterproductive if a slump is imminent. In fact, the central bank’s emergency policies may appear to be (or actually be) pro-inflationary, not anti-inflationary. The bank must focus, instead, on holding the financial system together. That is also the treasury’s overriding goal. The combination of urgency and overwhelmingly common interests virtually cries out for cooperation, even though central banks and treasuries have different operating styles, capabilities, legal authorities, institutional concerns, and prejudices. Each has its comparative advantages, and two can do the job better than one. Third, cooperation may be crucial to calming nervous or panicky financial markets. The need for coordination has operational, political, and psychological dimensions. Only the central bank has unlimited means—and the ability to deploy them quickly without legislative approval. But the treasury is needed for political legitimacy, and few things could be worse in a crisis than seeing the central bank and the treasury at loggerheads. The two must present a united front to jittery markets, and maybe also to a jittery legislature. In fact, having two agencies working hand-in-glove is not enough in the United States, where bank supervision and deposit insurance are in different hands; all the relevant authorities should be on the same page. A fourth reason for close cooperation is that the central bank may be drawn into what are called quasi-fiscal operations. The adjective tells you immediately that the neat separation between monetary policy and fiscal policy has gone by the board. Modest-sized lender-of-last resort operations along Bagehot lines are one thing. If the collateral is really good, they do not resemble appropriations. But massive lending and/or asset purchases involving possible losses are akin to backdoor spending, or at least to putting taxpayer money at risk. This resemblance to fiscal policy has not gone unnoticed in the United States, the United Kingdom, or the Eurozone. Fifth, when it comes to deciding which specific financial institutions should live on with taxpayer support (such as Bank of America, Citigroup, AIG,…) and which should die (such as Lehman Brothers violently, Bear Stearns peacefully,…), political legitimacy is critically important. The central bank merits a prominent place at the table, but it should not make such decisions on its own. If the issue becomes politicized, as is highly likely, the treasury, not the central bank, should be available to take most of the political heat—even if the central bank provides most of the money. That is part of what I meant by having different comparative advantages. Finally, in worst-case scenarios, the central bank’s accounting solvency might come into question. The Federal Reserve never came close to such a situation; but if it had, the treasury would have had to backstop it.Footnote 1 If, at some future date, the European Central Bank (ECB) suffers sizable losses on its then-large portfolio of sovereign bonds, the constituent governments will have to stand behind it. Those are a lot of reasons to cooperate, and you can probably think of others. My conclusion is that, in a crisis, preserving strict CBI is neither possible nor desirable. It would have been foolish, in September 2008, for Ben Bernanke to refuse to take Hank Paulson’s phone calls.",4
48,3,Business Economics,13 September 2013,https://link.springer.com/article/10.1057/be.2013.14,A Credible Path for Ending Too Big to Fail,July 2013,Richard Fisher,Harvey Rosenblum,,Male,Male,Unknown,Male,"The mere 0.2 percent of banks deemed ""too big to fail"" (TBTF) are treated differently from the other 99.8 percent, and differently from other businesses. Small and midsized institutions—that each have an asset size of less than $250 billion and, as a whole, account for about one-third of industry assets—face the credible and palpable threat of closure from their regulators, as well as market discipline from owners and/or creditors. The largest institutions, however, are not subject to a sufficient amount of regulatory or market discipline, rendering them seemingly exempt from the normal processes of bankruptcy and creative destruction. Implicit government policy has allowed the banking giants and their counterparties to take excessive risks without fear of failure if their bets go bad. The banks’ creditors believe the government will backstop their investments, simultaneously undermining the profitability and viability of smaller institutions that enjoy no such implicit guarantee. Economics 101 tells us that if you want more of something, you should subsidize it. The irony is that government policy subsidizes excessive risk taking—the very thing we want to reduce. Although TBTF financial institutions were not the sole cause of the financial crisis, they were a primary mechanism through which shocks were transmitted throughout the financial and economic systems. Many TBTF banks and their subsidiaries were major players in shadow banking activities dependent on short-term, nondeposit wholesale funding—using financial instruments such as commercial paper and money market funds—that spread systemic risk pervasively at the height of the crisis. The TBTF mindset also emboldens a sense of immunity from the law. As Attorney General Eric Holder acknowledged in testimony to the U.S. Senate on March 6, 2013: when banks are considered TBTF it is ""difficult to prosecute them … if we do bring a criminal charge, it will have a negative impact on the national economy” [Zibel and Kendall 2013]. Megabanks fund themselves more cheaply than do smaller banks. Studies, including those published by the Bank for International Settlements and the International Monetary Fund, estimate this advantage to be as much as 1 percentage point, or some $50 billion to $100 billion annually, in the period surrounding the financial crisis [Bank for International Settlements 2012; Ueda and di Mauro 2012]. In a popular post by editors at Bloomberg [2013], the 10 largest U.S. banks are estimated to enjoy an aggregate longer-term subsidy of $83 billion per year. Andrew Haldane, executive director for financial stability at the Bank of England, estimates the current implicit TBTF subsidy to be roughly $300 billion per year for the 29 global institutions identified as “systemically important” [Haldane 2012]. Whatever the precise subsidy number is, it is significant, it has not disappeared,Footnote 1 and it enables the biggest banking organizations, along with their many nonbank subsidiaries (investment firms, securities lenders, finance companies, and so on), to grow larger and riskier. It makes for an uneven playing field, tilted to the advantage of the giant banking organizations, and it places the financial system and the economy in perpetual jeopardy.",
48,3,Business Economics,13 September 2013,https://link.springer.com/article/10.1057/be.2013.20,Crises and the Great Recession,July 2013,John A Tatom,,,Male,Unknown,Unknown,Male,"Initially (2006–08), the surge in foreclosures was driven by foreclosures on adjustable rate subprime loans. Many of these borrowers came late to the mortgage interest rate cycle when the mortgage rate outlook was deteriorating and informed borrowers knew that adjustable rates loans would re-price upward, making adjustable rate loans unaffordable. Some of these borrowers were more speculative and took out such loans anyway, planning to sell their houses and repay the loans before their loans were re-priced. When the recession began, the situation changed as individuals with relatively high credit ratings and fixed rate loans began to lose their jobs and income and, as a result, enter the foreclosure process. Figure 1 shows the explosion in mortgage foreclosure inventory for various types of mortgages from 2002 to the second quarter of 2012, except for prime loans, whose series began in 2005:Q1. The foreclosure inventory includes all mortgages at any stage of the foreclosure process. The mortgage inventory rate has declined little, except for subprime adjustable rate loans (ARM), which led the explosion of foreclosures in 2007–08. The subprime ARM crisis shifted to a fixed rate and prime crisis during the first year of the economic recovery, as subprime problem loans began to decline and the recession and high unemployment led to more loan defaults among previously highly rated and employed borrowers. The foreclosure crisis began a year before the recession and continued through the recession, the recovery, and well into the economic expansion. Foreclosure Rates Have Declined Little since the Great RecessionSource: Mortgage Bankers Association. Figure 2 shows mortgage foreclosure starts for the same groups as used in Figure 1 for the period 2005:Q1–2012:Q2. Foreclosure starts are the new filings for foreclosure in each quarter and are a leading indicator of the foreclosure inventory. There has been a marked improvement (decline) in foreclosures of new subprime adjustable rate mortgage loans, since 2008:Q2, about two quarters into the Great Recession. This is also reflected in the new foreclosure starts for all subprime loans. Both remain quite high, however. Prime loan and all loan foreclosure starts remain near their peaks in 2009. Despite improvements, the data in Figure 2 suggest that the mortgage foreclosure crisis will continue. Foreclosure Starts, Except for Subprime, Show Little Improvement since the Great Recession (percent in each class)Source: Mortgage Bankers Association.Subprime ARM: purple line. Subprime: green line. All Loans: blue. Prime loans: red.",1
48,3,Business Economics,13 September 2013,https://link.springer.com/article/10.1057/be.2013.21,The Effect of State Corporate Income Tax Rate Cuts on Job Creation,July 2013,Xiaobing Shuai,Christine Chmura,,Unknown,Female,Unknown,Female,"General economic theory on profit maximization indicates that firms seek to maximize profits and minimize costs. In those models, high taxes act as additional costs to reduce a firm’s profitability and its ability to invest and hire more workers [Wu 2010]. At the federal level, Romer and Romer [2010] used national time-series data to perform an exhaustive analysis on federal corporate taxes. They provided evidence that federal tax increases had “a large, rapid, and highly statistically significant negative effect on output.” In their study, output was measured as gross domestic product (GDP). The study also found that higher federal taxes increased the unemployment rate in the nation. There is a plethora of empirical studies on the effects of state and local taxes on regional economic activities, such as income growth, a firm’s location choice and establishment growth, capital investment, foreign investment, and employment. However, the empirical results of state corporate tax rates on economic activities are not conclusive [Deller and Stallmann 2007]. Early studies on state taxes and economic growth were summarized in a comprehensive review of literature conducted by Bartik [1992]. Using a modified Delphi method summarizing dozens of studies conducted since 1979, Bartik concluded “that taxes have quite large and significant effects on [economic] activity,” such as employment, output, business capital stocks, and number of business establishments. Of the 57 interregional studies reviewed, 70 percent reported at least one statistically significant negative effect of taxes on one or more measures of economic activity. More specifically, Bartik [1992] summarized that state and local business taxes (including business property tax and corporate income tax) had a negative impact on local business activities, with the average elasticity of business activities with respect to state and local taxes being −0.3, ranging between 0.1 and −0.6. A few years later, Wasylenko [1997], in a second review of the literature on the link between taxation and economic growth, pointed out that even though Bartik’s review identified negative effects of state and local taxes on economic activities, the magnitudes of the effects varied widely. The tax effect depends on the source of data and economic variables used—employment, income, investment, or firm locations. In terms of the effect of business taxes on employment, two of the three studies included in the review indicated a negative and significant effect of business taxes. Wasylenko found that over time, tax differences between states have become less important in employment growth, perhaps because states have adopted similar tax systems. Debates over the effect of state and local taxes continued. Since then, a study by Pjesky [2006] raised questions on the robustness of the effect of state and local taxes. Pjesky [2006] attempted to replicate and expand on what he identified as five “influential” studies on the roles of state and local taxes in regional economic performance. Through the replication effort, Pjesky [2006] concluded that the estimated tax effects in those studies were sensitive to model specification and the time periods examined. As a result, it is not prudent to make any definitive statements about the relationship between state and local taxes and regional economic activity, as taxes can have both positive and negative effects on economic activities. More recently, Reed [2008] attempted to resolve the conflicting results from the existing literature and provided a robustness test on the relationship between state and local taxes and personal income growth as the indicator of regional economic growth. This study tested the robust relationship between state tax and personal income by varying different control variables, different estimation methods, as well as data structures (annual data vs. five-year data interval), and time period. This study used per capita personal income as the dependent variable, and tax burden (total state and local taxes as a percentage of personal income) as the key independent variable. He found that state and local taxes used to fund general expenditures are associated with significant, negative effects on per capita income growth. This finding is generally robust across alternative control variable specifications, alternative estimation procedures, and different time periods. Although this study is illuminating on the effect of state and local income taxes on per capita income and the approach to achieve a robust estimate, it did not address the issue of the effect of state corporate taxes on job creation. The above review of the literature suggests that the effects of state and local taxation on economic activities vary and it depends on the types of state taxes and economic activities. Studies should choose carefully the economic indicators and tax types. As Wasylenko [1997] pointed out, “job growth is still the variable politicians identify most often with prosperity.” Under the current economic environment of inept job creation, we chose employment as the dependent variable in our study. We also choose the corporate income tax because that is the most important business-related tax for states when they try to attract business and improve their business climate. The remainder of this literature review summarizes studies that are narrowly focused on state corporate income taxes and/or state employment growth. Even with a narrowed focus, the literature remains inconclusive on state corporate taxes and job creation. Goss and Phillips [1994] found that state personal income taxes had a negative and significant impact on state-level employment growth from 1982 to 1992 but corporate taxes had little effect. Carroll and Wasylenko [1994] implemented a switching regression model to analyze a set of state and local tax variables on state-level employment from 1967 to 1988. They found that corporate income tax had little effect on nonfarm employment for both the 1970s and 1980s. But the same tax had a negative effect on manufacturing employment in the 1970s but not the 1980s. The conflicting literature justifies another look at how state corporate income tax can affect job creation. Our study contributes to the literature in the following three aspects. Firstly, it evaluates the effect of state corporate income tax rates on employment growth directly, rather than other variables that have been explored such as per capita income, business establishment, and capital investment. In addition, our study focuses not only on the level of the tax rate, but also on whether the action of a cut in the state corporate income tax rate can benefit job creation. Thirdly, using the most up-to-date employment and tax data, our study covers the period of the most recent recession and subsequent slow recovery.",6
48,3,Business Economics,13 September 2013,https://link.springer.com/article/10.1057/be.2013.16,The Trucking Industry: The Lynchpin of the U.S. Economy,July 2013,Bob Costello,,,Male,Unknown,Unknown,Male,"Figure 2 shows that as recently as 1945, the rail industry hauled more freight tonnage than the trucking industry. However, a few key developments over the decades swung the pendulum toward highways. The first major transformation was the construction of the National Interstate Highway System, which started after the Congress passed legislation to build the network in 1956. Much of the current Interstate Highway System was first built from the late 1950s to the mid-1970s. Today, it is over 46,000 miles long. As highways were built and roads improved, trucking saw a surge in demand. Historical Freight Tonnage (1940-2012)Sources: U.S. Freight Transportation Forecast to 2024, Eno Foundation; American Trucking Associations. The second major change was the Motor Carrier Act of 1980, which significantly opened up competition in the industry. Prior to the 1980 Act, the Interstate Commerce Commission (ICC) regulated entry into the market, contracts, and pricing. Through economic deregulation in 1980, market entry became much easier and rates were set by the marketplace, not regulated by the ICC.Footnote 5 The 1980 Act produced fierce competition among carriers, which resulted in many failures as companies accustomed to protection from competition were unable to adapt to the new environment. Indeed, 62 of the top 100 for-hire motor carriers in 1980 have gone out of business in the years since [Transport Topics Publishing Group 2011]. Of the remaining 38 companies, many have been absorbed by other motor carriers. Only a handful of the companies operating in 1980 survive today. Freight rates, in real terms, plunged in the years following deregulation. Prior to 1980, freight rate increases tracked inflation very closely. From 1980 through 2000, real revenue per mile plummeted 47.2 percent, with much of the reduction occurring within the first 12 years. Since 2000, real revenue per mile has rebounded 29.3 percent, but it still remains 31.8 percent below 1980 levels (see Figure 3). Real Average Revenue per Mile (1980-2012)Source: American Trucking Trends. The third major trend that caused trucking to increase market share over the years stemmed from changes in supply chain practices that enabled businesses to hold less inventory. As the so-called just-in-time (JIT) inventory management was more broadly adopted and refined in the 1980s and 1990s, trucking’s market share increased at a faster pace than in the 1970s. Although it is not the cheapest mode of transportation, the flexibility that trucks can provide enabled the adoption of JIT strategies, whose supply chain advantages more than offset the additional transportation costs. Unlike other modes, which operate on a severely limited, fixed network, trucks can go pretty much anywhere the 4 million mile highway network allows, and in some cases, can even operate off-road.Footnote 6 When freight is solely transported on a truck, it doesn’t need to be transloaded from one mode to another. Most of the time, it is picked up at a manufacturing plant or warehouse and moved directly to the customer. However, airplanes, trains, and water-borne vessels often have to rely on a truck for pick-up or final delivery. As JIT developed, delivery windows for freight shrank. Often, trucking companies have a delivery window of a couple of hours. In some cases, such as auto manufacturing, the truck essentially serves as the factory warehouse because inventories are very limited. From 1970 to 1980, as the Interstate Highway System neared completion, trucking gained 1.5 percentage points in tonnage market share from the rails. From 1980 to 1990, as the industry was deregulated and JIT became more widespread, trucking gained another 2.7 percentage points in market share from its largest competitor. In the 1990s, trucks gained an additional 2.3 percentage points as JIT became more prevalent. However, since 2000, trucking’s market share has stagnated. Last year, trucks hauled about the same percentage of tonnage that they moved in 2000. Significant changes in market share among modes are unlikely in the years ahead.Footnote 7",5
48,3,Business Economics,13 September 2013,https://link.springer.com/article/10.1057/be.2013.17,"James Owen Weatherall, The Physics of Wall Street: A Brief History of Predicting the Unpredictable",July 2013,Gerald L Musgrave,,,Male,Unknown,Unknown,Male,,3
48,3,Business Economics,13 September 2013,https://link.springer.com/article/10.1057/be.2013.19,"Tomas Sedlacek, Economics of Good and Evil: The Quest for Economic Meaning from Gilgamesh to Wall Street",July 2013,Jan Kmenta,,,Male,Unknown,Unknown,Male,,
48,4,Business Economics,25 November 2013,https://link.springer.com/article/10.1057/be.2013.30,From the Editor,October 2013,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
48,4,Business Economics,25 November 2013,https://link.springer.com/article/10.1057/be.2013.25,Financial Services and the Trust Deficit: Why the Industry Should Make Better Governance a Top Priority,October 2013,Roger W. Ferguson Jr.,,,Male,Unknown,Unknown,Male,"To begin, let us look at the landscape in which the financial services industry is operating today. Americans have not forgiven the industry for its role at the center of the financial crisis. But that is perhaps not surprising given continued scandals around issues like LIBOR rate-fixing, insider trading, and multibillion-dollar losses from risky trades. Against this backdrop, the financial services industry has become one of the least-trusted industries in the world. This year, for the third consecutive year, banks and financial services firms placed dead last in the Edelman global ranking of trust in industries. Respondents cited both poor performance and the perception of unethical behavior as reasons for the lack of trust. A Gallup poll conducted last year found equally grim results for the industry when it measured perceptions of honesty and ethics among different professions. Nurses came out on top in this survey, with 85 percent of respondents giving them high marks for honesty and ethics. Only 28 percent rated bankers as highly. Stockbrokers were viewed even worse, with just 11 percent of respondents giving them high ratings for honesty and ethics. They barely edged out members of Congress and car salespeople to escape landing at the bottom of that list too.",
48,4,Business Economics,25 November 2013,https://link.springer.com/article/10.1057/be.2013.27,Small Business Borrowing and the Bifurcated Economy: Why Quantitative Easing Has Been Ineffective for Small Business,October 2013,Michael J. Chow,William C. Dunkelberg,,Male,Male,Unknown,Male,"Generally speaking, lending to small businesses mirrored credit behavior in the broader economy in the lead-up to the financial collapse. Lacking the access to capital markets that large firms frequently have available, the primary source of external financing for small businesses are depository lending institutions (commercial banks, thrifts, and credit unions) [U.S. Small Business Administration, Office of Advocacy 2013]. According to the most recent Survey of Small Business Finances conducted by the Federal Reserve Board, commercial banks are the most common supplier of credit lines, loans, and capital leases to small businesses, providing about 41 percent of small firms with such services in 2003 [Board of Governors of the Federal Reserve System 2006]. The commercial and industrial (C&I) loans provided by depository institutions that small firms use (defined as loans of $1 million or less) to fund operations and finance capital investments grew steadily from 1995 to 2007, the peak of the housing boom, as shown in Figure 1. Loans to large firms grew far faster and also peaked in 2007 before plunging during the recession. Although loans to large firms have since recovered to their precrisis peak, loans to small firms steadily declined into late 2012 before showing some signs of improvement. This trend occurred even as excess reserves of banks held at the Federal Reserve (the basic ingredient in lending) have risen to record high levels, rising from $13 billion in 2007 to $2 trillion dollars, earning just 0.25 percent interest. Since the mid-2000s, the share of C&I loans going to small firms has steadily decreased in spite of lower interest rates on loans, as shown in Figure 2. In 2012, the share going to small firms fell to one of the lowest levels on record, as shown in Figure 3.Footnote 5 Small Business Borrowing Sources: FDIC, Statistics on Depository Institutions Average Rate Paid on Short-Term Loans Small Business Share of C&I LoansSource: FDIC, Statistics on Depository Institutions Loan growth is impacted by both the supply and the demand for credit. In the wake of the financial crisis, Washington policymakers appeared to place greater weight on supply constraints as the cause of fewer credit transactions, officially expressing the view that banks had become too risk averse and excessively reluctant to lend, creating a major impediment to recovery. Undoubtedly, some banks did become more risk averse. Certainly mortgage credit became more difficult to obtain compared with the pre-2007 housing boom because traditional underwriting standards were restored. Some banks, especially the “Too Big to Fail” (TBTF) banks, did restrict lending to small firms as their capital became impaired. But on Main Street, there was less evidence of a restriction in credit supply to small businesses (higher lending standards). Loan interest rates fell dramatically, hardly a restrictive policy. As was the case before the crisis, loan committees continued to “know a good loan when they see one,” but simply saw far fewer applicants that were truly creditworthy (those that had good sales prospects). This observation is supported by the monthly surveys (started in 1973) of the approximately 350,000 member firms of the NFIB.Footnote 6 As shown in Figure 4, even during the so-called credit crunch period, when asked what the “most important problem” facing their business was, no more than 5 percent of small business owners cited “Finance and Interest Rates” as their top problem. Postcrisis, this measure reached a record low of 1 percent in December 2012. In contrast, this measure went as high as 37 percent in 1982 during the Volcker era of high inflation and Federal Reserve-induced double-digit interest rates and credit restriction. Clearly, owners are willing to identify credit supply constraints when they occur as in the 1979–84 period. However, it is not possible to identify the so-called credit crunch on Main Street following the most recent financial crisis, despite what popular financial media headlines might have read.Footnote 7 Single Most Important Problem Facing Owners: Finance and Interest Rates The NFIB data provide considerable insight to the recent weakness in small business’ weak loan demand, especially among qualified borrowers. The sources of this phenomenon are fairly clear. The small business sector had to re-size from the over expansion that occurred during the housing boom, during which too many strip malls, restaurants, and retailers (as well as construction firms) were created. When the recession hit, these firms struggled to survive a severe contraction in consumer spending by competing for shares of a shrinking pie. The collapse in consumer spending beginning in late 2008 coincided with the increase in the savings rate from around 1 to 6 percent. Each percentage point increase in the saving rate represents a reduction of approximately $100 billion in consumption spending. In 2008 and 2009, about a million small firms were lost, well above the normal rate of attrition, as seen in Figure 5. Many of them sought loans to help them survive, and the composition of loan demand shifted in favor of lines of credit from longer-term loans typically used to finance expansion.Footnote 8 Births and Deaths (Quarterly)Source: http://www.bls.gov/news.release/cewbd.t08.htm",1
48,4,Business Economics,25 November 2013,https://link.springer.com/article/10.1057/be.2013.26,"Bubbles Tomorrow and Bubbles Yesterday, but Never Bubbles Today?",October 2013,John C Williams,,,Male,Unknown,Unknown,Male,"Let’s now consider standard asset price theory, according to which the price of an asset equals the discounted expected return of holding the asset for one period. For example, take a share in a corporation. The return consists of two parts: the dividend payment the owner receives and the capital gain or loss from selling the share. The same formula applies to owning a house or a bond, or any asset for that matter. For the house, the dividend payment is the service flow the owner derives from living in it or renting it out. For the bond, it is the coupon payment. According to this theory, three variables can affect asset prices: the discount factor, the dividend payment, and the expected future price appreciation. It helps to simplify things a bit further. Under certain assumptions, including the absence of bubble-like behavior, Myron Gordon [1959] developed over 50 years ago an illuminating way of presenting this asset price formula. He noted that the ratio of the asset price to the dividend payment is inversely related to the difference between the expected future rate of return and the growth rate of inflation-adjusted dividends. That is, all else equal, the price-to-dividend ratio should be high when expected future dividend growth is high or when the expected future return to the asset is low. This is a classic case of an elegant and parsimonious theory. So, how does it stand up to the data? The first hurdle the model faces is the long history of boom and bust cycles in a variety of different asset prices. These were thoroughly documented by Robert Shiller [2005] in his book Irrational Exuberance.Footnote 2 I’m an economist, so I need to show some numbers here. Figure 1 shows two well-known recent U.S. asset price booms and busts. The solid line shows the price-to-dividend ratio of the S&P 500 stock index from 1990 to the present. The dashed line shows the time series of the house price-to-rent ratio from the CoreLogic home price index, in which the rent data are the Bureau of Economic Analysis data on owners’ equivalent rent. In the stock market boom of the late 1990s, the price-to-dividend ratio rose over 100 percent in the five years up to the end of the boom in 2000. The recent housing boom was relatively tame by this standard. The house price-to-rent ratio climbed around 50 percent during the five years before the market peak in 2006. To put these numbers in perspective, according to flow of funds data, in the five years before they peaked, U.S. stock market wealth soared $12 trillion and housing wealth increased some $10 trillion. Assset Price Booms and Crashes Sources: CoreLogic, BEA and S&P 500 data from Shiller [2005, updated]. What does the Gordon model have to say about these and other large surges in asset prices? Two explanations are possible based on changes in economic fundamentals. One is an upward shift in the expected growth rate of future dividends. The second is a reduction in investors’ expected returns on the asset. Importantly, in standard asset pricing theory, expectations of future dividend growth and future returns on assets are assumed to be rational. That is, expectations are assumed to be consistent with the structure of the model. I’ll start with the first explanation, that a rise in the price-to-dividend ratio is caused by higher expected dividend growth. The evidence on this is clear and negative. With respect to U.S. stocks, over history, the price-to-dividend ratio is uncorrelated with future real dividend growth [Cochrane 2008]. A similar pattern is seen with regard to the U.S. housing market. The price-to-rent ratio is uncorrelated with future real rent growth [Campbell and others 2009; Gelain and Lansing 2013]. The international evidence is somewhat more mixed. But a recent cross-country study found that, in most countries, the correlation between the house price-to-rent ratio and future real rent growth is either statistically insignificant or has the opposite sign of that predicted by the theory [Engsted and Pedersen 2012]. Indeed, in the most recent U.S. housing boom, the high house price-to-rent ratio observed during the boom did not foreshadow subsequent high real rent growth. In fact, the growth rate of real rents actually declined in the period following the peak price-to-rent ratio. It’s simply not the case that asset price movements can be explained by shifts in rational expectations of future dividend growth. So that leaves the possibility that a lower expected return might be driving the increase in asset prices during a boom. The lower expected return could reflect a combination of lower alternative investment returns, say as measured by the general level of real interest rates, and/or a lower risk premium on the asset in question. As in the case of dividend growth, the evidence on expected future real interest rates driving asset prices is negative. Equity dividend-to-price ratios are generally not correlated with future changes in real interest rates [Campbell and Shiller 1988]. Thus, expectations of future dividends or real interest rates fail to explain asset price movements. Given that, standard approaches ascribe much of the variation in asset prices to movements in the discount factor used to compute the present value of future dividends. This is the logic of Sherlock Holmes [Doyle 1890], who said that, “when you have eliminated the impossible, whatever remains, however improbable, must be the truth.” Time variation in the discount factor is the only remaining rational explanation. However, on their own, you can’t really judge whether movements in the discount factor are reasonable. After all, they are simply defined as the residual component of an identity implied by the theory. In this regard, the discount factor is akin to total factor productivity, which Moses Abramovitz [1956] famously described as “a measure of our ignorance.” Moreover, the explanation that the discount factor is the main driver of movements in the price-to-dividend ratio has potentially falsifiable implications. In particular, it says that when the price-to-dividend ratio is high, rational investors should expect a relatively low rate of return on the asset. When valuations are low, rational expected returns should be high [Greenwood and Shleifer 2013]. For example, if rational investors discount future dividends by less, perhaps owing to a reduction in risk aversion, then the price-to-dividend ratio rises and we see a boom in the asset price. And the expected future rate of return on the now higher-priced asset will be correspondingly lower. So, are the data consistent with this prediction of the theory? One test is to compare real-world measures of investors’ expected returns with the expected returns implied by the theory. Fortunately, there are a number of surveys of investor expectations of future returns on stocks and houses that can be brought to bear on this question. Let me jump to the bottom line. The evidence from surveys of investors’ expected returns is directly at odds with the implications of standard asset price theory. For one, stock market investors tend to expect high future returns when the price-to-dividend ratio is high, contrary to the theoretical prediction of a negative relationship between rational expected returns and the level of asset prices relative to dividends [Greenwood and Shleifer 2013].Footnote 3 A picture tells the story. Figure 2 shows Gallup survey results on the relationship between the S&P 500 price-to-dividend ratio and investor optimism regarding stock market returns over the next year. Gallup asks whether people are optimistic, pessimistic, or neutral about future market returns. The figure reports the difference between the number saying they are optimistic or very optimistic and those saying they are pessimistic or very pessimistic. As the figure shows, periods of high stock valuations, such as the late 1990s and mid-2000s, are when investors were more optimistic regarding future stock gains. And during periods of relatively low valuations, such as the early 2000s and the period of the global financial crisis, investors had relatively low expectations of stock market returns. The positive relationship between current stock prices and expected future returns is consistent across a variety of surveys and alternative model specifications. Stock Prices and Investor Optimism Sources: Greenwood and Shleifer [2013], Wells Fargo/Gallup, and S&P 500 data from Shiller [2005, updated]. Data from 1996 to 2013. This same relationship is evident in data on house prices. Figure 3 plots the level of house prices and expected future house price appreciation in the United States over the past decade. Each data point represents one of four major cities in a given year [Case, Shiller, and Thompson 2012]. The pattern is clear. Optimism about future house price appreciation tends to increase when house prices are high. Just as with stocks, the survey evidence directly contradicts the fundamental story that high house prices can be explained by a decline in the rational expected return from homeownership. House Prices and Expected Price Appreciation Sources: S&P/Case-Shiller, FHFA and Case, Shiller, and Thompson [2012]. Cities included are San Francisco/Alameda County, Boston, Los Angeles/Orange County, and Milwaukee. Data from 2003 to 2012. Let me sum up my points so far. According to standard asset price theory, an increase in asset prices must reflect either an increase in expected future dividend growth or a reduction in the expected return on the assets. We see large run-ups in equity and home prices. But they are not associated with higher future dividend growth rates or lower expected returns based on surveys. So far, this evidence is mainly destructive. But, in fact, a first step to understanding asset price bubbles can be found in the survey data I just discussed. The key is to relax the assumption of rational expectations and allow people’s decisions to be driven by their perceptions of what the future may hold.",9
48,4,Business Economics,25 November 2013,https://link.springer.com/article/10.1057/be.2013.29,Commodity Markets and Commodity Mutual Funds,October 2013,L Christopher Plantier,,,Unknown,Unknown,Unknown,Unknown,,
48,4,Business Economics,25 November 2013,https://link.springer.com/article/10.1057/be.2013.22,"Professional Occupations, Knowledge-Driven Firms, and Entrepreneurship: A National and Regional Analysis",October 2013,Andrew C Gross,Mark Holtzblatt,Emeric Solymossy,Male,Male,Male,Male,"Knowledge-intensive services are planned and implemented by a professional work force. The original three professional fields are said to be divinity, medicine, and law; later came engineering, architecture, surveying, accounting, and consulting. Professions involve the application of specialized knowledge: a professional occupation is one where the individual’s background is characterized by high-level skill, acquired learning, and commitment to both ethical standards and client confidentiality. Continuing education, certification, and membership in a professional body or association is often required to demonstrate ongoing expertise. Growing domestically or internationally is no easy task. In Table 2, we delineate how entry and expansion take place for four key sectors—accounting, legal services, engineering consultancy, and management consultancy. Professionals are generally held in high esteem; they can form their own firms or be hired by business, government, and nonprofit organizations. Regardless of their post, they tend to assert their status and autonomy to achieve power, capitalize on their expertise by charging high fees, and cluster in associations from which they exclude those they deem not qualified. An 85-year-old, oft-cited quote, attributed to G. B. Shaw, states: “All professions are conspiracies against the laity.” Indeed, the histories and the characteristics of various professions are colorful and controversial; and their evolution, along with their respective associations, can be traced back several centuries [Carr-Saunders and Wilson 1933; Parsons 1939; Wilensky 1964; Maister 1993; Dezalay and Sugarman 1995; Morgan 1998; Brock, Powell, and Hinings 1999; Baker and Dunn 2003]. Professionals stress technical expertise, codes of conduct, and confidentiality for clients; yet government bodies and associations often insert themselves in each sector by requiring demonstration of qualification (or to limit supply). Such certification or licensure can be demanding: for example, to qualify as a professional engineer, the State of Ohio in the United States requires (1) an engineering degree from an accredited university, (2) four years of practice, (3) thorough examinations, and (4) continuing education. As a partial offset to the power of expert individuals and their associations, regulators can opt for certification or licensure, with professionals accepting this as a sign of accreditation to their skills. In some nations, public authorities yield the licensing or certifying process to professional associations. For example, according to Wikipedia, there are now “ten accountancy bodies in the United Kingdom, all of which have been given a Royal Charter although not necessarily considered to hold equivalent-level classifications.” Illustrating further the diversity and the requirements in the case of one set of professional services, Table 3 shows the certification steps required by two management consultancy groups. Clearly, it is no small task to seek either licensure or certification in professional fields. But a key question raised frequently is where professionalism should reside—with the individual or with the organization. According to one expert, “engineers would be licensed by the state, accountants certified by the CPA examination, and lawyers regulated by the bar examination, but management consultants actively campaigned to avoid government oversight” [McKenna 2000]. Our research has shown also that solo or small firm management consultants like the certification, but larger consultancies discourage their staff, insisting that professionalism resides with the organization [Gross and Poor 2008; O’Mahoney 2010].",4
48,4,Business Economics,25 November 2013,https://link.springer.com/article/10.1057/be.2013.28,Economics in a Major Bank,October 2013,John E. Silvia,,,Male,Unknown,Unknown,Male,"Also essential for any successful economist is an understanding of your institution and its culture. The merger between Wachovia and Wells Fargo brought a different culture to my job—and a much larger footprint for the bank. The stagecoach is a strong symbol for the bank. A successful economist, over time, works with the culture of the institution and understands its priorities. Unfortunately, in my career I have known a number of economists who did their own thing, and their careers were cut short. Wells Fargo consists of three main divisions, and my job is to provide economic analysis for them all. Although the risk/reward balance differs for each division, it is important to recognize the nuances. Wholesale banking encompasses what many would call investment banking. In this area, there is a higher sensitivity to daily economic releases, and therefore an incentive for me to provide a quick analysis of each release. This is an area where analysts, traders and sales people expect a brief, concise explanation of the data and what they mean for the market; thus, the popularity of our one-page indicator reports. This is not an area for long reports that might better satisfy the purist. Community banking is another division of Wells Fargo. For an economist, this area presents the challenge of communicating to people who are not involved in the markets day to day, but who seek some explanation of what is happening in the economy that better enables them to make very personal financial decisions. This is a serious responsibility that I and many of my fellow economists take to heart. Here is where we must be very sensitive to the framing biases of decision makers. Framing simply means that some individuals who must balance risk/reward will favor risk—this is often the mindset of the entrepreneur. In contrast, many households dealing with their own finances have a risk/reward profile that avoids risk in favor of more modest returns. Communicating to community banking is communicating to the man/woman on the street; it is well worth our time, and we must always be aware of framing bias. For community banking, we typically publish reports on state and local economies as well as on housing and consumer spending/credit. Wealth and Securities Management is the last division that I will discuss. Here the focus is on the link between economic developments and financial markets over time so that households and money managers can better develop lifetime investment plans for their clients. The emphasis is on a longer-term focus and a careful explanation of the role of economics in wealth management. For the economist, the challenge at Wells Fargo is to be sensitive to the unique needs of each of our divisions. We tailor our products (whether written or oral) in recognition of each division’s needs.",
48,4,Business Economics,25 November 2013,https://link.springer.com/article/10.1057/be.2013.24,Balance: The Economics of Great Powers from Ancient Rome to Modern America,October 2013,John C Goodman,,,Male,Unknown,Unknown,Male,,
48,4,Business Economics,25 November 2013,https://link.springer.com/article/10.1057/be.2013.23,Red Ink: Inside the High-Stakes Politics of the Federal Budget,October 2013,Devon M Herrick,,,,Unknown,Unknown,Mix,,
49,1,Business Economics,07 March 2014,https://link.springer.com/article/10.1057/be.2014.5,From the Editor,January 2014,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
49,1,Business Economics,07 March 2014,https://link.springer.com/article/10.1057/be.2013.32,Persistent Unemployment and Policy Uncertainty: Numerical Evidence from a New Approach,January 2014,Patrick L Anderson,,,Male,Unknown,Unknown,Male,"The pace of employment growth in the four years since the end of the Great Recession was much slower than in the recoveries that the United States enjoyed after most recent recessions. Furthermore, the pattern of weak employment growth persisted in both the first two years, and the second two years, after the trough of the recession:
 During the four previous recoveries, the average change in overall nonfarm payrolls during the seven quarters after the trough of the downturn was a rise of 1.8 percent. In contrast, overall employment rose during the analogous time period after the recent recession by only 0.2 percent.Footnote 1 Civilian unemployment rates remained over 10 percent in several states through the spring of 2011—fully a year and a half after the recession was declared to be over. In the succeeding two years, the civilian unemployment rate dropped back under 8 percent for the country as a whole, with a handful of U.S. states and territories (including the quite-different states of California, Mississippi, Nevada, and North Carolina, as well as Puerto Rico) still experiencing rates above 9 percent in the first quarter of 2013.Footnote 2 This extended duration was “an unprecedented combination” according to the Federal Reserve Bank of Cleveland, noting that the unemployment rate peaked (above 9 percent) in the Great Recession more than 20 months after the beginning of the recession, and remained near that level for another 20 months.Footnote 3
 Examining the same phenomenon using an arguably more robust measure, the employment-to-population ratio, paints an even more depressing picture. Peaking above 64 percent in 2000, this ratio dropped to under 60 percent during the Great Recession, and remained around 59 percent through the first half of 2013. These levels had not been experienced since the early 1980s. Let us briefly consider the possible macroeconomic causes of such sluggish employment. We have separated these into two categories associated with the dominant schools of economic thought for the past half-century, plus the category that we wish to consider in the remainder of this paper: The economics profession focused on this trio of topics for much of the twentieth century, particularly after the searing experience of the Great Depression and the emergence of the macroeconomic theories originated by John Maynard Keynes. Keynes, and later Milton Friedman and others, criticized the response of the United States federal government to the Great Depression. However, the response of the federal government to the recent recession was quite different from that during the Great Depression. U.S. policymakers aggressively loosened monetary policy, contrary to the effective contraction of the Great Depression. In addition, the nation began stimulative fiscal policies in the last year of President Bush’s second term of office, and expanded that stimulus during the initial years of President Obama’s first term. Furthermore, and again unlike the Great Depression, the United States did not retreat into protectionism. The “real business cycle” school of thought that emerged in the later years of the twentieth century provides a competing set of possible causes of persistent unemployment. In particular, real business cycle theory suggests that consumers and business owners respond to trade disruptions, cycles in commodity prices, and technology shifts, as well as to changes in government policy that affect prices. In the case of the Great Recession and its recovery, there was no shock to the system on the order of magnitude of the oil embargo in the 1970s. It is similarly difficult to blame a real estate bubble that evaporated years ago. Finally, production technology has evolved slowly during this time period. Although the real estate bubble and technological change undoubtedly had some effect on the real economy, they cannot explain persistent unemployment four years and more after the beginning of the recovery. One aspect of real business cycle theory worth considering is the presumption that consumers and businesses exhibit “rational” expectations, of the kind first proposed by John Muth [1961] and greatly developed by Robert Lucas and others. One classic implication of rational expectations—that an increase in inflationary expectations would be caused by stimulative monetary policy—has not evidenced itself: indeed interest rates have been at historic lows. As will be shown further, the model presented here involves a different kind of expectation about the future than that proposed by Muth. Recently, a new hypothesis for the cause of persistent unemployment has gained attention: that uncertainty about government policies that could impose significant additional taxes and costs is responsible for a reluctance of private-sector firms to hire workers.Footnote 4 In particular, the hypothesis focuses on burdens on employers to fund health care and comply with related laws, pay higher assessments to fund extended unemployment benefits, as well as pay higher income taxes on earnings from business operations and from related investments. Reports from a variety of businesses and areas within the country, and from multiple sources, provide evidence that business hiring and investment plans were negatively affected by policy cost uncertainty during this time period [Federal Reserve Board 2012 and 2013; National Federation of Independent Businesses 2011 and 2013]. Of course, economists tend to place much less emphasis on what people say they are doing than on what they actually do, but these reports—together with persistent unemployment in the economy—provide a strong motivation for this test of the policy cost hypothesis. This characterization of policy cost uncertainty represents, of course, an incomplete assessment. However, it focuses our attention on the possibility that standard macroeconomic theory has been inadequate to the task of modeling the hiring decisions of private employers and that this inadequacy has been exposed by the slow recovery to the Great Recession. We define policy cost uncertainty as: A nonzero likelihood, as perceived by employers, of a significant increase in government policy‐imposed costs on businesses, including payroll taxes, health insurance costs and compliance burdens, and income taxes. Note that this definition focuses on the likelihood of certain events occurring, rather than the realization of possible events. The likelihood of an increase in taxes and costs is expressed as a probability between zero and one.Footnote 5 How businesses react to the likelihood that their costs and taxes will change is indeed a question of business decisions under uncertainty. Such a measure is distinct from other indicators of “uncertainty.” In particular, it is distinct from statistics on the variation in a random variable, such as the standard deviation, and from the “uncertainty index” proposed by Baker, Bloom, and Davis [2012].Footnote 6 However, the assumption here of a significant likelihood of policy cost increases in the 2011–13 era is consistent with the level of the uncertainty index during that time.Footnote 7
 Finally, the forward-looking expectations in this model are not the same as “rational expectations,” and the likelihood of a policy cost increase is not the same as the best estimate of future costs. We discuss this topic further in the final section of the article. Given the observations about the sluggish economy, and the definition of policy cost uncertainty, we formalize the policy cost uncertainty hypothesis as follows: Given policy cost uncertainty during the 2011–13 time period:
 some employers rationally avoided hiring to avoid possible future policy-imposed costs; and this avoidance is responsible for some significant part of the employment gap. The observation that uncertainty about the future can affect business decisions is hardly novel. However, as described below, standard microeconomic models fail to adequately capture how policy uncertainty affects business decisions. In addition, standard macroeconomic models often omit policy uncertainty entirely, relegate it to a minor factor, or confine it to a variable that affects only part of the economy. In particular, such models do not describe why firms might rationally not expand their operations when a recovering economy offers them the possibility of increased profits by immediately hiring workers. The embedded assumption in these models—that firms always maximize profits—is a time-honored one. Indeed, it is a foundation of the neoclassical economics that has been the beginning point for economic theory for well over a century [Marshall 1920]. Replacing the profit-maximizing principle with an alternate principle is a step that should be taken gingerly. However, there is now a competing model of firm behavior, which allows us to examine the policy cost uncertainty hypothesis directly. This is the path we take in this paper. In this analysis, we use a novel “value functional” or “recursive” model of firm behavior. The underlying principle in this model is that managers do not maximize profits in any single period, but instead maximize value. They do so by recursively solving a series of two-period optimization problems. The mathematics behind the value functional approach to optimization problems has been developed over approximately the past 50 years. However, only recently has it been explicitly applied to firm behavior, and even more recently has it been possible to numerically model business decisions using data representative of actual operating businesses. Such an approach shares certain fundamental presumptions with standard neoclassical, classical, Keynesian, and real business cycle models. These include the presumption that business managers are aware of multiple possible economic conditions that could prevail in any period, and that they take actions when conditions change. However, the business manager in the value functional approach is also presumed to have some knowledge about the probabilities that the state could change in the future, as well as knowledge of what changes in business operations the manager could exercise if such a change occurs. This knowledge has important ramifications, and the value-maximizing objective allows the business manager to behave in a manner that takes advantage of that knowledge before conditions change. Thus, this approach involves a different structure entirely than traditional classical, Keynesian, and neoclassical models, all of which rely to some extent on the profit-maximization principle, as well as real business cycle or other models that add “rational expectations” about costs of information available to business managers and then assume that managers maximize profits in the standard manner. In applying the value-maximizing approach, we consider the following questions:
 Can a practical model of firm behavior, built on the assumption that firms maximize value, be created and numerically solved, using data representative of actual operating firms? In such a model, can policy cost uncertainty cause rational employers to defer hiring or reduce employment? Does this approach provide analytical power worth exploring for other tasks of the business economist, such as evaluating potential investments, modeling “real option” opportunities for firms, and evaluating asymmetric and “black swan” risks in portfolios and financial institutions? The remaining sections of the paper address these questions as follows:
 Section 2 presents the value functional approach to business valuation, including the mathematical form, the fundamental building blocks of states, actions, and probabilities of operating in those states in the future, and the optimizing behavior of business managers in such a model. Section 3 develops the specific model of business risk and policy decisions. Section 4 outlines a value functional model of employer firms; and a numerical procedure for composing, formulating into a mathematical equation, and solving the model. Section 5 describes the results of testing the policy cost uncertainty hypothesis using the data, assumptions, and model described in Section 4. It also discusses the insights gained from numerically solving such a model; the practical questions outlined above; and areas where this method could be applied to numerous other issues confronting the business economist. Appendices contain additional information on the model, data, numerical procedure, and software used.",3
49,1,Business Economics,07 March 2014,https://link.springer.com/article/10.1057/be.2013.33,The Economic Recovery Five Years after the Financial Crisis,January 2014,James H Stock,,,Male,Unknown,Unknown,Male,"Before turning to those two topics, let me start with a brief overview of the recovery and the current situation. Since peaking at 10 percent in October 2009, the unemployment rate has fallen to 7.3 percent as of September 2013. During the previous 24 months the economy has added, on average, approximately 180,000 jobs per month. Indeed, as Figure 1 shows, 12-month employment growth has hovered just above two million jobs per year for the past two years, whether measured by establishment employment or by household survey employment. Although we all focus on establishment employment because its survey sample size is so much larger, this figure also includes the 12-month growth of the BLS-adjusted household employment series, where the BLS adjustment puts household employment on a conceptually comparable basis to the establishment employment series. Both series provide the same picture, albeit with much more noise in the household survey. Twelve-Month Change in EmploymentSource: Bureau of Labor Statistics. On the output side, real GDP has continued to grow at a moderate but steady pace, with average GDP growth over the first half of the year coming in at 1.8 percent. Based on the July GDP revisions, the level of real GDP per capita has essentially returned to its value at the cyclical peak of the fourth quarter of 2007. This growth in GDP comes despite substantial fiscal contraction over 2012 and especially over 2013 so far, a contraction that has come about in part because of the failure of Congress to address the sequester or to pass the additional jobs measures proposed by the President. Figure 2 shows the deficit as a fraction of GDP since 2000, including its 10-year projection by the CBO under current law and under the President’s proposed Budget. The deficit as a fraction of GDP fell from 8.7 percent in fiscal year 2011 to 7.0 percent in fiscal year 2012, for a total of 1.7 percentage points of fiscal drag. This fiscal drag partly came from reductions in automatic stabilizers in the budget as the economy recovered, but it mainly came from the wind-down of spending from the Recovery Act. Indeed, from 2009 to 2012 the deficit fell by 3.1 percentage points of GDP, the largest three-year decline since 1949. According to the CBO’s most recent projections, under current law the FY13 deficit is projected to be 4.0 percent of GDP, which means that FY13 will have experienced another 3.0 percentage points of fiscal drag. Some of this fiscal drag arises from automatic stabilizers as the economy recovers, but much of it is attributable to the end of the temporary payroll tax holiday at the start of this year and from the imposition of the sequester, which the Congressional Budget Office (CBO) estimates will have reduced government outlays by $42 billion over FY13. Federal Budget DeficitSource: U.S. Department of Treasury, Congressional Budget Office. The growth of GDP has been achieved by solid private sector growth offsetting this federal fiscal drag. As Figure 3 shows, growth of real private domestic final purchases, that is, consumption plus investment excluding inventory investment, has been solid, with quarterly growth averaging 2.6 percent at an annual rate over 2012 and 2013 and coming in at an average of 2.0 percent in the first half of this year. Because the bulk of the fiscal contraction under CBO’s current law scenario will have occurred by the end of this fiscal year, I am optimistic about next year’s growth potential. This will be particularly true if Congress acts, as it should, to provide relief to the sequester so that the federal government can properly carry out its many important functions that are hamstrung by the across-the-board sequester cutbacks. Real Private Domestic Final Purchases GrowthSource: Bureau of Economic Analysis.",1
49,1,Business Economics,07 March 2014,https://link.springer.com/article/10.1057/be.2014.1,Beyond Big Data,January 2014,Hal R Varian,,,Male,Unknown,Unknown,Male,"According to published reports, Google has seen 30 trillion URLs, crawls over 20 billion of those a day and answers 100 billion search queries a month [Singhal 2012]. Ordinary databases just can’t handle these magnitudes, so we have had to develop new types of databases that can store data in massive tables spread across thousands of machines and can process queries on more than a trillion records in few seconds. We published descriptions of these tools in the academic literature, and independent developers have created open source tools that have similar functionality. These tools are now widely available and run on cloud computing engines such as Amazon Web Services, Google Compute Engine, and other services. From the economic point of view, what was previously a fixed cost (deploying and managing a data center capable of dealing with massive data) is now a variable cost. As any economist knows, if you lower the barriers to entry you will get lots of new entrants and we have seen a number of startups in this area. But tools for data manipulation are only part of the story. We have also seen significant developments in methods for data analysis that have emerged from the machine learning community. Nowadays we hear a lot about “predictive analytics,” “data mining,” and “data science.” The techniques from these subjects, along with some good old-fashioned statistics and econometrics, have allowed for deeper analysis of these vast data sets, enabled by computer-mediated transactions. There have also been significant developments in open source programs that can be used to apply these tools, such as the R language, Weka (Waikato Environment for Knowledge Analysis), and others. One of most important features of these languages is the thriving communities of users who provide peer support on the web. Given the fact that cloud hardware, database tools, analysis tools, and developer support is now widely available it is not surprising that we have seen new entrants in the data analysis area. Back in 2006, NetFlix realized that 75 percent of movie views in its library were driven by recommendations. They created the “NetFlix Prize” of $1 million that would be awarded to the group that developed the best machine learning system for recommendations, as long as it improved the current version by at least 10 percent. They provided training data of about 100M ratings, 500,000 users, and 1,800 movies. A year later, the prize was won by a team that blended 800 statistical models together using model averaging. The success of the NetFlix challenge led to the establishment of a startup named Kaggle, who will set up NetFlix-like challenges. (Note: I am an investor and an adviser to Kaggle.) Nowadays, there are many organizations that have interesting data but no internal expertise in data analysis. As the same time, there are data analysts all over the world that have expertise but no data (and could use some money). Kaggle puts the two sides of the market together. They now have 114,000 data scientists who tackle the submitted problems. Their motto is “We make data science a sport.” Here are some examples of their projects:
 Heritage Health Prize: $3 million to predict hospital re-admits Gesture recognition for Microsoft Kinnect: $10,000 GE flight optimization: $250,000 Belkin energy disaggregation for appliances: $25,000 Recognizing Parkinson’s disease from smartphone accelerometer data: $10,000 … and many more When you combine Data+Tools+Techniques+Expertise, you can solve a lot of hard problems!",58
49,1,Business Economics,07 March 2014,https://link.springer.com/article/10.1057/be.2013.31,Financial Performance in Manufacturing Firms: A Comparison Between Parametric and Non-Parametric Approaches,January 2014,Eleonora Bartoloni,Maurizio Baussola,,Female,Male,Unknown,Mix,,
49,1,Business Economics,07 March 2014,https://link.springer.com/article/10.1057/be.2014.3,The Evolving Global Wine Market,January 2014,Barbara Insel,,,Female,Unknown,Unknown,Female,"The quality revolution in wine that has now spread across the world began in 1944 with the publication by Maynard Amerine and Albert Winkler, professors at the University of California, Davis, of their landmark Composition and Quality of Musts and Wines of California Grapes. This work initiated a rigorous approach to wine growing based on varietal selection, climate, and geographical location. UC Davis’ research integrated science (chemistry, genetics, microbiology, chemical engineering, plant physiology, and biochemistry) with sensory analysis, wine production, horticulture, and traditional agriculture. It became the global model for viticulture and enology research and training, generating constant improvements in practices and products. The very phrase “wine growing”—combining grape growing and wine making into one concept—symbolized these new practices, now practiced in every continent except Antarctica. Advanced research and experimentation enabled grape growers to select the most appropriate grape varieties for individual regions and vineyards and develop improved growing practices, including trellising and vine management, pruning, and harvesting. Research and training in enology—winemaking—introduced better techniques for crushing, fermentation, blending, bottling, and aging of wines. New materials were introduced; and traditional materials and equipment have been consistently upgraded, even to the point of ensuring that the most traditional of wine materials—the cork from ancient Portuguese forests—is now repeatedly quality tested with modern equipment. Better science and skills also opened the path to more consistently good quality wine, less dependent on annual vintage conditions, and an expanded focus on varietal wines and production at a wider range of price points. Today there is little “bad”—as in poorly made or spoiled—wine produced almost anywhere, although the range of quality may vary widely. Science has not “industrialized” wine, as the “traditionalists” sometimes argue. Wine is still an agricultural product, reflecting terrain and climate, and is also a very personal product, crafted by individuals of differing skills and tastes. These innovations simply acknowledge that making wine is both science and craft.",11
49,1,Business Economics,07 March 2014,https://link.springer.com/article/10.1057/be.2014.2,"The Map and the Territory: Risk, Human Nature, and the Future of Forecasting",January 2014,Diane C Swonk,,,Female,Unknown,Unknown,Female,,
49,1,Business Economics,07 March 2014,https://link.springer.com/article/10.1057/be.2014.4,Firm Commitment: Why the Corporation is Failing Us and How to Restore Trust in It,January 2014,Thomas A Hemphill,,,Male,Unknown,Unknown,Male,,
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.17,From the Editor,April 2014,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.13,"U.S. Economic Prospects: Secular Stagnation, Hysteresis, and the Zero Lower Bound",April 2014,Lawrence H Summers,,,Male,Unknown,Unknown,Male,"Let me turn, then, to the first of these propositions. It has now been nearly five years since the trough of the recession in the early summer of 2009. It is no small achievement of policy that the economy has grown consistently since then and that employment has increased on a sustained basis. Yet, it must be acknowledged that essentially all of the convergence between the economy’s level of output and its potential has been achieved not through the economy’s growth, but through downward revisions in its potential. In round numbers, Figure 1 shows that the economy is now 10 percent below what in 2007 we thought its potential would be in 2014. Of that 10 percent gap, 5 percent has already been accommodated into a reduction in the estimate of its potential, and 5 percent remains as an estimate of its GDP gap. In other words, through this recovery, we have made no progress in restoring GDP to its potential. Downward Revision in Potential GDP, U.S.A. Source: CBO. Information on employment is similarly sobering. Figure 2 depicts the employment/population ratio in aggregate. Using this relatively crude measure, one observes almost no progress. It has been pointed out repeatedly and correctly that this chart is somewhat misleading because it neglects the impact of a range of demographic changes on the employment ratio that would have been expected to carry on even in the absence of a cyclical downturn. Employment/Population Ratio, Aggregate Source: U.S. Department of Labor: Bureau of Labor Statistics. But that is not the largest part of the story. Even if one looks at 25-to-54 year-old men, a group where there is perhaps the least ambiguity because there is the greatest societal expectation of work, Figure 3 shows that the employment/population ratio declined sharply during the downturn, and only a small portion of that decrease has been recovered since that time. Employment/Population Ratio, Men 25–54 Source: Organization for Economic Co-operation and Development. The recovery has not represented a return to potential; and, according to the best estimates we have, the downturn has cast a substantial shadow on the economy’s future potential. Making the best calculations one can from the CBO’s estimates of potential (and I believe quite similar results would come from other estimates of potential), one can see from Figure 4 that this is not about technological change. Slower total factor productivity than we would have expected in 2007 accounts for the smallest part of the downward trend in potential. The largest part is associated with reduced capital investment, followed closely by reduced labor input. Let me emphasize that this is not a calculation about why we have less output today. It is a calculation about why it is estimated that the potential of the economy has declined by 5 percent as a consequence of the downturn that we have suffered. Why did Potential GDP Fall? Source: CBO data. Author calculations. The record of growth for the last five years is disturbing, but I think that is not the whole of what should concern us. It is true that prior to the downturn in 2007, through the period from, say, 2002 until 2007, the economy grew at a satisfactory rate. Note that, there is no clear evidence of overheating. Inflation did not accelerate in any substantial way. But the economy did grow at a satisfactory rate, and did certainly achieve satisfactory levels of capacity utilization and employment. Did it do so in a sustainable way? I would suggest not. It is now clear that the increase in house prices shown in Figure 5 (that can retrospectively be convincingly labeled a bubble) was associated with an unsustainable upward movement in the share of GDP devoted to residential investment, as shown in Figure 6. And this made possible a substantial increase in the debt-to-income ratio for households, which has been reversed only to a limited extent, as shown in Figure 7. Home Prices Source: Robert Shiller’s website. Housing Share of GDP Source: U.S. Department of Commerce: Bureau of Economic Analysis. Debt/Income Ratio for Households Source: Federal Reserve (FRED). It is fair to say that critiques of macroeconomic policy during this period, almost without exception, suggest that prudential policy was insufficiently prudent, that fiscal policy was excessively expansive, and that monetary policy was excessively loose. One is left to wonder how satisfactory would the recovery have been in terms of growth and in terms of achievement of the economy’s potential with a different policy environment, in the absence of a housing bubble, and with the maintenance of strong credit standards. As a reminder, prior to this period, the economy suffered the relatively small, but somewhat prolonged, downturn of 2001. Before that, there was very strong economic performance that in retrospect we now know was associated with the substantial stock market bubble of the late 1990s. The question arises, then, in the last 15 years: can we identify any sustained stretch during which the economy grew satisfactorily with conditions that were financially sustainable? Perhaps one can find some such period, but it is very much the minority, rather than the majority, of the historical experience. What about the rest of the industrialized world? I remember well when the Clinton administration came into office in 1993. We carried out a careful review of the situation in the global economy. We consulted with all the relevant forecasting agencies about the long-term view for global economic growth. At that time, there was some controversy as to whether a reasonable estimate of potential growth for Japan going forward was 3 percent or 4 percent. Since then, Japanese growth has been barely 1 percent. So, it is hard to make the case that over the last 20 years, Japan represents a substantial counterexample to the proposition that industrial countries are having difficulty achieving what we traditionally would have regarded as satisfactory growth with sustainable financial conditions. What about Europe? Certainly, for some years after the introduction of the euro in 1999, Europe’s economic performance appeared substantially stronger than many on this side of the Atlantic expected. Growth appeared satisfactory and impressive. Fears that were expressed about the potential risks associated with a common currency without common governance appeared to have been overblown. In retrospect, matters look different. It is now clear that the strong performance of the euro in the first decade of this century was unsustainable and reliant on financial flows to the European periphery that in retrospect appear to have had the character of a bubble. For the last few years, and in prospect, European economic growth appears, if anything, less satisfactory than American economic growth. In sum, I would suggest to you that the record of industrial countries over the last 15 years is profoundly discouraging as to the prospect of maintaining substantial growth with financial stability. Why is this the case? I would suggest that in understanding this phenomenon, it is useful at the outset to consider the possibility that changes in the structure of the economy have led to a significant shift in the natural balance between savings and investment, causing a decline in the equilibrium or normal real rate of interest that is associated with full employment.",540
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.11,Central Banking in the Crisis: Conceptual Convergence and Open Questions on Unconventional Monetary Policy,April 2014,Jean-Claude Trichet,,,Male,Unknown,Unknown,Male,"I see many reasons why the financial system of the advanced economies proved as fragile as a house of cards. Without being exhaustive, I would propose five major reasons, which are mutually reinforcing:
 First, the extreme sophistication of financial instruments, securitization, the generalization of derivatives markets of all kinds, the very rapid growth of shadow banking, and the emergence of highly leveraged institutions created a new financial environment that was complex, obscure in many respects, and difficult to decipher. Second, there was an extraordinary increase of interconnectedness between all financial and nonfinancial institutions, markets, and economies at national and international levels, fostered by the advances of information technologies and by globalization. This unseen level of interconnectedness has given rise to new, untested properties of global finance [Yellen 2013]. Third, a generalized excess of leverage, private and public, was progressively built in the advanced economies [Kindleberger 1978; Turner 2013]. This phenomenon was almost totally neglected by the international community over many years before the crisis, as were forgotten the financial instability hypothesis of Hyman Minsky [1986] and the debt-deflation analysis of Irving Fisher [1933]. This third reason was strongly underlined during the crisis [Reinhart and Rogoff 2009; Shirakawa 2012]. Four, there was a progressive generalization of a sentiment of excessive tranquility and confidence, both in the public and the private sector. The “great moderation,” marking the period from mid-1980s to mid-2000s, gave the false impression that the low volatility of both output and inflation—in a context of steady growth and low inflation—would last for a considerably longer period of time and no longer required traditional prudent and cautious policies. The governance of many private financial institutions was exceptionally loose, and the risk management culture was dramatically risky. This relative ignorance of longer-term economic and financial risks was largely shared in the public sector, including in central banking, even when the build-up of potentially deflationary and inflationary risks (because of public and private sector excessive indebtedness) was particularly accentuated [Taylor 2009]. And fifth, closely linked with the previous reason, there was a consensus of the international community on the efficiency of markets in almost all circumstances (and therefore on the virtues of large deregulation exercises) on the related belief that the financial system could never be far away from a Pareto-optimal single equilibrium. This implied that the possibility of multiple equilibria should be neglected. As a matter of fact, dominant macro models failed to predict the crisis and seemed, during the three quarters following Lehman Brothers, largely incapable of explaining what was happening to the economy in a convincing manner. “In the face of the crisis, we felt abandoned by conventional tools” [Trichet 2011a]. The first two reasons mentioned suggest that the important recent structural changes observed in global finance and in the global economy are presenting important new challenges, both for economic theory and for policymaking. Information technology advances, globalization, and related ongoing financial and economic interconnectedness and innovation are likely to give birth to new emerging properties of global finance that are far from being fully elucidated. The international community can be forgiven for having missed some of these new emerging properties—including the near-immediate global transmission of financial shocks—that contributed significantly to the acuteness of the crisis. The three last reasons are less forgivable. Forgetting Kindleberger, as well as Fisher and Minsky, at a time when the debt outstanding was piling up in many advanced economies, was strange. Believing that central banks should neglect an analysis of money, of its components, and of its counterparts—as recommended by the mainstream of central banking economists—appears hard to believe with the benefit of hindsight. Displaying an excessive confidence in models that were mathematically oversimplified and ignoring the possible materializations of tail risks proved mistaken. The crisis was a cruel reminder not only of the “financial instability hypothesis” or of the “debt-deflation” analysis but also of “Knightian uncertainty” [Knight 1921], which refuses to be encapsulated in probability modeling. Having to cope with these dramatic events occurring in the advanced economies, central banks had the lucidity and the courage to take bold and swift decisions. They were coping with very different economies, with significantly different financial structures, and with diverse historical and cultural backgrounds and conceptual references. One could have expected that, under the pressure of their own economies’ idiosyncrasies, the shock of the crisis would have accentuated their differences and given rise to an even more diverse set of decisions, of utilization of policy tools and instruments, and of concepts of monetary policy in a selfish, inward-looking mode. But my thesis is that—contrary to what could have been expected and, perhaps, feared—central banks appeared to be practically and theoretically somewhat closer when confronted by economic and financial turmoil. This phenomenon was spectacular almost immediately after the Lehman Brothers bankruptcy, with the closest central bank international cooperation ever, including through a multilateral network of swaps lines, which remains a historical accomplishment [Papadia 2013]. This unseen level of close cooperation has also been symbolically illustrated by the coordinated decrease of interest rates that took place on October 8, 2008. But the crisis has also started or accelerated a multidimensional process of convergence of key elements of thinking about and making monetary policy. I propose to characterize this phenomenon as a process of “conceptual convergence.” It is far from being achieved, if it ever can or should be. But my own perspective suggests that it is an ongoing global process that should call for great attention both from academia and from policymakers.",2
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.8,Time to Close the Opportunity Gap,April 2014,Kevin Brady,,,Male,Unknown,Unknown,Male,"In the context of this anemic recovery, President Obama raised the issues of economic inequality and mobility. These are real issues. Indeed, the Joint Economic Committee (JEC) Republican staff has conducted extensive research on economic inequality and mobility, which is available on the JEC’s website [Joint Economic Committee 2012a, 2012b, 2012c]. We are not all blessed with equal desires and talents. Therefore, we should not expect equal outcomes. In America, however, everyone should have an equal chance to climb the ladder of success—driven upward by his or her initiative—and to achieve his or her American dream. What role should the federal government play in addressing economic inequality and mobility? President Abraham Lincoln—perhaps the greatest “equalizer” to inhabit the White House—provided the answer. In his message to Congress on July 4, 1861, he made clear that the proper role of government in promoting economic opportunity is “to elevate the condition of men—to lift artificial weights from all shoulders—to clear the paths of laudable pursuit for all—to afford all, an unfettered start and a fair chance, in the race of life.”",1
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.10,Central Banks and Macroeconomic Ambidexterity,April 2014,Andrew G Haldane,,,Male,Unknown,Unknown,Male,"Accompanying these monetary policy changes has been a marked shift in regulatory philosophy. Since the crisis, financial regulation has become explicitly macroprudential [Morris and Shin 2008; Bank of England 2011; Hanson, Kashyap, and Stein 2011]. This is an expression much-used, but generally little-understood. In a nutshell, it means that policymakers have begun using prudential means to meet macroeconomic ends. Those macroeconomic ends include tempering swings in credit and leverage—the classic credit cycle. The credit cycle is a long-established feature of the financial landscape [Aikman, Haldane, and Nelson 2014]. Figure 1 shows its pattern in the United Kingdom over the past 130 years or so. The credit cycle is every bit as regular as the business cycle. But it differs from the business cycle in two critical respects: its amplitude is at least twice as large and its duration at least twice as long. Both are important for the design of macroprudential policy regimes. Credit and Business Cycles Source: Bank calculations. The larger amplitude of the credit cycle is one reason why credit booms have, more often than not, resulted in banking crises, as demonstrated in Table 1. Because financial crises cause large and long-lived disruption to the economy, this suggests a strong empirical link between credit cycles and macroeconomic destabilisation. Or, put differently, curbing the credit cycle appears to be an important ingredient of broadly based macroeconomic stability. In principle, monetary policy could be used to curb the credit cycle. In practice, the differing duration and synchronicity of the credit and business cycles means this is unlikely to work well. Precrisis experience illustrates well just that point. At the same time as the wider economy was operating in cruise control, credit markets were in overdrive. Hitting these two birds—one flying high, the other low—with one monetary policy stone would have defied even the most astute marksman [King 2013]. What is needed, in these instances, is a second instrument. In the language of Tinbergen [1952], two cycles and two objectives call for two instruments. This is where macroprudential policy comes in. One of the aims of macroprudential policy is to act countercyclically on the credit cycle, constraining credit booms and cushioning busts [Aikman, Haldane, and Nelson 2014]. In this role, macroprudential policy is complementing monetary policy in its role of stabilising the macroeconomy. Macroeconomic policy then becomes, in effect, two-handed or ambidextrous. Since the crisis, this two-handed approach to policy has been taken up actively by a number of countries internationally [International Monetary Fund 2013]. For example, countercyclical prudential policy is now baked into new international regulatory rules. The so-called Basel III reforms introduced for the first time a “Counter-cyclical Capital Buffer” (CCB) to be adjusted to counteract the credit cycle [Basel Committee on Bank Supervision 2010]. Although a small step for mankind, this is a giant one for bank regulators. It is also, inevitably, something of a step into the unknown. What will be the impact of changes to the CCB on credit and growth? Will the two arms of policy (monetary and macroprudential) be better than one? And, if so, what institutional arrangements best deliver those benefits? Policy experience from the recent past and the present can shed light on these questions.",
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.14,Remarks on Banking Reform,April 2014,Thomas M Hoenig,,,Male,Unknown,Unknown,Male,"I want to spend a few more minutes on this last topic, as it remains a critical step to a more sound financial system. The chart titled Consolidation of the Credit Channel (Figure 1) shows the trend in concentration of financial assets since 1984. The graph shows the distribution of assets for four groups of banks, ranging in size from less than $100 million to more than $10 billion. The chart shows that in 1984, the control of assets among the different bank groups was almost proportional. Also, within each group if a single bank failed, even the largest, it might shock the economy, but most likely would not bring it down. Today this distribution of assets is dramatically different. Banks controlling assets of more than $10 billion have come to compose an overwhelming proportion of the economy, and those with more than a trillion dollars in assets have come to dominate this group. If even one of the largest five banks were to fail, it would devastate markets and the economy. Consolidation of the Credit Channel Source: FDIC. Reflects the aggregation of total assets of FDIC-insured institutions by bank holding company and also includes charter-level assets for banks with no holding company. Title I of the Dodd-Frank Act is intended to address this issue by requiring these largest firms to map out a bankruptcy strategy. This is referred to as the Living Will. If bankruptcy fails to work, Title II of Dodd-Frank would have the government nationalize and ultimately liquidate a failing systemic firm. Although these mechanisms outline a path for resolution, success will be determined by how manageable large and complex firms are under bankruptcy and whether under any circumstance they can be resolved without major disruption to the economy. This is a daunting task, and increasing numbers of experts question whether it can be done given current industry structure.Footnote 2 Two impediments are most often highlighted to organizing an orderly bankruptcy or liquidation for these firms. First, it is not possible for the private sector to provide the necessary liquidity through “debtor in possession” financing due to the size and complexity of the institutions and due to the speed at which crises occur. There simply would be too little confidence in bank assets and the lender’s ability to be repaid, and too little time to unwind these firms in an orderly fashion in a bankruptcy. Under the current system, it would have to be the government that provides the needed liquidity, it is argued, even in bankruptcy to avoid a broader financial meltdown. Second, when a mega banking firm goes into bankruptcy, capital markets and cross-border flows of money and capital most likely would seize up, intensifying the crisis, as happened following the failure of Lehman Brothers, for example. International cooperation is critical in such circumstances, and it would be ideal if creditors, bankers and governments acted calmly and rationally in a crisis. It would be ideal also if all contracts were honored and if collateral and capital were free to move across borders. But, experience suggests otherwise. Panic is about panic, and people and nations generally protect themselves and their wealth ahead of others. Moreover, there are no international bankruptcy laws to govern such matters and prevent the grabbing of assets, sometimes known as ring-fencing. This raises the important question of whether firms must simplify themselves if we hope to place them into bankruptcy. This is no small question, and it must be addressed. A further sense of the importance of these unresolved issues can be gained by working through the annual report of any one of these largest firms. These reports show that individual firms control assets close to the equivalent of nearly a quarter of U.S. GDP, and the five largest U.S. financial firms together have assets representing just over half of GDP. The reported composition of firm assets represents a further challenge in judging their resolvability, as it is opaque and the relationship among affiliate firms is sometimes unclear. A host of assets and risks are disclosed only in footnotes, although they often involve trillions of dollars of derivatives that are not shown on the balance sheet. Intercompany liabilities are in the hundreds of billions of dollars and if any one link fails, it can initiate a chain reaction of losses, failure and panic. And should crisis emerge, liquidity is sought through the insured bank, not through the provisions of bankruptcy. One failure means systemic consequences. These conditions mean “too big to fail” remains a threat to economic stability. They necessarily put the economic system at risk should even one mega bank fail. And they allow these mega banks to operate beyond the constraints of economies of scale and scope, and provide the firms an enormous competitive advantage—all of which is antithetical to capitalism.",
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.16,Another Look at the Case for International Diversification,April 2014,Mark J Purdy,Kuangyi Wei,,Male,Unknown,Unknown,Male,"Building on the seminal work of Hymer [1960] and Vernon [1971], early studies in this field emphasized the benefits businesses can derive from international diversification. In the foreign direct investment literature, international diversification allows firms to overcome market imperfections [Ruigrok and Wagner 2003] and trade barriers [Dunning 1993]. Benefits include gains from economies of scale and scope [Caves 1971; Kogut 1985] and spreading of risks through portfolio diversification [Kim, Hwang, and Burgers 1993]. In the literature on multinational firms, diversification allows firms to exploit intrafirm comparative advantages, such as improving operational flexibility or arbitrage of opportunities within a global network [Kogut 1989]. The resource-based view of the firm highlights the role of international diversification in creating greater scope for organizational learning and development [Zahra, Ireland, and Hitt 2000] and access to cheaper resources, such as labour and technology, in foreign countries [Porter 1990]. Other studies have focused on the costs that arise from multinationality. Siddharthan and Lall [1982] was one of the first studies to point out that an increasing degree of internationalization may exhaust managerial activity. Some of the difficulties associated with international diversification include information asymmetries and coordination challenges between headquarters and subsidiaries [Denis, Denis, and Yost 2002], cultural diversity leading to communication and motivation problems [Hofstede 1984], increases in transaction costs due to cultural and geographical dispersion [Jones and Hill 1988], and increases in governance costs [Hitt, Hoskisson, and Kim 1997]. There are also increasing financial and political risks, such as exchange rate volatility and inflation differentials [Reeb, Kwok, and Baek 1998], and risks such as boycotts and fund remittance control [Boddewyn 1988]. Empirical studies of the impact of diversification have proved to be no more conclusive than the theoretical debate. One of the earliest empirical studies was Vernon [1971], which pointed toward a positive linear relationship between international diversification and firm performance. However, this was challenged by later studies that found evidence of nonlinearity. Geringer, Beamish, and DaCosta [1989] found evidence of an inverted U-shaped relationship. This suggests that firms were only able to exploit market imperfections existing across countries up to a certain point before the costs of coordination started to dominate. This diminishing return to diversification was echoed by a more complete study by Hitt, Hoskisson, and Kim [1997], drawing on the resource-based, transaction cost, and organizational learning theories of the firm. Many of the early studies implied that beyond a certain threshold the cost of internationalization will outweigh the potential benefits (an inverted-U shape). This view was challenged by Ruigrok and Wagner [2003]. Using a sample of German manufacturing companies, they consistently found that there was a U-shaped relationship across all statistical techniques applied, including panel analysis. This suggests that only when the firm has reached a certain level of international expansion does it start to benefit from economies of scale and scope. The authors explained that initial foreign expansion may not reduce operating costs. Cost efficiency from access to lower labour costs and cheaper raw materials appear to be exploitable only by firms with high degrees of internationalization, while newcomers have to undergo a learning period. Capar and Kotabe [2003] showed that a U-shaped relationship also existed in the services sector. Firms were likely to face declining performance with initial attempts at international diversification due to countries’ strict control over foreign involvements and the liabilities of foreignness. Attempts have been made to reconcile the above contradictory findings. Contractor, Kundu, and Hsu [2003] tested a three-stage theory of international expansion. They found a cubic (horizontal S-curve) relationship in the service industry using a cross-sectional analysis. Owing to data limitation, early studies predominantly used cross-sectional analysis. Recently, looking at the manufacturing sector, Lampel and Giachetti [2013] used panel-data analysis on firms in car and light truck production and showed that there was an inverted-U shaped relationship. On the other hand, Bobillo, López-Iturriaga, and Tejerina-Gaite [2010] tested the relationship using panel data on manufacturing firms in five European countries and found evidence for the S-shaped relationship. There are also a number of recent country-specific studies. For example, using a sample of U.S. listed companies, Lee and Li [2012] attempted to reconcile contradictory results in the literature by employing a panel analysis with firm-specific effects and a quantile approach.Footnote 1 They found that the effect of diversification on performance varied across different types of firms. However, Lee, Hooy, and Hooy [2012] found no evidence that international diversification has any impact on firm value in Malaysia. In the United Kingdom, Nath, Nachippan, and Ramanathan [2010] found that logistic firms are better off when operating in a diverse geographical market. A number of conclusions can be drawn from the above review. First, there are many channels through which international diversification could affect firm performance. Second, while many studies find some impact of diversification on performance, there is little consensus on the shape and form of that impact. Third, many studies focus on specific industries or firms in specific countries, raising questions around how general the results are to a wider set of firms. In this paper we attempt to overcome some of these issues through use of a panel data set of the world’s largest 500 companies, encompassing firms in a range of different industries and based in all the major economic regions of the world. We focus principally on the role of international diversification in helping firms to follow growth opportunities and exploit production efficiencies overseas.",
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.15,Economics at the American Chemistry Council,April 2014,Thomas Kevin Swift,,,Male,Unknown,Unknown,Male,"Now, our team’s focus is narrower (we no longer provide environmental risk support nor manage a resin statistics program) and the Economics & Statistics department now includes two economists, a statistician (who is studying to be an economist), and a business research professional. I currently report to the ACC’s Chief Financial Officer. We are organized as shown in Figure 1. ACC Economics & Statistics Department Organization I had a colleague remark once that I “was the only person not interested in building an empire” at ACC, and it’s true. I want to do good economics and have a little fun while doing it. I’ve always tried to “run lean” with the economics function. I’d rather not be a large target and prefer to keep busy with a nice backlog of projects. Over the years, advances in information technology and automation of business processes have reduced the need for a full-time staff assistant. We have access to one for the hour or so every other week when we need a check request, purchase order, supplies ordered, or assistance with a meeting we are hosting. In doing business economics, the availability of purchased databases and industry-specific publications and software further promote our productivity and enhance our ability to provide economic and other analytical services for our internal clients. During the summer of 2013, we had one position open and one colleague on maternity leave. I recently reflected that the two of us in that time were able to accomplish more (nearly two-fold by my reckoning) than five did 20 years ago! That said, having more support would be very helpful in extending our service offerings, especially with database development. The current mission of the ACC Economics & Statistics Department is to provide a full range of statistical and economic advice and services for ACC and its members and other partners. The group works to improve overall ACC legislative and regulatory advocacy impact by providing statistics on American chemistry as well as preparing information about the economic value and contributions of chemistry to our economy and society. We function as an in-house consultant, providing survey, economic analysis, and other statistical expertise, as well as monitoring business conditions and changing industry dynamics. The group also offers extensive industry knowledge, a network of leading academic organizations and think tanks, and a dedication to making analysis relevant and comprehensible to a wide audience. The latter is very important, and we strive to present our research findings in as simple a manner as possible. I keep a box of crayons on my desk to remind me of this!",
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.9,Business Economics and the Printing Industry,April 2014,Ronnie H Davis,,,,Unknown,Unknown,Mix,,
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.6,Initial Results of the 2012 Economic Census,April 2014,Robert P Parker,,,Male,Unknown,Unknown,Male,"The EC is mandated by law under Title 13 of the United States Code (sections 131, 191, and 224) and requires firms to respond, specifying penalties for firms that fail to report. The law also requires the Census Bureau to maintain confidentiality, and individual responses may be used only for statistical purposes. Individual responses may be seen only by people sworn to uphold Title 13. No data are published that could reveal the identity or activity of any individual or business. Confidential information on census forms is exempt from the Freedom of Information Act, and copies retained in respondents’ files are immune from legal process. In an EC, the Census Bureau collects and publishes data primarily on an “establishment” basis, where an establishment is a business or industrial unit at a single physical location that produces or distributes goods or performs services, such as a single store or factory. The EC features this basis, as opposed to an “enterprise” basis, because many companies own or control more than one establishment, and those establishments may be located in different geographic areas and may be engaged in different industries. By collecting separate information for each establishment, the EC can provide users with comprehensive data for detailed industries and geographic areas.Footnote 3 In addition, establishment data provide industry information on the specific inputs and outputs and how they relate to each other. These data enable the Census Bureau to provide measures of value added, which is considered to be the best measure of industry output because value added measures the contribution of each industry to total output by eliminating duplication. The 2012 EC includes a new report that, as described below, aggregates establishment data to the enterprise level. An EC has been taken at regular five-year intervals since 1967; the three previous censuses had been taken in 1954, 1958, and 1963. The 1954 EC was the first census to be fully integrated as it provided comparable data across economic sectors, using consistent time periods, concepts, definitions, classifications, and reporting units. It was the first census to be taken by mail, using lists of firms provided by the administrative records of other federal agencies. Since 1954, administrative records also have been used to provide basic statistics for very small firms, reducing or eliminating the need to send them census questionnaires. Also beginning with the 1954 EC, a distinction is made both in the collection and publication of EC data between “employer” and “nonemployer” businesses. Employer business are defined as firms with one or more paid employees at any time during the year as recorded in the administrative records of other federal agencies; nonemployer businesses are defined as firms subject to federal income tax with no paid employees during the year recorded in these administrative records. Prior to 1954, individual components of the EC were taken separately at varying intervals. The EC had its origin with the 1810 Decennial Census, when questions on manufacturing were included. Coverage of economic activities was expanded for the 1840 Decennial Census and subsequent censuses to include mining and some commercial activities. The 1905 Manufactures Census was the first time an EC was taken separately from a decennial population census. Censuses covering retail and wholesale trade and construction industries were added in 1930, as were some covering service trades in 1933. Censuses of construction, manufacturing, and the other business service censuses were suspended during World War II. The industry coverage of the EC has been expanded substantially since 1954, mostly between 1967 and 1992. A census of construction industries was added in 1967, and the scope of service industries was expanded in 1967, 1977, and 1987.Footnote 5 Although a few transportation industries were covered as early as 1963, it was not until 1992 that the census was expanded to include transportation except for railroads and passenger air, communications, and utilities. Also in 1992, coverage was expanded to include the financial, insurance, and real estate industries. The most recent change in coverage was in 2007, when scheduled passenger air transportation was added. With these additions, the EC covers roughly 98 percent of the private nonfarm economy. Coverage of the government (public administration) and farm sectors is provided by a census of governments, collected by the Census Bureau, and a census of agriculture, now collected by the U.S. Department of Agriculture. Beginning with the 1954 EC, data on nonemployer businesses for retail trade and selected services were obtained from tax return records, and these data were integrated where feasible into the EC reports. Since 1972, a separate report on all nonemployer businesses has been issued as part of the EC, and an annual series was added beginning with the 1998 report. After the 1982 EC, nonemployer business data were dropped from the regular EC reports until the 2007 EC, when a report was introduced that includes summary data for both nonemployer and employer businesses.Footnote 6 The EC is the most comprehensive source of economic information on business establishments available. It provides detailed industry and geographic statistics that are used by businesses, researchers, and government policy makers. In addition, it provides the detailed data used by the Board of Governors, U.S. Federal Reserve System [2012] to benchmark the Index of Industrial Production and Capacity Utilization, by the Bureau of Labor Statistics [2014] to develop weights for the Producer Price Index, and by the Bureau of Economic Analysis to prepare input-output accounts and quarterly GDP. Universe-level estimates from the EC also are used to benchmark most of the Census Bureau’s annual, quarterly, and monthly economic surveys—such as the annual surveys of manufacturing and wholesale trade and the monthly survey of retail trade—and to update their survey sample frames for changes in the composition and organization of the economy. Benchmarking of these series is critical to the reliability of the survey-based estimates. As previously noted, the estimates from the 2012 EC are not the first estimates for 2012 released by the Census Bureau. For industries covered by its monthly, quarterly, and annual industry surveys, the first estimates for 2012 were released as the sum of the months/quarters for manufacturing, merchant wholesale trade, retail trade and food services, and selected service industries. For the monthly and quarterly surveys, estimates for 2012 were released at the same time as the release of the estimates for December 2012 (or fourth quarter 2012). For example, the first estimates of sales and inventories for retail trade were released in February 2013. About a year later, estimates from the 2012 annual survey were released and incorporated in the monthly release resulting in revisions into time series back to 2011 and forward from December 2012. (Table 2 shows for each of these monthly, quarterly, and annual surveys the items covered, industry detail, and date of release of the first 2012 estimates.) Final and more reliable estimates for 2012 will be released in early 2015 or 2016 when the 2012 EC data are incorporated in the monthly estimates. The 2012 data from the Census Bureau’s monthly, quarterly, and annual sample surveys provide timely estimates that enable users to track short-term trends.Footnote 7 However, when compared with the data from the 2012 EC, these data have the following significant limitations: (1) their estimates are based on small samples and subject to significant sampling errors, whereas the EC data, except for the census of construction industries, are based on a complete enumeration; (2) reporting is mandatory in the EC, whereas the data from most of the surveys are reported on a voluntary basis; (3) there are no surveys of construction or mining sectors, which are covered in the EC; (4) for industries covered by the EC, the surveys do not provide data for all six-digit NAICS industries; (5) the surveys provide little or no geographic coverage—only annual state data for manufacturing; and (6) the surveys—especially the monthly and quarterly surveys—have limited data content. (The annual surveys of manufacturing, wholesale and retail trade, and services collect data such as operating expense, purchased services, class of customer, and class of customer data.) By collecting these items on a sample basis, the Census is able to reduce reporting burden on millions of small businesses. Industry coverage of the 2012 EC will be the same as the 2007 EC; it will cover almost the entire private nonfarm economy as well as selected government activities. Table 3 identifies the industries that will not be covered in 2012. EC data also cover government hospitals and government-owned liquor stores, even though the data for the EC do not generally include government-owned establishments.Footnote 8 The sectors and industries shown in Table 3 reflect the 2012 version of the NAICS. The 2012 EC will cover 1,056 of the 1,065 industries of the 2012 version of U.S. NAICS, as announced in the August 17, 2011 Federal Register. The 2012 EC covered the business activity of about 28 million business establishments. To obtain data for these establishments, the Census Bureau uses both mail questionnaires and administrative records from other federal agencies—the Internal Revenue Service, Social Security Administration, and Bureau of Labor Statistics. Also for the 2012 EC, first-time electronic reporting was formally permitted for firms with a single establishment, and about 30 percent of them filed electronically. Mail questionnaires were used for establishments of large- and medium-size firms, all firms known to operate more than one establishment, all employer firms with payroll above a specified cutoff during 2012 (based on administrative records), and a sample of single-establishment employer firms with payroll below a specified cutoff in classifications for which the need for specialized data preclude reliance solely on administrative records sources. For the 2012 EC, about 4 million census forms were mailed using over 500 different census forms, each customized to particular industries. Mail questionnaires were not used for single-establishment employer firms with payroll below a specified cutoff and for nonemployer businesses. Data for about 3 million small employers on sales, payroll, and employment, as well as information on location, legal form of organization and industry classification, are derived or estimated from administrative records. These records also are the source of data for about 21 million nonemployer businesses on sales, location, legal form of organization, and industry classification. The major EC reports now exclude nonemployer businesses, primarily because of the limited information available from administrative records, of their impact on the EC measures of business activity for most industries, and of the cost of collecting additional information from these businesses. (Only the Economy-Wide Key Statistics Report provides data for employer and nonemployer businesses.) The Census Bureau has estimated that for 2012, nonemployer businesses accounted for roughly 4 percent of business activity, measured using sales, and 70 percent of all number of businesses. For 2012, the number of these businesses and their sales, receipts, or shipments by industry and geographic area, will be released in the Nonemployer Statistics report scheduled to be released in April 2013 on the American Fact Finder on the Census Bureau’s website. Each establishment covered in an EC is assigned a NAICS industry code based on the primary products shipped, sold, or produced. The product detail for service industries collected in the 2012 EC is based on the North American Product Classification System (NAPCS), the 1999 product classification agreement with Canada and Mexico.Footnote 9 For the other industries, the product detail used for the 2012 EC is consistent with NAICS. For the new 2012 EC Enterprise Statistics report, each enterprise or company reporting in the EC will be assigned a special industry code based on the primary industry of its establishments.",1
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.7,The Great Deformation: The Corruption of Capitalism in America,April 2014,Devon M Herrick,,,,Unknown,Unknown,Mix,,
49,2,Business Economics,05 June 2014,https://link.springer.com/article/10.1057/be.2014.12,"The Great Escape: Health, Wealth, and the Origins of Inequality",April 2014,W Steven Barnett,,,Unknown,Unknown,Unknown,Unknown,,
49,3,Business Economics,17 September 2014,https://link.springer.com/article/10.1057/be.2014.26,From the Editor,July 2014,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
49,3,Business Economics,17 September 2014,https://link.springer.com/article/10.1057/be.2014.18,CBO’s Outlook for the Economy in February 2014,July 2014,Douglas W Elmendorf,,,Male,Unknown,Unknown,Male,"As a companion to The Budget and Economic Outlook: 2014 to 2024, the Congressional Budget Office (CBO) also released The Slow Recovery of the Labor Market [Congressional Budget Office 2014b]—a report that takes a closer look at developments in the labor market since the recent recession and at CBO’s projections for the labor market for the next decade. The deep recession that began in December 2007, when the economy began to contract, and ended in June 2009, when the economy began to expand again, has had a lasting effect on the labor market. More than four and a half years after the end of the recession, employment has risen sluggishly—much more slowly than it grew, on average, during the four previous economic recoveries that lasted more than one year. At the same time, the unemployment rate has fallen only part way back to its prerecession level; and a significant part of that improvement is attributable to a decline in labor force participation that has occurred as an unusually large number of people have stopped looking for work. In CBO’s assessment, the slow recovery of the labor market largely reflects slow growth in the demand for goods and services, with a smaller role played by structural factors that stem from the recession and the slow recovery of output but that are not directly related to the economy’s current cyclical weakness. CBO estimates that employment at the end of 2013 was about 6 million jobs short of where it would be if the unemployment rate had returned to its prerecession level and if the participation rate had risen to the level it would have attained without the current cyclical weakness. My comments about the labor market summarize CBO’s analysis of the unemployment rate, the labor force participation rate, and the employment-population ratio. Of the roughly 2 percentage-point net increase in the unemployment rate between the end of 2007 and the end of 2013, the portions that CBO estimates can be attributed to different factors are:
 about 1 percentage point to cyclical weakness in the demand for goods and services; and about 1 percentage point to structural factors, of which:
 ° about ½ percentage point to the stigma and erosion of skills from long-term unemployment; and ° about ½ percentage point to a decrease in the efficiency of matching workers and jobs, at least partly from mismatches in skills and locations. CBO estimates that GDP was 7½ percent smaller than potential (maximum sustainable) GDP at the end of the recession; by the end of 2013, that gap had declined to about 4 percent (see Figure 1). CBO expects that, under current laws governing federal taxes and spending, output will grow more rapidly in the next few years than it has in the recent past. The agency projects that by the second half of 2017, the gap between actual and potential GDP will return to its average historical relationship—bringing the effects of cyclical conditions on unemployment and labor force participation back to their average historical values in 2018. Gap between Actual and Potential GDP Source: The Budget and Economic Outlook: 2014 to 2024 (February 2014). Notes: Values are annual. Actual data are plotted through 2012; projections are plotted through 2024. With the narrowing of the gap between actual and potential output, as well as improvement in the structural factors that have raised the unemployment rate since the beginning of the recession, CBO expects the unemployment rate to decline over the next decade. The agency anticipates that, under current law, the unemployment rate will decline from the average of 7.0 percent in the fourth quarter of 2013 to 5.8 percent in the fourth quarter of 2017 and 5.5 percent in 2024 (see Figure 2). The natural rate of unemployment—which is the rate arising from all sources other than fluctuations in the overall demand for goods and services—is projected to decline from an estimated 6.0 percent at the end of 2013 to 5.6 percent in the fourth quarter of 2017 and 5.2 percent in 2024. Actual and Natural Rates of Unemployment Source: The Slow Recovery of the Labor Market (February 2014).Notes: Values are quarterly. Historical data are plotted through 2013; projections are plotted through 2024. The projected decline in the unemployment rate through the end of 2017 reflects primarily the expected continued economic expansion, because the increase projected in the overall demand for goods and services will create a need for businesses to hire more workers. With the gap between actual and potential GDP anticipated to narrow to one-half of a percent in the second half of 2017, the unemployment rate would be expected to drop considerably. However, CBO anticipates that this effect will be dampened because the improvement in labor market conditions will induce some of the people who have left the labor force to return to it. The projected decline in the unemployment rate through the end of 2017 also reflects an expected waning of some of the structural factors that have raised unemployment since the beginning of the recession. The effect on the unemployment rate of the extension of unemployment insurance (UI) benefits essentially ended with the expiration of that extension at the close of 2013. And the effect of unusually large problems in matching unemployed workers with available jobs—estimated at about one-half percentage point in late 2013—is expected to recede almost entirely by 2017 (and to disappear by 2018). In contrast, the effects of stigma and erosion of skills for the long-term unemployed—also estimated to have raised the unemployment rate by about one-half of a percentage point in late 2013—will continue to hamper the ability of some people who remain in the labor force to find work. The projection of the small additional decline in the unemployment rate over the 2018–24 period stems from CBO’s anticipation that the effects of stigma and skill erosion will wane as more of the long-term unemployed either leave the labor force or find jobs. Even in 2024, however, CBO expects that those effects will boost the unemployment rate by about one-quarter of a percentage point. CBO projects that the unemployment rate will be about one-quarter of a percentage point above the natural rate in the latter part of the coming decade. That gap is not based on a forecast of specific cyclical movements in the economy; instead, it is based on CBO’s estimate that the unemployment rate has been, on average, higher than the natural rate since 1947 and, notably, during each of the past five business cycles. The difference arises because the shortfalls in output relative to CBO’s estimate of potential output during economic downturns have been larger and more frequent than the excesses during economic expansions. Of the roughly 3 percentage-point net decline in the labor force participation rate between the end of 2007 and the end of 2013, the portions that CBO estimates can be attributed to different factors are:
 about 1½ percentage points to long-term trends (primarily the aging of the population); about 1 percentage point to cyclical weakness in job prospects and wages; and about ½ percentage point to discouraged workers who have dropped out of the labor force permanently. CBO projects that the labor force participation rate, which averaged 62.9 percent in the fourth quarter of 2013, will be at the same level, on average, in 2014 but will fall to 62.5 percent by the end of 2017 and to 60.8 percent by the end of 2024 (see Figure 3). Historical and Projected Rate of Participation in the Labor Force Source: The Slow Recovery of the Labor Market (February 2014). Notes: Values are quarterly. Historical data are plotted through 2013; projections are plotted through 2024. For 2014–17, CBO anticipates that many of the people who left the labor force because of a lack of job opportunities will return as labor market conditions improve and that the number of people who stay out of the labor force (for example, to attend school) will diminish. However, CBO expects, the cyclical recovery in participation will be more than offset by the downward pressure on participation stemming from other changes. The most significant downward pressure will come from the aging of the population (and other demographic changes), but federal tax and spending policies will also play a role. The Affordable Care Act (ACA) will tend to reduce participation, with the largest impact stemming from new subsidies that reduce the cost of health insurance purchased through exchanges. Specifically, by providing subsidies that decline with rising income and by making some people financially better off, the ACA will create an incentive for some people to choose to work less. The structure of the tax code—in which rising real (inflation-adjusted) incomes push some people into higher tax brackets—will also reduce labor force participation slightly. The further projected decline in the participation rate after 2017 primarily reflects the ongoing aging of the population, with smaller further effects of the federal fiscal policies just discussed. Those effects are offset in part by a reduction in the number of people who will have permanently stopped looking for work because of the recession and slow recovery; many of them would, by that period, have left the labor force anyway through retirement or by some other means. Still, CBO estimates that the lasting effects of the recession and slow recovery will depress labor force participation by 0.4 percentage points in 2024. The difference of roughly 5 percentage points between the labor force participation rate at the end of 2007 and CBO’s projection of that rate for the end of 2024 can be attributed to the following factors:
 about 3½ percentage points to long-term trends (primarily the aging of the population); a little under ½ percentage point to discouraged workers who have dropped out of the labor force permanently; and about 1 percentage point to the ACA and real bracket creep in the individual income tax. CBO’s projections for the growth of employment during the next decade reflect the agency’s projections of growth in the population and of the changes in the rate of labor force participation and the unemployment rate. All told, CBO projects, the employment-to-population ratio will edge upward over the next few years, reaching 58.9 percent at the end of 2017—just 0.4 percentage points above the ratio at the end of 2013 and well below the peak of 63.3 percent reached at the end of 2006 and the beginning of 2007, before the recession (see Figure 4). After 2017, the growth of employment slows somewhat more in CBO’s projections as demographic changes continue to push the labor force participation rate downward. CBO anticipates that, under current law, the employment-to-population ratio will fall to 57.5 percent in 2024. Historical and Projected Share of the Population that Is Employed Source: The Slow Recovery of the Labor Market (February 2014). Notes: Values are quarterly. Historical data are plotted through 2013; projections are plotted through 2024.",
49,3,Business Economics,17 September 2014,https://link.springer.com/article/10.1057/be.2014.23,The Labor Market in the Aftermath of the Great Recession,July 2014,Mary C Daly,Elliot M Marks,,,Male,Unknown,Mix,,
49,3,Business Economics,17 September 2014,https://link.springer.com/article/10.1057/be.2014.19,The Economics of Japan’s Stagnation,July 2014,Tanweer Akram,,,Unknown,Unknown,Unknown,Unknown,,
49,3,Business Economics,17 September 2014,https://link.springer.com/article/10.1057/be.2014.25,State Income Taxes and Interstate Migration,July 2014,Roger Cohen,Andrew Lai,Charles Steindel,Male,Male,Male,Male,"Table 1 shows numbers on the movements of taxpayers and aggregate gross income between regions, based on IRS migration data.Footnote 5 Between 1970 and 2010, the Northeast and Midwest consistently saw net losses from domestic migration, whereas the South experienced strong net gainsFootnote 6 [Conway and Hountenville 2003; Ihrke and Faber 2012]. This pattern may well partly reflect demographics. The population in the Northeast tends to be older than in the South and West (the median age in the Northeast is 39.2 years vs. 37.0 years in the South and 35.6 years in the West). Outmigration by Northeast retirees could partially reflect their preferences for warmer climates, as well as lower tax regimes. Table 1 also reports average regional state income tax rates. Southern states typically have low tax rates, whereas Northeastern states typically have higher ones. Part II of Table 1 breaks down migration flows by region and migration share. Southern states have relatively high gross outmigration (relative to their share of the U.S. population), but most Southern migrants remain in the region. In 2010, 55.5 percent of movers in the South relocated to another Southern state; in contrast, only about 39 percent of Northeast movers remained in the Northeast, while another 39 percent of Northeasterners relocated to a Southern state. Aside from any other factors (including climate and business taxation) affecting their attractiveness, states with larger migration gains tend to have lower income tax rates. Table 2 lists the top and bottom 10 states by net population flows. Among the top 10 migration gainers, three states (Florida, Texas, and Tennessee) have no tax on wage income.Footnote 7 The South tends to fare better with respect to migration gains than other regions, claiming eight of the top 10 migration-gaining states. Among the bottom 10 states in net migration, four are located in the Northeast and none are in the South. Figure 1 shows general levels of net migration for all 50 states in 2011. Net Migration in the 50 States, 2010 Source: Authors’ calculations using IRS SOI data. New Jersey has figured prominently in the state tax debate, having experienced a steady outmigration flow over the past 30 years. This a trend that has been attributed to the state’s high cost of living, along with the general decline of manufacturing in the Northeast, but also to its relatively high tax rates [Laffer and Moore 2009; Ebeling 2010]. But has New Jersey’s income tax system actually boosted outmigration? A study of New Jersey taxpayers concluded that New Jersey’s 2004 “millionaires’ tax,” which raised the state’s top marginal rate from 6.37 percent to its current rate of 8.97 percent on taxable incomes above $500,000,Footnote 8 did not noticeably affect migration decisions of the directly affected group [Young and Varner 2011]. However, the study suffers from a number of flaws [Davies and Pulito 2011; Cohen, Lai, and Seindel 2014]: notably, it assumed ex ante that the millionaires’ tax had no effect on annual migration of taxpayers making below its $500,000 threshold, even for those who made more in other years, and did not specifically control for many of the factors that could have affected migration, such as housing prices and unemployment. More critically, although the count of migrants to New Jersey were included in the study, the authors did not explicitly control for factors that could have affected the relative appeal of New Jersey for potential movers from other states. Finally, in contrast to their highlighted results, Young and Varner also reported a statistically significant increase in net migration from New Jersey when they confined their sample to parties whose entire income was subject to New Jersey’s tax.Footnote 9 Thus, our view is that Young and Varner’s study did not settle the issue on the effect of state income taxes on migration, either for New Jersey or for other states. It should be acknowledged at the start that income tax differentials are unlikely to be the key determinants of interstate migration flows. It also seems unlikely that typical state income tax hikes would be of such magnitude as to result in immediate large-scale increases in outmigration, or that the scale of such tax-motivated departures would be sufficient to reverse the gains in revenue resulting from a rate increase, at least in the near term. But state policymakers may have concerns beyond immediate revenue needs. If tax hikes induce increases in outmigration, they may have deleterious long-term effects on the state’s economy and on local businesses.Footnote 10 Given the body of suggestive evidence—the broad pattern of outmigration to lower tax states, the Coomes–Hoyt results, a critical examination of the Young–Varner study—the potential existence of notable “tax flight” effects should not be dismissed. We proceed to a more systematic study of some factors, including income tax differentials, which would likely affect interstate migration.",1
49,3,Business Economics,17 September 2014,https://link.springer.com/article/10.1057/be.2014.22,Economics at Work in an Economic Development Organization,July 2014,Patrick Jankowski,,,Male,Unknown,Unknown,Male,"Every task mentioned—big or small, mundane or substantive—supports the mission of the organization. In chamber of commerce parlance, that mission is “to make Houston a better place to live, work and build a business.” In less prosaic terms, we recruit corporations, facilitate foreign trade, advocate for legislation that enhances the region’s business climate or quality of life, and engage our members in task forces and committees. In another city, the Partnership would be considered a mega-chamber of commerce. In the past, I often referred to the Partnership as a chamber of commerce on steroids, but that was before steroids had a bad name. GHP’s strength derives from our 120-member board of directors. That is not a typo. Our board is larger than many small towns in Texas. The board truly represents the leadership of Houston’s business community. To serve on our board, an individual must hold the most senior position in Houston in his or her organization. Consequently, our board includes Fortune 500 CEOs, international division presidents, managing partners of professional service firms, owners of private business concerns, university presidents, and executive directors of large nonprofits.",
49,3,Business Economics,17 September 2014,https://link.springer.com/article/10.1057/be.2014.24,"The Coatings Industry—Beautifying, Protecting, and Preserving the Nation’s Assets",July 2014,H Allen Irish,,,Unknown,Unknown,Unknown,Unknown,,
49,3,Business Economics,17 September 2014,https://link.springer.com/article/10.1057/be.2014.20,Stress Test: Reflections on the Financial Crisis,July 2014,Diane C Swonk,,,Female,Unknown,Unknown,Female,,1
49,3,Business Economics,17 September 2014,https://link.springer.com/article/10.1057/be.2014.21,"Mass Flourishing: How Grassroots Innovation Created Jobs, Challenge, and Change",July 2014,Derk A Wilcox,,,Male,Unknown,Unknown,Male,,1
49,4,Business Economics,22 December 2014,https://link.springer.com/article/10.1057/be.2014.36,From the Editor,October 2014,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"I am pleased to welcome Gad Levanon to the Editorial Board. Gad is the director of macroeconomic research at The Conference Board and has also worked at the Israeli Central Bank. He holds a Ph.D. in economics from Princeton University. As you will have noted in the third article of this issue, Gad and his colleagues won this year’s NABE Contributed Paper Award. Also, he was an advisor to Bruneel, Juneau, and Wang, who won the Mennis Award.",
49,4,Business Economics,22 December 2014,https://link.springer.com/article/10.1057/be.2014.34,Strategies for NABE’s Long Term,October 2014,Jack Kleinhenz,,,Male,Unknown,Unknown,Male,"Let us look at a couple of destinations on that roadmap. First, what is our organization’s purpose? What value does NABE provide? To answer these questions of purpose and value, the NABE Board enlisted the help of a strategic planning consultant to interview members and thought leaders. The first question we asked was simple: “Why did you join NABE?” The overwhelming response was also simple: “networking” and joining a community of common interests. This truly validated the fact that NABE serves a special need for its members that cannot be met elsewhere—a need to associate with other practitioners who are also working to monitor, analyze, and interpret key trends in the economy—both in the United States and abroad. Let’s face it—we business economists are detectives trying to make sense of things for our employers and clients, and many times the best resources we have is each other. It’s interesting to look back at the official history of NABE [Regan 1989]—and for those of you who don’t know, our founders established NABE in 1959, more than 55 years ago—and find that its founders stated that its purpose is to, “provide for the mutual association of business economists and to strengthen professionalism.” This translates into today’s speak to facilitate networking and the sharing of views and information, and it also hints at the importance of continuing education. It is nice to know that we are continuing that legacy of 55 years ago. It was also no surprise to me when our consultants told us that it was clear to them that a significant amount of intellectual capital resided in this association and its members. This intellectual capital is a valuable asset that is simply not reflected on our balance sheet or profit and loss statement. But it’s not a one-way street. It’s not just that NABE provides value to its members: You—the NABE members—provide value to the organization. We do not have to look far for examples of seemingly impossibly difficult tasks that have been reduced to manageable processes thanks to the energy, wisdom, and networks of our members. Just look at this Annual Meeting. Its success has been due in large part to the role played by members serving on multiple committees and planning sessions and working diligently on recruiting speakers and developing valuable programs. And your value to the organizations is not just apparent in this annual meeting. Through your efforts, NABE launched its Transfer Pricing Symposium. Our Annual Economic Policy Conference in Washington DC is one of the premier policy events in the nation’s capital, thanks in no small part to your participation. This year, with your help, NABE developed a new professional certification: The Certified Business Economist—CBE. (More about that later, but it is certainly in line with the founders’ desire to “strengthen professionalism.”) There are too many accomplishments to mention. But it is the contributions of NABE members that make this association, “the preeminent business economics association globally.” I want to say that again because that is our new vision statement. NABE is, “the preeminent business economics association globally.” So, we’ve dealt with the first couple of questions: what is our purpose and what value do we provide. Next, the Roadmap project looked at where should we be going? The Roadmap identifies a number of initiatives for growth, including areas where NABE could expand our network and increase the information-sharing opportunities that are central to NABE’s existence. I’ll outline four of these. The CBE is really NABE 2.0. The growth of professional development opportunities offered by NABE is a game-changer, both for the profession and for NABE. Quite frankly, this sort of hands-on, applied training we now offer is not available anywhere else. And even more important, it is tailored specifically for us. The CBE Certification will bring clarity to hiring and promotion decisions and reduces onboarding time for new hires. I’ll say it again—the CBE Certification program is a game changer. The Roadmap examines the anticipated growth phase of the CBE and explores how its distribution method can be expanded and its course content can be renewed and updated to keep us on the cutting edge. We’ve only just scratched the surface with the CBE exam and course offerings. The first CBE exam will be held at this Annual Meeting this afternoon and again tomorrow. I wish good fortune to all those candidates. Thanks to the roadmap, we have launched an “international initiative.” The goal of this initiative is to expand the footprint of NABE abroad and provide an avenue that can better connect U.S. business economists with our foreign colleagues. This initiative includes exploring and offering opportunities to host events abroad, reaching out to economists at embassies in Washington, DC (the second meeting takes places next month) and working to recruit high-quality international speakers for our conferences held here in the United States. NABE has long advocated for high-quality federal statistics, but primarily on an ad-hoc, reactive basis. The Roadmap sets forth the need to take a more proactive approach. The NABE Board has, in fact, already begun this effort, creating a Public Policy Steering Committee. This group is charged with tracking programmatic and budget issues at the various government statistical agencies and recommending when and how NABE should get involved and/or alert members about issues that may affect their data. As economists, we all know how vital reliable data is to our work. This initiative allows NABE to work proactively to protect the variables that we preciously require for our analysis, forecasts, and decision-making. NABE members told our Roadmapping consultants that they are increasingly being asked by the leadership of their organizations to be point persons on big data. NABE now has plans to foster education and training in the knowledge and use of big data analytics. While we economists have many of the tools needed to work with large, unstructured data sets, there are gaps in that knowledge and in the effectiveness of those tools. NABE intends to bring this content to members so that they can meet the demands of their employers. We have asked and answered the questions of who we are, what is our purpose, what value do we provide, and, finally, where to do we go from here. Now we ask the question that neither Alice nor the Cheshire Cat answered: How do we get there? The Roadmap is “constructed” in a way that will inform priority setting and decision-making within the Association in the years ahead and guide NABE to achieve our goal of being “the preeminent business economics association globally.” How will that work? All new projects and initiatives will be evaluated in a simple, rational framework. Quite simply: Does the project fit within the framework of our roadmap? Will it help us to achieve our strategic priorities in an efficient manner? If the answer is yes, great—we move forward. If the answer is no, the project will be tabled; or in some cases the answer may spur a re-evaluation of our Roadmap due to changing trends and developments. Remember, this Roadmap is meant to be a living document, and conditions change. So, Mr. Lewis Carroll—we’ve provided our own response to Alice and the Cheshire Cat: we now know where we are going and—unlike Alice—we can see the roads which will take us there. Of course, there will be disagreement on any initiative which some may view as moving the organization in a new direction. Creating our own specialized CBE certification did not receive unanimous and unqualified support when first proposed by Frank Schott’s at his 1978 NABE Presidential Meeting address [Schott 1979]. To quote Frank: We are looking for ways to increase our professional satisfaction, for ways of raising our advancement potential at least within our present niche, and for elevating the status of the profession. Is it not clear that more intense and possibly more formalized continuing education is one method that may not be surefire but that is certainly in line with other professions and has the immense advantage of being fairly well under our own control? During the 1980s and 1990s, it became evident from our few—but highly successful—forays into education that there was something to this. Some of you recall the advanced training in economics—I have my certificate for the 1987 week-long advanced training in economics from Carnegie Mellon University—to our more recent Economic Measurement Seminars, our econometrics courses, and skills sessions at conferences. It has become clear that the opportunities to improve our members’ skills are highly desired. And there were few other sources for this sort of training. The NABE leadership eventually recognized the “value added” of continuing education and to the merits of recognizing members’ achievements through a formalized certification program. Thus, a bridge was built between an idea advanced in Frank Schott’s presidential address at the 1978 annual meeting and today’s fulfillment of that possibility—an idea realized through the intensive efforts of more than 60 members over the better part of the last decade. I want to digress a bit to talk about how education serves individuals and the public good. Interestingly, the word economist come from the Greek for house manager or house steward—their role was to prepare the living quarters to make sure everything functioned properly. To do that, they needed to be educated about how to run a household. Many centuries later, St. Ignatius of Loyola, founder of the Jesuit religious order, instilled in his followers a zeal for study because by learning more we can better serve others. His view: education is not just for self but to prepare men and women for others! Let me share one additional thought on education, and why the CBE is such a notable endeavor. In 1793, Alexander von Humboldt wrote that “the ultimate task of our existence is to give the fullest possible content to the concept of humanity in our own person through the impact of actions in our own lives.” (John Stuart Mill in his essay, On Liberty, gave credit to some of his thinking to Von Humboldt.) This task “can only be implemented through the links established between ourselves as individuals and the world around us … Self-education can only be continued in the wider context of the development of the world.” In other words, individuals are not only authorized but required to be a force in shaping the world around them through education. Education is a means of realizing individual possibilities, but also a means by which we can contribute to a greater effort in shaping the world. As economists, we need to be engaged in the world. That is who we are and what we do, and for most us, why we are in the field we are in.",
49,4,Business Economics,22 December 2014,https://link.springer.com/article/10.1057/be.2014.33,An Econometric Study of Quits Rate Trends in the United States since the Great Recession: Cyclical Movement or Structural Change?,October 2014,Didier Bruneel,Stephen Juneau,Simiao Wang,Male,Male,Unknown,Male,"Past studies have not provided strong evidence on the driving forces of quits rates due to the constraints imposed by the unavailability of a comprehensive cross-sectional or time-series data set. Over the past 12 years, the Bureau of Labor Statistics (BLS) has released three new data products that measure the dynamics of the U.S. labor market: Job Openings and Labor Turnover Survey (JOLTS), Business Employment Dynamics (BED), and Current Population Survey (CPS) labor force status flows. “These data illustrate the fluid nature of the labor market by highlighting the millions of jobs that appear or disappear and the millions of individuals who become employed, become unemployed, or leave the labor force entirely every month” [Boon and others 2008]. In 2002, the BLS began releasing data from the JOLTS. As this is the first data set in the United States that provides establishment-level information on both worker turnover and vacancies and the rising importance of the quits rate as a labor market indicator, the topic is very likely to attract more scholars and more public attention. Stoikov and Raimon [1968] grouped the determinants of quits rates into two parts, which was consistent with the March-Simon two-component formulation [March and Simon 1958]. The first group includes variables that relate to “the worker’s perception of the desirability of quitting or remaining with his employer,” such as wages, size of establishment, union occupancy rate, and layoff rate. The second group pertains to “the worker’s perception of the ease or difficulty of changing employers,” such as GDP growth, percentage of female workers, and length of service. Stoikov and Raimon formulated a linear regression model to help obtain an overall relationship between quits rate and the determinants. The hypothesis and test results proposed by this paper are very informative. However, it is a dated study of the industry quits rate, as the research was conducted in the 1960s. With our focus on quits rate behavior following the Great Recession, we used the proposed variables in their article only as a reference during our variables selection process. In our paper we analyzed some of the essential determinants proposed by their article such as wages, percentage of female worker, and layoffs rate; but we also added variables on the unemployment level and the Consumer Confidence (current conditions) index, as we aim to see the problem from a different and more comprehensive perspective. Other studies propose that the quits rate can be affected largely by other factors as well. Faberman and Nagypál [2008] used JOLTS data to study the relationship between quits, recruitment, and establishment growth. They concluded that “quits generally decline with establishment growth and even though quits fall and the vacancy and hiring rates rise with employer growth, there is a robust, positive relationship between hires, vacancies, and the incidence of a previous quit.” They also found that at the establishment level, the incidence of a quit substantially increases the probability of a subsequent hire or vacancy in all industries and for all size classes. Therefore, we excluded the hires rate from our list of determinants due to the lack of statistical independence between the quits rate and the hires rate. Other interesting findings on the quits rate include those of Peter Mattila [1974]. He suggests that the quits rate is higher with white-collar jobs than with blue-collar ones due to the different job search methods adopted by the two groups. Rees and Shultz [1970] in studying the search behavior of workers in Chicago concluded that “white-collar workers such as typists, keypunch operators, tabulating machine operators, and accountants do in fact rely heavily upon ‘formal’ methods of search (private employment agencies, training school agencies, newspaper ads) whereas blue-collar workers typically found jobs by ‘informal’ methods (gate applications, referral by a friend).” Therefore, the faster and relatively convenient search method of white-collar workers assists them in smoothing out the process of job hunting, whereas the process for blue-collar workers can be harder and could take a much longer time. It is worth mentioning that there have been past studies of quits rate analysis across industries in a given year, and past researches focused on the dynamics between quits rate and one category of determinants over time. However, there has been little research on multiple determinants of the quits rate and its behavior across both industries and regions and over consecutive years—particularly for years after the Great Recession. In this paper we will try to complement the past studies by focusing on several important determinants of the quits rate and their special effects across time, across industries and across regions.",
49,4,Business Economics,22 December 2014,https://link.springer.com/article/10.1057/be.2014.35,The Risk of Future Labor Shortages in Different Occupations and Industries in the United States,October 2014,Gad Levanon,Ben Cheng,Michael Paterra,Male,Male,Male,Male,"This section draws from Levanon and Cheng [2014]. As the rapid decline in the unemployment rate continues in the United States, the risk of labor shortages is growing. During the 56 months since October 2009, when the unemployment rate peaked at 10 percent, the unemployment rate has declined by about 4 percentage points. This decrease is faster than in the previous two recoveries, despite the relatively slow growth of GDP in this most recent recovery period; and the rapid decline in the unemployment rate is likely to continue in the coming years. Many economists expect the unemployment rate to reach its natural rate (long-term NAIRU) of about 5.5 percent by 2015, as shown in Figure 1.Footnote 2 U.S. Unemployment Rate and the NAIRU Sources: Bureau of Labor Statistics and Congressional Budget Office. After the unemployment rate reaches its natural rate, an additional 15 years of very slow growth in the U.S. labor supply is expected. Current population projections predict that the working age population (persons aged 18 to 64), which has been growing by only 0.3 to 0.4 percent per year in recent years, will continue to slow—from 2020 to 2030 it will grow by just 0.15 to 0.25 percent, as shown in Figure 2. Moreover, the Bureau of Labor Statistics [2013] projects that the U.S. labor force will grow by a mere 0.5 percent per year between 2012 and 2022. These projections assume that there will not be a surge in immigration. Resident Working Age Population: 18–64 Years Old Source: U.S. Census Bureau. To put this into perspective, if employment grows as fast as the labor force at 0.5 percent per year, resulting in an unchanged unemployment rate, jobs would increase by a mere 50,000 per month. In contrast, average employment growth over long periods of time has typically been much higher than that. In order to not constrain the economy, labor force participation would have to increase significantly which, given an increasingly aging population, is unlikely to happen. Labor shortages for the nation as a whole, however, does not mean that each organization, occupation, or industry will have a similar experience. Some are highly at risk for labor shortages, and others may not suffer any shortage at all. There are several main drivers that will impact the development of labor shortages in each occupation, and the variation of these drivers across occupations is large. For example, all else equal, occupations that grow faster are more likely to experience labor shortages; and, according to the Bureau of Labor Statistics, occupations such as personal care aides and occupational therapy assistants are expected to grow by more than 40 percent between 2012 and 2022. On the other hand, occupations such as printing and logging workers are projected to shrink over that same decade. Additionally, occupations vary significantly in the share of older workers who must be replaced in the coming decade. In Figure 3, we show the unemployment rates for the latest 12 months to May 2014 compared with 2005–06, which we assume to be a period of “normal” unemployment. Using the unemployment rate, the most commonly used measure of labor market tightness, we observe the gap between the two periods to reflect how tight the specific labor market has been recently compared with “normal” times. An unemployment rate well below the normal or “natural” rate signifies a tight labor market where there are more jobs than workers available to fill them. Unemployment Rate Gap for Selected Occupations, 2005–06 Compared with June 2013 to May 2014 Source: Bureau of Labor Statistics. Figure 3 is sorted according to the ratio between the unemployment rate in 2005–06 and the current unemployment rate for each occupation. At the bottom, we see the occupation groups in which the current unemployment rate is presently at or below the prerecession rate. Perhaps somewhat surprisingly, some of the occupations that appear to be the tightest presently are occupations that do not require a bachelor’s degree. For example, rail and water and air transportation workers, firefighting and law enforcement workers, supervisors of cleaning and maintenance workers, and plant and system operators are all among the occupations facing the tightest labor markets right now. Large shares of the workforce have been retiring in recent years for some of these occupational groups as the baby boom generation is reaching retirement age. In particular, some occupations with mandatory retirement ages (such as firefighting and law enforcement and some air transportation workers) presently appear to have especially tight labor markets. In these types of occupations in particular, the retirement of the baby boomers has been occurring for more than a decade, leading to current labor market tightness. For example, in an occupation with a mandatory age of retirement of 55, the oldest baby boomers reached the mandatory age in 2001. Thus, for such an occupation, the baby boomers have been already retiring for 13 years. In some ways these occupations have already experienced what the rest of the economy is likely to experience in the coming decades. Perhaps surprisingly, some of the science, technology, engineering, and mathematics (STEM) occupations, as well as lawyers, are experiencing well above normal unemployment rates in the past 12 months.",10
49,4,Business Economics,22 December 2014,https://link.springer.com/article/10.1057/be.2014.27,Is Fiscal Stimulus a Good Idea?,October 2014,Ray C Fair,,,,Unknown,Unknown,Mix,,
49,4,Business Economics,22 December 2014,https://link.springer.com/article/10.1057/be.2014.32,Is the Fed Funds Rate Still Effective?,October 2014,John Silvia,Azhar Iqbal,,Male,,Unknown,Mix,,
49,4,Business Economics,22 December 2014,https://link.springer.com/article/10.1057/be.2014.29,Confronting the Gray Market Problem,October 2014,Peggy E Chaudhry,,,Female,Unknown,Unknown,Female,"The gray market can boost incremental sales of the firm, such as serving markets not in direct competition with its authorized dealers, providing access to products in a market with supply shortages, and allowing firms to assess market segmentation information [Anita and others 2004; Xiao, Palckar, and Liu 2011]. The resale of luxury cars from the United States to China has provided a lucrative goods arbitrage situation for gray marketers like Automotive Consultants of Hollywood to serve the insatiable demand of Chinese consumers for Mercedes, BMW, and Range Rover vehicles [Goldstein 2014]. In 2009, Unilever faced a gray market situation in Jordan, Syria, and Lebanon—an estimated $4 million in unauthorized sales in this region. The firm decided to launch its Clear shampoo with media on the pan-Arab satellite networks to build a brand awareness of this new product in the region—even in markets like Jordan that were not part of the company’s official channel of distribution. The managers at Unilever used the gray market sales of Clear to actually better understand the market, such as where the product was sold and the type of consumer purchasing the shampoo in order to officially launch the product one year later in Jordan [Mahajan 2012, p. 20]. The gray market can stimulate competition and benefit the consumer—the purchaser receives a lower-priced good and this benefits consumer welfare [Nolan-Haley 1983, p. 233]. In 2013, Pompei, a well-known gray marketer in the Hong Kong retail market, started selling its product online through Alibaba’s TMall (formerly Taobao Mall), a Chinese-language website [http://www.tmall.com/]. Pompei Chairman Vincent Wong reported that their virtual sales of gray goods have increased around 20 percent each month at the TMall site [Chiu and Chu 2014]. Another gray marketer, Zhenpin.com, sources from distributors in Europe, such as Prada and Gucci, and resells the product in China at a 10–20 percent discount less than authorized retailers in China [Chiu and Chu 2014].",5
49,4,Business Economics,22 December 2014,https://link.springer.com/article/10.1057/be.2014.28,An Overview of the U.S. Regulated Fund Industry,October 2014,Brian Reid,Emily Gallagher,,Male,Female,Unknown,Mix,,
49,4,Business Economics,22 December 2014,https://link.springer.com/article/10.1057/be.2014.30,"Tyler Cowen, Average Is Over: Powering America beyond the Age of the Great Stagnation",October 2014,John C Goodman,,,Male,Unknown,Unknown,Male,,
49,4,Business Economics,22 December 2014,https://link.springer.com/article/10.1057/be.2014.31,"Thomas Piketty, Capital in the Twenty-First Century",October 2014,Devon M Herrick,,,,Unknown,Unknown,Mix,,
50,1,Business Economics,18 March 2015,https://link.springer.com/article/10.1057/be.2015.6,From the Editor,January 2015,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
50,1,Business Economics,18 March 2015,https://link.springer.com/article/10.1057/be.2014.38,An Interview With Ben S. Bernanke,January 2015,Ben S Bernanke,Kai Ryssdal,,Male,Male,Unknown,Male,,6
50,1,Business Economics,18 March 2015,https://link.springer.com/article/10.1057/be.2015.1,An Ordered Probit Approach to Predicting the Probability of Inflation/Deflation,January 2015,John Silvia,Azhar Iqbal,,Male,,Unknown,Mix,,
50,1,Business Economics,18 March 2015,https://link.springer.com/article/10.1057/be.2014.37,How to Curb Short-Termism in Corporate America,January 2015,Robert C Pozen,,,Male,Unknown,Unknown,Male,"Many critics of short-termism point to the rising volume of equity tradingFootnote 1 and the increase in stock turnoverFootnote 2 in U.S. equity markets. These trends are indeed significant over the last few decades. However, during the last few years, the trading volumeFootnote 3 and turnover rateFootnote 4 for U.S. stocks has decreased substantially. If these two factors were key drivers of short-termism, we should have seen a marked shift from short-term to long-term decision making by corporate executives. Yet, there is no evidence of such a shift during the last few years. More fundamentally, the trading and turnover data reflect the AVERAGE behavior of investors in U.S. stock markets, and these averages are misleading. U.S. institutional investors, which dominate U.S. stock markets, should be divided into three distinct categories based on the continuity of share ownership within a portfolio and the size of stakes in portfolio companies. According to Wharton Professor Brian Bushee [2004, pp. 30–31], 31 percent were transient investors, 8 percent were dedicated investors and 61 percent were quasi-indexers. Most of the increase in short-term trading derives from the rise of transient investors, including high frequency traders whose holding periods may be measured in minutes or seconds. With well-diversified portfolios, transient investors pursue short-term profits through high turnover and heavy use of momentum trading. But the behavior of transient investors should not influence the investment decisions of corporate executives. Transient investors trade on the basis of technical factors or momentary price differences between related markets; they do not care a whit about company expenditures or business plans. The majority of public company shares are owned by dedicated investors and quasi-indexers. Dedicated investors have substantial investments in a relatively small number of portfolio companies; they hold a high percentage of their portfolio shares for two years or more. Quasi-indexers, as their name implies, hold well-diversified portfolios of publicly traded securities; they also have a high degree of ownership continuity since they tend to hug the broad-based indices like the S&P 500. Thus, the majority of public company shares are owned by relatively stable institutional investors, with much lower trading rates and longer holding periods than transient investors. Dedicated investors and quasi-indexers are less interested in short-term trading profits and more focused on monitoring long-term performance of public companies. These institutional investors will look closely at the quality of long-term plans of public companies and their performance in carrying out such plans.",2
50,1,Business Economics,18 March 2015,https://link.springer.com/article/10.1057/be.2015.5,The Impact of a Rising Dollar on the U.S. Economy,January 2015,Joel L Prakken,Chris P Varvares,,Male,,Unknown,Mix,,
50,1,Business Economics,18 March 2015,https://link.springer.com/article/10.1057/be.2014.39,Banking and the Real Economy,January 2015,Ed Clark,,,Male,Unknown,Unknown,Male,"First let me start with my big learning. Banks matter. It may seem a bit strange for someone trained as an economist who became a banker to say that was his learning. In truth, I did not take banking’s role that seriously—I do not intend to exaggerate my importance. But we clearly learned that banks do matter. Economies with strong banking systems survived and thrived. Others with weaker systems struggled. Places like Europe where banks played even a bigger role in the economy than in North America suffered more—and still have trouble recovering. Have we solved the problem? If the problem is ensuring that banks are well capitalized and have stronger liquidity, I think we can say that, at least in North America, we are in good shape. But have we restored trust in banks, have we reached equilibrium in the regulatory system? The answer is clearly no.",
50,1,Business Economics,18 March 2015,https://link.springer.com/article/10.1057/be.2015.3,Estimating the Impact of NABE Member Characteristics on Compensation,January 2015,Christopher Swann,Anessa Custovic,,Male,Unknown,Unknown,Male,"NABE offers all members an opportunity to participate in the bi-annual survey through electronic and paper response. The 2014 survey, while largely similar to previous questionnaires, had a number of changes. The survey was expanded to reflect the growing NABE membership and a wider range of professional and industry affiliations. For example, members were able to identify with several additional occupational descriptions, including risk management, planning and strategy, and transfer pricing. Two additional reporting relationships were included: managing/senior economist and Chief Financial Officer. Members were able to identify themselves as having less than a Bachelor’s degree, an M.A. or MBA and were also able to indicate professional certifications, such as a Chartered Financial Analyst certification. These enhancements to the survey give a more detailed profile of the NABE membership. There were 337 NABE members who responded to the survey. The results of the survey are best viewed in the Salary Characteristics 2014 document itself [NABE Foundation 2014]. However, there are some observations about the sample that are pertinent to the model estimation. The overall sample for 2014 was lower than in previous model years. In previous years, the number of respondents generally exceeded 600. In previous models, a number of cases were excluded for qualitative reasons, but the resulting degrees of freedom were in the 590–605 range. The number of respondents in 2014, 337 in total, represents a response rate of 20.6 percent, which is a drop from previous surveys [NABE Foundation 2014]. The sample revealed several interesting items about compensation among NABE members. The median base salary was roughly $120,000, about 10 percent greater than in the 2006 survey. About 70 percent of members reported additional compensation that averaged roughly 20 percent of base salary. Roughly, one quarter (150) of responding members reported earning additional income from secondary employment, worth $10,000 on average. The sample indicated that, as expected, on average, higher levels of formal education results in higher compensation. Median salaries ranged from about $94,000 for those holding bachelor’s degrees in economics to $145,000 for those holding Ph.D.’s. Those with masters degrees averaged (median level) roughly $103,000, while those who indicated having completed all but the dissertation (ABD) for the Ph.D. earned roughly $120,000 on average (median). The median base salary for entry-level members was about $60,000, an increase of about 12 percent over the 2006 survey. About 10 percent of the sample reflected members with less than five-year’s experience. Experience does pay, as expected, and as is shown in the results of our regression equation (below). Compensation levels in the sample clearly rise with experience before leveling out at about 30-years’ experience. The sample also suggests that compensation levels rise with supervisory experience. Indeed, administrative positions averaged (median) nearly $160,000, among the highest salaries reported. Interestingly, the size of the organization did not appear to matter to compensation earned except for very large firms. In addition, while there were a number of industry affiliations represented in the sample, only a few were statistically significant and included in the model. Compensation variation by gender is one of the more interesting aspects of the survey, one that we explored econometrically and that warrants consideration in subsequent surveys and analysis. There were 262 male respondents and 63 female respondents in the final sample, and within each group several very high salaries skewed the compensation distribution. (43 respondents chose not to indicate gender.) The median compensation for females in the sample was $100,000, $24,000 lower than the median compensation reported for males. The mean compensation level for males was just above $148,000 ,with a standard deviation of $153,000. For females, the mean compensation was about $118,000, with a standard deviation of $77,000. In both cases, the variability reflects extreme values at high compensation levels, and particularly so with males, explaining the high standard deviation relative to mean (coefficient of variation). However, when removing the effect of extreme values in both groups, the compensation levels both tend to cluster around their median values and with much less variation. Finally, it appears that economists employed in financial services industries and occupations tend to realize higher compensation levels. The highest median base salary and the highest additional gross compensation levels were reported by members working in the securities and investments industry. Similarly, we estimated a significant compensation effect associated with employment in New York City over and above the industry effects. This likely reflects the fact that many members in the sample are employed in financial services firms that are located in New York City.",1
50,1,Business Economics,18 March 2015,https://link.springer.com/article/10.1057/be.2015.4,"House of Debt: How They (and You) Caused the Great Recession, and How We can Prevent It from Happening Again",January 2015,Derk A Wilcox,,,Male,Unknown,Unknown,Male,,
50,1,Business Economics,18 March 2015,https://link.springer.com/article/10.1057/be.2015.2,Bootleggers and Baptists: How Economic Forces and Moral Persuasion Interact to Shape Regulatory Politics,January 2015,Gerald L Musgrave,,,Male,Unknown,Unknown,Male,,1
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.18,From the Editor,April 2015,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"As noted in the January issue, Business Economics is celebrating its 50th anniversary this year and will have a few special features in upcoming issues to commemorate the occasion. A new development is that Palgrave Macmillan, who publishes Business Economics on NABE’s behalf, will publish a special commemorative book on the best articles of Business Economics’ first 50 years. The Editorial Board has been engaged in the daunting task of making these selections. It is an exciting project, and I look forward to the finished product.",3
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.12,Thoughts about Monetary and Fiscal Policy in a Postinflation World,April 2015,Alice M Rivlin,,,Female,Unknown,Unknown,Female,"The second half of the twentieth century was a good time for the advanced economies. Economic policy, both monetary and fiscal, got a lot of the credit, probably more than it deserved. The consensus goal of economic policy was to keep the good times rolling by smoothing out the ups and downs of the business cycle. At least at the beginning of the period fiscal policy was thought to be useful in stimulating aggregate demand and job creation in recovery; central bankers were thought to be useful in the boom part of the cycle when the problem was how to stop inflation before it became unstoppable. Central banks warmed to the task and got increasingly explicit about their mandates to moderate inflation. They set numerical targets for inflation and emphasized their commitment to achieving them, in hopes that credible intentions could make painful policy measures less necessary. Even the Federal Reserve, with its dual responsibility for price stability and maximum employment, concentrated on moderating inflation on the grounds that inflation was the biggest threat to sustainable growth and employment. The challenge was to find the right moment for removing the proverbial punch bowl as the boom phase of the cycle gathered steam. Timing was everything. Tightening too soon could slow growth, throw people out of work, and undermine prosperity; waiting too long was dangerous because inflation was so hard to stop once it got started. Inflation was perceived as a lurking predator that had to be stopped in its tracks before it took off in a self-perpetuating upward spiral. Fear of inflation had institutionalized mechanisms to protect the public from its ravages. Escalator clauses in multiyear union contracts and indexing of public pensions served this purpose, but they perpetuated inflation once it accelerated. Even more worrisome were inflationary expectations that became self-fulfilling prophesies. Once inflation got out of control, the only remedy was hard monetary tightening that risked sending the economy into a tailspin. Examples of over-tightening that precipitated recession included the actions of the Volcker Fed. Back in the twentieth century, adults in the United States and Europe had personal experience of inflation in their lifetimes and had heard about the chaos of hyperinflation when grocery-shopping required wheel-barrels full of cash and savings and pensions became worthless. Germans relived the runaway inflation of the 1920s for decades as tales of that desperate period imbedded fear of inflation in their culture. Developing countries provided proof that hyperinflation was not just a scary tale from a bygone era. But by the turn of the century economists were talking about “The Great Moderation” [Stock and Watson 2002]. Recessions were shorter and shallower; inflation was more contained. Central banks got—or took—much of the credit for the Great Moderation, although the stabilizing effect of welfare spending and progressive taxation may well have played a role. Markets hung on the words of central bankers; the financial press perceived them as endowed with magical powers. Politicians understood that the unpopular action of raising interest rates at the critical moment was necessary to moderate inflation. They didn’t want to take unpopular actions themselves, so they respected central bank independence. In the United States there were still vocal critics of the Federal Reserve, who thought the central bank focused too much on inflation. They carried the populist banner of opposition to money and power, but they did not seriously threaten Fed independence. But even as the prestige of inflation-fighting central banks soared, other threats appeared. After 1989, Japan was battling deflation, and other countries realized that deflation could happen to them [Bernanke 1999]. Monetary authorities began to talk about minimum, as well as maximum, targets for inflation. Zero inflation seemed risky in a down-turn, because it could so easily lead to deflation, and it was undesirable in a recovery because the downward stickiness of money wages made it hard to move labor into more productive uses [Akerlof, Dickens, and Perry 2000]. The prospect that inflation could start suddenly and get out of control quickly also became increasingly remote. The self-perpetuating mechanisms, such as escalator clauses in labor contracts, were disappearing. Internet communication, outsourcing, and increasingly competitive wage and price markets were reducing upward pressure on prices. Above all, the longer inflation remained quiescent, the less people expected it to flare up. These days, when I give my students a monetary policy exercise, I can’t to get them to regard inflation as a serious threat.",
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.11,The Decline in the Natural Rate of Interest,April 2015,John C Williams,,,Male,Unknown,Unknown,Male,"By the natural rate of interest I mean the real federal funds rate consistent with the economy operating at its full potential once transitory shocks to aggregate supply or demand have abated [Williams 2003]. This definition of the natural rate takes a “longer-run” perspective, in that it refers to the level expected to prevail, say, 5–10 years in the future, after the economy has fully recovered from the recent recession and is expanding at its trend rate. Importantly, there is no reason to expect the natural rate to be constant: highly persistent shifts in aggregate demand and supply will affect the natural rate. As discussed by Laubach and Williams [2003], Congressional Budget Office [2014], International Monetary Fund [2014], and Hamilton and others [2015], there are numerous potential influences on the natural rate of interest, including, but not limited to, fiscal policy, technological change, and demographics. Therefore, in trying to gauge the natural rate of interest, one needs to apply methods that allow for time variation in the natural rate, but at the same are not overly sensitive to cyclical influences on interest rates. In the following, I focus on the model developed by Thomas Laubach and myself [2003] that is designed for exactly that purpose. In the Laubach–Williams (LW) model, the natural rate of interest is assumed to change over time due to various unobservable influences. In principle, one would like to use data on the factors that influence the natural rate in order to quantify the effects. In practice, these are difficult to measure using available data and methods. In light of these data limitations, in the empirical implementation of the LW model, a parsimonious specification for the determinants of the natural rate is used. Specifically, the natural rate is assumed to depend on the (estimated) contemporaneous trend growth rate of potential output and a time-varying unobserved component that captures the effects of other unspecified influences on the natural rate. The model is estimated using the Kalman filter [Laubach and Williams 2003]. In words, the Kalman filter works on the principle that one should partially adjust one’s estimate of the natural rate of interest based on the distance between the model’s prediction of GDP and actual GDP. If, for example, the model prediction for GDP proves to be too optimistic, the estimate of the natural rate is lowered accordingly. Estimation of the trend growth rate of potential output follows a similar logic. In this way, this procedure is designed to allow for the possibility of a changing natural rate, but guard against overreacting to short-term movements in real interest rates. The LW estimates of the natural rate of interest display two periods of significant declines: a moderate secular decline over the two decades preceding the Great Recession, and a second, more substantial decline during the Great Recession. Figure 1 shows the LW estimates of the natural rate of interest from 1980 to 2014. These are the so-called one-sided estimates, where the model parameters are estimated using the entire data sample through 2014, but the estimates of the natural rate are based on the past history of the data on GDP, interest rates, and other model variables only through the date indicated. As can be seen in the figure, the estimate of the natural rate was about 3.5 percent for 1990. It fluctuates over time but exhibits a downward trend, reaching about 2 percent in 2007, on the cusp of the Great Recession. Then, in the recession years of 2008 and 2009, the estimated natural rate plummets by nearly 2 percentage points. It has remained in the vicinity of zero for the subsequent 5 years. This represents an unprecedented decline and historical low level of the natural rate over the past half century for which we have estimates. Laubach–Williams Estimates of the Natural Rate of Interest Note: Gray bands denote NBER recessions. This pattern of declining natural rates of interest is also seen in estimates implied by economic forecasts and yields on Treasury inflation-protected securities (TIPS), reported in Table 1. The first row of the table reports natural rate estimates implied by the long-run forecasts from the Blue Chip survey of forecasters. The second row reports real interest rates 5–10 years in the future based on TIPS yields. Note that TIPS did not exist in 1990. The third row reports the LW estimates. Note that the LW model uses the personal consumption expenditure price index for the measure of inflation adjustment, while the Blue Chip and TIPS-based measures use the Consumer Price Index, so these are not directly comparable in the levels. However, this does not materially affect comparisons of the different measures’ changes over time, shown in the final two columns of the table. Interestingly, the pattern of decline in natural rates is consistent across the three measures, although the movements in the LW estimates are much larger than the other two measures. What accounts for these periods of declining natural rates? While it is not possible to ascertain with any certainty the reasoning behind the changes in the Blue Chip and TIPS-based measures, the LW model estimates provide a partial accounting for these developments. In the LW model, a falling trend rate of growth of potential output, coupled with other factors, contribute to the decline in the estimated natural rate of interest in both periods. The final two rows of Table 1 report the contributions from changes in trend growth and the catch-all “other factors” to the decline in the estimated natural rate for the two periods, 1990–2007 and 2007–14. Overall, the decline in trend growth accounts for nearly one-half of the decline in natural rate growth and a little over one-half is accounted for by other unspecified factors. For reference, Figure 2 shows the LW model estimates of the trend growth rate of potential output over 1980–2014. The estimated trend potential output growth was about 3.5 percent in 1990, declining to 3 percent in 2007, then falling sharply to about 2 percent. Note that the model does not attribute these movements in trend potential output growth to specific sources, instead these are the values generated by the model estimation procedure that maximizes the fit to the data. Laubach–Williams Estimates of the Trend Growth Rate of Potential Output Note: Gray bands denote NBER recessions. One criticism of the Laubach–Williams natural rate estimates are that they can be subject to revision as additional and revised data become available [Clark and Kozicki 2005]. Although this can be an issue with any model estimates, it is striking that in the recent episode, the LW estimates of the natural rate have been remarkably consistent. Figure 3 compares the ex post (one-sided) LW estimates of the natural rate to the corresponding real-time estimates over 2007–14. The real-time estimates represent the estimate of the natural rate based solely on data available a few months after the end of the quarter for which the estimate is reported. By construction, the real-time and ex post estimates are identical for the final data point in our sample (fourth quarter of 2014). Real-time Laubach–Williams Estimates of the Natural Rate of Interest Note: Gray bands denote NBER recessions. The real-time LW estimates fell sharply during the recession and remain low through the end of the sample. Thus, these estimates provided a strong early signal of a lower natural rate. In addition, the estimates have sent a consistent signal of a low natural rate over time. Indeed, the real-time and ex post estimates are close to the same from 2011 to 2014. In this episode at least, revised estimates do not tell a different story than real-time estimates. A second criticism of the LW model is that the assumed relationship between the trend growth rate and the natural rate of interest, although justified by economic theory, does not have strong empirical support [Hamilton and others 2015]. In this regard, it is worth noting that the decline in factors besides trend growth explains more than half of the decline in the estimated natural rate of interest since 1990, as shown in Table 1. Therefore, even if one discounts the role of trend growth in affecting the natural rate, the implied decline in the natural rate over the past few decades, and especially in the aftermath of the Great Recession, has been remarkably large.",21
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.14,Progress and Challenges for International Financial Reform,April 2015,Nathan Sheets,Michael Pisa,,Male,Male,Unknown,Male,"While the U.S. economy continues to gain steam, with real GDP growing above trend, job creation picking up sharply, and unemployment declining, global growth remains slow and uneven. For a strong and sustained global recovery, all major economies should employ a comprehensive strategy that marshals all levers of economic policy—monetary, fiscal, and structural—to address shortfalls in domestic demand, create jobs, and raise incomes. Failure to take the necessary actions leaves the door open to further disinflationary risks for the global economy. The world fundamentally needs more demand. These general themes are echoed in the recent performance of the major foreign economies. For example, in Europe the recovery remains tentative, demand is weak, and in some countries growth is still overly dependent on exports. Accordingly, a strong case can be made that those countries with large external surpluses and fiscal space should pursue bolder policies to boost domestic demand, including investment in infrastructure. We encourage European policymakers to work toward a comprehensive approach to promote economic growth, using the full range of policy tools. We continue to monitor the situation in Greece closely, encouraging all sides to work pragmatically together to find a path forward. Greece has made enormous progress in addressing its fiscal imbalances, but further structural reforms are necessary to allow the supply-side of its economy to compete successfully in Europe and internationally. It is important that Greece work with its European partners and the international community to sustain its progress toward recovery and reform. In Japan, the weakness of economic growth through the last three quarters of 2014 underscores the need for a reinvigorated application of all three arrows of Prime Minister Abe’s economic program, particularly structural reforms with an eye toward catalyzing domestic demand over the medium term. Resolute action by the authorities will improve Japan’s economic prospects and help foster the escape from deflation, which in turn should contribute positively to an improved global outlook. China faces difficult challenges, amid moderating growth, as it shifts its economy away from an unusually high investment share of GDP toward greater reliance on consumption. Chinese policymakers have recognized the need to transition to a more market-oriented economy, and they have tools to manage this transition. Also critical to China’s success is accelerating its move to a market-determined exchange rate; progress has been made in recent years, but such efforts must be broadened and become more entrenched.",
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.15,Central Banks: Confronting the Hard Truths Discovered and Tough Choices Ahead,April 2015,Philipp Hildebrand,,,Male,Unknown,Unknown,Male,"Of course, the need for central banks to be concerned by more than inflation alone has by now become a well-accepted truth, but I vividly remember the then Economic Adviser of the BIS being shot at with heavy intellectual artillery when he effectively first put this notion forward at the Jackson Hole Federal Reserve Symposium early on in the Great Moderation period [Borio and White 2004]. Back in the days when inflation, rather than the lack of it, was a problem, the central banking community developed the view that to do its job a central bank had to focus on inflation; and the narrower the focus, the more effective it would be. But in fact, it is not that simple. During the Great Moderation, if you looked only at inflation you would have thought policymakers were doing a fine job. And indeed GDP was growing nicely—until it collapsed, along with inflation. Figure 1 shows that what happened was that this great stability bred low volatility, and this led to excessive risk taking and a sharp build-up of leverage and debt. In the words of Mark Carney, “risk was at its highest when risk premia were at their lowest”. Inflation, Output and Volatility in the United States Source: St Louis FED (http://research.stlouisfed.org/fred2); U.S. Bureau of Labour Statistics (http://www.bls.gov/data/); Thomson Reuters Datastream; Bureau of Economic Analysis (http://www.bea.gov/national/index.htm#gdp). So, we have had to learn that, actually, central banks had better keep a close eye on the stability of the system as well as inflation. Build up of leverage and debt matters enormously. That was the prime lesson of the crisis. Today, the tools are still not perfect but the lesson has been learned; and all the key central banks except for the Bank of Japan (BOJ) now have a macroprudential arm/mandate attached to them, distinct from monetary policy. There is, of course, a difficult choice we now face, though we have not confronted it yet: what if the macroprudential tools are not enough and a conflict arises between financial stability (requiring an interest rate rise) and price stability (requiring accommodating macroeconomic policy, say because of deflation risk)? What might be the answer here?
 First, I believe that one should not be overly pessimistic about the efficacy of macroprudential tools. Of course, many are untested; and there are difficult calibration issues. But let us remember that it was not so long ago that people thought price stability was unachievable. Second, one needs to think courageously about better cooperation between monetary policy and government policies, notably fiscal. This is not at all to say that central bank independence should be revisited, quite the opposite. But recognizing that these different policies can be supportive of each other can also help alleviate the trade-offs that each of them faces. And while independence is absolutely of the essence when fighting inflation, greater emphasis on cooperation may be more relevant when fighting disinflation (the reduction or elimination of inflation), let alone deflation (an actual fall in the price level). Having said that, how to operationalize this is not obvious and raises slippery slope issues.",
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.13,Transfer Pricing for the Rest of Us,April 2015,Constance L Hunter,Thomas Herr,Marcus Heyland,Female,Male,Male,Mix,,
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.16,Time-Series Momentum Trading Strategies in the Global Stock Market,April 2015,Gagari Chakrabarti,,,Unknown,Unknown,Unknown,Unknown,,
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.10,Opportunities and Challenges Facing the Bureau of Labor Statistics,April 2015,Erica L Groshen,,,Female,Unknown,Unknown,Female,"At the BLS, we have recently made some notable recent improvements and innovations. For example, one recent BLS innovation came in response to Hurricane Sandy. In the fall of 2012, we first matched the Quarterly Census of Employment and Wages (QCEW) employer file against hurricane flood zone maps. In June 2014 we released new maps and tables showing QCEW employment, wages, and establishment counts in all hurricane flood zones on the Gulf and Atlantic coasts. These custom maps and tables are now available online for the public to examine before or after a hurricane. They are used within the BLS for research into the data collection and economic effects of a storm. Externally, labor market information offices and others use the maps for emergency preparedness and response. Firms may use such maps to guide location decisions and assess risk. An example of such a map for Queens County in New York is shown as Figure 1. In this map, Zone 1 is the area that would be flooded by a Category 1 hurricane, Zone 2 by a Category 2, and Zone 3 by a Category 3. Employment in Hurricane Storm Surge Flood Zones, Queens County, NY In February 2014, the Producer Price Index (PPI) Program more than doubled its coverage of the U.S. economy. By transitioning from a Stage-of-Processing aggregation system to a Final Demand-Intermediate Demand aggregation system, we now cover more than 75 percent of domestic production. We got there because this new system enables the PPI to include producer prices for services, government purchases, and exports. The BLS now publishes new data on employment in the nonprofit sector, which has been a missing part of the labor market puzzle. These new data cover the portion of the IRS nonprofit category under the 501(c)(3) designation. This project is based on merging existing BLS data from the QCEW program with publicly available data from the Internal Revenue Service. Thus, there is no new data collection or respondent burden involved in this new data product. Of course, new products and improvements to our data and methodologies are notable, but they mean little if users cannot get their hands on the data. So, in an effort to create transparency and encourage technological innovation, we developed an application programming interface, or API, to help users more easily access and customize our data products. The API allows developers to create applications, tools, and visualizations that can provide novel, powerful, and shareable content and analysis of BLS economic data to a wider audience. It is our hope that talented developers and programmers will use the API to create original, inventive applications with published BLS data. We have provided sample code in the more popular languages currently available to developers and programmers, and an updated API 2.0 was released in October 2014.",
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.9,International Economics in the Automotive Industry,April 2015,David P Teolis,,,Male,Unknown,Unknown,Male,"As part of the Global Public Policy staff, the economics team is responsible for assessing the impact of worldwide economic developments on the company. In addition to our primary responsibility for forecasting economic growth and global vehicle sales, the economics team is responsible for providing advice on competitive and economic policy issues. The organization of the economics staff has evolved since I joined the company, largely reflecting global economic shifts—naturally toward China. Today there are five regions covered by three senior-level economists. The regional breakdowns include North America, South America, Europe, Greater China, and International, which includes Asia-Pacific, the Middle East, and Africa. The Detroit team consists of the Chief Economist, two senior-level economists—one covering North America and Greater China, and myself, covering South America and the International markets—an industry analyst, and an energy economist. The European economist is unique in that it is the only economics position located outside of Detroit—situated at the Adam Opel Headquarters in Rüsselsheim, Germany. While our team could be considered lean in relation to our workload, we do benefit from the support of third-party economic models and outside consultants. This outside support allows us to respond quickly to our customers’ queries without which we would find ourselves deep underwater. The economics staff has direct lines of communication with nearly all major functional areas of the company, but most notably we field queries from Treasury, Sales and Marketing, Risk Management, Public Policy, Corporate Finance, Communications, Global Manufacturing Strategy and Planning, and Product Planning. In addition to the broad range of support we provide across various functional staffs, we interact closely with senior leadership, especially the company’s Chief Financial Officer. While ad hoc requests dominate most of our daily workload, there is a defined set of assignments that are required on a regular basis. First, our role is to prepare a monthly assessment of current economic conditions and, if necessary, provide an update of our real GDP growth forecasts—highlighting major developments or changes since the previous month. This assessment is prepared in a presentation format and is reviewed during a global conference call with regional sales teams. We solicit feedback from the sales teams that helps us refine the economic message that will be disseminated across the entire company. Second, we are also responsible for providing monthly updates of our foreign exchange-rate forecasts for 50 currencies. The currency outlook is then used throughout the organization for the purpose of updating monthly financial forecasts, revising business case analyses by product program teams, or assessing hedging opportunities. Third, we support the local sales organizations by advising on industry matters, including the short-term impact from swings in energy prices, price elasticity analysis to estimate the industry sales impact during periods of extraordinary competitive price pressure, and to predict the effect on vehicle sales resulting from policy changes such as taxes, interest rates, or government-sponsored scrappage programs. This is a critical supporting role we play in terms of managing production and inventory levels as well as meeting vehicle demand expectations. In addition, on an annual basis, we are responsible for updating the 10-year business plan, which includes a detailed economic, demographic, and policy analysis to support our long-term industry sales outlook.",
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.7,"The Home Improvement, Maintenance, and Repair Industry",April 2015,Emil B Berendt,,,Male,Unknown,Unknown,Male,"Although maintenance and repairs use nearly the same inputs as home improvement activities, they have different effects on the housing stock. Understanding the distinction between them is important for several reasons. First, improvement is an investment that impacts the value of housing capital and therefore can affect the value of homeowners’ equity. Maintenance, on the other hand, is an expense that preserves capital and does not lead to a wealth effect. Second, the two are difficult to distinguish empirically, which creates challenges in measurement for economists in the industry. Third, forecasters need to take into account the fact that these categories of spending each have their own unique drivers. Home improvement (sometimes referred to as remodeling or renovation) is an investment activity made to the existing housing stock that adds to value or useful life.Footnote 1 The BEA defines home improvement as follows: Improvements to residential structures … consist of additions, alterations, and major replacements to structures subsequent to their completion. They include construction of additional housing units in existing residential structures, finishing of basements and attics, remodeling of kitchens and bathrooms, and the addition of swimming pools and garages. They include major replacements—such as new roofs, water heaters, furnaces, and central air conditioners—that prolong the expected life of the structure or add to its value; routine maintenance and repair work is not included. [Bureau of Economic Analysis 2014] The Census Bureau estimates that home improvement spending on private, owner-occupied dwellings was $133.1 billion in 2013 [Census Bureau 2014c]. According to the American Housing Survey (AHS) 50 percent of homeowners made improvements in 2011–12. Of these, 36 percent were do-it-yourself projects, and 64 percent were done by professionals [Census Bureau 2013]. Owners choose a wide range of different projects reflecting a diversity of trades and materials. Table 1 ranks projects by major category from the AHS survey. Historically, bathroom and kitchen remodeling have been the two most frequent types of home improvement. Housing capital depreciates; that is, it undergoes obsolescence and normal wear-and-tear. Maintenance is preventative care that conserves existing capital at its present functioning condition whereas repair restores the capital to a previous operating condition. The Census Bureau offers a succinct description of these activities: Expenditures represent current costs for incidental maintenance and repairs that keep a property in ordinary working condition, rather than additional investment in the property. Maintenance includes expenses for painting, papering, floor sanding, furnace cleaning or adjustment, etc. Repairs include many kinds of expenditures for plumbing, heating, electrical work, and other kinds of activity involved in the upkeep of residential properties. Repairs also include replacements of parts while replacement of entire units are classified as alterations to housing structures. [Census Bureau 2006] The Census Bureau also provides illustrations to help distinguish among spending categories: For example, roof repairs (including replacement of shingles, gutters, etc.) are classified under maintenance and repairs, but a complete reroofing is classified as an alteration to housing structures. Plumbing repairs may include extensive replacement of water pipes, but if the entire piping system is removed and a new one put in, the expenditures for the work are classified as an alteration to housing structures. Maintenance and repairs do not include expenses for trash and snow removal, lawn maintenance and landscaping, or cleaning and janitorial services. [Census Bureau 2006] The NIPA treats maintenance and repairs as an expense rather than an investment. These outlays are reported as a component in computing net value added in the housing sector. The BEA also includes property insurance, brokers’ commissions on land, closing costs, and property management fees as housing expenses in its NIPA value-added figures. Although the markets for new construction, home improvement, maintenance, and repair have their unique characteristics, the boundaries between them can be porous. This is especially the case in the upstream links of the supply chain. To take an example, wallboard can be installed in a new home (new construction), used to finish a basement or attic (improvement), or applied to a small water-damaged section of a wall (repair). There is substitutability of labor even at the installation stage. For example, during the housing downturn new home builders serviced the home improvement market to supplement their revenue. Furthermore, if the cost of repair is high enough it can result in a renovation [Joint Center 2013]. Another commonality is that similar regulations and costs of compliance apply to contractors across the markets. It is common for local governments to require permits, even for major renovations, and an inspection to ensure compliance with building codes. In addition, many local and state governments have requirements for licensing and registration. Contractors must also meet federal standards such as those set by the Occupational Health and Safety Administration and the Environmental Protection Agency.",
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.8,Open Secret: The Global Banking Conspiracy That Swindled Investors Out of Billions,April 2015,William C Dunkelberg,,,Male,Unknown,Unknown,Male,,1
50,2,Business Economics,27 July 2015,https://link.springer.com/article/10.1057/be.2015.17,Trillion Dollar Economists: How Economists and Their Ideas Have Transformed Business,April 2015,Douglas J Lamdin,,,Male,Unknown,Unknown,Male,,
50,3,Business Economics,29 September 2015,https://link.springer.com/article/10.1057/be.2015.26,From the Editor,July 2015,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"Mark Dadd, a past president of NABE in 1996–97 and a long-time member of the Business Economics Editorial Board, has died after a long illness. Those who knew Mark will remember his warmth and good humor as well as his thoughtfulness and capability as an economist. He was a willing and productive member of the Editorial Board until his illness prevented him from continuing. Those who knew him are saddened by his loss.",
50,3,Business Economics,29 September 2015,https://link.springer.com/article/10.1057/be.2015.19,Central Banking After the Crisis: No Return to Past Certainties,July 2015,Adair Turner,,,,Unknown,Unknown,Mix,,
50,3,Business Economics,29 September 2015,https://link.springer.com/article/10.1057/be.2015.21,The Changing Nature of the Long-Run Federal Budget Problem,July 2015,Rudolph G Penner,,,Male,Unknown,Unknown,Male,"No one is very good at forecasting interest rates. But the CBO has to decide on some sort of interest rate path in order to fulfill its obligation to project budget aggregates for 10 years. In compiling their January 2015 baseline, the CBO assumed that the 3-month Treasury bill rate will gradually rise from essentially 0 in 2014 to 3.5 percent in calendar 2028 and remain at 3.4 percent from 2019 through 2025. The 10-year Treasury note rate is projected to rise from 2.5 percent in 2014 to 4.6 percent by 2020 and then be constant through 2025. As the CBO explains, the assumed increase is relatively modest. The implied 10-year real interest rate of 2.2 percent from 2020 to 2025 is 75 basis points below the average real rate between 1990 and 2007, a period of relatively stable inflationary expectations. Nevertheless, interest becomes the government’s most rapidly growing expenditure, rising at 12.4 percent per year. If interest rates turn out to be 1 percentage point higher, the annual rate of increase would rise to 15.4 percent per year. But as this was written in April of 2015, interest rates seem to be running somewhat below CBO expectations, so it may be some time before the interest bill really begins to soar. Would a rapidly rising interest bill make voters more concerned about the deficit? It is difficult to explain how deficits harm long-run living standards by drawing down national savings and wealth, but interest is something that most people have to deal with in their ordinary lives. They know that it is not a good thing if their interest bill is absorbing a higher and higher portion of their income, and they know that it is not a good thing if the government’s interest bill is equivalent to a higher and higher proportion of tax revenues. Consequently, rising rates might very well raise public concern about the deficit. Because interest rates are so hard to forecast, a considerable element of uncertainty has been added to the nation’s long-run fiscal outlook. If interest rates remain near today’s unusually low levels for several years and all else remains equal, it is difficult to argue that the country faces a very serious long-run budget problem, because the debt-GDP ratio would be put on a declining path. If interest rates should soar above the CBO’s assumed path for a significant period in the next 10 years, the fiscal outlook would be downright frightening. Debt would be significantly higher, and GDP would very likely be lower. Even the modest interest rate increase projected by the CBO is cause for grave concern. The fiscal risks created by interest rates can only be reduced by reducing the debt-GDP ratio. Options for doing that will be considered later in this article.",
50,3,Business Economics,29 September 2015,https://link.springer.com/article/10.1057/be.2015.24,Gauging the Financial Capability of Americans,July 2015,Mark A Johnson,Douglas J Lamdin,,Male,Male,Unknown,Male,"It is useful to place the question of financial capability and its importance in perspective. Campbell [2006] points out that the academic study of financial economics is organized around asset pricing and corporate finance. What he labels “household finance” lacks definition and status. The financial crisis and recession that followed has served as a catalyst to make the financial capability of individuals and households (household finance) a more prominent area of study. An example of this recent interest is the survey by Lusardi and Mitchell [2014] of financial literacy, decisions, and outcomes. In earlier work, Lusardi and Mitchell [2007] provided evidence of a link between financial knowledge and preparing for retirement. They show that holding constant demographic variables such as education and race, those with more basic financial knowledge are more likely to prepare for retirement. It is this sort of research finding that is the impetus for calls for increased household financial knowledge. Financial knowledge is not the goal. Rather, improved financial outcomes are the goal, and enhanced knowledge is the path toward it. Financial capability was a concern of former Federal Reserve Board Chairman Alan Greenspan [2002]. He made the case for financial education and its role in making decisions about home ownership, small business ownership, and savings. Chairman Greenspan’s successor, Ben Bernanke [2013], stated in a 2013 speech: “Among the lessons of the recent financial crisis is the need for virtually everyone—both young and old—to acquire a basic knowledge of finance and economics. Such knowledge is necessary for anyone who will be faced with managing a household budget, making financial investments, finding reliable information about buying a car or a house, and preparing financially for retirement and other life goals.” As is often the case, what appears to be new is not. Consider the following quote: “The efficiency and the nation’s stability of economy depend on the wisdom with which you select your purchases and the restraint you show in alternating between buying a lot and feeling saturated with goods.” Although this could surely have been written in recent years, this quote is from a consumer economics textbook by Morgan [1955, p. iii] from over half a century ago. This quote also makes the link between individuals and the aggregate economy. Poor decisions at the individual level can reverberate throughout the economy. High levels of debt and poor housing purchases or mortgage decisions of individuals played a role in precipitating the recent financial crisis. Moreover, these poor decisions likely made the subsequent recession longer and deeper than it otherwise would have been. Although the financially capable may avoid poor decisions on an individual level, the impact of others’ poor decisions on the aggregate economy will affect everyone.",9
50,3,Business Economics,29 September 2015,https://link.springer.com/article/10.1057/be.2015.25,"The Solar Panel Manufacturing Industry’s Boom, Bust, and Future",July 2015,Leah Goddard,,,Female,Unknown,Unknown,Female,"Solar panels are comprised of one or more solar modules, which consist of photovoltaic cells that convert sunlight into electricity. Solar panels themselves are then assembled into a photovoltaic array to form a complete power-generating unit, or solar system. Semiconductor-grade polysilicon, a material processed from quartz and used extensively in electronics manufacturing, is the primary input into solar cells; as such, the price of polysilicon impacts production costs as well as selling prices for solar modules. The spot price of polysilicon peaked at about $475.0 per kilogram in February 2008 before bottoming out at $15.83 per kilogram in December 2012, according to Bloomberg News [Mufson 2013]. Similarly, the average price of solar modules per peak watt, or output power per watt installed under full solar radiation, fell by half from 2008 to 2012 [Energy Information Administration 2013]. Excess supply of polysilicon and solar modules led prices for these materials to deteriorate, while technological advancements cut production costs and exacerbated the drop in prices. This made solar power more competitive with other forms of energy generation, encouraging solar panel installation. Traditional crystalline silicon solar cells and modules and second-generation thin-film solar cells and modules dominate the industry. According to the EIA, crystalline silicon cells and modules accounted for 91.5 percent of the value of domestic shipments in 2012, whereas thin-film cells and modules represented just under 8.0 percent [2013]. On average, crystalline silicon modules have a higher energy conversion efficiency, which is the percentage of sunlight that is converted into electricity, than thin-film modules. In contrast to crystalline silicon modules, thin-film modules are made from nonsilicon materials, primarily cadmium telluride, copper-indium gallium-selenide, or amorphous silicon, which lacks a crystalline structure. The layers of semiconductor material in thin-film cells are only a few micrometers thick, which results in lightweight, flexible modules that can double as shingles and tiles on rooftops and facades. Although less expensive to produce than crystalline silicon modules due to highly automated processes that eliminate the need to assemble modules from individual cells, thin-film modules tend to be less efficient in converting solar radiation into electricity [International Energy Agency (EIA) 2014]. According to the EIA [2013], crystalline silicon modules had an average energy conversion efficiency of 16.0 percent in 2012, compared with 13.0 percent for thin-film panels. First Solar, which accounted for 13.5 percent of industry revenue in 2014, popularized cadmium telluride thin-film solar modules by achieving below-average production costs through its fully integrated and automated manufacturing process [IBISWorld 2014]. Amorphous silicon technology is less widespread, as it offers lower efficiency, lower longevity, and higher degradation rates than its competitors [International Energy Agency 2014]. Although First Solar has steadily improved the average energy conversion efficiency of its cadmium telluride solar modules to more than 14.0 percent, plummeting polysilicon prices in recent years have reduced thin-film modules’ cost advantage. As a result, thin-film cells and modules have lost market share to crystalline silicon cells and modules after reaching a record high in 2007. In terms of US manufactured domestic shipments by peak kilowatt, which is the industry’s standard measurement of output power per kilowatt installed under full solar radiation, thin-film cells and modules have fallen from 39.1 percent in 2007 to 31.5 percent in 2012 [EIA 2013]. Figure 4 illustrates solar cell and module shipments by type. 2012 Cell and Module Shipments (peak kilowatts) *Source: EIA. However, GTM Research reports that the price of polysilicon has risen modestly since 2012 to $21.70 per kilogram in the third quarter of 2014 [Solar Energy Industries Association (SEIA) 2014]. With polysilicon prices trending upward, crystalline silicon cells and modules will become more expensive to manufacture and purchase. Nevertheless, this technology will continue to offer greater efficiency than thin-panel technology. Concentrator solar technology is more efficient than crystalline silicon and thin-film cells and modules, but the high cost associated with this technology has limited its use to niche markets, such as space applications and the utility sector. According to the EIA [2013], concentrator solar technology had an average energy conversion efficiency of 30.0 percent in 2012. Concentrating solar power focuses sunlight on photovoltaic cells to increase energy conversion efficiency through reflective or refractive surfaces. According to the IEA [2014], low-concentrating photovoltaic systems track the sun on one access and have a concentration ratio of about 10, whereas high-concentrating photovoltaic systems track the sun on two axes and have a concentration ratio of 300 or more. Although the former can use high-efficiency crystalline silicon cells, the latter relies on multijunction cells. Multijunction cells consist of two or more stacked solar cells, with transparent upper layers that allow sunlight to reach the lower layers. These layers are able to capture a broader spectrum of sunlight in a smaller semiconductor area, meaning more expensive materials can be used to increase efficiency.",1
50,3,Business Economics,29 September 2015,https://link.springer.com/article/10.1057/be.2015.20,The Frackers: The Outrageous Inside Story of the New Billionaire Wildcatters,July 2015,Jesse S Hixson,,,Male,Unknown,Unknown,Male,,
50,3,Business Economics,29 September 2015,https://link.springer.com/article/10.1057/be.2015.23,Free Market Environmentalism for the Next Generation,July 2015,Gerald L Musgrave,,,Male,Unknown,Unknown,Male,,
50,4,Business Economics,18 December 2015,https://link.springer.com/article/10.1057/be.2015.36,From the Editor,October 2015,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,,
50,4,Business Economics,18 December 2015,https://link.springer.com/article/10.1057/be.2015.29,Designing Federal Budget Policy to Spur Economic Growth,October 2015,Douglas W Elmendorf,,,Male,Unknown,Unknown,Male,"Most federal investment is funded through the discretionary appropriations that Congress approves each year. Not surprisingly, defense investments primarily enhance our ability to protect the country, although they sometimes have positive spillovers to economic growth. I will focus on nondefense investments, which CBO analyzed in a report two years ago (CBO, 2013). Under the current caps on discretionary spending, federal nondefense investment in infrastructure, education and training, and research and development soon will be smaller relative to GDP than at any time in at least 50 years. Specifically, as shown in Figure 2, the current caps imply that nondefense discretionary spending will be a smaller percentage of GDP each year after 2016 than at any time in at least 50 years, and those investments have represented about half of nondefense discretionary spending nearly every year in the past 50. Therefore, unless the composition of nondefense discretionary spending shifts toward investments and away from other items to an extraordinary degree, the decline in that spending relative to the size of the economy will mean a decline in key federal investments relative to the size of the economy. Nondefense Discretionary Spending Source: CBO, 2013. That is not forward-looking, growth-oriented budget policy. Spending for infrastructure like roads and airports decreases the cost of moving people and delivering goods and services; spending for education and training enhances the skills of our workforce; and spending for research and development promotes innovation. Cutting those federal investments will reduce total output and income relative to what they would otherwise be. In particular, cutting federal investment in education and training will reduce the incomes of lower- and middle-income people who are dependent on government help to have a real opportunity to advance. To boost economic growth, we should raise the caps on nondefense discretionary spending substantially, in order to maintain federal investment as a share of GDP. That increase would also have the short-term benefit of providing a little more fiscal stimulus to an economy that needs it; I will come back to this issue later. We should also work to increase the return on federal investments. Sometimes we build critical transportation links, and sometimes we build bridges to nowhere; sometimes education funding supports a breakthrough in a person’s life, and sometimes it is dissipated. CBO’s central estimate is that additional federal investment yields half of the typical return on private investment, in part because federal investment can crowd out investment by private entities or state and local governments, and in part because federal investments are not always chosen with an eye to maximizing their effect on future income. We can do better. For example, we should increase the role of careful cost-benefit analysis in deciding which specific investments to undertake. In addition, it is important to understand that some federal spending outside nondefense discretionary spending also represents an investment in future income, especially for lower-income people. There is a growing body of evidence that certain health care benefits, housing subsidies, education subsidies, and other means-tested benefits raise future incomes of some young people. We should protect those investments as well. You will note that I have made the case merely for preserving the current amount of federal investment relative to the size of the economy. One can make a case for increasing federal investment as well. But even preserving federal investment would require a significant increase relative to what will happen under current law.",
50,4,Business Economics,18 December 2015,https://link.springer.com/article/10.1057/be.2015.28,A Career in a Less-than-Perfect Economy,October 2015,John Silvia,,,Male,Unknown,Unknown,Male,"As a profession we are challenged by the questions: What good are economists and where are the one-armed economists? However, the stark reality is that in the real world, there are no more one-armed economists than there are one-armed attorneys, doctors, or political consultants. Linear thinking and one-factor explanations are the currency of childhood. Our challenge is to express the underlying flow of an imperfect economy. We leave the simplistic, superficial bromides to popular commentary. Doctors cannot predict what disease anyone may have two years ahead, lawyers cannot predict case outcomes, and political pundits certainly cannot predict the actual winning gaps in elections. Moreover, Presidents Carter, Clinton, and Obama were all unknowns at the start of their presidential campaigns. As David McCullough noted in his book on Harry S. Truman, the man who wanted a one-armed economist, he often asked for several opinions on many issues, especially when considering whether or not to drop the Atom bomb [McCollough 1992]. Kennedy’s failure at the Bay of Pigs was the result of listening to only one view. Kennedy’s success at the Cuban missile crisis is the result of his listening to two opposing views [Neustadt and May 1986].",
50,4,Business Economics,18 December 2015,https://link.springer.com/article/10.1057/be.2015.27,The Home Purchase Sentiment Index: A New Housing Indicator,October 2015,James A Wilcox,,,Male,Unknown,Unknown,Male,"Households’ expectations have become central to economic analysis, modeling, and policymaking. Long recognized in principle, expectations now figure prominently in practice, permeating discussions and forecasts of consumer spending, business investment, labor costs, inflation, bond yields, and monetary policy. Incorporating expectations is problematic, however, in large part because most data do not directly measure expectations about the future but, instead, measure “the way we were.” One long established way to get data about expectations is through surveys. Both the demand for and supply of surveys of business analysts, forecasters, consumers, and businesses rose noticeably over the past decade. Because studies increasingly concluded that survey data can be informative, and because the costs of conducting and processing surveys decreased, more surveys emerged. That so many private sector organizations produce and pay for access to survey data for consumer and business attitudes and conditions testifies to the value of such data. One practical advantage of survey data for analysis and forecasting is that they are often produced quickly, perhaps a few days after the end of a month or even during the month. Second, survey data can be especially valuable in the wake of unexpected or especially large developments. For example, consumers can quickly change their views, as well as their spending and borrowing, in response to a surprising vote in Congress or to an event in the Middle East. In addition, survey data can be particularly valuable when the news changes consumers’ views about longer-run outcomes. Views about longer-run outcomes are particularly important for housing and mortgage markets, where longer-run assets and liabilities predominate. And, by their very nature, longer-run outcomes do not produce much traditional data in the shorter run. Surveys are used in dozens of countries to build indices of overall consumer sentiment. Two long-running, well-regarded monthly indices of overall consumer sentiment in the United States are the University of Michigan’s Survey of Consumer’s ICS and the Conference Board CCI.Footnote 2 For similar purposes, businesses are also surveyed in many countries. The Michigan Survey uses phone interviews to get 500 households’ views about their personal finances, overall business conditions, and buying conditions. For several decades, the ICS has been calculated from the same five questions. In addition to questions about recent and expected conditions, both personal and national, one of the five questions selected for the ICS asks whether it is a good time to buy big-ticket items. To produce its monthly CCI, the Conference Board uses the (approximately) 3,500 responses returned to them from the 5,000 questionnaires they send out by U.S. mail. Like the ICS, the CCI is calculated from five questions. The questions selected for the CCI ask con`sumers about current and expected business and job market conditions, and about their expected future incomes. Although their questions and calculation methods differ somewhat, in effect, both the ICS and the CCI are based on averages across their five questions of the “net percent positive” responses in their surveys. After the number of negative (or worse or pessimistic) responses is subtracted from the number of positive responses for each question, that difference is expressed as a percent of aggregate responses. The newer, higher-frequency Bloomberg U.S. Weekly Consumer Comfort Index is based on 1,000 phone interviews that elicit consumers’ views on the national economy, the buying climate, and their personal financial conditions. In effect, the Consumer Comfort Index is an equally weighted average of the net percent positives for its three categories of questions. Although the ICS equally weights its questions, the weights applied to the questions in the CCI vary over time. Some housing-related indices are built from surveys, while others rely on more traditional data. The monthly Housing Market Index (HMI) of the National Association of Home Builders (NAHB) and Wells Fargo is based on mail surveys of NAHB members. The surveys ask builders for their attitudes and expectations for the demand for single-family (SF) homes, and ask them to rate housing market conditions. Essentially, the HMI is a weighted average of the net percent positives for three questions about conditions in and expectations for their local SF markets, with weights determined by each question’s historical correlation with future SF housing starts. The National Association of Realtors (NAR) surveys its members monthly about real estate market conditions and expectations. Rather than forming an index by combining responses to several questions, the NAR’s Confidence Index is a collection of indicators. Each indicator is the average of the members’ responses to one of the survey questions, each of which is scored as 0, 50, or 100. Freddie Mac introduced its Multi-Indicator Market Index (MiMi) in March 2014. According to the Freddie Mac website, “MiMi measures local housing market conditions by combining recent, local-market data with Freddie Mac data … MiMi assesses where each market is relative to its own, long-term, stable range ….” Rather than being based on survey data, MiMi is based on an average of four objective variables. The monthly NHS started in June 2010. The NHS is the only large-scale, monthly survey of consumers that focuses exclusively on housing. The NHS uses phone interviews of 1,000 consumers to get their housing-related attitudes, expectations, and intentions.Footnote 3 The NHS is designed to produce a sample of respondents that is nationally representative along eight socioeconomic dimensions. To reduce remaining deviations of the sample from national representativeness, each respondent’s answers are weighted before net percent positives are calculated. The NHS asks consumers for their housing-related views about the recent past, the present, the future, and even about hypothetical situations. The NHS asks consumers about their personal economic and financial conditions, as well as about the economy as a whole. The NHS asks both homeowners and renters about owning and renting homes, home and rental prices, home ownership distress, household finances, and views about the condition and outlook for the economy. Thus, the NHS provides valuable information about housing markets that is otherwise not available.Footnote 4 The resulting, wide-ranging, up-to-date database of consumers’ attitudes, conditions, and expectations can provide information quickly and easily, not just to housing analysts and economic forecasters, but to borrowers and lenders, to home sellers and home buyers, and to investors and policymakers.",6
50,4,Business Economics,18 December 2015,https://link.springer.com/article/10.1057/be.2015.34,Three Challenges to Central Bank Orthodoxy,October 2015,James Bullard,Kevin L Kliesen,,Male,Male,Unknown,Male,"What we are calling the “classic” or “traditional” way to view current U.S. monetary policy emphasizes the cumulative success that has been achieved so far with respect to Committee goals. The Committee has clear objectives associated with labor market performance and inflation. Regarding inflation, the Committee set an official target of 2 percent beginning in 2012. Concerning labor market performance, the Committee, through its September 17, 2015 Summary of Economic Projections (SEP), has indicated that an unemployment rate of around 4.9 percent is likely to be consistent with longer-run equilibrium, as indicated in Table 1.Footnote 3
 The value of the longer-run unemployment rate has drifted down recently—it was 5.6 percent within the last few years.Footnote 4
 Is the Committee achieving these objectives? The classic view emphasizes that, indeed, these Committee objectives are close to being met. As shown in Figure 1, the unemployment rate as of September 2015 is 5.1 percent and has been on a downward trend. Given the large amount of uncertainty around the concept of a long-run or natural rate of unemployment, the current 5.1 percent value is statistically indistinguishable from the Committee’s statement of the likely long-run level. Unemployment Rate 
Source: U.S. Bureau of Labor Statistics In the last two expansions, unemployment fell well into the 4 percent range; and, barring a major recessionary shock, unemployment is likely to fall to similar levels in the quarters and years ahead. This is likely regardless of the date of liftoff, because monetary policy will remain exceptionally accommodative even after normalization begins. In short, the Committee has already hit its objective on this dimension, as shown in Figure 1. In addition, labor markets are likely to continue to improve going forward, barring a major negative shock. Many have argued that other dimensions of labor market performance should be considered in the current environment. We think this is fair, since labor markets were severely impaired in 2007–09. Indicators such as job openings and initial unemployment insurance claims look very good, while other indicators like working part time for economic reasons and long-term unemployment seem not as good. One way to get a handle on this issue is to consider a labor market conditions index. Such an index can be constructed by combining many different indicators of labor market performance into a single index number and then taking that index number as a better and more informed judgment of the state of the overall labor market than the unemployment rate alone. The Board of Governors has calculated such an index [Chung and others 2014]. As shown in Figure 2, the current level of the index is well above its average level since 1976. Labor markets might be viewed as even better than normal according to this metric. Labor Market Conditions Index 
Source: Board of Governors of the Federal Reserve System What about the inflation side of the Federal Reserve’s dual mandate? Inflation is certainly low today; in fact, it is near zero on a year-over-year basis due in part to the very large decline in oil prices beginning in 2014. In addition, recent oil price volatility suggests stabilization of oil and related commodities prices may still be some ways in the future. Although the drop in oil prices is a net positive for the U.S. economy, the sharp downward movement does inhibit year-over-year readings on headline inflation. The classic view has an answer for this—it suggests looking through large oil price shocks, either positive or negative. The reason is that energy price shocks are usually limited in their duration. Thus, relatively large increases (decreases) tend to be followed by relatively large decreases (increases). Accordingly, at this particular juncture, it may be more useful to consider the Dallas Federal Reserve’s trimmed mean PCE inflation measure, as seen in Figure 3. As of August 2015, this measure was running at about 1.7 percent year-over-year, about 30 basis points below the Committee’s target. This is low, but still reasonably close to target. Inflation Rate 
Source: Federal Reserve Bank of Dallas The classic view, as we are outlining it here, would then say that unemployment of 5.1 percent and underlying inflation of 1.7 percent constitute values that are exceptionally close to the objectives of the Committee. One easy method of calculating how close the Committee is to its dual objectives uses a quadratic function to approximate the Committee’s objective function. In effect, it measures deviations of unemployment and inflation from target: where π
t
 is the actual inflation rate at time t; π* is the Committee’s 2 percent inflation target; u

t
 is the actual unemployment rate at time t; and u* is the median longer-run value of the unemployment rate from the Committee’s September SEP (4.9 percent).Footnote 5 Importantly, this version of the objective function puts equal weight on inflation and unemployment and is sometimes used to evaluate various policy options. Figure 4 shows that today’s combination of labor market performance and inflation performance is about as good as it has ever been in the past 50 years or so.Footnote 6
 Distance From Goals 
Source: U.S. Bureau of Economic Analysis Although the metrics concerning Committee objectives are close to normal, the policy settings are not. The Committee has used two tools in the last seven years to conduct monetary policy. One tool has been to set the policy rate—the federal funds rate—to a near-zero value, where it remains today (see Figure 5). Federal Funds Rate 
Source: Board of Governors of the Federal Reserve System Recall from Table 1 that the Committee’s SEP indicates that participants view the longer-run level of the policy rate to be about 3.5 percent. Thus, the current policy rate is more than 325 basis points lower than the long-run level. The other tool has been QE. As a result of several rounds of QE, the Federal Reserve’s balance sheet has increased from a precrisis value of about $800 billion to about $4.5 trillion today (see Figure 6). Federal Reserve Balance Sheet 
Source: Board of Governors of the Federal Reserve System These considerations—objectives met, but policy settings far from normal—suggest a policy path that will return the economy to the well understood precrisis equilibrium. Based on central bank orthodoxy, the most prudent course of action is to begin to normalize the policy rate slowly and gradually, under the interpretation that the Committee will still be providing considerable monetary policy accommodation to the economy to guard against potential pitfalls and risks as the quarters and years ahead unfold. By adopting this prudent approach to monetary policy strategy, the Committee may be able to lengthen the expansion longer than it may otherwise extend. However, failure to promptly begin the process of normalization runs the risk of settling into an equilibrium of unknown duration and uncertain consequences.Footnote 7
 We have set up this simple classic view because we think that, on balance, this view suggests the best path forward for U.S. monetary policy. But there are certainly other views with considerable merit, and we will now turn to a discussion of these alternatives. Each of the alternatives departs from an important aspect of the classic view. Again, we would hesitate to associate these alternatives with specific individuals or organizations, as most or all of us (including us at times) appeal to parts of these arguments when discussing contemporary monetary policy.",2
50,4,Business Economics,18 December 2015,https://link.springer.com/article/10.1057/be.2015.35,Integrating Financial Stability into Monetary Policy,October 2015,Stephen S Poloz,,,Male,Unknown,Unknown,Male,"The debate about using monetary policy to respond to financial imbalances has evolved rapidly since the precrisis era. A decade or so ago, the discussion was essentially between those who argued that monetary authorities should lean against imbalances such as asset price bubbles and those who said that monetary policy should be reserved for cleaning up the mess after the bubble popped. Before the financial crisis, the Bank of Canada basically straddled the two camps. On the one hand, we argued that it is very difficult to identify an asset price bubble, and central bankers have no comparative advantage in making this determination. Like many, we questioned the wisdom of using the blunt instrument of interest rates on a bubble that could be confined to one asset class. Indeed, if a bubble was particularly large and persistent, a central bank that used the cure of higher interest rates could end up causing the very economic damage it was trying to prevent. On the other hand, the Bank recognized that price stability was a necessary, but not sufficient, condition for financial stability. Given our keen interest in a well-functioning financial system and our macro perspective, we worked to raise awareness of stability threats. Our decision to begin publishing our Financial System Review (FSR) in 2002 showed our early commitment to promoting financial stability. We used the phrase “global imbalances” a lot in speeches leading up to the crisis. In other words, we were not content to just stand on the sidelines and wait to clean up messes. The widespread and extremely high cost of the Great Recession made it clear just how difficult the clean-up job can be. It has been roughly seven years since the crisis, and the damage done to the global economy has left many central banks still struggling with weak growth and inflation. However, the more fundamental lesson we learned is that “lean vs. clean” is a false dichotomy. It is far too simplistic to say that financial stability threats compel central banks to choose between leaning and cleaning. In a perfect world, we would have a macroeconomic model sophisticated enough to capture the emergence and resolution of financial imbalances, along with their related impacts on the real economy. With such a model, we would be able to incorporate financial stability threats into our reaction function, if not with absolute precision, then at least as well as we incorporate other economic variables. Unfortunately, we do not live in that perfect world. A general-equilibrium model containing a grand synthesis of real and financial variables does not exist and is not likely to. I do not mean to downplay the importance of research and the development of stylized models, which are crucial in helping us understand aspects of the relationship between the real economy and financial stability. But the reality is that central banks have to cope with tremendous uncertainty regarding financial stability issues, and this is layered on top of the regular uncertainties of monetary policy concerning unobservable variables such as potential output. Given all of the uncertainty, it seems to me the proper response of the monetary authority is to acknowledge and accept all the things we do not know, gauge the risks facing the economy as best we can, and manage those risks as we conduct monetary policy. I’ll describe our risk-management framework in detail later on. But we still have the question of how policymakers should respond to financial imbalances, particularly those that are concentrated in a specific sector or asset class. The Bank of Canada’s view is that monetary policy should be the last line of defence against threats to financial stability, behind the joint responsibility of borrowers and lenders, appropriate regulatory oversight within the financial sector, and sound macroprudential policies. Let me say a little bit about each of these.",7
50,4,Business Economics,18 December 2015,https://link.springer.com/article/10.1057/be.2015.30,The First 50 Years of Business Economics,October 2015,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"The first issue of Business Economics appeared in July 1965. Soon after the creation of the National Association of Business Economists (NABE) in 1959, it produced a newsletter; but William Charterner,Footnote 2 the editor of the newsletter, felt that a professional journal would be appropriate and made that recommendation to the NABE Council (the predecessor of the Board of Directors) in the spring of 1964. In January 1965, the Council approved the plan for a journal that would incorporate the proceedings of the Annual Meeting, the quarterly newsletter, and other professional articles. Charterner volunteered to serve as the unpaid editor of the new journal and assembled an editorial team that consisted of two members from for-profit firms and two from nonprofit research institutes. There was also an editorial advisory board that consisted of two members from for-profit firms, eight from universities, a journalist, and one member from the Federal Reserve. Thus, from its very beginnings, Business Economics relied heavily on individuals who were not directly tied to for-profit business.Footnote 3 By a slim majority, the Council voted to call the new journal Business Economics rather than The Business Economist. The rationale was to focus on the profession rather than the individuals in it. The following is excerpted from Regan [1989, p. 25]: Chartener requested, and the Council agreed, that Business Economics have an initial printing of 2,000 copies per issue. NABE’s membership stood at 837 in the spring of 1965, but Chartener hoped to sell the surplus to libraries and individual nonmembers or corporate subscribers. Members would, of course, receive the journal free as a privilege of membership. The Council approved and allocated $7,000 for the journal’s first year. This amount was a large sum from a NABE operating budget of less than $19,000; and, while Executive Secretary-Treasurer Ralph Burgess did not anticipate a deficit resulting, he did note that “the estimated surplus” was going to be considerably reduced. Late in the summer of 1965 the first issue of Business Economics appeared. Its publication, NABE President George James [Battelle Memorial Institute] noted on its opening page, was the realization of a long-time objective: “Since its founding, a dream and ambition of NABE has been to publish a professional journal, an essential step before any organization truly gains professional stature.” Such dreams, James remarked, “begin with men of imagination—and are carried out by men of ambition. ‘Prominent among those combining both qualities was William Chartener,’ who led the rest of us in producing Business Economics.” 
The existence of the new journal would, James continued, “provide business economists with an exchange of views” and prevent their working in a vacuum—certainly the fulfillment of Adolph Abramson’s dream that NABE be “a forum for the discussion of common problems.”
Footnote 4
The journal could also provide a means through which to “exhibit our professional thinking to our peers and our management, as well as our academic counterparts.” But it would accomplish these things, James warned:
 … only if the members of NABE take it on themselves to contribute to the journal through unique and well-developed articles concerning their own experience and work results. These articles, in turn, should enhance the work of our colleagues while withstanding the constructive criticism of our contemporaries. Although it was not officially viewed as such, perhaps the most important function Business Economics had in its early years was to provide a form of continuing education for NABE members. Nearly seven years after its founding, NABE was still comprised primarily of economists trained for academic careers. The appearance of Business Economics provided a tangible if informal means of furthering their knowledge about the problems that working business economists had to solve every working day. Chartener and his successor, Jack McCroskey,Footnote 5 who took over the editorial duties with the spring 1967 issue (vol.2, no. 2) devoted the first several issues of Business Economics to the publication in article form of selected papers from NABE seminars and Annual Meetings. The exigencies of starting a new journal no doubt influenced the editors in the choice of material. However, by publishing the seminar and Annual Meeting papers they made available to those who could not attend what were in effect short courses on diverse economic topics. In addition to regular articles, each issue had at least one book review, a “President’s Message,” and various communications and notes. Beginning in Volume 1, each volume has included the address of the outgoing NABE president, which is delivered at NABE’s Annual Meeting. Business Economics was published three times per year, with one additional issue devoted to a membership directory. Rising publication costs and a shortage of acceptable articles limited publication to three issues per year until 1974 [Regan 89, p. 46].Footnote 6 Morris Cohen became the Editor in 1969.Footnote 7 The complete list of Editors is shown in Table 1. In 1970, the Editorial Advisory Board was disbanded; and its members became Associate Editors. There was no change in membership of the consolidated group, and there was no apparent change in editorial direction. However, book reviews ceased to be a regular feature by 1970 and appeared only sporadically until 1982.Footnote 8illustration David L. Williams of the Acme-Cleveland Corporation took over the editorship in 1973, with no apparent change in editorial policy or content, except for dropping the “President’s Message” as a regular feature. It has appeared only sporadically since then, and it is not known whether the change was due to editorial policy or presidential preference. However, in March 1975, the membership issue began to include two or three articles in addition to its normal content of a list of members, by-laws, and other material. Also, the average number of pages per article grew, and there was less reliance on conference and seminar proceedings. However, it is not known how many papers came “over the transom” as unsolicited papers that were refereed and how many were invited.Footnote 9",1
50,4,Business Economics,18 December 2015,https://link.springer.com/article/10.1057/be.2015.31,"The World Market for Light Sources: Lamps and LEDs, 2006–21",October 2015,Jennifer Mapes-Christ,Andrew C Gross,,Female,Male,Unknown,Mix,,
50,4,Business Economics,18 December 2015,https://link.springer.com/article/10.1057/be.2015.32,Boss Life: Surviving My Own Small Business,October 2015,Charles Steindel,,,Male,Unknown,Unknown,Male,,
50,4,Business Economics,18 December 2015,https://link.springer.com/article/10.1057/be.2015.33,Mastering ‘Metrics’: The Path from Cause to Effect,October 2015,Jan Kmenta,,,Male,Unknown,Unknown,Male,,
51,1,Business Economics,31 March 2016,https://link.springer.com/article/10.1057/be.2016.11,From the Editor,January 2016,Robert Thomas Crow,,,Male,Unknown,Unknown,Male,"This issue is my last as Editor of Business Economics. I am retiring after more than 16 wonderful years—the longest I have held any job in my professional career. My successor—already at work on the April issue with my enthusiastic assistance—is Charles Steindel. Many NABE members will know Charlie from his long and distinguished participation in NABE. He is a Certified Business Economist, a past member of the Board of Directors, a current member of Business Economics’ Editorial Board, and a NABE Fellow. He will be a great Editor. Why am I retiring? I have several big personal projects that have nothing to do with economics that must either be done soon or not at all. I have to get after them. Also, on a more altruistic note, after one’s 77th birthday, life becomes increasingly less certain: I do not want to leave NABE in the lurch by suddenly becoming unavailable. Thus, some months ago, I informed Tom Beers that it was time to look for a successor. That said, my health is good so far—better than most geezers, I suspect. There are many people to thank for my enjoyable long run with Business Economics. I will do so more or less chronologically. First, there was Enrique Sanchez who told me that NABE was looking for a Business Economics Editor. I am certainly grateful to Susan Doolittle, NABE’s Executive Director at the time, and Diane Swonk, NABE’s President at the time, for taking a gamble that I would be up to the job. Moreover, Susan—in addition to her regular duties as Executive Director—served as copyeditor, content reviewer, and executive publisher of Business Economics until NABE established its current relationship with Palgrave. The members of the Editorial Board, past and present, have served admirably in providing referee reviews of submitted papers, selecting Abramson and Mennis Award winners, and other help. I am deeply grateful for their expertise, wisdom, and willingness. When Tom Beers succeeded Susan Doolittle, support for Business Economics did not lose a beat. I am most appreciative. Also, he has maintained and augmented a most helpful and congenial staff. In particular, Colette Brissett has been my cheerful, knowledgeable, and efficient go-to resource on any and all administrative matters ever since I became Editor. Tara Munroe has been invaluable in the management of the Abramson and Mennis Awards—often acting as Jiminy Cricket to my Pinocchio in doing her best to make sure that I did the right thing when I should do it. Melissa Golding has been most helpful in obtaining transcripts of presentations at NABE meetings that often turn into Business Economics articles and letting the press know what is coming. Also, I appreciate the help of Pam Ginsbach and then Chris Jonas in getting items related to Business Economics into NABE News and its successor, NABE NewsDigest. I would also like to thank my production editors at Palgrave—Kirsty Lockett, Julie Abbott, Michelle Lo, and James Keane. Their patience, diligence, and good humor made my job easier and made Business Economics better than it would have been otherwise. Finally, a whole-hearted thank-you to all of the authors that I have had the pleasure of working with over these past 16 years. They have come from every corner of economic thought and practice—from distinguished guest speakers at NABE conferences to young professionals sharing their fresh insights in submitted papers. Keep the articles coming!",
51,1,Business Economics,31 March 2016,https://link.springer.com/article/10.1057/be.2016.5,Economic Implications of Demographic Change,January 2016,James Poterba,,,Male,Unknown,Unknown,Male,"Two forces underpin the shift in the age distribution of the U.S. population that is currently under way. One is the ongoing decline in mortality rates at older ages, and the other is the lingering effect of a decline in birth rates that took place in the 1960s and 1970s. Death rates have declined for decades. Although there is disagreement about the likely pace of future declines, consensus forecasts suggest continued improvement. Table 1 shows life expectancy estimates for men and women born in various birth cohorts between 1900 and 2000. Life expectancy for a male born in 1950 is estimated to be 73.7 years. As many of those born in 1950 are still alive, and these survivors are currently 65 and 66 years old, estimates of life expectancy depend in part on projections of future age-specific mortality rates. These projections suggest that remaining life expectancy for a man who celebrated his 65th birthday in 2015 is 19.3 years. The entries in Table 1 show that during the last century, life expectancy at birth for males rose by about 30 years. For females, the increase was somewhat smaller, about 27 years. Life expectancy for a 65-year-old man increased by about nine years, and for a woman by around three years. The chance that a newborn male in 1900 would reach age 65 went up by about 40 percentage points; the change was about 35 percentage points for females. To offer another perspective on changing late-life mortality rates, Table 2 shows the age at which there is a 20 percent chance that someone who reached age 65 would still be alive. The table presents this information for various cohorts. The table shows, for example, that if a woman born in 1900 reached age 65, she would have a 20 percent change of living beyond the age of 91. Mortality rate projections suggest that for a woman born in 2000 who reaches 65, the analogous age will be 96.6 years. The probability of living for many years beyond age 65 has risen sharply. For the 1900 birth cohort, there was almost a 20 percent chance that a man would live to 86 if he lived to 65. For the cohort born in 2000, mortality rate projections suggest a 20 percent chance of almost reaching 95 conditional on reaching 65. The second component of demographic change is the decline in fertility. The two decades after World War II were a period of unusually high fertility, particularly in comparison to more recent decades. Figure 1 shows the historical U.S. fertility rate since 1900, along with projections from the Social Security Administration (SSA) of fertility rates through 2050. The total fertility rate is a measure of the number of children who would be born to a woman over the course of her child-bearing years, calculated using current age-specific fertility rates. Fertility Rate, United States, 1940–2015 Source: U.S. Social Security Administration, Trustees Report [2015], Table V.A1. The total U.S. fertility rate reached about 3.5 per woman in the late 1950s and then fell in the late 1960s and the 1970s to a level close to its current value, just below two. Today’s fertility rate is close to the value at which the number of births exactly offsets the number of deaths. Because the United States also experiences net immigration, even at this fertility rate the U.S. population is projected to grow. Projections by SSA and others call for current levels of fertility to persist for the foreseeable future. One of the great uncertainties of the demographic future is whether the fertility rate will vary substantially from its current level. The combination of falling fertility rates and declining mortality rates over the last century has led to an ongoing change in the age structure of the U.S. population. Figure 2 shows the fraction of the U.S. population under the age of 18. This fraction fell from over 30 percent between the 1950s and the 1970s to the low 20s today and is projected to decline further, to just over 20 percent, by 2050. Changing Age Structure of U.S. Population Source: Poterba [2014], Table 4. Figure 2 also shows the fraction of the population over 65, which is projected to rise from about 12 percent in 2000 to just over 20 percent by 2050. The figure also displays the share of the population over the age of 85, which in percentage terms is the fastest growing part of the U.S. population. That group will represent nearly 5 percent of the population by 2050. The age structure of a population is often summarized with the dependency ratio, the ratio of the number of individuals who are in need of support in the economy to the number who are in the traditional working age groups. A standard definition is the ratio of the population below age 15 and above 65 to the working age population (15 to 64). The dependency ratio is projected to rise over time. Figure 3 shows the total, youth, and elderly dependency ratios. Dependency Rates for U.S. Population Source: Social Security Administration, Trustees’ Report [2015], Table V.A2. Over the last half century, the composition of the dependent population in the United States has shifted. This group was predominantly young in the 1960s and 1970s; today, the number of aged dependents is approaching the number of young dependents, and the numbers will be roughly equal by 2050. The economic burden of the elderly population has increased even more rapidly than the population data suggest because the price of medical care, which is disproportionately consumed by the elderly with financial support from government insurance programs, has increased faster than the overall price level.",2
51,1,Business Economics,31 March 2016,https://link.springer.com/article/10.1057/be.2016.6,Public and Private Challenges of an Aging U.S. Population,January 2016,Olivia S Mitchell,,,Female,Unknown,Unknown,Female,"Several factors will set the stage for this discussion. One is that incomes have not risen quickly at the bottom of the pay distribution, making it difficult for many to save—despite the reality that living longer requires more savings. Also in the past, employers often provided active and retiree health-care insurance as well as pensions; a minority of firms outside of the private sector do so now. This changing reality means that today’s workers will not live as well in retirement as did their parents and grandparents. Another concern is that our Social Security program is not aging well. Established in the mid-1930s, it is still the single biggest U.S. government program with “defined benefits,” set according to a formula based on years of work and pay levels. It is mainly a pay-as-you go system with revenues on the order of $760 billion coming in payroll tax; 166 million of us pay those taxes, and our average tax amounts to around $4,500 per worker per year. A related point that many people do not appreciate is that a majority of American workers today pay more in Social Security tax than in income tax, making it difficult to raise more revenue to keep the program going. Already Social Security costs exceed payroll tax revenue, at $848 billion going to 59 million recipients for an average annual benefit of about $14,500. The system is also being questioned since many of today’s workers already expect to receive less than they paid in. Of course this was not always so: the initial generation of Social Security beneficiaries received a virtually infinite rate of return, since they paid little in payroll tax but received lifetime benefits. Even for my mother’s generation, born in the 1920s, the system was not a bad deal: women received about a 6 percent real internal return and men 3 percent. Couples usually did much better because of the spouse benefit, for which traditional one-earner households did not pay extra. Today’s workers and those retiring in the future, however, face a much less appealing deal. Social Security returns have been dropping since people now pay higher taxes for more years than in the past. So the question is: if we’re getting such low returns on our Social Security taxes, why can’t we just let people invest their savings on their own? The answer is that, in a pay-as-you-go system, today’s taxes are needed to pay today’s retirees (our parents and grandparents). Although there is a Trust Fund, this represents IOUs from one government agency to another; there are no assets on hand that can simply be cashed out to pay current and future retirees, without Congress raising taxes and/or debt. The size of the gap to be filled, by one measure the “open group obligation,” is about $26 trillion in present value. Some compare this with our current U.S. G.D.P. of about $18 trillion—and the gap is $15 trillion larger than the typical 75-year gap that the government usually talks about. If the Social Security System cannot pay all scheduled benefits, what then? The Congressional Budget Office has estimated that scheduled benefits will need to fall by between one-quarter and one-third to keep the system in balance. Barring that, payroll taxes must rise by 50-80 percent. Population aging is also causing other federal resources to be spread very thin. Most of the projected rise in federal spending projected from now until the year 2040—55 to 60 percent of it—is directly attributed to population aging. Moreover, much of the remainder of the spending is driven by increased health-care spending per capita, also a result of older longer-lived citizens.",
51,1,Business Economics,31 March 2016,https://link.springer.com/article/10.1057/be.2016.1,Economic Implications of Demographic Change: Diversity Dividend or Deficit?,January 2016,Marta Tienda,,,Female,Unknown,Unknown,Female,"Table 1 summarizes the U.S. diversification narrative over the past century as the population quadrupled to 310 million. The White population share declined from 89 percent in the middle of the twentieth century to about 71 percent in 2000 and 62 percent by 2010. The Black population share, which remained rather stable throughout the twentieth century at 11-12 percent of the total, inched up to 13 percent in 2010. In contrast, the Hispanic and Asian populations have surged since 1970, largely due to the resurgence of mass migration after the 1965 Amendments to the Immigration and Nationality Act lifted restrictions on immigration from Asia and made family reunification the centerpiece of the admission regime.Footnote 1 Unlike the Asian population share, which has risen gradually even as the total population has increased, the Hispanic population has more than trebled over the last 50 years, rising from about 4.5 percent in 1970 to more than 16 percent by 2010. Put differently, Latinos represented 36 percent of the net 100 million persons added to the U.S. population between 1966 and 2006—more than Whites (34 percent), and more than Blacks (16 percent) and Asians (13 percent) combined [Pew Hispanic Center 2006]. The relative growth of the Hispanic population is projected to continue for the foreseeable future, albeit at a slower rate [Frey 2015] The economic significance of the unprecedented population diversification partly derives from the accompanying and equally profound, generational transition characterized by an aging White majority and a youthful minority population. Figure 1 illustrates the ethno-racial contours of the changing age structure. Thanks to the baby boom—a period of above-replacement fertility between 1946 and 1964—in 1970 over one-in-three U.S. residents was under age 18 and less than 10 percent was ages 65 and over; by 2010, the youth share had dropped to less than one-in-four; and, as the outsized Baby Boom cohorts approached retirement, the senior share rose to 13 percent [Howden and Meyer 2011]. These changes have important implications for old-age support burdens, the solvency of the Social Security system, and the nation’s ability to compete in a globalized economy. U.S. Age Structure by Race, 2010 Source: Census 2010, Census Bureau; Social Explorer Tables SE:T12 and SE:T41.",1
51,1,Business Economics,31 March 2016,https://link.springer.com/article/10.1057/be.2016.8,Secular Outlook for Global Growth: 2015–34,January 2016,Irina Tytell,Lisa Emsbo-Mattingly,Dirk Hofschire,Female,Female,Male,Mix,,
51,1,Business Economics,31 March 2016,https://link.springer.com/article/10.1057/be.2016.9,Exploring a Measurement of Analytics Capabilities,January 2016,Suzanne Heller Clain,Matthew J Liberatore,Bruce Pollack-Johnson,Female,Male,Male,Mix,,
51,1,Business Economics,31 March 2016,https://link.springer.com/article/10.1057/be.2016.10,Fed Funds Rate Surprises and Financial Markets,January 2016,John Silvia,Azhar Iqbal,Alex Moehring,Male,,Male,Mix,,
51,1,Business Economics,31 March 2016,https://link.springer.com/article/10.1057/be.2016.3,The Courage to Act: A Memoir of a Crisis and Its Aftermath,January 2016,Charles Steindel,,,Male,Unknown,Unknown,Male,,
51,1,Business Economics,31 March 2016,https://link.springer.com/article/10.1057/be.2016.4,"The Midas Paradox: Financial Markets, Government Policy Shocks, and the Great Depression",January 2016,John C Goodman,,,Male,Unknown,Unknown,Male,,
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.20,From the Editor,April 2016,Charles Steindel,,,Male,Unknown,Unknown,Male,,
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.23,Measuring the Natural Rate of Interest Redux,April 2016,Thomas Laubach,John C Williams,,Male,Male,Unknown,Male,"Knut Wicksell [1936] famously characterized the natural rate of interest: “There is a certain rate of interest on loans which is neutral in respect to commodity prices, and tends neither to raise nor to lower them.” We operationalize this notion by defining the natural rate as the real short-term interest rate consistent with the economy operating at its full potential once transitory shocks to aggregate supply or demand have abated. Implicit in this definition is the absence of upward or downward pressures on the rate of price inflation relative to its trend. Our definition takes a “longer-run” perspective, in that it refers to the level of real interest rates expected to prevail, say, 5–10 years in the future, after the economy has emerged from any cyclical fluctuations and is expanding at its trend rate. In contrast, other research has focused on short-term fluctuations in the natural rate of interest, assuming the longer-run value is constant [Neiss and Nelson 2003; Woodford 2003; Andrés, López-Salido, and Nelson 2009; Barsky and others 2014; Cúrdia and others 2015; Goldby, Laureys, and Reinold 2015]. These studies define the natural rate to be the real interest rate that would prevail if all prices were flexible. However, we do not view this as a competing or contradictory approach to defining the natural rate; rather, the short-term perspective is complementary to our longer-run approach. Figure 1 portrays a highly stylized model of the determination of the natural rate. The downward-sloping line, labeled the IS curve, shows the negative relationship between aggregate spending and the real interest rate. The vertical line indicates the level of potential GDP, which is assumed to be unrelated to the real interest rate for this diagram. In principle, potential GDP may also be a function of the real rate, but this modification does not affect the basic point of the analysis. At the intersection of the IS curve and the potential GDP line, real GDP equals potential, and the real interest rate equals the natural rate of interest. Determination of the Natural Rate of Interest The natural rate of interest may change over time owing to highly persistent structural shifts in aggregate supply and demand. For example, Laubach [2009] finds that increases in long-run projections of federal government budget deficits are related to increases in expected long-term real interest rates. In Figure 1, an increase in long-run projected budget deficits moves the IS curve to the right, implying a higher natural rate. As discussed by Bomfim [1997], Laubach and Williams [2003], Congressional Budget Office [2014], International Monetary Fund [2014], Council of Economic Advisers [2015], Hamilton and others [2015], and Pescatori and Turunen [2015], there are myriad influences on the natural rate, including, but not limited to, productivity growth, demographics, and the evolution of the global economy.",92
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.12,Financial Stability and the Hemianopsia of Monetary Policy,April 2016,Peter R. Fisher,,,Male,Unknown,Unknown,Male,,2
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.14,Can Monetary Policy Meet the Needs of Financial Stability? Remembering the Frb Alongside the Fomc,April 2016,Paul Tucker,,,Male,Unknown,Unknown,Male,"When we talk about financial stability problems, we are concerned with two different types of social cost or negative externality:
 The boom phase of the asset-price/credit cycle gives rise to a misallocation of resources and, in particular, to over-indebtedness among households and firms? An implosion of the financial system itself cuts off the supply of vital services to the real economy, throwing the economy into recession, potentially putting the economy onto a lower growth path and, if attitudes to risk are persistently changed, possibly lowering the “trend” rate of growth. Talk about monetary policy—or about macroeconomic policy more generally—addressing longer-run intertemporal stabilization problems is essentially about the first of those two varieties of social cost. Of course, dampening booms, if it were feasible, would on the face of it reduce the probability of vicious busts and, thus, the probability of the financial system collapsing. But, realistically, that can never be sufficient to guard against instability. Imagine a financial system in which many core firms have next to no tangible common equity—that is not unlike much of the financial world in the run up to 2007/08. That would be a system that could buckle in the face of even relatively small disturbances—as, one might argue, the U.S. subprime mortgage problems should have been in a half-decent national and international finance system. In other words, for the vulnerability of the financial system to be irrelevant, the macroauthorities’ stabilization credentials would need to be superhuman. The only way to be confident of avoiding collapse is to have a resilient financial system. The instruments to require that in the face of private incentives to run stretched balance sheets lie with regulators, not with monetary policymakers. And, within the Federal Reserve System, they lie with the Board, not with the Open Market Committee. Indeed, the FOMC has only one instrument—the terms of the Federal Reserve’s open market operations—and thus, to the extent that it may legitimately and, in practice, could effectively be engaged in stability policy, its locus could at most be only the first of the two broad externalities. By contrast, the Federal Reserve Board has a range of policy tools available to it: regulatory parameters for banks and certain nonbank groups designated as systemically significant by the multiagency Financial Stability Oversight Council; approving the terms and framework for Discount Window facilities; deciding on various Lender of Last Resort (LOLR) operations, subject to Treasury approval; setting minimum margin requirements in various capital markets; and deciding whether to vary dynamically certain regulatory requirements in pursuit of relevant objectives set by Congress. There are three things to be said about this:
 Even though the Board of Governors does not have an umbrella statutory responsibility for financial stability, some of its specific powers are to be used to preserve system stability, and its longstanding bank-regulatory powers are dedicated to firms’ safety and soundness, which can more than arguably be construed in terms of stability.Footnote 1 The Board and, thus, individual Board members are responsible for policy directed to both types of social cost/negative externality, separate from and therefore in addition to their permanent membership of FOMC. The Board’s regulatory powers and role concern not only the static regime but also any permitted dynamic variation of core regulatory parameters in the pursuit of stability.Footnote 2
 Putting those first two points together, the Board and, thus, individual Board members are responsible for policy directed to both types of social cost/negative externality, separate from and therefore in addition to their permanent membership on the FOMC. The governance or, as I prefer to put it, the political economy of the Board is, therefore, very important—for the United States and, indeed, for the rest of the world. But any interest in that gets crowded out by the longstanding soap opera around the FOMC’s monetary-policy settings. I will expand on this in two respects: by elaborating a little on what I mean by dynamic prudential policy and how it fits into a regime for stability; and by contrasting the feasible options for communications policy available to, respectively, the Board and the FOMC.",
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.13,Promoting Economic Development in a Volatile World,April 2016,Sri Mulyani Indrawati,,,,Unknown,Unknown,Mix,,
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.15,Policy Imperatives for Boosting Global Growth and Prosperity,April 2016,David Lipton,,,Male,Unknown,Unknown,Male,"The weak recovery is taking place in the context of unresolved legacies. In many parts of Europe, for instance, sovereign and private sector balance sheets remain highly leveraged and banks’ nonperforming loans high. In the United States, aging-related spending pressures and unfulfilled infrastructure needs diminish economic prospects. And in Japan, deflation is putting the recovery at risk. At the same time, we are witnessing an emergence of new risks. The global economic slowdown is hurting bank balance sheets and financing conditions have tightened considerably. In emerging markets, excess capacity is being unwound through sharp declines in capital spending, while rising private debt, often denominated in foreign currency, is increasing risks to banks and sovereign balance sheets. Concerns about the global outlook have weighed heavily on world financial markets. The decline in equity price indices in 2016 so far this year has averaged over 6 percent, implying a loss of global market capitalization of over U.S. $6 trillion (or 8.5 percent of global GDP). This is roughly half the U.S. $12.3 trillion loss incurred in the most acute phase of the global financial crisis. Some Asian markets, such as in China and Japan, have been particularly hard hit, with losses of over 20 percent since the beginning of the year. Meanwhile, emerging market currencies have weakened, while their sovereign credit spreads have continued to widen—in Latin America and Africa by over 300 basis points over the past year. What may be most disconcerting is that the rise in global risk aversion is leading to a sharp retrenchment in global capital and trade flows. Last year, for example, emerging markets saw about $200 billion in net capital outflows, compared with $125 billion in net capital inflows in 2014. Trade flows meanwhile are being dragged down by weak export and import growth in large emerging markets such as China, as well as Russia and Brazil, which have been under considerable stress. Furthermore, inflation has fallen to historical lows. Headline inflation in advanced economies in 2015, at 0.3 percent, was the lowest since the financial crisis, and in emerging markets core inflation remains well below central bank targets. First, because protracted low global demand, and adverse feedback loops between the real economy and markets may generate additional deflationary pressures, putting us at risk of secular stagnation. Second, and equally relevant, is that labor supply and labor productivity growth have fallen considerably over the past decade, further aggravating these adverse dynamics. Although some aspects of the weak recovery are clear, we and many others in the policy world and in the markets are still debating and analyzing the role and the severity of several key transitions now under way:
 How will China’s transition—with the deceleration in export-oriented manufacturing activity and a pickup in sectors satisfying household demand—alter patterns of global trade and investment? Will the transition to lower oil and commodity prices be a plus, as predicted, or a minus? The expected pickup in consumption in commodity importers has been weaker than expected, possibly reflecting continued deleveraging in some of these economies and a limited pass-through of price declines to consumers. At the same time, declining prices have reduced investment in extractive industries, pushed some producers to or beyond the edge of profitability, and weighed on growth prospects for commodity-exporting countries. Will geopolitical tensions, the related refugee crisis, and global epidemics further increase uncertainty and weigh on economic activity? With all these uncertainties, even our latest baseline for global growth may no longer be applicable. In any case, the downside risks are clearly much more pronounced than before, and the case for more forceful and concerted policy action, has become more compelling.",
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.18,Household Economic Security and Public Policy,April 2016,Karen Dynan,,,Female,Unknown,Unknown,Female,"As is well known, market incomes of households in the lowest and middle parts of the distribution have failed to keep pace with growth of the overall economy during the past several decades. And, we have learned from the experience of the past several years that the underlying economic forces that generated that pattern have not abated. For example, as you can see in the slide (Figure 1), the Congressional Budget Office [2014] estimates that the lowest quintile of households saw an increase in their inflation-adjusted market incomes of 16 percent between 1979 and 2011, and those in the middle quintile saw an increase of just 9 percent. For context, aggregate real personal income per capita rose 75 percent over this period. In other words, if market incomes of the middle quintile had kept pace with overall per-capita income growth, those people’s incomes would have risen eight times as much as they actually did. While the CBO numbers end in 2011, more timely data, such as median household cash income from the Census, suggest that incomes for many households have continued to grow slowly. Real Income Growth Source: Middle Income Quintile Series are from the Congressional Budget Office [2014]. Real Aggregate per Capital Income is from the Bureau of Economic Analysis. What can public policy do to help? One set of appropriate policy responses aims to raise workers’ productivity by increasing their skills, especially for those in the lower part of the wage distribution. We can boost community college attendance, for example, by cutting net tuitions. We can strengthen Pell grants for low-income college students, for example, by indexing them to inflation and making other reforms that would encourage completion. We can improve worker training, for example, by increasing funding for job training, grants to promote education, and programs to increase apprenticeships. And, we can increase education at younger ages by expanding high-quality preschool, for example, by increasing funding for Head Start and supporting state initiatives in this area. Another set of appropriate policy responses is to encourage labor force participation so that more people earn wages and salaries. We can increase tax credits for working, for example, by expanding the earned income tax credit for childless workers. We can increase subsidies for childcare, for example, by boosting tax credits for childcare. We can help states to adopt paid leave policies. And, we can remove obstacles to entry into better jobs, for example, by reforming occupational licensing rules. A third set of appropriate policy responses is to strengthen the safety net. It is crucial that we keep the expanded health-care subsidies under the Affordable Care Act, which have reduced the number of people without health insurance so dramatically in the past few years. As you can see from the slide (Figure 2), that number has fallen nearly in half since the passage of the law. In addition, we must support programs with demonstrated long-term benefits for poor families. A small but growing body of high-quality research shows that children in poor families that receive certain health-care subsidies, housing subsidies, food assistance, and high-quality preschool earn substantially more later in their lives. We should also increase portability of noncash benefits of working such as pensions. Finally, we should strengthen insurance for people who are laid off—including enhancing unemployment insurance and expanding wage insurance. Individuals Without Health Insurance Source: National Health Interview Survey.",3
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.21,"Inequality, Mobility, Real Earnings, and Real Anger",April 2016,Jared Bernstein,,,Male,Unknown,Unknown,Male,,1
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.22,Policy Options to Address Slow Income Growth and Improve Income Mobility,April 2016,Scott Winship,,,Male,Unknown,Unknown,Male,,
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.17,Sluggish Wage Growth and Depressed Labor Force Participation,April 2016,Gregory Daco,,,Male,Unknown,Unknown,Male,"At a national level, the ECI, wages, and salaries for civilian workers has progressed quite slowly; a cumulative 15.1 percent since the end of the Great Recession, or less than 2 percent per year over the past eight years. Using Oxford Economics’ Global Economic Model, we find that each 1.0 pp boost to real income growth would have boosted consumer spending by 0.7 pp. As such, much of the postrecession consumer spending sluggishness can be attributed to slow wage growth. Overall, there is relatively little dispersion around mean wage growth, with the goods-producing sector up 15.1 percent and the services sector up 15.7 percent (Figure 2). However, two sectors lag behind the national average by about 3 percentage points: construction and leisure and hospitality. Interestingly, both of these sectors display evidence of high nominal wage rigidity. The idea, discussed by Federal Reserve Chair Yellen Janet [2012, 2014] on numerous occasions, is that businesses generally favored payrolls cuts, furloughs, and reduced-weeks over outright reductions in wages during recessions. As companies did not cut wages on the downside, they are more cautious about raising them on the upside. U.S.: Employment Cost Index in 2015:Q4 Source: Oxford Economics.",
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.16,The Defense Industry: Tradeoffs Between Fiscal Constraints and National Security Challenges,April 2016,Nayantara Hensel,,,Unknown,Unknown,Unknown,Unknown,,
51,2,Business Economics,11 July 2016,https://link.springer.com/article/10.1057/be.2016.19,Live Free and Prosper: Restoring America by Increasing Freedom,April 2016,Merrill Matthews,,,Male,Unknown,Unknown,Male,,
51,3,Business Economics,13 October 2016,https://link.springer.com/article/10.1057/s11369-016-0009-x,From the Editor,July 2016,Charles Steindel,,,Male,Unknown,Unknown,Male,,
51,3,Business Economics,19 September 2016,https://link.springer.com/article/10.1057/s11369-016-0011-3,Northern America’s Production of Technology Capital Is Transforming the World Economy,July 2016,Edward C. Prescott,,,Male,Unknown,Unknown,Male,"Business economists mostly use the language of microeconomic theory to address microeconomic problems. It is a good language for this purpose, and its use has improved the productivity of businesses. Some microeconomists use micro theory to address aggregate questions, where aggregate theory is needed, and typically fail spectacularly. I use the expression “aggregate economic theory.” Some of you probably wonder what I mean by it. For example, how is it different from micro theory? Both are neoclassical economics. But the languages used are different. In this lecture, I will attempt to make clear how they differ. The field of aggregate economic theory is young, having started about 1970 by Robert E. Lucas, Jr., who developed dynamic aggregate economic tools. These tools permitted the use of economic reasoning in the study of dynamic aggregate economic phenomena. A decade later, a methodology was developed that made aggregate economics a hard quantitative science. Much progress has been and is being made since then. Over the last 15 years, with improved sets of economic statistics and more powerful computational capabilities, aggregate economics has flourished. In this very short time, so much was learned. So much is being learned. The good news for quantitative aggregate economists is that so much remains to be learned. There is a unified aggregate theory. It accounts for the variations in output across countries and time. The same theoretical framework is used to understand (i) those aggregate fluctuations that are called business cycles, even though these fluctuations are not cyclical; (ii) the Great U.S. Depression that spanned the entire decade of the 1930s; (iii) growth miracles as occurred in Japan and the four Asian Tiger economies in the second half of the twentieth century and is happening in other Asian countries today; (iv) the variation in the value of corporations relative to gross national income; (v) the depressed state of the American economy over the last 7 years; (vi) the boom in hours worked in the last half of the 1990s [McGrattan and Prescott 2010b]; (vii) the depressed state of the Western European countries, where market hours are 30 percent less than those in other advanced industrial countries; (viii) the behavior of the U.S. current accounts; (ix) the consequences, or near lack thereof, of monetary policy for the behavior of aggregate output and employment; and (x) the consequence of China’s FDI policy for China and for the rest of the world. The theory is unified. Given a question, a model economy is constructed that is consistent with the assumptions made in previous successful applications of the theory. The model economy is selected from a parametric set of model economies to mimic reality on selected dimensions. Which dimensions depends upon the question. The aggregate model is restricted by micro observations as well as by aggregate statistics. The model, of course, is an abstraction and therefore false. It is a tool for drawing quantitative scientific inference concerning the given question. I emphasize, what is a good model to address a given question depends upon the question. There is no disciplined competing approach. There is the animal spirit crowd that can explain anything. They just say that the Great Depression during the 1930s and the current not so great depression are the result of contagious cases of laziness. This is not science. It is neo-Keynesian theory, and I don’t see it as being useful. Fortunately, there is established theory that has been tested through successful use. A science is successful if it is useful in dealing with the complex reality. Modern aggregate theory meets this test. Both the theme of this conference and the title of the lecture match well with what I will be talking about today. The theme is that the introduction of technology capital into the quantitative dynamic aggregate economic theory is a major step forward. It justified Adam Smith’s statement that the extent of specialization is limited by the extent of the market. Already there are three important applications, which I will briefly review, in this lecture. This introduction of technology capital is done in such a way that Adam Smith’s invisible hand works. The Schumpeterian argument that monopoly rents are needed to finance investments in technology capital and therefore Smith’s invisible hand cannot produce an efficient outcome is wrong. The first application is economic development, in particular why are there such large gains from openness. The second application is modeling the behavior of the capital accounts. The finding is that what many perceived as imbalances are just the result of world capital working. The third is evaluating China’s foreign direct investment policies for China and for the rest of the world. Technology capital is different than other forms of capital. Technology capital can be used at multiple locations throughout the world. Forms of technology capital include brand names and the technical knowledge embodied within a business organization. This feature of reality is brought into modern quantitative aggregate theory in a way that results in model economies where Adam Smith’s invisible hand works and the gains from specialization are limited only by the extent of the market. It overthrows the Schumpeterian view that monopoly rents are needed to finance investments in developing new technologies. An issue is, what are the empirical counterparts of various statistics of the model economy? The fact that they have the same name, such as aggregate output, does not mean that the reported aggregate output is the same statistic as aggregate output in the model economy. For example, reported aggregate output does not include most of intangible capital investment, and this component has been found to be big, as big as investment in tangible capital. The way this problem is handled is to introduce into the model economy the accountants who construct the accounts for the national economy. The model accountants construct for the model economy the statistics in the same way as accountants do for the actual economy. This means that quantitative aggregate theorists must know how the accountants constructed the statistics being used. Modern quantitative aggregate theory is a relatively recent development. It uses dynamic economic reasoning in constructing models to draw scientific inference. The basic theoretical framework is neoclassical growth theory with its aggregate production functions and aggregate households. Incidentally, a sequence-of-markets framework is used with the outcome at each market in the sequence being a valuation equilibrium. In Arrow–Debreu event-contingent valuation equilibrium theory, there is no concept of income. If the capital theoretic version with a sequence of markets is used, there are concepts of income and savings. This is the accounting system used by businesses to assess their performances and to determine their tax liabilities. It is the system used by national accountants. The production of technology capital, because it can be used at multiple locations, justifies Adam Smith’s statement that the degree of specialization is limited by the extent of the market. It is as if there were increasing returns to scale when there is not. Most had concluded incorrectly that to finance investment in developing superior technologies, there must be monopoly rents, and therefore Adam Smith’s argument that the invisible hand with competitive business enterprises is impossible, as there must be monopoly rents to finance investments in new technologies. The work reported on here extends now established dynamic aggregate theory to investment in better technologies in a way consistent with the working of Adam Smith’s invisible hand. If the valuation equilibrium has a Pareto optimum allocation, then we say that the invisible hand works, and the degree of specialization is limited by the extent of the market. This is the first of this lecture sequence delivered by a dynamic aggregate theorist. A number of the previous lectures were delivered by great economists. But they used the language of micro theory and not the language of aggregate economic theory, which was not yet developed when they made their contributions. Generally, equilibrium reasoning is fundamentally different than partial equilibrium reasoning. There is a unified dynamic quantitative aggregate theory. This theory has been tested through successful use. When using this theory to address an issue, the findings that economists come up with are the same independent of the political persuasions of the economists. The theoretical framework used is the neoclassical growth theory with its aggregate sector production functions and its aggregate household types. Two crucial allocation decisions are (i) the share of productive time allocated to market activities and (ii) the shares of output allocated to current consumption and to investment. The amount of output depends upon the quantities of inputs, namely the quantities of the services of human, tangible, and other forms of capital. Total output also depends upon the productivity of the business sector—that is, the number of units of the composite output produced by the business sector per unit of the composite input. Productivity is by far the most important factor in accounting for differences in output across countries at a point in time and across time. Productivity is used as a residual measurement of the performance of the regulatory and legal system governing the business sector in much the same way that profits are used to measure the performance of a business enterprise. Productivity comparisons across time in a given country are accounted for largely by growth in the stock of publically available knowledge useful in production. The productivity comparisons across economies at a point in time are accounted for largely by the nature of governance. Associated with a policy regime change, there is a transition to a new balanced growth path. Balanced growth paths all have the same growth rate of the real variables. This rate is the trend growth rate which is common across countries. Over the last 150 years, this rate has been approximately 1.8 percent per year, which implies a doubling of living standards every 39 years. Growth rates temporarily deviate from this common trend growth rate if there is a regime change. They differ only during the transition to the balanced growth path associated with the new policy regime. The point is to think in terms of relative levels, not growth rates. Solow [1957] made this point using the classical growth models of Solow [1956] and Swan [1956]. He repeatedly pointed out that saving rates do not determine growth rates.",1
51,3,Business Economics,21 September 2016,https://link.springer.com/article/10.1057/s11369-016-0007-z,Reflections on Macroeconomics Then and Now,July 2016,Stanley Fischer,,,Male,Unknown,Unknown,Male,"Well, are the answers all different than they were 50 years ago? No. The basic framework we learned a half-century ago remains extremely useful. But also yes: Some of the answers are different because they were not on previous exams because the problems they deal with were not evident 50 years ago. So the advice to potential policymakers is simple: Learn as much as you can, for most of it will come in useful at some stage of your career; but never forget that identifying what is happening in the economy is essential to your ability to do your job, and for that you need to keep your eyes, your ears, and your mind open, and with regard to your mouth—to use it with caution. Many thanks again for this award and this opportunity to speak with you.",1
51,3,Business Economics,05 August 2016,https://link.springer.com/article/10.1057/s11369-016-0002-4,Challenging the Groupthink of the Guild,July 2016,Kevin Warsh,,,Male,Unknown,Unknown,Male,"The central bankers and treasury officials of the largest economies are in strong agreement on one thing: the foreign economy is weak…I am still trying to figure that one out. The fall in global nominal GDP growth—by more than one-third—from the precrisis period to the post-crisis period is alarming. We’ve seen a further weakening of global economic growth since the summer of 2015. I suspect some members of our guild will be calling this ‘the newer normal’…as we persist in defining deviancy down. And we are told by the preachers of secular stagnation that our fates are sealed. This dangerous defeatism by some of the leading lights of our guild is owed more to rationalization than reflection, I am afraid. As the year advances, economic forecasts from the guild are likely to be marked down yet again…for the seventh straight year…some would call that a trend. And we should do our best to understand it, not excuse it. In the U.S., the Fed assures us that while growth has undershot its forecasts since the darkest days of the crisis, the unemployment rate improved faster—as if to suggest forecasting errors are two sided. Former Fed governor Larry Lindsey’s detailed analysis finds that the fall in the unemployment rate is a function of three phenomena, only one of which, I should note, offers any consolation: the drop in participation by those among the prime working-age population; weak productivity growth; and some modest real economic growth. Hard-working Americans are still waiting for the vaunted portfolio balance channel from quantitative easing (QE) to trickle down. The wealth effect is an older, more accurate moniker for what QE was hoping to accomplish. After all, the wealth effect works best for the wealthy. Our fellow Americans do not resent wealth accumulation by their fellow citizens. But they are rightly troubled by wealth increases that are bestowed by central bank policy rather than earned. The increase in household wealth is impressive for those who already possessed financial assets. But about half of American households have no accumulated balance sheet wealth. They are living on their stagnant incomes. And these hard-working folks are likely to be the hardest hit when the economy falters, and their incomes deteriorate. Perhaps that’s why we are seeing a surge in precautionary savings by consumers in recent months. And why the politics of 2016 seem at odds with the official government statistics which the Fed describes as nearing ‘full employment.’ We should recognize the broken compact between the government and the citizenry. Economic policymakers, not just central bankers, failed to deliver that which they promised. We should not risk the credibility of the guild or the public institutions we hold most dear by a pattern of overpromising and under-delivering. Time is short. Reform is urgently needed. The Fed is right that economies don’t die of old age. Economies die of policy error. So, let’s get to work to improve the conduct of monetary policy.",2
51,3,Business Economics,25 August 2016,https://link.springer.com/article/10.1057/s11369-016-0001-5,Finding the Equilibrium Real Interest Rate in a Fog of Policy Deviations,July 2016,John B. Taylor,Volker Wieland,,Male,Male,Unknown,Male,"The real equilibrium interest rate is usually defined as the real interest rate consistent with the economy reaching both potential output and price stability. In other words, it is the real interest rate where real GDP equals potential GDP and the inflation rate equals the target inflation rate.Footnote 1 The semi-structural time-series and DSGE models used to find this equilibrium real interest rate are complex and difficult to understand intuitively, but the logic—and thereby the pitfalls—can be explained in simple terms if we focus on three relationships common to macroeconomic models. The methodology described here is closest to that used by Laubach and Williams [2016], but we think it also applies to the model-based studies such as Barsky and others [2014], Curdia and others [2015], or Justiniano and Primiceri [2010].Footnote 2
 The first relationship is the intertemporal substitution equation (aka Euler equation or IS curve) between real GDP and the real interest rate. For simplicity, we can write this as a linear equation in terms of percentage deviations of real GDP from potential GDP and the deviations of the real interest rate from the equilibrium real interest rate: where y is the log of real GDP, y* is the log of potential GDP, r is the real interest rate, and r* is the equilibrium real interest rate. Assume that we have time-series observations on y and r, and that the parameter β can be calibrated or estimated. The second relationship is about price adjustment. Again for simplicity assume that it too can be represented as a linear equation, this time between the rate of inflation and the gap between real GDP and potential GDP: where π is the inflation rate. This equation could easily be generalized to incorporate staggered wage setting, expectations, or indexing as in a typical new Keynesian model, but the logic of the argument would not change. Note that having the gap on the right-hand side implies that y being equal to y* is consistent with price stability (steady inflation at a target, such as 2 percent). We assume that we have observations on π and that the parameter θ can also be calibrated or estimated. Model-based studies focus on these two relationships and the task is to use them to find the equilibrium real rate of interest r*. If one knew potential GDP (y*), then a seemingly reasonable method for finding r* would be to see if Eq. (1) generates an output gap (y − y*) that is different from what is predicted, P(y − y*), based on information on the right-hand side. If there is a difference, then one must adjust (the estimate of) r* up or down until it gives the correct prediction. For example, if (y − y*) < P(y − y*), then r* is too high and it must be lowered. Of course, y* is also unknown, but Eq. (2) can be used to help find it following the same logic used to find r*: If π is not equal to the prediction, Pπ, from Eq. (2), then adjust y*. For example, if π > Pπ, then lower (the estimate of) y*. Now consider the omitted variable problem. Suppose that another variable, or several variables, can shift the intertemporal relationship in Eq. (1) around. For example, costly regulations might lower the level of investment demand associated with a given real interest rate. This would mean that rather than Eq. (1) we would have Eq. (1′): where the variable x* could represent a variety of influences on real GDP from regulations that negatively affect investment to tax policy that negatively affects consumption. With Eq. (1′), if one finds that y − y* is lower than the prediction P(y − y*), then the implication is not necessarily that the estimate of r* is too high and must be lowered. Now, there is the possibility that x* is too low and must be raised. In other words, the possibility of an omitted variable that is not in the macro model makes it more difficult to find the equilibrium real interest rate. There is also another important problem of omission which makes it even more difficult to find r*. According to most macroeconomic models, there are also a financial sector and a central bank reaction function, which create another relationship. To capture this relationship, suppose we add a monetary policy rule to the model which makes the nominal interest rate and thus the real interest rate endogenous: where i is the nominal interest rate set by the central bank and d* is a possible deviation from the policy implied by the rule. As with Eqs. (1) and (2), if i is not equal to the prediction Pi, then one can adjust r*, but one can also adjust d*. For example, if i < Pi then one might conclude that it reflects a lower r*, but an alternative interpretation is a decline in d*. In fact, given what has happened to monetary policy in recent years around the globe, it would be a big mistake not to consider this. In Figure 1, which updates charts created by Hofmann and Bogdanova [2012] at the BIS, Hyun Shin [2016] shows how large and significant the variable d* has been around the world recently when the policy rule is the Taylor rule and r* is calibrated with respect to the estimated trend of output growth.Footnote 3
 It is our view that the existing studies referred to at the start of this paper underestimate the influence of x* and d* in their analysis and their search for the equilibrium real interest rates. This is easiest to see in the case of Laubach and Williams [2016] who use versions of Eq. (1) and (2), but it is also a good characterization of the DSGE models which do not include variables such as regulation and tax inefficiencies.",30
51,3,Business Economics,25 August 2016,https://link.springer.com/article/10.1057/s11369-016-0005-1,Why We Need Sound Public Finances,July 2016,Ludger Schuknecht,,,Male,Unknown,Unknown,Male,"Some economists and policymakers have been calling for more fiscal stimulus—even though growth in most advanced economies is near or above trend today, including in the eurozone, the United Kingdom, and the United States. I do not know any model that can rationalize further expansionary fiscal policies now. Moreover, public finances are only just being brought on a sustainable path again. The ratio of government debt in the G7 countries averages 120 percent of GDP, and it is over 100 percent of GDP in the United States and nearly 250 percent of GDP in Japan. Figure 1 illustrates that average debt ratios are back to levels that were posted after the Second World War. After 40 years of deficit spending and one-sided Keynesianism, we are now seeing debt ratios that 10 years ago would have caused a widespread sense of alarm; today we are almost used to it. 
Sources: IMF Historical Debt Database; own calculations. Development of the Public Debt Ratio of G7 Countries Consisting of Canada, France, Germany, Italy, Japan, United Kingdom and the United States as Percent of GDP Currently, we are not seeing any impact on interest rates on government debt, but this is due in part to the fact that monetary authorities are buying up government debt and thereby pushing down interest rates and spreads artificially. In that sense, we have partly switched off the signaling function of the markets regarding the sustainability of government debt. But this does not mean that we should engage in renewed fiscal expansion. If a recession were to occur and a country still features a significant deficit, as is still the case in some countries, fiscal policies have little leeway to react without deficits entering risky territory. My conclusion therefore is that we are not in a situation where public balance sheets would indicate much scope for fiscal expansion. On the contrary, there is still a need for completing fiscal consolidation as part of a medium-term strategy to reduce deficits and debt in some countries.",1
51,3,Business Economics,08 August 2016,https://link.springer.com/article/10.1057/s11369-016-0006-0,The Case for an Active Fiscal Policy in the Developed World,July 2016,Angel Ubide,,,Male,Unknown,Unknown,Male,,1
51,3,Business Economics,11 August 2016,https://link.springer.com/article/10.1057/s11369-016-0003-3,Housing Conundrum: A Shortage of Demand or Supply?,July 2016,Stephen D. Oliner,,,Male,Unknown,Unknown,Male,,2
51,3,Business Economics,14 September 2016,https://link.springer.com/article/10.1057/s11369-016-0008-y,The US Aerospace Industry: A Manufacturing Powerhouse,July 2016,Maksim Soshkin,,,Male,Unknown,Unknown,Male,"The aerospace manufacturing industry is one of the United States’ few production powerhouses. The industry is a vital component of globalization, creating products necessary for worldwide interconnectivity. Commercial aircraft allow for the rapid transportation of people and goods throughout the world. Just over half of international trips and 35 percent of the value of goods transported are conducted by air [United Nations World Tourism Organization 2015; Air Transport Action Group 2016]. Furthermore, the development of space launch vehicles and spacecraft has led to satellites that enable everything from instant worldwide communication to observation and navigation. Aerospace products such as combat aircraft and missiles have also become an essential part of warfare and account for a large share of United States’ government spending. As a result, aerospace manufacturers lay at the nexus of business, globalization, politics, and fiscal policy. From the U.S. economic prospective, the aerospace manufacturing industry is a major source of well-paid employment, investment, and exports. The industry employed nearly 490,000 people in 2015, generating $47.5 billion in wages. These figures translate into nearly 5 percent of employment and about 8.7 percent of wages in the US manufacturing sector. In particular, industry average wages reached nearly $98,000 in 2015, which is 72 percent higher than the overall manufacturing average [IBISWorld Staff 2016a, b]. Moreover, the industry has been able to successfully supply the growing global demand for commercial aircraft and defense equipment, accounting for an estimated 9.6 percent of the value of goods exported by the United States. Buoyed by rising global and domestic demand, after 2010, the U.S. industry revenue climbed at a rate of 6.3 percent to a level of $259.2 billion in 2015 (Figure 1). 
Sources: IBISWorld Staff [2016a]. Aircraft, Engines & Parts Manufacturing in the U.S, Industry Report 33641a and Space Vehicle & Missile Manufacturing in the U.S, Industry Report 33641b. The U.S. aerospace manufacturing industry’s revenue This paper will review ongoing developments in the demand for various types of the products produced by the industry, and discuss the competitive environment faced by the firms. In general, private demands appear to be strong, while military and space demand seems to be stabilizing after some tailing-off. Overall, the sector looks like it will sustain its role as a robust part of the U.S. manufacturing.",5
51,3,Business Economics,09 September 2016,https://link.springer.com/article/10.1057/s11369-016-0010-4,When Technical Skills Are Not All You Need to Succeed,July 2016,Constance L. Hunter,,,Female,Unknown,Unknown,Female,,1
51,3,Business Economics,08 August 2016,https://link.springer.com/article/10.1057/s11369-016-0004-2,Concrete Economics: The Hamilton Approach to Economic Growth and Policy,July 2016,Francis Schott,,,Male,Unknown,Unknown,Male,,
51,4,Business Economics,02 December 2016,https://link.springer.com/article/10.1057/s11369-016-0019-8,From the Editor,October 2016,Charles Steindel,,,Male,Unknown,Unknown,Male,,
51,4,Business Economics,18 October 2016,https://link.springer.com/article/10.1057/s11369-016-0012-2,Ultra-Easy Money: Digging the Hole Deeper?,October 2016,William R. White,,,Male,Unknown,Unknown,Male,"Let me begin by saying that it is a great honor to have been awarded the Adam Smith prize. I am conscious of both the importance of the awarding body and the distinguished list of previous recipients. Perhaps even more important, I recognize that my policy views diverge significantly from what has, at least to date, been mainstream thinking about monetary policy. I thank you for your open mindedness and the opportunity to bring these views to a wider audience. There should be no monopoly on “truth” in this crucially important area, particularly given how frequently and radically views about the conduct of monetary policy have changed over the last fifty years or so.Footnote 1
 It is broadly agreed that the decline in U.S. house prices late in 2005 was the initial phase of the subsequent economic and financial crisis in the United States. Since then all parts of the world economy have come to bear its imprint, with many harboring fears that the Second Great ContractionFootnote 2 is by no means over. The duration, scope, and magnitude of what has happened cannot be explained by a process of contagion. Rather, there were credit driven “imbalances” accumulating in the complex, adaptive system we know as the global economy. The collapse of the subprime mortgage market in the United States, and the complex financial instruments based on such mortgages, was simply the trigger that revealed a prevailing systemic fragility. In this presentation I will try to trace the origins of the crisis, and the particular contribution made by expansionary monetary policies before (unnaturally easy) and after (ultra-easy) the crisis broke. I will contend that the situation we face in late 2016, both in the advanced market economies (AMEs) and the emerging market economies (EMEs), is arguably more fraught with danger than was the case when the crisis first began. By encouraging still more credit and debt expansion, monetary policy has dug the hole still deeper. Accordingly, I will finish by suggesting some government policies that might be more effective in restoring the “strong, sustainable and balanced growth” desired by the leaders of the G20. I am aware that the current consensus is that global economic prospects are likely to improve next year. I would remind you, however, that actual outturns have generally been weaker than predicted (as of the previous spring) in each of the last seven years. This is not surprising since the models underlying most forecasts (including those of the Fed, OECD and IMF) do not adequately recognize the vital importance of credit and the financial system. The fundamental ontological error has been to model the economy as a relatively simple machine, whose properties can thus be known and controlled by its policy operator. In reality, it is an evolving system, too complex to be either well understood or closely controlled. Moreover, it is a system in which stocks and “imbalances” build up over time in response to monetary stimulus. This reality makes future prospects totally path dependent, and we are on a bad path. For the same reason, it is also overly simplistic to suggest that central banks should reduce the “financial rate” of interest in response to a presumed fall in the “natural rate” of interest (the expected rate of return on capital) since the crisis started.Footnote 3 If expected profits have collapsed as a side effect of monetary policies followed in the past, this hardly seems a justification for maintaining such policies. A simple, single period model, stripped of all policy side effects except near-term inflation, is simply not adequate to deal with such dynamic processes. It will be argued below that other side effects, particularly those affecting supply potential and financial instability, demand much greater attention. Looking at the individual regions in the global economic system also reveals potential weaknesses. The United States is furthest ahead in the recovery but faces declining labor participation rates and (like others) weak capital investment. With “potential” lower, the risks of inflation are higher. Europe faces its own idiosyncratic problems, not least a still weak banking system and potential fallout from the vote on Brexit. Japan is conducting an unprecedented experiment with “Abenomics,” but inadequate results to date suggest even greater experimentation going forward. China must make a transition to a different growth model, based on internal consumption, but all transitions are difficult and carry significant risks. Moreover, in our increasingly integrated global economy, problems anywhere will quickly become problems everywhere. As an example, think of the implications of China’s slowdown for other emerging markets and beyond, particularly for commodity producers. Note too that the EMEs have expanded rapidly in size in recent decades and developments there are now likely to have a big effect on AMEs. In sum, there are valid reasons for concern about the prospects for the global economy.",8
51,4,Business Economics,21 November 2016,https://link.springer.com/article/10.1057/s11369-016-0018-9,The Business Economist’s Toolkit,October 2016,Lisa Emsbo-Mattingly,,,Female,Unknown,Unknown,Female,"Data analytics come in multiple forms, econometrics is one example. My rule of thumb is there’s never enough econometrics out there. Understanding nonlinear systems is another example. How did what many perceived to be as a little subprime problem turn into the great financial crisis? And finally, behavioral economics helps us understand the intricacies of human decision-making. When it comes to data analytics, I would encourage you to be agnostic, and take advantage of the countless data available to help us do your job better.",1
51,4,Business Economics,07 November 2016,https://link.springer.com/article/10.1057/s11369-016-0015-z,Manipulation and Deception as Part of a Phishing Equilibrium,October 2016,George A. Akerlof,Robert J. Shiller,,Male,Male,Unknown,Male,"The word phish, according to the Oxford English Dictionary, was coined in 1996 as the Internet was getting established. That dictionary defines phish as “To perpetrate a fraud on the Internet in order to glean personal information from individuals, esp. by impersonating a reputable company; to engage in online fraud by deceptively ‘angling’ for personal information.” We are creating a new, broader meaning for the word phish here. We take the computer definition as a metaphor. Rather than viewing phishing as illegal, we present a definition for something that is much more general, that goes much further back in history. It is about getting people to do things that are in the interest of the phisherman, but not in the interest of the target. It is about angling, about dropping an artificial lure into the water, and sitting and waiting as wary fish swim by, make an error, and get caught. By the laws of probability, there are so many phishers and they are so ingenious in the variety of their lures that we all get caught sooner or later, however wary we may try to be. No one is exempted. By our definition, a phool is someone then who, for whatever reason, is successfully phished. There are two kinds of phool, psychological and informational. Psychological phools, in turn, come in two types. In one case, the emotions of a psychological phool override the dictates of his common sense. In the other case, cognitive biases, which are like optical illusions, lead him to misinterpret reality, and he acts on the basis of that misinterpretation. Information phools act on information that is intentionally crafted to mislead them. Enron stockholders are an example. The rise of Enron was based on the adoption of misleading (and, then later, fraudulent) accounting. Its extraordinary profits were the result of its “mark-to-market” accounting, whereby future expected profits from an investment could be booked when the investment was made. The more usual practice is to wait until the profits are actually realized. From 1996 to 2001, Fortune named Enron the country’s Most Innovative Company. That was right; Fortune just failed to understand the nature of Enron’s innovations. Whether or not businessmen have good (or bad) morals is not the subject of this book, although sometimes both of these sides will appear. We see the basic problem as lying, instead, in the nature of competitive markets. They are terrific at incentivizing and rewarding businessmen heroes with innovative new products for which there is a real need. However, unregulated free markets rarely reward the different kind of heroism, of those who restrain themselves from taking advantage of customers’ psychological or informational weaknesses. Managers who restrain themselves in this way tend to be replaced through competitive pressures by others with fewer moral qualms. Civil society and senses of business integrity from social norms do place some brakes on such phishing; but, in the resulting market equilibrium, if there is an opportunity to phish, even firms guided by those with real moral concerns will usually have to do so in order to survive. We anticipated as we wrote this book that it would be unpopular (to say the least) with those who think that people all but invariably make the best decisions for themselves. Indeed, the commentary we received was mixed, sometimes positive, sometimes downright hostile. Of those who were critical, the essence seemed to be to ask “who are Bob and George to say that individual people are not themselves—always and invariably—the best arbiters of the decisions that affect them?” We do not have to be presumptuous to see that people are making such decisions. We know because we see people making decisions that NO-ONE-COULD-POSSIBLY-WANT. Henry David Thoreau remarked that “the mass of men lead lives of quiet desperation.” Remarkably, a century and a half later, in the US, almost the richest country the world has ever known, much too much of our lives are still led in quiet desperation.",6
51,4,Business Economics,01 November 2016,https://link.springer.com/article/10.1057/s11369-016-0013-1,The Evolving Contours of Productivity Performance and Automation Investment in U.S. Manufacturing,October 2016,Cliff Waldman,,,Male,Unknown,Unknown,Male,"For all of its importance, productivity is really a straightforward concept. It is a measure of how much output the economy—or some sector of the economy-produces for a given amount of inputs. Symptomatic of an increasingly data-rich period, there has been growth in the number of metrics used to gauge productivity performance. Two metrics, however, remain critical and dominate economic analysis as well as public discourse. Chief among them is labor productivity, defined as the ratio of the output of goods and services to the labor hours used in their production. Most commonly, labor productivity is measured as the output per hour of “all persons.” A second measure, multifactor (or sometimes called “total factor”) productivity measures the output generated by a combination of inputs. In U.S data this combination consists of capital, labor, energy, raw materials, and business services (often denoted by the acronym KLEMS). Productivity performance impacts long-term economic growth, wage growth, and competitiveness. The potential growth of an economy, which is the sustainable rate of output gain without inflation, is the sum of the growth of labor productivity and the growth of labor hours. In the manufacturing sector, strong productivity performance is needed to meet the globally driven challenges of cost pressures and competitiveness. Thus, for both manufacturing and the economy as a whole, the dramatic slowdown in productivity growth in the years following the deep and destabilizing 2007–09 recession is of increasing concern. Figure 1 shows a 4-year moving average of the growth of non-farm labor productivity in the U.S. The data extend back to 1951. The history supports the general notion of productivity cycling, pronounced slumps followed by pronounced accelerations. 
Source(s): U.S. Bureau of Labor Statistics. U.S. Non-Farm Labor Productivity Growth, 4-Year Moving Average For much of the 1950s, there was a slowing trend in labor productivity growth, which reversed after 1959 with productivity growth peaking in 1964 and 1965. Volatility aside, it is clear that from the mid-1960s to the early 1980s there was a sustained slowing in labor productivity growth to near zero in 1982. A partial rebound pushed productivity performance into a moderate range, where it stayed until the mid-1990s. Subsequently, the now well-recognized information technology (IT) boom catalyzed a reacceleration of productivity growth to the peak rates of the 1960s. The post-1996 productivity acceleration reached an apex during 2003. This was followed by a lengthy slowdown that bottomed in 2008. A short-lived post-recession bounce peaked in 2010.Footnote 1 The dramatic subsequent slowing took labor productivity growth to 0.6 percent for the latest data in 2015. Figure 2 shows a 4-year moving average of labor productivity growth in the U.S. manufacturing sector. Unfortunately, these data only extend back to 1991. As is the case for the total economy, the data point to an IT-led productivity acceleration which peaked in 2005, decelerated to 1.6 percent by 2009 and then experienced a post-recession bounce to 3.5 percent in 2010. In spite of an upward blip to 4.4 percent during 2013, labor productivity growth in manufacturing decelerated to 0.4 percent by 2015, the weakest in this history. 
Source(s): U.S. Bureau of Labor Statistics. U.S. Manufacturing Labor Productivity Growth, 4-Year Moving Average Fortunately, while concerns about manufacturing productivity are growing, so are opportunities for real change. Rapid and potentially disruptive technological advancement is holding out the prospect of significant evolution in the very nature of manufacturing. The range of such advancements that are slowly working their way into goods supply chains around the world is daunting. From 3D printing to robotics to software-led factories, technology is high on the list of topics for all who are concerned with the future of the factory sector in the U.S. Two forces are playing a role in the technological dynamism of the current period. The first is the growing intensity of global manufacturing competition. The advancing integration of heavily populated low labor cost countries into the global trading network and the ever-tightening connection between the manufacturing sectors of key countries, through not only increasingly complex supply chains but also innovation spillovers and trade channels, has intensified cost pressures and performance pressures. U.S.-based manufacturers began to address such challenges in the 1990s with the implementation of lean manufacturing, a production paradigm adopted from the Toyota of the 1990s and the Ford Motor Company of the early part of the 20th century. But the lean journey only set the framework for change that is now moving to a second stage with new technology and a new vision for what a factory looks like, how it operates, and what it actually does. The second motivator takes the form of a host of demographic shifts that have been occurring across the world. Population growth and labor force growth are slowing not just in the advanced economies but also now in many emerging market nations. Key countries in the developed and developing world are feeling the pressure of lower births, declining labor force shares of their total populations, and a swelling retirement cohort. There are powerful exceptions, most notably India and most of the African continent, but for much of the world labor is becoming an increasingly scarce input for production. With intensified competition on cost and performance and slowing labor supply growth, it is certainly reasonable to predict that technology innovators will see increased demand for their long-simmering innovations as manufacturers have a growing incentive to make the most effective use of these promising new tools that they possibly can. But with all the public dialogue about new manufacturing technologies and manufacturing automation investment in particular, there are little or no data that offer a coherent picture of the automation investment dynamic. How much automation investment has there been? What are the short-term drivers? What is the outlook for automation investment? The interesting reality of slowing manufacturing productivity growth, on the one hand, and an expanding basket of new manufacturing technologies on the other, motivates three tasks for this study. First, an understanding of the reasons for the manufacturing productivity slowdown is necessary for prescribing solutions. Second, credible data on manufacturing automation investment are needed. Finally, a consideration of the relationship between the manufacturing productivity slowdown and the positive manufacturing automation technology shock offers a rich framework for effective policy analysis. In the next section, I review key papers from the large economics literature on productivity performance. Section 3 displays a wide span of rankings and other descriptive data in order to draw a picture of subsector dynamics in the U.S. productivity evolution. Section 4 postulates a testable framework for identifying key drivers of labor productivity growth and multifactor productivity growth for a sample of manufacturing subsectors and Section 5 reviews the empirical results. Section 6 describes the survey method that was used to gather national data on U.S. manufacturing automation investment behavior. Section 7 presents the survey results. Section 8 analyzes the automation dynamic suggested by the survey results and suggests a framework for considering the still unresolved question of the relationship between automation activity and productivity performance in manufacturing. Section 9 offers a forecast of manufacturing productivity performance in light of the results of this study. Section 10 concludes by discussing the policy implications of this study.",4
51,4,Business Economics,19 October 2016,https://link.springer.com/article/10.1057/s11369-016-0014-0,A New Framework to Estimate the Near-Term Path of the Fed Funds Rate,October 2016,Sam Bullard,Azhar Iqbal,John Silvia,,,Male,Mix,,
51,4,Business Economics,01 November 2016,https://link.springer.com/article/10.1057/s11369-016-0017-x,Is Predicting Recessions Enough?,October 2016,Azhar Iqbal,John Silvia,,,Male,Unknown,Mix,,
51,4,Business Economics,25 October 2016,https://link.springer.com/article/10.1057/s11369-016-0016-y,Does Economic Activity Slow in Election Years?,October 2016,Michael A. Brown,Michael Pugliese,,Male,Male,Unknown,Male,"Academic researchers and commentators have drawn attention to the potential link between the heightened political uncertainty experienced in presidential election years and the impact this uncertainty may have on the economy [Gulen and Ion 2016; Julio and Yook 2012; Jamrisko 2016]. While equity market trends in presidential election years have been well documented [Allvine and O’Neill 1980; Colón-De-Armas 2013], we seek to analyze the performance of real economic variables during presidential election years. In this report, we look at some of the key economic variables typically used by the National Bureau of Economic Research (NBER) to date business cycles and compare the performance of these economic indicators between presidential election and non-election years since 1960. Our results suggest that the general argument that uncertainty during presidential election years results in slower economic activity does not hold water. In fact, based on our analysis, we find that real GDP growth, real consumer spending growth, real business fixed investment growth, real disposable income growth, and industrial production growth are actually stronger during presidential election years compared to non-election years (Figure 1). We find that there are no statistically significant differences in total real government spending growth, real federal government spending growth, or job growth between presidential election and non-election years. In short, we do not find any evidence of adverse effects on economic activity in presidential election years. We conclude by exploring several possible explanations for stronger real economic growth in election years, including party control of the White House, the share of quarters spent in recession during election and non-election years, and structural economic differences between periods within our sample. 
Sources: U.S. Depts. of Commerce & Labor, Federal Reserve Board and Wells Fargo Securities. U.S. Economy in Presidential Election Years",
52,1,Business Economics,12 May 2017,https://link.springer.com/article/10.1057/s11369-017-0030-8,From the Editor,January 2017,Charles Steindel,,,Male,Unknown,Unknown,Male,,1
52,1,Business Economics,25 April 2017,https://link.springer.com/article/10.1057/s11369-017-0028-2,Comments at the Panel “Responders of First or Last Resort: Central Bank Strategies in an Era of Ultra-Low Interest Rates”,January 2017,Charles L. Evans,,,Male,Unknown,Unknown,Male,,
52,1,Business Economics,28 March 2017,https://link.springer.com/article/10.1057/s11369-017-0031-7,Monetary Policy Accommodation at the Lower Bound,January 2017,Signe Krogstrup,,,Female,Unknown,Unknown,Female,"It is often claimed that monetary policy has become ineffective since the global financial crisis, based on a perception that monetary policy has been ultra accommodating, yet the recovery has been delayed in some countries, or not yet in sight in others. The implications of this view is that monetary policy cannot address such shocks and should not be used, or in the extreme, that monetary policy accommodation may somehow perversely have been responsible for the delayed recovery, and should therefore have been removed or reversed much sooner. However, monetary policy has not been ultra-accommodating, and hence, there is little reason to conclude that monetary policy accommodation has become ineffective or contractionary. Instead, the lower bound on interest rates and constraints on the adoption of unconventional measures prevented sufficient monetary accommodation since the financial crisis in most advanced economies. Monetary policy has consequently been too tight despite low nominal interest rate levels and unprecedented expansions of central bank balance sheets. Nominal policy rates and quantitative measures are just tools, and do not measure monetary policy accommodation per se. What matters is how these tools affect real interest rates relative to neutral real rates. The distinction between reduced monetary policy ineffectiveness and constraints on monetary policy accommodation is crucial. It suggests that it is the lack of sufficient space for monetary accommodation at the lower bound, rather than ultra-accommodating monetary policy, that may have prolonged the recession, and that raising interest rates sooner could have made matters much worse. These points are illustrated with two examples. First, an assessment of the degree to which monetary policy has been accommodating in the US suggests that the lower bound prevented monetary policy from sufficiently accommodating, given the large negative financial shock of 2008, the subsequent decline in inflation and inflation expectations, and drops in the US neutral real interest rate. Second, an assessment of the degree of accommodation achieved through negative interest rates in the five countries that have adopted these, shows that cuts into negative territory were too small to meaningfully reduce real interest rates, given the declines in inflation expectations that they were generally responding to. With the benefit of hindsight that we have today, unconventional measures and negative interest rates should have been adopted faster and more forcefully in response to the crisis, to the extent that this had been possible. The many constraints and uncertainties associated with unconventional measures may prevent central banks from effectively and sufficiently accommodating at the lower bound in the future too, however. It is therefore urgent that a broader discussion takes place on how to adapt monetary policy tools and frameworks, as well as the fiscal monetary policy mix at the lower bound, to create space for an effective policy response to shocks in a low real interest rate future.",
52,1,Business Economics,27 March 2017,https://link.springer.com/article/10.1057/s11369-017-0029-1,The Man Who Knew: The Life and Times of Alan Greenspan by Sebastian Mallaby,January 2017,William Poole,,,Male,Unknown,Unknown,Male,"Here is the summary conclusion of the FCIC on Greenspan’s responsibility: “The prime example [of pervasive permissiveness] is the Federal Reserve’s pivotal failure to stem the flow of toxic mortgages, which it could have done by setting prudent mortgage-lending standards. The Federal Reserve was the one entity empowered to do so and it did not” (Financial Crisis Inquiry Commission 2011, p. xvii). Consider assignment of responsibility for a murder. A cop might have prevented it. But doesn’t the blame rest squarely with the person who pulled the trigger rather than the cop? To me, the conclusion that Greenspan is responsible for the crisis because he was an inadequate regulator is simply strange. The argument that Greenspan’s monetary policy caused the bubbles deserves attention—an issue I take up below. Of course, just as we might investigate what clues the cop missed, so also we should investigate what regulators might have done. Nonetheless, the regulator is not the guilty one. The FCIC blames the cop for the murder. The FCIC Majority Report discusses at various places, from page 178 to page 506, conclusions drawn from its interview on March 4, 2010 with Thomas Lund, former EVP of Fannie Mae Single Family Business  (Financial Crisis Inquiry Commission 2010a).Footnote 5 A huge amount of material is now available but it would most likely not have been available to Mallaby as he was finishing the book. However, could Mallaby have interviewed Lund? Given controversy over the role of affordable housing goals, several such interviews would have made perfect sense. When you read both the Lund interview and the FCIC Majority Report, you realize how selective the Report is in quoting what Lund said. Consider four examples from the Lund interview with the staff of the FCIC: (a) “In his role as Chief Acquisition Officer, Mr. Lund had increased interaction with regulators from OFHEO and HUD.Footnote 6 He stated that he felt pressure to meet HUD housing goals. The housing goals were raised in 2000 and again, substantially, in 2004, and Fannie Mae received pressure from the administration to expand its business beyond the traditional 15- and 30-year fixed-rate mortgages that were previously the core of the Company’s business.” (b) “Mr. Lund said that he was extremely concerned about the impact of some of these new initiatives and the risk associated with expanding into riskier, nontraditional mortgage products to meet housing goals. He said that he was in constant communication with HUD about housing goals, but the goals were still raised.” (c) “Mr. Lund wanted to have the housing goals declared ‘unfeasible’ since the only way Fannie Mae could meet the goals was to access the nontraditional mortgage market and expand into riskier products since these borrowers were not going through traditional channels.” (d) “Mr. Lund responded that Fannie Mae had to cross-subsidize products ‘all day long.’ Mr. Lund explained that the 15-year, fixed-rate mortgage was the most profitable product, and if the GAP reports were examined, it was obvious that the Company used its profits on the 15-year, fixed-rate product to subsidize the rest of its product offering” (Financial Crisis Inquiry Commission 2010a, pp. 2–3). The FCIC essentially ignored Lund’s information and instead argued that Fannie’s actions were driven by a desire to maintain market share in what the Commission majority claimed was the profitable subprime market. That cannot be correct if Fannie “had to cross-subsidize products ‘all day long.’” A Mallaby conclusion: “Greenspan was ultimately guilty of one serious analytical error—admittedly, a consequential one. Although he understood the frailty in finance, he underestimated the cost in doing little about it” (p. 675). The reader of the biography should understand that Mallaby did not, apparently, believe that he needed to explain why the crisis occurred. Greenspan is obviously the villain. Peter Wallison argues—persuasively—that Fannie Mae and Freddie Mac accumulated subprime paper on their own balance sheets under pressure from HUD. HUD in turn was acting in response to political pressures from Congress, President Clinton and later President George W. Bush. Here are some quotes from Wallison’s work that state the points clearly. In his Dissent to the report of The National Commission on The Causes of The Financial And Economic Crisis In The United States, Wallison states that … I believe that the sine qua non of the financial crisis was U.S. government housing policy, which led to the creation of 27 million subprime and other risky loans—half of all mortgages in the United States—which were ready to default as soon as the massive 1997–2007 housing bubble began to deflate. If the U.S. government had not chosen this policy path—fostering the growth of a bubble of unprecedented size and an equally unprecedented number of weak and high risk residential mortgages—the great financial crisis of 2008 would never have occurred. … Ultimately, all these entities [Fannie Mae, Freddie Mac and some others]… were compelled to compete for mortgage borrowers who were at or below the median income in the areas in which they lived. This competition caused underwriting standards to decline, increased the numbers of weak and high risk loans far beyond what the market would produce without government influence, and contributed importantly to the growth of the 1997–2007 housing bubble (Wallison 2011, p. 444). … …Although at the end of 2005 Fannie was exposed to $311 billion in subprime loans, it reported in its 2005 10-K (not filed with the SEC until May 2, 2007) that: ‘The percentage of our single-family mortgage credit book of business consisting of subprime mortgage loans or structured Fannie Mae MBS backed by subprime mortgage loans was not material as of December 31, 2005’. [emphasis supplied] Fannie was able to make this statement because it defined subprime loans as loans it purchased from subprime lenders… (Wallison 2011, p. 467). The conventional “Greenspan guilty” view has been that the private sector originated and spread subprime debt to private bank and nonbank balance sheets around the world. Wallison demonstrates that the conventional facts are simply wrong. “… government agencies now look to have guaranteed, originated or underwritten 60 percent of all ‘non-traditional’ mortgages, which totaled $4.6 trillion in June 2008”Footnote 7 (Wallison 2015, pp. 5725–5727). The GSEs were desperate to increase their holdings of mortgages that met the ever-expanding affordable housing targets set by HUD after 1992. These targets required loans to homebuyers below median income, and subprime credit was the only way to create an adequate supply of such mortgages. In his dissent and his book, Wallison offers extensive data and quotes from various HUD, internal Fannie and Freddie memos and other documents to support his case. You will have to read Wallison’s book to understand the full extent of his argument and evidence. Mallaby could have done so, but apparently did not. Mallaby refers briefly to the Commission’s Report, but not to Wallison’s Dissent to the majority position in the report. Yes, investment banks and some commercial banks bought subprime paper and should be held responsible for doing so. Yes, investment banks securitized some of this paper. That was a mistake for which managements and directors are responsible. However, those were not Greenspan mistakes nor did the Federal Reserve have regulatory authority over investment banks. The question on Greenspan is whether he could have stopped the GSEs and other investors from buying subprime mortgages, either directly or by cutting off the supply from Countrywide and other originators of subprime mortgages. Keep in mind that the affordable housing goals were policy of the United States Congress and HUD under both Clinton and Bush. If Greenspan had attempted a frontal assault, would many members of Congress and affordable housing groups have attacked without mercy? Attempting to confine Fannie and Freddie to properly underwritten prime mortgages would have failed. Greenspan would have then been a villain because he wanted to block low-income families, especially minority ones, from buying houses, contrary to the policy of Congress. Keep in mind also that, due to accounting irregularities at both Fannie and Freddie, neither firm presented standard financial reports for several years. When they finally did, as Wallison demonstrates, they presented misleading reports. They lied. In December 2011—well before Mallaby finished writing—the SEC sued the companies and six of their former officials alleging misrepresentation of the financial condition of the firms. The complaint alleged that as of March 31, 2007 Fannie disclosed Alt-A mortgage exposure of 11 percent of its book of business whereas the actual figure was 18 percent. The SEC also alleged that as of the second quarter of 2008 Freddie Mac disclosed total subprime exposure of $6 billion when the actual exposure was $250 billion and for Fannie the disclosed exposure was $ 8 billion whereas estimated actual was $110 billion. The SEC complaint would not have been available to Greenspan attempting to make a public case to rein in the GSEs. Wallison’s Dissent contains page after page of tables and quotes from internal Fannie, Freddie and other documents. With respect to non-traditional mortgages (NTMs), Wallison concludes: The use of the affordable housing goals to force a reduction in the GSEs’ underwriting standards was a major policy error committed by HUD in two successive administrations, and must be recognized as such if we are ever to understand what caused the financial crisis. Ultimately, the AH goals extended the housing bubble, infused it with weak and high risk NTMs, caused the insolvency of Fannie and Freddie, and—together with other elements of U.S. housing policy—was the principal cause of the financial crisis itself (Wallison 2011, p. 518). If Greenspan could not have used regulatory authority to restrict subprime mortgage issuance, could he have raised the federal funds rate sufficiently to pop the bubbles? John Taylor has argued that he could have and should have because interest rates were too low during the 2002–05 period. My view, biased no doubt because I was a member of the FOMC at the time, is that the macroeconomy was doing pretty well. Thus, the right way to think about the issue is in the context of a macroeconomy in decent shape, gradually recovering from the 2001 recession, but with the housing sector out of equilibrium. Was Fed monetary policy too easy 2002–05? Perhaps, but the magnitude of the error seems too small to explain the financial crisis. In assessing the Fed’s monetary policy role, an evaluation must come to grips with the enormous GSE contribution to subprime loan origination, a magnitude described earlier in this paper based on Wallison’s data. If the GSEs had continued to buy and securitize prime mortgages only—their traditional function—they would have owned and guaranteed millions fewer subprime mortgages. Moreover, the GSE activity pumped up the size and profitability of subprime mortgage originators; thus, most likely, without the GSE demand for subprime mortgages there would also have been fewer private (non-agency) subprime mortgages. The house price bubble could not have inflated to the extent it did, nor would the deflation have been so painful as subprime mortgages failed. All of us knew that there were subprime mortgage abuses. Let me speak of my own attitude here. Financial abuses have a long history. Charles Ponzi has a particular scheme named after him and frauds of this type continue to this day. I did not and do not condone subprime mortgage abuse schemes.Footnote 8 To my knowledge, none of us realized that the GSEs were the principal source of demand for subprime mortgages. I had always understood that Fannie and Freddie securitized prime mortgages. While in office, I gave several speeches warning of GSE dangers, but I focused on interest-rate risk and thin capital rather than credit risk. Alan Greenspan retired from the Fed at the end of January 2006, at almost the exact peak of U.S. house prices. Could he, “The Man Who Knew,” have known or figured out the scale of issuance of subprime mortgages? The extensive evidence Wallison accumulated, most of it after the crisis, was not available to Greenspan. To my knowledge, the Fed did not have any moles inside Fannie or Freddie, or HUD for that matter, with access to the information we now have. As noted earlier, Wallison quotes Fannie’s statement that its “subprime mortgage book was not material as of December 31, 2005.” That statement was in Fannie’s 2005 10-K report, filed May 2, 2007, which was two years late. How could that be? Both Fannie and Freddie had accounting irregularities so severe that the auditors would not sign their financial statements. I have no idea what Greenspan knew about the financial condition of Fannie and Freddie in the period they were not issuing financials, but would it not have been an appropriate question for Mallaby to put to Greenspan in one of his many interviews? Mallaby asserts that, “… as Fed chairman, Greenspan turned away suggestions that the Fed should regulate the GSEs, preferring to encourage reformers privately behind the scenes rather than joining the front lines of the struggle” (p. 678). Did Mallaby check opensecrets.org, the website of The Center for Responsive Politics, before he wrote these words? Not only did Fannie Mae have a PAC but also individual executives of the company made extensive contributions to members of Congress, well over half the members as I recall. Greenspan made exactly the right decision. Those of us who worked on GSE issues understood that Fannie, especially, had a formidable political machine. Fannie bought off academics by commissioning academic papers, for which it paid handsomely. The Fannie Mae Foundation showered funds on affordable housing nonprofits. For the Fed to assume responsibility, or even have suggested the possibility, would have embroiled the Fed in political controversies it was not well positioned to confront. Greenspan was working behind the scenes to broker GSE reforms at least as early as 2001. I had a speech scheduled for October 2001, and as was my usual practice I distributed a draft for comment. A senior economist at the Board of Governors who worked closely with Greenspan on GSE issues called me. The speech draft contained some critical comments about the GSEs; he said that he did not disagree with me but that the Chairman was working to broker reforms and thought my rather outspoken speech would not be helpful. I toned it down (Poole 2001). In subsequent years, I presented several more speeches on risks created by the GSEs, some of which drew considerable press attention. I did not hear from Greenspan again, except for a wry remark just before an FOMC meeting after a speech I presented in Washington in March 2003 (Poole 2003).",1
52,1,Business Economics,15 February 2017,https://link.springer.com/article/10.1057/s11369-017-0020-x,Are Young People Becoming More Risk Averse? An Analysis of Factors Contributing to the Rise in Precautionary Savings Among Young Adults,January 2017,LaVaughn M. Henry,,,Unknown,Unknown,Unknown,Unknown,,
52,1,Business Economics,22 February 2017,https://link.springer.com/article/10.1057/s11369-017-0022-8,When is a Firm Born? Alternative Criteria and Consequences,January 2017,Paul Davidson Reynolds,,,Male,Unknown,Unknown,Male,"There is currently an enormous amount of attention to new firms, not only because they are a major source of job creation, improved productivity, economic adaptation, and innovation, but also because new firm creation—or entrepreneurship—is a major career option for a large minority of those in the labor force. The result has been a considerable expansion of research on the firm creation process for both scholarly and policy objectives. The origin of this research emphasis was the discovery in the 1970s that new, small firms were a major source of new job creation. This finding was based on a reorganization of the establishment listings in the Dun and Bradstreet credit rating files to identify multi-unit enterprises, which made it possible to identify autonomous new listings distinct from new branches and subsidiaries and, in turn, estimate their contribution to job creation (Birch 1979, 1981). To the surprise of all, new firm job creation turned out to be substantial. Subsequently, both the U.S. Census Bureau and Bureau of Labor Statistics now identify autonomous new employer firms and their unique economic contributions (Haltiwanger et al. 2007). A parallel effort was initiated in 2005 to use the Current Population Survey to track those individuals in the initial month they reported investing 15 or more hours in self-employment, considered to be entrepreneurs for that month (Fairlie 2014; Fairlie et al. 2016).Footnote 1 All of these efforts to track different aspects of entrepreneurial activity or firm creation are currently active.Footnote 2
 Despite this enormous investment in tracking new firm creation, there is no consensus on either the conceptual (theoretical) or the operational (measurement) definitions of one of the most fundamental features of the process—the birth of a new firm. Firms are not created instantaneously. It takes a while for the individuals and teams that develop new firms to assemble and organize the resources and talent required for a viable new firm. This leads to considering firm creation as a multi-stage process, as illustrated in Figure 1. Steps in creating a new firm A precise description of the process requires the establishment of explicit criteria for the two transitions: entry into the firm creation process and the shift from start-up venture to a “new firm.” Most analyses tend to conflate or ignore the two phases of the process. Many assessments fail to make a clear distinction about which phase of the start-up process is represented by the data used in the analysis. This paper will produce evidence illustrating the value of clarifying this distinction. It will be shown that differences in the conceptual and operational definition of a “firm birth” lead to considerable variation in “post-firm birth” trajectories related to survival and growth measured by job creation. A unique dataset, the Panel Study of Entrepreneurial Dynamics (PSED), which provides a longitudinal description of a representative sample of nascent ventures in the start-up process, is used for the analysis. Once identified in a national screening, new firm initiatives are tracked from entry into the start-up process through firm birth, defined as initial profits, and for several subsequent years. This rich dataset allows the utilization of different criteria for a firm birth and the exploration of the impact of alternative views on one’s perception of the start-up process, post-firm birth trajectories, and contributions to job creation.Footnote 3 The box describes the PSED program.Footnote 4
 “The Panel Study of Entrepreneurial Dynamics (PSED) research program is designed to enhance the scientific understanding of how people start businesses. The projects provide valid and reliable data on the process of business formation based on nationally-representative samples of nascent entrepreneurs, those active in business creation. PSED I began with screening in 1998–2000 to select a cohort of 830 with three follow-up interviews. A control group of those not involved in firm creation is available for comparisons. PSED II began with screening in 2005–06, followed by six yearly interviews. The information obtained includes data on the nature of those active as nascent entrepreneurs, the activities undertaken during the start-up process, and the characteristics of start-up efforts that become new firms.” (http://www.psed.isr.umich.edu/psed/home). The program is supervised by Richard Curtin at the Institute for Social Research at the University of Michigan, with support from the E. M. Kauffman Foundation, the National Science Foundation, and the Small Business Administration.",13
52,1,Business Economics,10 March 2017,https://link.springer.com/article/10.1057/s11369-017-0023-7,"The U.S. Carpet Industry: History, Industry Dynamics, and a Simple Model for Short-Term Forecasting",January 2017,Kevin Swift,Emily Sanchez,,Male,Female,Unknown,Mix,,
52,1,Business Economics,22 March 2017,https://link.springer.com/article/10.1057/s11369-017-0026-4,The Competitive Dynamics of the Generic Drug Manufacturing Industry,January 2017,Evan Hoffman,,,Male,Unknown,Unknown,Male,"The American healthcare system is plagued by complexity, opacity, and policy uncertainty. It is becoming an increasingly burdensome task for economists to understand the fundamentals that drive our healthcare system, which is becoming an ever-more important component of GDP. Between 2004 and 2014, healthcare expenditures as a percentage of GDP rose from 15.1 to 17.1 percent, surpassing $3 trillion, and surpassing the shares in all other OECD countries (World Bank 2014a, b). Moreover, the sector as a whole is projected to add more jobs in the years leading up to 2024 than any other sector of the economy (Bureau of Labor Statistics 2015). However, the system has arguably fallen short in achieving greater overall health for the nation, while costing more. Per capita spending on healthcare has increased at an average annual rate of 4.0 percent between 2004 and 2014 or 1.3, percentage points faster than GDP per capita (World Bank 2014b), raising doubts from economists, lawmakers, and the public on the efficiency of the system as a whole. Spending on generic and brand name drugs has been an especially important matter, as it has grown into a significant subcomponent and cost driver of the greater healthcare system. Pharmaceutical spending continues to grow disproportionately relative to other subcomponents of health care such as hospital care, physician and clinical services, medical equipment, and home health care (Centers for Medicare & Medicaid Services 2016). U.S. spending on pharmaceuticals continues to outpace all other OECD nations, with the next nearest spender, Canada, spending nearly a third less per capita (Figure 1). 2014 Pharmaceutical spending per capita To better understand the drug manufacturing industry one must study the rise of generic drug manufacturers. The emergence of the generic part of the broader pharmaceutical manufacturing industry has been driven by the desire to create a cost-effective way to deliver drugs to consumers. Generic makers increased their share of dispensed prescription drugs from 55.5 percent in 2005 (Health Affairs 2008) to 88.7 percent in 2015 (IMS Health 2016a). Generics’ ability to reduce overall drug prices has been debatable. Nevertheless, it should be noted that between 2004 and 2014, the growth of pharmaceutical spending has slowed significantly, likely as a result of the proliferation of generic drugs. Growth of pharmaceutical spending tapered from an annualized 24.6 percent between 1990 and 2000 to an annualized 8.9 percent between 2004 and 2014 (Centers for Medicare & Medicaid Services 2015), even though the overall volume of drugs being dispensed to consumers continued to increase markedly. It is equally important to look at the broader pharmaceutical industry through the respective lenses of generic and brand name drug makers to understand the competitive forces that are affecting innovation and costs.",1
52,1,Business Economics,15 February 2017,https://link.springer.com/article/10.1057/s11369-017-0021-9,Money Changes Everything: How Finance Made Civilization Possible,January 2017,Larry Neal,,,Male,Unknown,Unknown,Male,,1
52,1,Business Economics,15 February 2017,https://link.springer.com/article/10.1057/s11369-017-0025-5,The Curse of Cash,January 2017,Stephen Williamson,,,Male,Unknown,Unknown,Male,,2
52,1,Business Economics,07 April 2017,https://link.springer.com/article/10.1057/s11369-017-0032-6,Erratum to: When is a Firm Born? Alternative Criteria and Consequences,January 2017,Paul Davidson Reynolds,,,Male,Unknown,Unknown,Male,,2
52,2,Business Economics,17 July 2017,https://link.springer.com/article/10.1057/s11369-017-0039-z,From the editor,April 2017,Charles Steindel,,,Male,Unknown,Unknown,Male,,
52,2,Business Economics,04 July 2017,https://link.springer.com/article/10.1057/s11369-017-0035-3,Secular stagnation or financial cycle drag?,April 2017,Claudio Borio,,,Male,Unknown,Unknown,Male,"Boiling down the two hypotheses to their bare essentials is a necessarily stylised treatment. But as long as it is recognised as such, it can help better highlight the main differences without running the risk of being seen as depicting straw men. The secular stagnation hypothesis can be summarised in three propositions. First, the world has been haunted for a very long time, well before the crisis, by a structural aggregate demand deficiency that is likely to persist well into the future and keep growth sluggish. Many factors are typically mentioned in this context, including ageing populations, growing income and wealth inequality, and falling tangible investment owing to technological change. Second, the pre-crisis financial boom (or “bubble”) was the only reason why output reached potential, i.e. full employment. Third, and more technically, the natural (or equilibrium) real interest rate has been falling steadily and has been negative for some time.Footnote 3 Now, the natural or equilibrium interest rate is typically defined as the rate that would prevail if output was at its potential level and hence inflation was stable. So, in plainer language, given the major structural demand deficiency, real (inflation-adjusted) interest rates must be negative in order to ensure that the economy operates at full employment and to avoid a costly deflationary spiral. Such a spiral would arise because, with nominal interest rates stuck at the zero lower bound, falling prices would raise real interest rates, which would cut spending further, which, in turn, would depress output and employment and hence prices and so on. The financial cycle drag hypothesis can also be summarised in three propositions—largely the mirror image of the previous ones. First, the world has been haunted by the inability to restrain financial booms that, once they turn to busts, cause huge and long-lasting economic damage—deep and protracted recessions, weak and drawn-out recoveries and persistently slower productivity growth. Such outsize financial cycles are best characterised by the joint fluctuations in credit and asset prices, especially property prices, as risk-taking ebbs and flows (Fig. 1). And they tend to be much longer than “traditional” business cycles (say, 15–20 years rather than 8–10) (Drehmann et al. 2012; Borio 2014a; BIS 2014).Footnote 4 Second, the pre-crisis boom actually pushed output above potential and undermined productivity. In other words, it was not even required to achieve full employment. Third, the natural or equilibrium real interest rate is positive and considerably higher than the secular stagnation hypothesis would suggest. There are two related reasons for this. Defining and measuring an equilibrium rate without explicitly considering the build-up of financial imbalances is too narrow an approach. In addition, the global demand deficiency has been overestimated, while the role of primarily positive, and benign, secular supply side global factors in driving inflation has been underestimated. 
Source: Drehmann et al. (2012), updated Financial and business cycles in the United States. The financial cycle as measured by frequency-based (bandpass) filters capturing medium-term cycles in real credit, the credit-to-GDP ratio and real house prices. The business cycle as measured by a frequency-based (bandpass) filter capturing fluctuations in real GDP over a period from 1 to 8 years. The graph compares the financial cycle with traditional measures of the business cycle. The picture would be similar based on other common methodologies (e.g. turning point (peak/trough) analysis). Thus, the two hypotheses have elements in common. They take a longer-term perspective, focusing on the deeper forces behind shorter-term economic fluctuations. They question the presumption that the economy is always self-equilibrating, quickly returning to a pre-existing trend. And they encourage us to question more thoroughly the prevailing analytical paradigms. But their differences are equally apparent—with respect to the balance between financial and real factors as well as between aggregate supply and aggregate demand, to the interpretation of the behaviour of interest rates and inflation, and hence, as we shall see, to the likely future scenarios and policy implications.",15
52,2,Business Economics,17 July 2017,https://link.springer.com/article/10.1057/s11369-017-0024-6,Does mismeasurement explain low productivity growth?,April 2017,Chad Syverson,,,Male,Unknown,Unknown,Male,,
52,2,Business Economics,30 June 2017,https://link.springer.com/article/10.1057/s11369-017-0034-4,"Prices of high-tech products, mismeasurement, and the pace of innovation",April 2017,David Byrne,Stephen D. Oliner,Daniel E. Sichel,Male,Male,Male,Male,"Economists and others have offered many explanations for the slowdown in U.S. productivity growth that began in the mid-2000s, with labor productivity in the business sector rising just over one-half percent at an annual rate from 2010 to 2015, well below the pace over the boom years of 1995–2004, and even below the already reduced rate that prevailed over 2004–2010. Focusing on the supply side of the economy, Gordon (2016) argues that the IT revolution is just not as big a deal as the second industrial revolution, and that the boost to productivity growth rates from IT largely is behind us.Footnote 1 Focusing on the demand side, Summers (2014) has resurrected the Depression-era term “secular stagnation,” arguing that the economy is generating insufficient demand. Others have argued that the tools of economic measurement have not kept up with the digital revolution and that economic growth has been stronger than reflected in official statistics. One strand of this argument focuses on items within the current scope of GDP, positing mismeasurement of key GDP components (Goldman Sachs (2015, 2016), for example.) Another strand looks beyond the current scope of GDP, making the case that economic welfare has improved much more rapidly than have the measures of productivity.Footnote 2
 This paper contributes to the “within GDP” debate, focusing on the mismeasurement of prices of high-tech products. As noted, Goldman Sachs and others have made the case that the productivity slowdown can be explained, at least in part, by mismeasurement of the digital economy. Since that argument emerged, two papers have countered that claim. Byrne et al. (2016a) carefully examined the evidence and concluded that mismeasurement does not provide an explanation of the slowdown. Syverson (2017), using a completely different methodology, also made a compelling case that mismeasurement cannot explain the productivity slowdown. But, is this the end of the story? Should we conclude that mismeasurement of high-tech prices and the digital economy have no important consequences for patterns of economic growth? This paper argues that mismeasurement does matter. In particular, mismeasurement matters for the allocation and pattern of multifactor productivity (MFP) growth across sectors. To demonstrate this, we take estimates of the amount of mismeasurement of prices of high-tech products from the literature and feed these through a standard growth-accounting framework to examine the implications of this mismeasurement for sectoral MFP growth. Our results show that the mismeasurement of high-tech prices has a dramatic effect on the pattern of MFP growth across sectors. Specifically, a faster decline of prices of high-tech products implies a faster pace of MFP growth in high-tech sectors and a slower rate of MFP advance outside the high-tech sector. If we take MFP growth as a rough proxy for the pace of innovation, our results suggest that innovation in the tech sector has been more rapid than the rate that would be inferred from official statistics (and even slower outside high-tech). At the same time, our results confirm that this mismeasurement does not explain the labor productivity slowdown, and has a relatively modest effect on aggregate MFP growth. We believe these results are important for three reasons. First, they deepen the productivity puzzle. If the pace of innovation in the high-tech sectors has been more rapid than indicated by official statistics, then it is perhaps even more puzzling that overall labor productivity growth has been so sluggish in recent years. Second, we believe narratives about the prospects for growth have been improperly darkened by the view that innovation, even in the tech sector, has been weak. According to official statistics, prices of tech products have barely been falling in recent years. And, that slow rate of price decline in the tech sector has implied, via the dual approach to productivity measurement, a slow rate of MFP growth. This has led, in turn, to inferences that the pace of innovation in the tech sector has faltered.Footnote 3 Finally, a faster rate of innovation in the tech sector implies, via a multi-sector growth model, a faster steady-state rate of growth in labor productivity, even with the slower rate of MFP growth outside the tech sector. Accordingly, we argue that the pattern of MFP growth across industries may presage a second wave of productivity advance supported by the digital economy.Footnote 4
",8
52,2,Business Economics,26 June 2017,https://link.springer.com/article/10.1057/s11369-017-0037-1,The necessity for a strategic approach to monetary policy,April 2017,Mickey D. Levy,,,,Unknown,Unknown,Mix,,
52,2,Business Economics,31 May 2017,https://link.springer.com/article/10.1057/s11369-017-0033-5,Threats to monetary policy independence: reasons to be concerned,April 2017,David J. Stockton,,,Male,Unknown,Unknown,Male,"Do we need to worry about threats to monetary policy independence? In a word—YES. In the current contentious climate in Washington, many officially independent institutions are encountering increased mistrust and heightened political pressure—and the Fed is among them. In the case of the Fed, these threats have taken concrete form in a variety of recent legislative proposals that have been advanced in the past couple of years to reform the institution. In the past, they had little chance of passing and being signed into law. That is no longer the case. Consequently, these legislative efforts deserve greater scrutiny than they have received in the past. As is often the case, these proposals are a hodge-podge of ideas. Some might do little harm or even make marginal improvements in the framework for monetary policy. But other proposals for so-called “reform” pose serious risks to the independence of monetary policy, and could ultimately place the Fed under greater and more persistent political pressure. One must remember that monetary policy independence is still a relatively recent concept, in the United States as well as elsewhere. It is not a state of nature but rather a political construct. Moreover, central bank independence is not a zero–one state. Central bank independence is always a matter of degree, and central banks will always be constrained to some extent by the political environment in which they operate. This is entirely appropriate, and these institutions need to be held accountable to the democratic process. What do we mean by independence and why is it valuable? In defining independence, a distinction must be drawn between instrument independence and goal independence. Goals for the central bank should be set by or in consultation with the government. In the case of the United States, Congress establishes the goals for the Fed—stable prices, maximum employment, and moderate long-term interest rates. Few would argue that central banks should be free to set their own goals—that is, that they should have goal independence. But after the goals have been established by the government, central banks should have independence in determining how best to pursue those goals, including choosing the appropriate instruments for their achievement. In other words, central banks should be given instrument (operational) independence. Of course, in the end, the central bank needs to be held accountable for achieving the established goals and should be required to explain any failure to do so. Although not conclusive, there is a considerable body of empirical evidence that suggests economic performance is superior for economies with independent central banks in comparison with situations in which central bank decisions are directed by the government. The biggest reason is that independent central banks insulate monetary policy decisions from political pressures to focus on the near term. A relentless focus on the next electoral cycle can lead to outcomes that are far from optimal. Independent central banks, free from political pressure surrounding their near-term policy actions, have the ability to focus on the longer-term attainment of price stability and full employment. What is disturbing about some of the recent legislative proposals under consideration is that they open the door, sometimes widely, to placing more political pressure on the near-term conduct of monetary policy.",1
52,2,Business Economics,10 March 2017,https://link.springer.com/article/10.1057/s11369-017-0027-3,Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,April 2017,Apurv Jain,,,Unknown,Unknown,Unknown,Unknown,,
52,3,Business Economics,24 August 2017,https://link.springer.com/article/10.1057/s11369-017-0048-y,From the Editor,July 2017,Charles Steindel,,,Male,Unknown,Unknown,Male,,
52,3,Business Economics,10 August 2017,https://link.springer.com/article/10.1057/s11369-017-0047-z,The overselling of globalization,July 2017,Joseph E. Stiglitz,,,Male,Unknown,Unknown,Male,"Economists’ belief in the virtues of free trade are so great and so long-standing that an economist who expressed skepticism was at risk of losing his “union card”—or at least his credibility as a serious economist. Indeed, one of the earliest contributions of Paul Samuelson, my thesis supervisor, was to show that the country as a whole was better off with trade (Samuelson 1938). This expanded on the earlier argument of David Ricardo, about the gains from trade that arise when each country increases production in what it does relatively well and Adam Smith, about the gains from trade that arise when each country specializes so that it can get better and better at what it does. But any theory is only as good as the assumptions that go into it—as the expression goes, garbage in, garbage out. If the assumptions are unrealistic, the conclusions are likely to be false or at least misleading. The standard models assume full employment. What workers worry about is jobs. The 2008 crisis showed that even a well-functioning economy like the US could have high unemployment for a long time. Indeed, even now, almost a decade after the onset of the crisis and more than a decade after the bursting of the housing bubble, long-term unemployment in the US remains elevated. In addition, the fraction of the population that is working is lower than it has been since women started entering the labor force. The standard theory recognized that the opening up of trade to cheap imports would result in the loss of jobs in the import-competing sectors. But it also assumed that new jobs would be created in the export sectors—and that those new jobs would pay far better than those that were lost. Contrary to what our politicians assert (including the US Trade Representative, or USTR, which is in charge of trade policy), trade agreements are not about creating jobs. Maintaining the economy at full employment is the responsibility of monetary policy (the Federal Reserve in the US, the Bank of England in the UK, and the European Central Bank in the eurozone) and fiscal policy (the setting of taxes and expenditure). It is not the purview of trade policy. Even the narrow argument put forward by the USTR that trade agreements create jobs is unpersuasive—indeed, almost certainly fallacious. If, as the USTR claims, exports create jobs, then imports destroy jobs, and if trade is roughly balanced, what advanced countries export uses less labor than what they import. Hence, net, for advanced countries like the US, any balanced trade agreement by itself destroys jobs. If monetary and fiscal policy work as they should, then new jobs will be created to offset the jobs lost. But too often monetary and fiscal policy aren’t working as they should, and there is a scarcity of jobs. This may be because in a deep downturn, monetary policy is ineffective, or it may be that (as now) politics constrains the effective use of fiscal policy. But whatever the reason, unemployment may be high, and those who lose their jobs worry about getting another. Jobs were at the center of the 1992 election, where Bill Clinton ran on the simple platform, “jobs, jobs, jobs.” And worries about jobs clearly played a role in the 2016 election. This problem of job loss from mismanaged globalization becomes, as I have noted, particularly salient when there already is an unemployment problem. The massive job losses in the decade since the financial crisis provide a clear and troubling case in point. The effects of globalization are real and palpable. Those parts of the country producing products that compete with Chinese imports, which surged after the country’s WTO accession, have lower wages and more unemployment. The surge of imports not only directly destroyed jobs, but as it did so, others in the community were affected, as housing prices fell and demand for non-traded goods decreased. What globalization is good for (when it works well) is thus not jobs, but standards of living. It increases overall productivity. The jobs created in the export sector are higher paying than the jobs lost in the import-competing sector. Trade liberalization is thus supposed to be about increasing GDP. With higher national income, in principle, everyone could be better off. There were, however, two problems with even this story of the benefits of globalization, which I will discuss at greater length below. First, the fact that everyone could be made better off doesn’t mean that they would be better off. But secondly, special interests saw trade agreements as an opportunity to distort the economy in their favor, to increase their rents and profits. The result was that the US never signed on to a free-trade agreement. Doing so would have entailed politically untenable compromises, for instance, giving up its massive agriculture subsidies. Agreements, like NAFTA, were called free-trade agreements, but they were really “managed” trade agreements serving the interests of large corporations. The problem is that most politicians did not understand the story I’ve just told—that trade is not about jobs but about standards of living, that it was not automatic that the standards of living of most citizens would increase with a trade agreement. Those that did understand it thought it was too complicated. So, they told what they thought was a white lie to the public—trade creates jobs. And when the evidence showed the contrary, especially when standards of living of large numbers of citizens declined, they lost their credibility.",52
52,3,Business Economics,31 July 2017,https://link.springer.com/article/10.1057/s11369-017-0043-3,"OECD’s interim economic outlook, March 2017",July 2017,Catherine L. Mann,,,Female,Unknown,Unknown,Female,,
52,3,Business Economics,10 August 2017,https://link.springer.com/article/10.1057/s11369-017-0049-x,Federal policy and economic growth,July 2017,Douglas W. Elmendorf,,,Male,Unknown,Unknown,Male,"When I look at the budget outlook a decade or two in the future, I am quite concerned about the high and rising levels of federal debt we will have unless our policies are changed. We have shown no ability as a country to adopt policies to address that challenge. Indeed, we have not even had an informed, honest discussion of that challenge outside of groups like this one. But as I look out to the next several years, I am much more worried about the prospect of sharp cuts in federal spending that would be damaging to the country’s long-term growth prospects through reductions in federal investment, and would be harmful to lower- and middle-income people’s well-being through reductions in federal services and benefits. I think that economic policy in this country should be oriented explicitly toward raising the living standards for lower- and middle-income people because they have gained the least from overall economic growth during the past several decades, and there are a number of steps we can take to do that. But the first thing is to do no harm—to not take away services and benefits that people depend on. So I worry a lot in the short term about inappropriate cutbacks in government spending. Beyond that, we can make changes to put federal debt on a more sustainable path. Those changes are less urgent than they might otherwise be because interest rates are so low and are likely to stay below their historical averages for a prolonged period. I have written a paper with Louise Sheiner from Brookings that will appear in the Journal of Economic Perspectives shortly, about the implications of low interest rates for fiscal policy. There are a number of possible contributing factors to low interest rates, and those factors have somewhat different implications for policy. But on balance they imply that it is appropriate to have more debt and more federal investment than if interest rates were higher. For the federal budget as a whole, and this is not a new line for me, we need to design a set of deficit-reducing changes that would phase in gradually over time. Because interest rates will probably remain significantly lower than their historical averages for an extended period, this phase-in process can happen more slowly, and then we can do more crucial public investment over the next decade. But ultimately, we will need a combination of cutbacks in Social Security and Medicare benefits relative to current law and increases in tax revenues relative to current law. It is much better to put those changes into law now. Remember that we are still in the midst of an increase in the retirement age for social security that was legislated in 1983—34 years ago. So if we will cut spending for certain programs 10 or 20 years from now, this would be the best time to put those cuts into law. However, I see no prospect of that happening. There is a persistent delusion—fed by some elected officials who do not know better, and some who do know better—that the way to tackle the federal budget imbalance is to cut the so-called waste. I am not against cutting waste, but that is not where the action is. The action is in benefit programs that people like. President Trump was elected in part because he promised not to cut Social Security, Medicare, and Medicaid benefits. So I see no prospect that Republican leaders will try to cut spending on those programs beyond bills that can be labeled “repeal and replace the Affordable Care Act”—a subject to which I return below.",1
52,3,Business Economics,03 August 2017,https://link.springer.com/article/10.1057/s11369-017-0044-2,Faster growth and greater fiscal responsibility are possible,July 2017,Glenn Hubbard,,,Male,Unknown,Unknown,Male,,
52,3,Business Economics,18 July 2017,https://link.springer.com/article/10.1057/s11369-017-0041-5,"What is the potential growth rate of the U.S. economy, and how might policy affect it?",July 2017,Jason Furman,,,Male,Unknown,Unknown,Male,"This section assesses the growth of potential real gross domestic product (GDP) over the next decade (2016–2026).Footnote 1 As such, it abstracts from any cyclical development that could cause growth to be higher or lower than potential—a reasonable assumption given the length of the period in question and the proximity of the U.S. economy to full employment today. The starting point for this assessment is the following identity: The same identity can be expressed in terms of growth rates: This identity says that the growth of potential GDP can be understood as the sum of the growth of potential output per hour (or labor productivity) and the growth in potential labor inputs. Labor input growth, in turn, depends on growth in average hours, the potential labor force participation rate, and the population. The Central Scenario in this forecast uses CBO’s 1.8% annual rate estimate for potential GDP growth from 2016 to 2026. CBO projects annual potential non-farm business sector productivity growth of 1.7% annually, the equivalent of 1.4% annual growth in potential labor productivity for the economy as a whole.Footnote 2 To put this forecast in context, potential non-farm productivity growth of 1.7% is a relatively pessimistic figure compared to the economic performance of the United States since World War II (it is at the 27th percentile of ten-year changes in CBO’s estimates of potential productivity growth since 1949) but very optimistic compared to its recent performance (1.2% annual growth in potential non-farm business productivity in the last decade, according to CBO’s estimates). On balance, CBO’s forecast is roughly in accord with its estimate of average annual potential productivity growth since 1973. The Central Scenario also uses, wherever possible, CBO’s assumptions for the other inputs into potential real GDP. CBO’s forecasts for inputs into GDP imply that average weekly hours per worker will remain constant over the next decade, as shown in Fig. 1. 
Source Bureau of Labor Statistics, Productivity and Costs; author’s calculations Average weekly hours per worker, non-farm business sector. CBO also assumes that the age–sex-adjusted labor force participation rate will increase slightly over the next decade. Nevertheless, because the overall age–sex mix of the population will change over the next decade, especially due to the aging of the baby boomers, the overall potential participation rate is expected to fall by about 0.2 percentage point each year, as shown in Fig. 2. This scenario is optimistic relative to the past performance of the participation rate, since the age-adjusted participation rate for men has fallen nearly continuously since the 1950s and the age-adjusted participation rate for women has fallen since around 2000.Footnote 3 This participation rate is applied to CBO’s forecast of future civilian non-institutional population growth, which is similar to the Census Bureau’s projections for growth of the resident population. 
Source Bureau of Labor Statistics, Current Population Survey; Congressional Budget Office; author’s calculations Potential labor force participation rate, 1950–2026. Overall, about two-thirds of the difference between CBO’s 1.8% projected annual growth rate of potential GDP over the next ten years and the 3.5% annual growth rate from 1949 to 2007 can be attributed to differences in demography between the two periods, with the assumption that potential productivity growth will be lower than its postwar average being responsible for the other third. The starkness of the demographic change can be seen in Fig. 3, which shows the growth of the prime-age (25–54) population. Annual prime-age population growth peaked above 2% a year in the 1980s but has since fallen to just above zero and is unlikely to rise above 0.5% a year over the next decade. 
Source Bureau of Labor Statistics, Current Population Survey; Social Security Administration; author’s calculations Growth of civilian non-institutional population 25–54. Moreover, the slowdown of productivity growth may be, at least in part, a result of this demographic shift. James Feyrer (2007) has found that changes in the age structure of the labor force are correlated with changes in labor productivity. Workers are generally at their most productive in middle age, so as demographic changes have led to a shrinking share of workers in their prime years, aggregate productivity has slowed. Additionally, recent research on the United States finds that a 10% increase in the fraction of the population over 60 is associated with a 5.5% drop in the growth rate of GDP per capita over ten years, with two-thirds of the slowdown due to reduced productivity growth (Maestas et al. 2016). In this section, I make use of a Monte Carlo simulation to generate a probability distribution of a range of outcomes for potential growth over the next decade. By construction, the distribution of outcomes is centered on CBO’s 1.8% estimate for annual potential growth over the next decade. For the simulation, the results of repeated draws from independent distributions of the inputs to potential growth shown in Eq. (1) are combined to generate estimates of potential growth. Potential productivity growth is drawn from a normal distribution with mean equal to CBO’s forecast and variance equal to that of the historical distribution of CBO’s estimates of ten-year changes in potential productivity from 1949 to 2016. Growth in average weekly hours is drawn from a normal distribution with mean zero and variance equal to the historical distribution of ten-year changes in average weekly hours reported by the BLS. In all cases, population growth is assumed to be equal to CBO’s forecast and is not subject to random draws. Forecasts for the overall potential labor force participation are aggregated up from forecasts for labor force participation rates for specific sex–age groups (male and female; 16–24, 25–54, and 55 and older) using forecasts of population shares for each group derived from Social Security Administration data (which are assumed, like overall population growth, to be deterministic): In the Monte Carlo simulation, repeated draws are taken for each age–sex group from independent distributions of 10-year changes in its participation rate. To approximate CBO’s projections, which attribute most of the decline in potential labor force participation to the effects of aging, all distributions are assumed to have a mean of zero (i.e., no change in within-group participation rates over the next decade). The variance for each distribution is taken from the variance of the historical distribution of ten-year changes in the participation rate for the age–sex group in question. Once aggregated, each combination of draws yields a path for the labor force participation rate over the next ten years relative to a baseline with no change in age–sex-adjusted participation; this path is then rebased using CBO’s estimate of potential labor force participation in order to yield a path for potential labor force participation from 2016 to 2026. The range of outcomes for potential growth from a simulation using 10 million draws is shown in Fig. 4a and b, which show the probability density function and the cumulative density function, respectively. Summary statistics are provided in Table 1. 
Source Bureau of Labor Statistics; Congressional Budget Office; Social Security Administration; author’s calculations Monte Carlo simulation (n = 10,000,000). To aid in the interpretability of these results, Table 2 displays some specific scenarios that generate potential real GDP growth at the 5th percentile, the 50th percentile, and the 95th percentile of the simulated distribution. The table shows three possible scenarios for productivity growth and three possible scenarios for the labor force participation rate, and displays the potential growth rate over the next decade resulting from the combination of each. All scenarios assume that average weekly hours are unchanged. The different scenarios for productivity growth and participation are shown in Fig. 5a and b. 
Source
a Congressional Budget Office; author’s calculations. b Bureau of Labor Statistics, Current Population Survey; Congressional Budget Office; Social Security Administration; author’s calculations 
a Potential labor productivity, non-farm business sector. b Potential labor force participation rate, 1950–2026.",2
52,3,Business Economics,21 June 2017,https://link.springer.com/article/10.1057/s11369-017-0038-0,Policy uncertainty and the economic outlook,July 2017,Douglas Holtz-Eakin,,,Male,Unknown,Unknown,Male,,1
52,3,Business Economics,03 August 2017,https://link.springer.com/article/10.1057/s11369-017-0050-4,New policy uncertainties in the economic outlook,July 2017,Joel Prakken,,,Male,Unknown,Unknown,Male,,1
52,3,Business Economics,19 July 2017,https://link.springer.com/article/10.1057/s11369-017-0042-4,Is there an easy cure for low growth?,July 2017,John Fernald,,,Male,Unknown,Unknown,Male,"The recovery from the Great Recession has been disappointingly slow. From the 2009 trough through 2016, GDP growth averaged only 2.1%. That rate is extraordinarily low compared with history. Hence, are we likely to do better than that going forward? Are there policies that can help? The Survey of Professional Forecasters from February 2017 expects growth over the next decade to be about a quarter percentage point faster than what we’ve seen since 2009. Higher growth would help alleviate concerns about government budgets, and higher per capita income growth would imply faster increases in material standards of living. But even the disappointing 2.1% growth rate since 2009 was enough to bring the unemployment rate down from 10% to around 4½% in early 2017. Hence, sustainable growth could plausibly be even lower than what we’ve observed recently. My own modal estimate is that longer-run GDP growth—say, over the next 5–10 years—will be in the 1½–1¾% range.Footnote 1 Relative to history, the biggest constraints on growth are headwinds from demographics and reduced gains in education. In particular, the labor force is projected to grow only slowly, and education will add less to labor quality and thus to labor productivity. Net of this labor quality slowdown, my benchmark forecast assumes that productivity runs at about its pace in the 1970s and 1980s, which is a little faster than what we’ve seen since 2004 and notably faster than it’s been since 2011. Of course, there is enormous uncertainty about any projection because low-frequency productivity trends are mysterious. In the last 45 years, the 1995–2004 period showed exceptional productivity gains. But neither professional forecasters nor newspaper articles in the early 1990s, predicted the pickup. And professional forecasters weren’t expecting the productivity boom to end after 2004. (Müller and Watson 2016 discuss confidence intervals on long-run projections.)",
52,3,Business Economics,21 June 2017,https://link.springer.com/article/10.1057/s11369-017-0036-2,"The end of alchemy: money, banking, and the future of the global economy",July 2017,Kevin L. Kliesen,,,Male,Unknown,Unknown,Male,,1
52,3,Business Economics,26 June 2017,https://link.springer.com/article/10.1057/s11369-017-0040-6,“The undoing project: a friendship that changed our minds”,July 2017,Ellen Hughes-Cromwick,,,Female,Unknown,Unknown,Female,,
52,4,Business Economics,30 November 2017,https://link.springer.com/article/10.1057/s11369-017-0060-2,From the editor,October 2017,Charles Steindel,,,Male,Unknown,Unknown,Male,,
52,4,Business Economics,14 November 2017,https://link.springer.com/article/10.1057/s11369-017-0059-8,Unconventional monetary policy and the role of central banks,October 2017,Raghuram Rajan,,,Unknown,Unknown,Unknown,Unknown,,
52,4,Business Economics,30 November 2017,https://link.springer.com/article/10.1057/s11369-017-0057-x,"Inflation, uncertainty, and monetary policy",October 2017,Janet L. Yellen,,,Female,Unknown,Unknown,Female,"Let me begin by reviewing recent inflation developments and the economic outlook. As the solid line in Fig. 1 indicates, inflation as measured by the price index for personal consumption expenditures (PCE) has generally run below the FOMC’s 2% longer-run objective since that goal was announced in January 2012.Footnote 1 Core inflation, which strips out volatile food and energy prices, has also fallen persistently short of 2% (the dashed line). Furthermore, both overall and core inflation, after moving up appreciably last year, have slipped again in recent months. Sustained low inflation such as this is undesirable because, among other things, it generally leads to low settings of the federal funds rate in normal times, thereby providing less scope to ease monetary policy to fight recessions. In addition, a persistent undershoot of our stated 2% goal could undermine the FOMC’s credibility, causing inflation expectations to drift and actual inflation and economic activity to become more volatile. Source: Bureau of Labor Statistics and Federal Reserve Summary of Economic Projections Inflation has been running persistently below 2%. Inflation measured as 12-month percent changes. Core inflation excludes food and energy prices. As noted in its recent statement, the FOMC continues to anticipate that, with gradual adjustments in the stance of monetary policy, inflation will rise and stabilize at around 2% over the medium term. This expectation is illustrated by the stars, which represent the medians of the inflation projections submitted by FOMC participants at our meeting last week. In part, this expectation reflects the significant improvement in labor market conditions over the past few years. As shown in Fig. 2, the unemployment rate (the solid line) now stands at 4.4%, somewhat below the median of FOMC participants’ estimates of its longer-run sustainable level (the dotted line). As the stars indicate, labor market conditions are expected to strengthen a bit further. The inflation outlook also reflects the Committee’s judgment that inflation expectations will remain reasonably well anchored at a level consistent with PCE price inflation of 2% in the long run, and that the restraint imposed in recent years by a variety of special factors, including movements in the relative prices of food, energy, and imports, will wane in coming quarters. Source: Bureau of Labor Statistics and Federal Reserve Summary of Economic Projections Labor market conditions have strengthened markedly. Monthly data. FOMC annual projections are fourth-quarter averages. To understand this assessment, it is useful to decompose the forces driving movements in inflation since the financial crisis, as estimated using a simple model of inflation that I presented in a speech 2 years ago.Footnote 2 Figure 3 reports this decomposition as the contributions made by various factors to the shortfall of PCE price inflation from 2%, year by year. As illustrated, labor underutilization, or “slack,” accounts for a shrinking share of the shortfall since 2012 and is now having a negligible effect. By comparison, the influence of changes in relative food, energy, and import prices has been more substantial in the past few years, although their contribution is estimated to have greatly diminished this year.Footnote 3
 Source: Federal Reserve Board using data from the BEA, BLS, CBO, and SPF. See the appendix for further details Why has PCE inflation been running below 2%? Deviation of PCE inflation (4th quarter to 4th quarter) from 2%, total and portion attributable to specific factor. Not surprisingly, the simple model does not account for all of the year-to-year movements in inflation. As indicated, the residual component of the shortfall was modestly positive on average from 2008 through last year. This year, however, inflation has been unexpectedly weak from the model’s perspective. This unusually large error does not necessarily imply that inflation is more likely to continue to come in on the low side in coming years.Footnote 4 Some of the recent decline in inflation, although not all, reflects idiosyncratic shifts in the prices of some items, such as the large decline in telecommunication service prices seen earlier in the year, that are unlikely to be repeated. As the dashed line in Fig. 4 illustrates, if the average change in consumer prices each month is calculated excluding items whose price changes are outliers on both the high and low sides, the resulting “trimmed mean” measure of inflation shows less of a slowdown this year.Footnote 5
 Source: Bureau of Labor Statistics and Federal Reserve Bank of Dallas Some of the recent inflation slowdown reflects idiosyncratic factors. Inflation measured as 12-month percent changes. Core inflation excludes food and energy prices. Based on analyses of this sort, my colleagues and I currently think that this year’s low inflation is probably temporary, so we continue to anticipate that inflation is likely to stabilize around 2% over the next few years. But our understanding of the forces driving inflation is imperfect, and we recognize that something more persistent may be responsible for the current undershooting of our longer-run objective. Accordingly, we will monitor incoming data closely and stand ready to modify our views based on what we learn.",31
52,4,Business Economics,30 October 2017,https://link.springer.com/article/10.1057/s11369-017-0056-y,The tax reform agenda,October 2017,Martin Feldstein,,,Male,Unknown,Unknown,Male,"Before I talk about the current agenda, let me provide a little historic background by focusing on the last major tax reform, the Tax Reform Act of 1986. TRA86 focused on the personal income tax. It combined base broadening with rate reduction to generate a reform that was revenue neutral on a static basis, i.e., even without taking into account the favorable effect of the lower tax rates on increasing taxable incomes. Here is the strategy that produced that important reform. For each income group—i.e., for each tax bracket—the Treasury calculated the revenue gain that would be achieved from eliminating a variety of “tax benefits”—i.e., exclusions, deductions, and also the “tax shelters” like cattle feeding and oil drilling used by high-income taxpayers. The Treasury then reduced the tax rate for each bracket to offset that revenue gain in a static calculation. Amazingly, because of the widespread use of tax shelters by high-income taxpayers, that strategy allowed reducing the top rate from 50 to 28%. Perhaps even more amazing, the legislation passed on a bipartisan basis with a Republican in the White House (Ronald Reagan) and a Democratic majority in the House of Representatives. Since the legislation reducing tax rates had come as a surprise to taxpayers, they had not been able to shift income from the high tax rate years before 1986 to the years after the rate reductions. It was a great natural experiment to see how taxpayers responded to a major change in marginal tax rates. Looking back after the fact, it was clear from the data that taxpayers responded to the reductions in marginal tax rates in ways that substantially increased taxable incomes. I studied that effect using a large panel of anonymous tax returns provided by the IRS (Feldstein 1995). The panel structure of the data meant I could compare the behavior of the same individuals in 1985 and 1988. The detailed data for each taxpayer meant I could exclude the effects on revenue of the tax shelter losses. Taxpayers who faced a marginal tax rate of 50% in 1985 had a marginal tax rate of just 28% after 1986, implying that the marginal net-of-tax rate rose to 72% from 50%, an increase of 44%. For this group, the average taxable income rose between 1985 and 1988 by 45%. A more detailed difference-in-difference analysis also implied that the elasticity of taxable income to the net-of-tax rate was about one. This dramatic increase in taxable income reflected three favorable effects of the lower marginal tax rate: the greater net reward for extra effort and extra risk-taking leading to increases in earnings, in entrepreneurial activity, in the expansion of small businesses, etc. Lower marginal tax rates also caused individuals to shift some of their compensation from untaxed fringe benefits and other perks to taxable earnings. Taxpayers also reduced spending on tax-deductible forms of compensation. Let me just note that this estimated response to lower marginal tax rates was not the full-scale dynamic scoring now used by the Congress, the Treasury, and the Congressional Budget Office. The response that I described was just the short-term change in taxable income in response to the change in marginal tax rates. The tax rate reductions of TRA86 were too good to last. President George H. W. Bush agreed to raise the top rate from 28 to 31%. President Clinton then raised that rate to 36%, adding what he called a “temporary” 10% increase to 39.6% for high-income taxpayers, an increase that still remains. And President Obama added a 3.8% extra tax on investment income of high-income taxpayers. One final bit of tax history: The Congressional Budget Office (2016) did a very careful study of effective tax rates by income quintiles for the 35 years from 1979 to 2013. The CBO concluded that while the effective tax rate fell in every income quintile over those 35 years, it rose to well above the 35-year average for taxpayers in the top one percent of the income distribution.",
52,4,Business Economics,03 July 2017,https://link.springer.com/article/10.1057/s11369-017-0045-1,Did Okun’s law die after the Great Recession?,October 2017,Giorgio Canarella,Stephen M. Miller,,Male,Male,Unknown,Male,"Since Okun’s (1962) seminal contribution, a large literature documents substantial evidence of the correlation between changes in real output and changes in the unemployment rate, an enduring relationship that has become known in the macroeconomic literature as Okun’s law.Footnote 1 Okun’s law, in its difference version, postulates that the growth rate in real gross domestic product (GDP) drives the change in the unemployment rate. Theoretically, the relationship links aggregate demand and the Phillips curve; empirically, the “Okun’s coefficient” provides a useful “rule of thumb” in macroeconomics, economic forecasting, and policy modeling and evaluation. The value of the Okun’s coefficient gives a benchmark for policy makers to measure the unemployment costs of higher growth fluctuations. Despite its popularity, however, the usefulness of Okun’s law depends on the stability, linearity, and symmetry assumptions implicit in the 1962 version. While conventional macroeconomics accepted Okun’s law as an empirical regularity, recent research questions the robustness of the relationship and the validity of the underlying assumptions.Footnote 2 Moreover, strong evidence for structural breaks in the relationship exists, which Lee (2000) attributes to rising female labor force participation, productivity and wage slowdowns, and corporate restructuring. More recently, Owyang and Sekhposyan (2012) and Abdel-Raouf (2014), using a rolling regression approach, detect significant changes in the unemployment rate–output growth nexus during the Great Recession of 2008–2009. Significant evidence also supports the idea that macroeconomic time series, in general, exhibit non-linear or asymmetric behavior over various phases of the business cycle. Silvapulle, Moosa, and Silvapulle (2004), Harris and Silverstone (2001), and more recently, Chinn, Ferrara, and Mignon (2014) and Valadkhani and Smyth (2015) note that ignoring asymmetry when it is present leads to a misspecified model, which produces not only erroneous inference in hypothesis testing but also poor policy evaluation results and inadequate economic policies. Macroeconomic forecasting could benefit from a better understanding of structural breaks and non-linearities in Okun’s law. Okun’s law can provide policy makers with a benchmark to measure the relative cost of output in terms of the unemployment rate. Stabilization policies designed to mediate the effect of output on the unemployment rate during recessions could benefit from an understanding of how the responsiveness of the unemployment rate to output growth changes when a given level of the growth of output (or the unemployment rate) is exceeded. Such analysis can be done with the use of a relatively econometric tool that tests for threshold non-linearity. A process may behave differently when the value of a given variable surpasses a threshold value, because a different model specification may apply. The remainder of the paper is organized as follows: Section 2 outlines the empirical methodology. Section 3 displays the findings of the empirical analysis with three sets of results. First, we present the results from the constant parameter linear model, assuming no breaks and no thresholds within the breaks. Second, we present the results from the structural break model without threshold. Finally, we present the results from the structural break model with threshold. In other words, we first investigate whether the model exhibits structural breaks. Upon finding two structural breaks and, thus, three regimes, we then examine each regime for evidence of threshold non-linearity. Section 4 concludes.",1
52,4,Business Economics,20 September 2017,https://link.springer.com/article/10.1057/s11369-017-0053-1,"America’s online ‘jobs’: conceptualizations, measurements, and influencing factors",October 2017,Christopher Alex Hooton,,,Male,Unknown,Unknown,Male,"For approximately three decades, the internet has been developing into a unique economy with its own assets, currencies, goods, services, and more. One critical component of the internet sector’s maturation has been the revolutionary development of new online markets for work and commercial opportunities. There are two aspects to this: (1) the development of new ‘traditional’ jobs through new products and services and (2) the facilitation of new types of work, job arrangements, and other commerce opportunities through new technologies. This latter aspect is defined by a very wide array of activities conducted by a wide array of individuals. These range from work for traditional firms to microbusinesses to freelance labor and more. The exact nature of the work (i.e., full-time versus part-time, primary income source versus supplementary, freelance versus salaried) varies tremendously depending on each individual’s preferences and needs, while the terms lobbed at the positions and activities are often oversimplified, weighted with connotation, and representative of only a fraction of the overall online market. The one important aspect that unites these commercial activity types across their differences is that they allow individuals to earn income. The defining characteristic of the market overall has been its elusiveness to accurate measurement up to now, likely in part due to conceptual limitations linked to traditional labor models. Currently in 2017, there are (at least) approximately 23.9 million online income positions
Footnote 1 in the United States. To provide a scale and more context for that number, there are 20.7 million jobs in Professional and business services, 15.7 million Healthcare jobs, 12.4 million Manufacturing jobs, and 0.8 million Telecommunications jobs in the United States as of June of 2017. State by state, online income positions (OIPs) range from approximately 5.8 million (the most) in California to just under 20,000 (the least) in North Dakota. In addition to this OIP market, the internet sector supported an additional 3–3.6 million traditional jobs as of 2014.Footnote 2 The one important note of temperance, however, is that this paper is unable to examine the intensity of these positions and so it is inappropriate to think of each one as the equivalent of a regular, full-time position.Footnote 3
 The paper finds that the number of OIPs is largely driven by relative income levels (cost of living, poverty, and GDP per capita levels), but not by unemployment. In other words, there is evidence that OIPs are serving as income supplements—for example in high-cost areas or in areas with weaker economies—but not primarily as job replacements. These figures come directly from Internet Association’s (IA) member companies, which represent a significant portion of the internet sector, and the author argues that the data offer a largely accurate estimate for the internet sector overall.Footnote 4 The paper has compiled this proprietary information through a survey of its members along with publicly available resources from its member companies. All data have been aggregated and anonymized at the US state and national levels to allow for analysis and in accordance with antitrust regulations. To the extent of the author’s knowledge, this is the only report and IA the only organization with access to this data. The report begins in Sect. 2 with a discussion of the existing literature estimating the OIP market or some component thereof. Section 3 describes the data and methodology. Section 4 offers a series of analyses to describe the OIP market including some simple modeling of OIPs using standard multiple regression. The report finds a surprisingly robust model for the number of OIPs in states through a series of exploratory specifications. Section 5 discusses policy implications and Sect. 6 concludes.",2
52,4,Business Economics,18 September 2017,https://link.springer.com/article/10.1057/s11369-017-0052-2,Quantifying the housing recovery: which MSAs are experiencing bubbles?,October 2017,Azhar Iqbal,Mark Vitner,,,Male,Unknown,Mix,,
52,4,Business Economics,10 October 2017,https://link.springer.com/article/10.1057/s11369-017-0054-0,Business economics in a post-truth era,October 2017,Stuart P. M. Mackintosh,,,Male,Unknown,Unknown,Male,"We need to recognize our limits and be frank about our failures, because recognizing our limits is key to our influence, now and in the future. In a post-truth era, we must, first of all, look to our professional standards and defend our norms and culture. NABE professional guidelines provide direction: It is important to us that we exercise balance, transparency, and honesty in our professional relationships in a way that does not exceed the reasonable boundaries of what our profession can offer on subject matters. I recall when Chris Varvares, Co-Founder of Macroeconomic Advisors, was formulating our professional guidelines with colleagues on the board. There was debate about whether we really needed them. Surely such norms were already well understood? Would such norms be almost anodyne in their nature and degree of obviousness? It turns out that you can answer yes to those questions but still ensure that the standards are embedded within our culture and our profession of business economics. We need to be willing to recognize our failures. Nasib Teleb goes too far in his excoriation of economics and economists in his book, The Black Swan: The Impact of the Highly Improbable. But we have plenty to be frank and sometimes embarrassed about. Take, for example, the failure of almost everyone, except perhaps Raghuram Rajan, University of Chicago Professor of Finance, and Bill White, former Head of Research at the Bank for International Settlements, to see the global financial crisis risks in advance. And, more recently, the loud warnings of disaster due to Brexit by many, including Mark Carney, Governor of the Bank of England, which proved to be, at least in the short term, wrong. Or warnings by NABE economists about the possible effects of the new administration on the economy. We forecast a Trump slump, and did not see the Trump bump coming. So we, as economists, need to be cautious about the reach of our analysis and the limits of our judgment.",3
52,4,Business Economics,17 August 2017,https://link.springer.com/article/10.1057/s11369-017-0046-0,"Paradox resolved? A review of the Rise and Fall of American Growth, by Robert J. Gordon",October 2017,John G. Fernald,,,Male,Unknown,Unknown,Male,,
52,4,Business Economics,26 October 2017,https://link.springer.com/article/10.1057/s11369-017-0055-z,The age of experts: a review of Marc Levinson’s an extraordinary time,October 2017,Stephen C. Sexauer,Laurence B. Siegel,,Male,Female,Unknown,Mix,,
53,1,Business Economics,01 February 2018,https://link.springer.com/article/10.1057/s11369-018-0071-7,From the Editor,January 2018,Charles Steindel,,,Male,Unknown,Unknown,Male,,3
53,1,Business Economics,14 February 2018,https://link.springer.com/article/10.1057/s11369-018-0067-3,Global economic challenges and opportunities,January 2018,Tao Zhang,,,,Unknown,Unknown,Mix,,
53,1,Business Economics,01 February 2018,https://link.springer.com/article/10.1057/s11369-017-0064-y,Between debt and the devil: beyond the normalization delusion,January 2018,Lord Adair Turner,,,Unknown,Unknown,Unknown,Unknown,,
53,1,Business Economics,07 December 2017,https://link.springer.com/article/10.1057/s11369-017-0062-0,China’s belt and road initiative and regional perceptions of China,January 2018,John O’Trakoun,,,Male,Unknown,Unknown,Male,"Global corporations operating in Asia-Pacific markets benefit from access to some of the largest markets in the world with, in general, strong growth potentials, supported by technological improvements, favorable demographic trends, continuing structural reforms across the region, and supportive fiscal and monetary policy. However, the tremendous opportunities found in Asia-Pacific are offset, to varying extents, by persistent geopolitical risks, such as North Korean nuclear arms testing, Taiwan strait relations, South China Sea sovereignty disputes, territorial disputes between China and Japan, and border disputes between China and India. While these conflict points have not escalated to full-crisis levels, the ever-shifting political and economic background requires businesses to constantly re-evaluate the risk assumptions surrounding their Asia-Pacific forecasts. China’s growing outward investments in the Asia-Pacific region have the potential to significantly alter the area's risk environment. Among these investment programs, China’s ambitious and expansive Belt and Road Initiative (BRI), announced in 2013 as a means of enhancing regional connectivity, deepening trade and economic relations, and expanding ties between citizens of participating countries, best exemplifies the linkage between economic investment and geopolitics in the region (see Table 1). The “Belt” refers to the Silk Road Economic Belt, a proposed network of transportation and communication infrastructure that harkens back to China’s ancient Silk Road trade routes, while the “Road” refers the 21st Century Maritime Silk Road, a string of ports connecting China with Southeast Asia, the South Asian subcontinent, Africa, the Middle East, and Europe (see Fig. 1). The combined BRI spans regions in Africa, Asia, and Europe, and encompasses nearly 65% of the world’s population and 35% of global GDP. The economic impact of the BRI is expected to be immense, with the China Development Bank reporting that approximately $900 billion in overseas infrastructure projects are currently underway or being planned. (Liao et al. 2017). 
Source Rolland (2017). Originally appearing in “West China seeks fortune on modern silk road,” Xinhua, May 15, 2016, http://news.xinhuanet.com/english/2016-05/15/c_135360904.htm. Originally published in Chinese in Xinhua, 2014 China’s silk road economic belt and 21st Century maritime silk road. While in China BRI is mostly portrayed as primarily an economic initiative—one that benefits China’s neighbors first and China itself second—outside scholars have also emphasized the geostrategic aspects of China’s effort. According to Paul Haenle, director of the Carnegie-Tsinghua Center for Global Policy, “there is an inherent duality in many BRI infrastructure projects, from foreign ports to dams. While ostensibly a commercial or soft power venture, the resulting infrastructure could have dual-use applications that would allow China to enhance its hard power projection.” (Haenle 2017) Nadège Rolland, senior fellow at the National Bureau of Asian Research, identifies 4 key strategic rationales underlying the BRI initiative: (1) enhancing political stability by developing China’s interior regions; (2) securing energy resources; (3) strengthening China’s “Neighborhood Diplomacy”; and (4) pivoting westward to counter the US pivot to Asia (Rolland 2017). Although all four of the strategic rationales for BRI have important implications for the business risk environment, we focus on the third aspect of strengthening regional diplomacy. As Rolland writes, “this idea is at the core of BRI. Economic cooperation is not just a way to boost development or to bring financial returns. It is also a tool to be used for political and strategic gain—a method that Renmin University’s Shi Yinhong has labeled ‘strategic economy.’” (Rolland 2017) If the BRI is successful in generating positive perceptions of China and stronger diplomatic relations, BRI markets stand to benefit from further upside risks stemming from the stronger political and trade relationship with China. In this article, we examine the relationship between China’s outward investments and regional perceptions of China’s influence. Using survey data from 2005 to 2016, we show that an increase in Chinese foreign direct investment (FDI) in a country improves local respondents’ perceptions of Chinese influence on their country. We also show that perceptions of China are correlated with future business confidence in Asia-Pacific countries. With China advancing plans for further regional investment, particularly through the BRI, the economic and business outlook for Asia-Pacific could improve significantly, adding to existing advantages from regional economic and demographic trends.",8
53,1,Business Economics,06 September 2017,https://link.springer.com/article/10.1057/s11369-017-0051-3,Drivers of international tourism demand in Africa,January 2018,Ogechi Adeola,Nathaniel Boso,Olaniyi Evans,Unknown,Male,Unknown,Male,"Tourism, a major world economic activity, is an important ingredient for the growth of an economy through its effects on employment generation, infrastructure provision, acceleration of income taxes and exports, and promotion of global peace (Eilat and Einav 2004). As a result, many African countries have started tapping into the potentials embedded in tourism. Tourism has been identified as boosting foreign exchange, enhancing infrastructure, and promoting international cooperation and understanding on the African continent (Kareem 2008). Tourism has become a key avenue through which many African countries can improve their income and export base (Kester 2003; Christie and Crompton 2001) as well as showcase their cultural heritage (Kareem 2008). Recent decades have seen growing expansion in tourism in Africa. Both private and public sectors have channeled substantial resources into the industry. Studies such as Christie and Crompton (2001) have highlighted that Africa has remarkable tourism prospects and that tourism is gradually contributing to the continent’s GDP and exports. Naudé and Saayman (2005) contend that Africa’s natural and cultural resource endowments are such that it ought to profit largely from international tourism, while Kester (2003) argues that tourism has the potential to significantly accelerate Africa’s economic growth and development. Empirical studies of international tourism demand have largely focused on developed countries, while Africa has remained under-researched (Xiao and Smith 2006; Rogerson 2007). This dearth of rigorous empirical studies has been attributed as the cause of inadequate policy guidance in the industry (Christie and Crompton 2001) and the continent’s largely underdeveloped and underutilized tourism endowments. The few empirical studies have suggested several factors as the leading drivers of international tourism demand. Since the estimated demand elasticity of each driver of international tourism found in the empirical literature varies significantly, the estimates cannot be regarded as conclusive. This study is an attempt to fill these voids, taking into consideration typical factors present within the continent. This study is a direct response to the number of important regional and national initiatives to enhance tourism in Africa. A comprehensive analysis of the drivers of international tourism demand in Africa may help advance understanding of tourist behavior and also help in the development of more effective international tourism public policy. The rest of this study is organized as follows: the next section provides the stylized facts of international tourism in Africa. Section 3 provides the review of the theoretical and empirical literature of the drivers of international tourism demand. Section 4 provides the empirical results. Section 5 concludes the paper.",33
53,1,Business Economics,22 January 2018,https://link.springer.com/article/10.1057/s11369-018-0069-1,Water and wastewater treatment worldwide: the industry and the market for equipment and chemicals,January 2018,Andrew Gross,Emily Park,,Male,Female,Unknown,Mix,,
53,1,Business Economics,07 December 2017,https://link.springer.com/article/10.1057/s11369-017-0061-1,The end of the Asian century by Michael R. Auslin,January 2018,Gene Huang,,,,Unknown,Unknown,Mix,,
53,1,Business Economics,07 December 2017,https://link.springer.com/article/10.1057/s11369-017-0058-9,The Great Convergence: Information Technology and the New Globalization,January 2018,Paul Thomas,,,Male,Unknown,Unknown,Male,,3
53,2,Business Economics,26 April 2018,https://link.springer.com/article/10.1057/s11369-018-0078-0,From the Editor,April 2018,Charles Steindel,,,Male,Unknown,Unknown,Male,,
53,2,Business Economics,29 March 2018,https://link.springer.com/article/10.1057/s11369-018-0074-4,Lessons from the global financial crisis,April 2018,Mervyn King,,,Male,Unknown,Unknown,Male,,1
53,2,Business Economics,16 April 2018,https://link.springer.com/article/10.1057/s11369-018-0077-1,R-star wars: the phantom menace,April 2018,James B. Bullard,,,Male,Unknown,Unknown,Male,"This article provides some commentary on issues around what is often called “r-star,” denoted “\(r^{*}\)”, the natural real rate of interest. According to leading contemporary theories, policymakers need to know the value of \(r^{*}\) to decide if the current policy rate setting is accommodative, neutral or restrictive. In practice, pinning down empirical values for the natural real rate of interest involves imputing an underlying trend from raw data, which can be difficult. Hence, this variable is something of a “phantom menace.” In this article, I will define the natural real rate of interest as a low-frequency trend measure of a short-term real interest rate. Further, I will take a regime-switching view of how to think about trend movements in the data.Footnote 1 Based on this analysis, I tentatively conclude that there appears to be a large demand for safe assets globally, and this may be the largest factor driving real interest rates to low levels over the past three decades. In addition, there appears to be only modest evidence that key trends influencing the natural rate of interest are changing today. At the end of this article, I will insert the current-regime low value of the natural rate in some Taylor-type monetary policy rules. These rules will generally recommend that the current low setting of the policy rate is broadly appropriate.",2
53,2,Business Economics,01 February 2018,https://link.springer.com/article/10.1057/s11369-018-0070-8,Adapting tax systems for population aging,April 2018,Karen Dynan,,,Female,Unknown,Unknown,Female,"A key way in which population aging affects the economy is that it changes the size of the older population relative to that of the working-age population. Figure 1 shows history and projections of old-age dependency ratios for various countries. The ratios capture how the size of the older population (age 65 and up) compares with the size of the working-age population for a given country. In the United States, for example, the oldest baby boomers reached the age of 65 in 2011, and, indeed, the U.S. dependency ratio (the dark solid line) began to trend up about that time. Today, the U.S. dependency ratio stands at 24 percent, but it will climb rapidly over the next 15 years and reach 35 percent by the early 2030s. Source World Bank Population 65+ relative to working-age population Figure 1 also shows that the pattern is not unique to the United States, although the timing and the magnitude of the rise in the old-age dependency ratio varies across countries. Old-age dependency ratios started to rise much earlier in Japan and somewhat earlier in Germany, with the ratio projected to eventually climb to close to 60 percent in Germany and to 70 percent in Japan over the next 35 years. Population aging is also occurring in countries outside of the advanced world. The dependency ratio is rising in China and it is even projected to begin to climb in Brazil over the next decade. Population aging has also contributed to the pronounced downtrend in real interest rates that has been seen over the past several decades. Figure 2 shows the nominal yield on 10-year U.S. Treasury bonds. Nominal interest rates have fallen sharply since the early 1980s. While some of this decline reflects lower inflation, also shown in the figure, the real interest rate has also declined significantly. Real interest rates have, of course, been particularly low in recent years because monetary policy remains accommodative in the United States and other major economies. They should rise as policy normalizes but are expected to return to levels that are considerably lower than the average for the past few decades. Indeed, formal studies have estimated a marked long-term downtrend in global real interest rates since the 1980s (see, for example, Yi and Zhang 2016). Source Federal Reserve Board and U.S. Labor Department U.S. interest rates and inflation Population aging is contributing to lower real interest rates through a couple of channels (Carvalho et al. 2016). First, people approaching retirement have been saving more to fund their post-retirement years, contributing to the global “saving glut” that has weighed on interest rates. (Other factors thought to have contributed to the global saving glut include the greater demand for “precautionary” foreign reserves by some countries in the wake of the Asian financial crisis in the late 1990s and the greater share of income going to richer households that have higher propensities to save.) People entering retirement may start to draw down their savings, beginning to reverse the effect just discussed, but their departure from the workforce will leave an abundance of capital relative to labor, which depresses the return on capital. Gagnon et al. (2016) explored these different channels using an overlapping-generations model and concluded that demographic developments might account 1.25 percentage points of the decline in equilibrium real interest rates. There is also speculation that population aging is contributing to a downtrend in productivity growth. Feyrer (2008) finds that productivity peaks for the average worker in her 40s. This result could be explained by how the combined effect of different forces affect productivity as people age—in particular, the gains from greater experience and wisdom may dominate the effect from worsening health and declining energy until the 40s but the reverse may be true after that point. That said, the evidence on this issue is mixed. While there some studies confirm the Feyrer results (for example, Aiyar et al. 2016) other studies suggest that the results are sensitive to specification (National Research Council 2012). Furthermore, other papers find no negative relationship between aging and GDP growth across countries (Acemoglu and Restrepo 2017).",1
53,2,Business Economics,17 January 2018,https://link.springer.com/article/10.1057/s11369-018-0066-4,Global aging and public finance,April 2018,Jason J. Fichtner,,,Male,Unknown,Unknown,Male,,10
53,2,Business Economics,04 April 2018,https://link.springer.com/article/10.1057/s11369-018-0075-3,What is holding back housing?,April 2018,Laurie S. Goodman,,,Female,Unknown,Unknown,Female,"The supply/demand imbalance at the national level is quantified in Table 1. This imbalance is reflected in the total new supply calculation, which compares net new units with total new demand, measured by net household formation. In 2017, there were 785,000 1–4 family units and another 347,000 units in multifamily structures (structures with 5 units or more) completed, for a total new construction of 1,132,000 units. To adequately capture gross new supply, we must add the 92,000 new manufactured housing units shipped, for a gross new supply of 1,224,000 units. To then derive net new supply from these gross new supply numbers, we also subtract units lost to obsolescence. There are just over 135 million housing units in the US. Using the Components of Inventory Change from the American Housing Survey,Footnote 1 we can calculate the net obsolescence rate, which includes homes that have been lost to disasters, homes that are uninhabitable without renovation, homes converted to commercial use, and commercial units converted to residential use. We find that the net obsolescence rate in 2017 was 0.31%. This does not suggest that the average home lasts 300 years. This calculation does, however, capture the significant number of commercial to residential conversions that have occurred from 2001 on. Applying our very conservative obsolescence rate of 0.31% to the 135 million housing units in the US suggests that 422,000 units were lost to obsolescence in 2017, leaving a net new supply of 802,000 units. We then compare the supply of 820,000 new units to net new household formation. Between decennial census surveys (the last was conducted in 2010, the next is to be conducted in 2020), two series are widely used: the American Community Survey (ACS) and the Current Population Survey (CPS). The two series show very different numbers for net household formation, with CPS much higher than ACS. Moreover, both series are quite volatile. As a result, we use the two-year rolling average for each series, and average the two. This methodology places total net new household formation in 2016 at 1.05 million; we estimate that this number will increase to 1.15 million in 2017. Thus, we have an estimated supply/demand gap of 348,000 units (1,150,000-802,000). That is, we estimate that 348,000 fewer units were produced than the rate of household formation in 2017. Figure 1 shows how this has looked through time. Net new supply exceeded demand in the early 2000s, as 1.8–2.0 million new homes were being built during this period. There has been a deficit of units since 2009. Sources U.S. Census Bureau, Urban Institute estimates Supply and demand over time. The reasons for this lack of supply are twofold: land costs and labor shortages. Figure 2 shows price indices for home prices, broken down into the price of land and the price of the structure. The series goes back to 1975 and it is scaled so that Q2, 2000 = 1.0. Note that land costs are far more variable than the costs of structures and have appreciated much more since 2000.Footnote 2 In particular, as of Q1, 2016, the total index stands at 1.72, while the land index stands at 1.95, and the structure index stands at 1.58. Land costs are high because there are a significant and growing number of land use restrictions. Emrath (2016) has shown that regulatory costs account for 24.3% of the price of a new single family home. The largest portion of this—approximately three-fifths—is due to the price of the finished lot, resulting from regulations imposed during the lot development, including zoning and sub-division approval as well as the cost of delay. The other two-fifths—9.7%—are the result of costs incurred by the builder after the lot has been finished and purchased. Ganong and Shoag (2015) use the per capita number of state appeals court cases that contain the phrase “land use” to measure land use restrictions over time; this measure is rising rapidly. They show that their measure of land use restrictions is robust, as it has a high correlation with the Wharton Residential Land Use Index (last updated using 2005 data), and it does a good job of explaining the relationship between income and home prices: states with more regulations have higher home values. Source Davis and Heathcote (2007) Price indices: land and structures. In addition to land, labor is very tight. Figure 3 shows the 12-month moving average of construction job openings; the current level is the highest it has been in over a decade. And the labor shortages are likely to grow more acute if immigration is made more difficult. Census data indicate that overall 17% of U.S. workers are foreign-born; in the construction and extraction occupations it is nearly double that share at 29%. Source U.S. Bureau of Labor Statistics retrieved from FRED Labor is tight. There is no easy solution to the supply/demand imbalance, which places significant upward pressure on both home prices and rents, decreasing affordability. While the Federal government can tie transit funding to increased density, this solution helps only at the margin. The more fundamental issue is that most of the constraints on land development are due to local zoning laws, with policies designed to limit growth. The Federal government has no tool that can offset these local laws.",2
53,2,Business Economics,09 April 2018,https://link.springer.com/article/10.1057/s11369-018-0073-5,Importance of the U.S. Bureau of Labor Statistics and critical issues it faces,April 2018,Erica L. Groshen,,,Female,Unknown,Unknown,Female,"Accurate, timely, and readily available statistics are an essential public good in a democratic nation and free enterprise economy. Such statistics help government and private entities make better decisions, producing a more vibrant and efficient economy. By the same token, lack of such statistics, or poor-quality statistics, can lead to bad choices that waste public and private resources and make people’s lives worse. For the United States, the Bureau of Labor Statistics (BLS) produces vital information about jobs and unemployment, wages, working conditions, and prices. A part of the Department of Labor (DOL), the BLS is the second largest, and oldest, statistical agency within the Federal government. For more than 125 years, the BLS has collected, analyzed, disseminated, and improved essential economic information, serving as a key pillar of the knowledge infrastructure of the nation. While I served as the 14th Commissioner of the BLS (from January 2013 to January 2017), I learned much about the agency that I think most data users should know—including the urgency of its funding situation. This article represents an effort to bring my non-BLS colleagues up to speed by organizing and sharing that information in a single document. The first two sections review evidence of the high importance and visibility of BLS data. The third section discusses how BLS maintains data integrity, followed by discussion of evidence of user trust. The fifth section talks about the agency’s continuous efforts to improve efficiency. The sixth part reviews the recent funding history, and the following one discusses what BLS could do with more adequate funding. The final section concludes with what NABE members and other data users can do to help assure that BLS continues to provide that data that we all need in order to preserve our national vitality.",
53,2,Business Economics,04 January 2018,https://link.springer.com/article/10.1057/s11369-017-0063-z,The Power and Independence of the Federal Reserve Peter Conti-Brown,April 2018,M. Cary Leahey,,,Unknown,Unknown,Unknown,Unknown,,
53,2,Business Economics,22 January 2018,https://link.springer.com/article/10.1057/s11369-017-0065-x,2017-12-18 Review of Nicholas Eberstadt’s book,April 2018,Michael W. Horrigan,,,Male,Unknown,Unknown,Male,,
53,3,Business Economics,21 June 2018,https://link.springer.com/article/10.1057/s11369-018-0086-0,From the Editor,July 2018,Charles Steindel,,,Male,Unknown,Unknown,Male,,
53,3,Business Economics,03 May 2018,https://link.springer.com/article/10.1057/s11369-018-0072-6,Is American manufacturing in decline?,July 2018,Kevin L. Kliesen,John A. Tatom,,Male,Male,Unknown,Male,"According to a recent Gallup poll, Americans believe that a vibrant manufacturing sector is “key” to boosting job growth.Footnote 1 At the same time, many appear to believe that American manufacturing is in an irreversible decline because of eroding competitiveness with foreign manufacturers—particularly those in Asia. The declinist view seemed to increase in popularity following the Great Recession and financial crisis. Between December 2007 (the previous business cycle peak) and March 2010, the number of employees in the manufacturing sector declined from almost 13.75 million to about 11.5 million—the lowest level since March 1941. However, the recent plunge is not a new development. In fact, manufacturing employment has been on a secular decline since the late 1970s. From its peak in June 1979 to its recent trough in March 2010, manufacturing employment has declined by about 8.1 million. Although the Great Recession was a severe shock to manufacturing, the longer secular decline in manufacturing employment has reflected other factors. One potential factor has been the persistence of manufacturing (goods) trade deficits. In the declinist view, growing imports have displaced domestic production, thereby triggering a wave of plant closures and lost jobs. Of course, growing trade deficits ultimately stem from the nation’s fundamentals—in this case, lower domestic saving rates. A second potential factor has centered on China and the potential adverse effects of its entry into the World Trade Organization at the end of 2001. This paper discusses these arguments and presents a competing narrative. We argue instead that the U.S. manufacturing sector is fundamentally strong. Why? Because it has historically experienced rapidly rising productivity and output that, despite falling employment, has resulted in a roughly constant share of domestic output (GDP). This has been evident in a historically falling relative price of manufacturing output. While beneficial to manufacturing and to other sectors, relatively high manufacturing productivity that exceeds its output growth implies that the sector’s employment level must decline—much as the agricultural sector has experienced declining employment and rising output in the twentieth century. That said, we are cognizant of the fact that, along with the aggregate economy, manufacturing output, productivity, and employment growth have been unusually slow since 2008.Footnote 2 But since the manufacturing sector is highly cyclical, manufacturing output growth reflects fundamentals in the aggregate economy. In that vein, the slowing growth of population and the labor force, dramatically slower capital formation, and consequent weaker aggregate labor productivity growth have helped to slow the economy’s potential and actual GDP growth rate. Thus, if the declinist view has any credence, it mostly stems from the nation’s slower economic growth process and very weak economic recovery. Should aggregate productivity rebound, this will be reflected in a tremendous boost to the manufacturing sector. The outline of the paper is as follows. In Sect. 2 we show that there are some key recent qualifications to the pattern of U.S. manufacturing trends that prevailed over the post-World War II period. Despite the influence of slowing population and labor force growth and its related effects on investment, capital formation, and productivity, U.S. manufacturing output and, less so, employment growth remains healthy. In fact, relatively faster productivity growth, reflecting innovation and the substitution of labor for capital, continues to shift employment shares away from manufacturing to other sectors, especially services. In Sect. 3, we examine the role of domestic factors—demographics and structural labor productivity growth—in explaining the slowdown in manufacturing employment and a lower manufacturing output share. In Sect. 4, we turn to the role of foreign factors, such as growing imports from abroad, especially from China, in possibly weakening U.S. manufacturing. This Chinese conjecture is termed the Chinese uncertainty hypothesis. Our analysis shows, instead, that there is a strong positive relationship between U.S. manufacturing output growth and growth of goods imports. This occurs because of the key role that imported materials and capital goods play in boosting U.S. manufacturing competitiveness. Although we find some evidence that the time period surrounding China’s entry into the WTO in December 2001 was associated with declines in U.S. manufacturing employment, consistent with the findings of other researchers, we also find that a surge in Chinese imports does not appear to be the dominant explanation for trends in U.S. manufacturing employment since the early 2000s. Section 5 concludes with a brief discussion of the outlook for U.S. manufacturing, given recent legislation to reduce the federal corporate tax rate from 35% to 21% and to introduce immediate expensing of outlays on equipment. We also briefly discuss the Trump Administration’s belief that unilateral and bilateral trade policies, including tariffs or quotas, will lower U.S. imports and the trade deficit. We argue that the adoption of such policies is likely to adversely affect the U.S. manufacturing sector. We undertake no formal modeling exercise to examine whether the net effect of these two developments will be positive or negative for the U.S. manufacturing sector. Nonetheless, our evidence supporting a positive link between imports and manufacturing suggests that trade policy efforts to restrain imports, if successful, will reduce the efficiency and productivity growth in manufacturing that is expected to result from beneficial new regulatory and tax policy initiatives.",1
53,3,Business Economics,04 June 2018,https://link.springer.com/article/10.1057/s11369-018-0083-3,Perspectives on U.S. Fiscal Policy,July 2018,Jeffrey Holland,Edward Leamer,Alice Rivlin,Male,Male,Female,Mix,,
53,3,Business Economics,05 June 2018,https://link.springer.com/article/10.1057/s11369-018-0084-2,Immigration and United States Economic Growth,July 2018,Frederick Treyz,Peter Evangelakis,,Male,Male,Unknown,Male,"Restrictive immigration policy is President Trump’s signature issue. Although any legislation on immigration is currently gridlocked, significant shifts in the status quo could have major implications for the U.S. economy. Immigration is a key contributor to long-term labor force growth, and a substantial change in the current rate of immigration will have significant long-term macroeconomic consequences for the U.S. This paper answers the counterfactual question, “what would happen to U.S. economic growth through 2060 without international immigration?” using the REMI PI+ model in a simulation context. To do this, we reduce by 100% the baseline immigration in the REMI national baseline economic forecast. From this policy shock, we then show the potential population, labor force, employment, and other demographic and economic consequences of ending migration. This scenario provides the foundation for understanding the long-term macroeconomic contribution of immigration, and it provides a baseline to consider the magnitude of the impact of restrictive immigration policies that may fall in between the status quo and the extreme no-immigration scenario.",4
53,3,Business Economics,22 May 2018,https://link.springer.com/article/10.1057/s11369-018-0079-z,The U.S. immigration debate: what’s all the shouting for?,July 2018,Jared Bernstein,,,Male,Unknown,Unknown,Male,,
53,3,Business Economics,14 June 2018,https://link.springer.com/article/10.1057/s11369-018-0081-5,The case against price-level targeting,July 2018,Peter Hooper,,,Male,Unknown,Unknown,Male,"How the Fed will deal with the next economic downturn has become a topic of considerable interest both within the Fed and among Fed watchers. This interest is not driven by fears of an impending downturn, but rather by a growing realization that when the next recession does occur, the Fed may have less ammunition to deal with it than it has had in the past. The central problem is that the Fed has less scope to cut policy rates than it has had in the past because the real neutral level of the fed funds rate (r*) has fallen substantially over time. Various solutions have been proposed. One in particular that is gaining some currency both within the Fed and in academic circles is for the FOMC to shift from targeting the inflation rate to targeting the price level. It is argued that this shift would allow the Fed to aim for a higher rate of inflation late in the business cycle, giving it more scope to cut the real fed funds rate into negative territory when expansion ends and downturn ensues. The main purpose of this note is to outline the case against moving to price-level targeting. The primary argument can be boiled down to the fact that the Fed’s control over inflation is very loose to begin with, and requiring it to hit a price-level target is considerably more demanding than just having to hit an inflation target. Doing so could result in a significant increase in the volatility of output or the depth of recessions. In what follows, we begin by reviewing the central policy challenge facing the Fed in a low r* world. We then touch briefly on the various countercyclical monetary and nonmonetary tools that policymakers will have at their disposal when the next recession hits. Next we drill down in more detail into the price-level targeting solution and review its advantages and drawbacks. We finish by outlining our preferred solution to the Fed’s quandary and by considering how the Fed might deal with the next recession.",
53,3,Business Economics,14 June 2018,https://link.springer.com/article/10.1057/s11369-018-0085-1,A new twist on an old framework: bounded price-level targeting,July 2018,David Altig,,,Male,Unknown,Unknown,Male,"The proposition that an effective monetary policy framework should include a clear definition of price stability might seem so obvious as to require no elaboration. But the essential characteristics of price stability are generally only implicit in the operational goals of central banks. The FOMC statement on longer-run goals, for example, defines 2 percent inflation as consistent with price stability, but leaves unstated the exact reasoning as to why it is so (let alone, why it is “most consistent”). Speaking at a recent Brookings Institution conference (https://www.brookings.edu/events/should-the-fed-stick-with-the-2-percent-inflation-target-or-rethink-it/) devoted to the question of whether the Fed should stick with its 2 percent inflation target, Rick Mishkin (2018) articulated one well-known—and I believe widely accepted—version of what price stability looks like: … [Former Federal Reserve Chairman Alan] Greenspan had this beautiful definition of price stability… he basically said that price stability is when economic agents are not spending a lot of time worrying about inflation in terms of what they’re doing. That is actually something very positive for the economy in terms of getting people to focus on what they really should do which is produce goods with very low cost rather than worrying about financial transactions and dealing with inflation.  Price stability, in other words, is about maintaining confidence in the purchasing power of money. Maintaining this confidence, and hence satisfying the Greenspan definition of price stability, seems to require two related criteria. First, the central bank is committed to reducing the uncertainty associated with inflation. Second, the central bank has a fair degree of credibility in its resolve to deliver on that commitment. To set up the framework discussion below, I am going to suggest two sufficient conditions for meeting the Greenspan definition. Specifically, price stability will be satisfied if: (a) Upon saving $1 today, you can be confident that the real value of that dollar will be within a pre-determined range of some pre-determined target value at any time in the future 
and (b) The range of uncertainty implied by the policy choices in (a) is small enough to yield minimal distortions on production and financial transactions. 
Following Atlanta Fed president Raphael Bostic’s (2018b) language, I’ll refer to (a) as the “principle of bounded nominal uncertainty.” Figure 1 shows a version of the principle of bounded nominal uncertainty in operation. In this example, the “pre-determined target value” follows 2 percent annual growth path for the price level, starting in 1995.Footnote 2 The “pre-determined range” is a symmetric corridor of plus-or-minus 5% about that path. These choices are just illustrative, of course. The underlying path of the price-level, as well as the shape and size of the corridor, is a policy choice. The important point is that at any horizon private decision makers can be assured that the price level will fall within the promised corridor. Sources Bureau of Economic Analysis, Author’s calculations; Haver Analytics A hypothetical bounded price-level target. The framework illustrated in Fig. 1 is a soft form of price-level targeting. It is “soft” because the framework does not strictly require that prices actually follow the 2 percent growth path that anchors the corridor, even in the long run. It only requires that prices not stray too far—in this example more than 5%—from the 2 percent path. For obvious reasons, I’ll refer to this framework as “bounded price-level targeting.”Footnote 3 In setting the acceptable bounds on price-level deviations, a central banker choosing a framework like that in Fig. 1 makes an explicit judgment about what is necessary to deliver acceptably small distortions on production and financial transactions. Bounded price-level targeting as I am presenting it here does not absolutely eliminate long-run uncertainty about the purchasing power of money. But it does limit it.",
53,3,Business Economics,11 April 2018,https://link.springer.com/article/10.1057/s11369-018-0076-2,"A review of  “Windfall: how the new energy abundance upends global politics and strengthens America’s power,” by Meghan L. O’Sullivan",July 2018,Marianne Kah,,,Female,Unknown,Unknown,Female,,
53,3,Business Economics,30 May 2018,https://link.springer.com/article/10.1057/s11369-018-0080-6,"James R. McGuigan, R. Charles Moyer, and Frederick H. deB. Harris (eds.): Managerial economics: applications, strategy, and tactics",July 2018,Thomas Kevin Swift,,,Male,Unknown,Unknown,Male,,
53,3,Business Economics,05 June 2018,https://link.springer.com/article/10.1057/s11369-018-0082-4,"Henry Kaufman: Tectonic shifts in financial markets: people, policies and institutions",July 2018,Francis H. Schott,,,Male,Unknown,Unknown,Male,,
53,4,Business Economics,15 October 2018,https://link.springer.com/article/10.1057/s11369-018-0097-x,From the Editor,October 2018,Charles Steindel,,,Male,Unknown,Unknown,Male,,
53,4,Business Economics,31 October 2018,https://link.springer.com/article/10.1057/s11369-018-0099-8,Monetary policy and risk management at a time of low inflation and low unemployment,October 2018,Jerome H. Powell,,,Male,Unknown,Unknown,Male,,7
53,4,Business Economics,31 August 2018,https://link.springer.com/article/10.1057/s11369-018-0095-z,Explaining the slow U.S. recovery: 2010–2017,October 2018,Ray C. Fair,,,,Unknown,Unknown,Mix,,
53,4,Business Economics,13 August 2018,https://link.springer.com/article/10.1057/s11369-018-0092-2,Improving the usefulness of the Purchasing Managers’ Index,October 2018,Rolando F. Peláez,,,Male,Unknown,Unknown,Male,"Dynamic economies are subject to unanticipated structural breaks. Stock and Watson (1996) note that structural instability characterizes 76 U.S. macroeconomic time series, and 5700 bivariate forecasting relations between those series. Given the magnitude of change in the structure of the US economy, the relationship between the Purchasing Managers’ Index (PMI) and economic activity may have changed. If so, ignoring the break may lead to substantial discrepancies between forecasts and outcomes. This paper shows that a break in the GDP growth–PMI relationship occurred in 2004Q1. Modeling the break significantly improves the PMI’s predictive power for current-quarter GDP growth (Henceforth, GDP refers to real GDP). The PMI—one of the most-closely followed of all indicators—is released on the first business day of each month as part of the Manufacturing Report on Business of the Institute for Supply Management (ISM). Each month the ISM surveys a sample of purchasing executives. Bretz (1990) provides a detailed, if somewhat dated, description of the questionnaire and data collection methods. Respondents compare production, new orders, employment, supplier deliveries, and inventories in the current month to the previous month in terms of higher, same, and lower. Each diffusion index consists of the percent of responses in a positive direction (higher), plus one-half of those reporting the same (Bretz 1990). The resulting indices are then seasonally adjusted. Diffusion indices do not measure rates of change. Several papers document the importance of the ISM indices. Out of twenty-four macroeconomic announcements, the ISM survey ranks seventh in order of importance for trading activity in the Treasury bond market (Fleming and Remolona 1997). Ederington and Lee (1996) report a statistically significant effect of the ISM survey on interest rates in both the U.S. Treasury and Eurodollar markets. Among others, Klein and Moore (1998), Harris (1991), Dasgupta and Lahiri (1992), Kauffman (1999), and Lahiri and Monokroussos (2013) find that the PMI conveys useful information about economic activity. Koenig (2002) notes that the PMI contains information about GDP growth beyond that contained in the Federal Reserve’s industrial production index, and in official reports on employment and retail sales. The next section describes the data. Section 3 tests for a break in the relationship between GDP growth and the PMI. Section 4 evaluates the predictive accuracy for GDP growth of a presumed stable link with the PMI, versus an alternative model that allows for a structural break. Section 5 discusses why the break may have occurred. Section 6 concludes.",
53,4,Business Economics,20 July 2018,https://link.springer.com/article/10.1057/s11369-018-0089-x,Behavioral attitudes toward current economic events: a lesson from neuroeconomics,October 2018,Kavous Ardalan,,,Unknown,Unknown,Unknown,Unknown,,
53,4,Business Economics,30 July 2018,https://link.springer.com/article/10.1057/s11369-018-0090-4,Retirement concerns and planning of cooperative members: a study in the Dutch healthcare sector,October 2018,George Apostolakis,Gert Van Dijk,,Male,Male,Unknown,Male,"During times of economic turmoil, uncertainty about future outcomes increases public interest in issues related to individuals’ future prospects for well-being. At present, the generosity of the welfare state is under pressure, as social-benefit policies are under review in the aftermath of the global financial crisis of 2008 and the European sovereign debt crisis of 2010. The unstable economic environment—together with ongoing discussions regarding pension reforms, which had previously focused on problems related to longevity and the aging population—has increased the concerns of the public, employees, employers, and policymakers regarding the prospects of individuals’ post-retirement conditions. This increased uncertainty has both piqued public interest in individuals’ financial conditions for retirement and induced people to think more about retirement issues. Simultaneously, as the population ages, the need for healthcare and other specialized services continues to grow. Furthermore, increased demand for healthcare services puts more pressure on public finance. Holland et al. (2018) raise the concern to policymakers that future increases in public spending will come from entitlements. Fichtner (2018) also discusses the issue of an aging society as prevalent social problem from the perspective of public finance. There is high concern about retirement issues, and the literature indicates that people are less than adequately prepared to handle future challenges. Several socioeconomic and psychological factors that relate to individuals’ levels of concern and that influence retirement planning decisions have been reported in the literature, including income and wealth levels, financial literacy, and future time perspective. Other factors related to uncertainty that have been less extensively examined in the literature include care provision, future health condition, social inclusion, and loneliness. Owen and Wu (2007) posit that an unstable financial environment is likely to increase the level of retirement-related concern. Knoll (2010) further argues that the recent economic turmoil has increased people’s concerns about their retirement savings. However, little research has focused on the effects of retirement concerns on retirement planning and preferences. This study aims to fill this gap by addressing whether and to what extent retirement concerns (1) are associated with retirement planning and (2) influence retirement preferences regarding the choice of an ideal post-retirement financial situation. We test whether retirement concerns have an influence on retirement preferences and on retirement planning. On the one hand, planning ordinarily acts as an alleviatory mechanism, decreasing concerns about future outcomes. On the other hand, a high level of concern increases the propensity for planning. However, we stress that we do not attempt to establish a causal relationship between retirement concerns and planning. We seek to investigate the relationships among retirement concerns, the perception of an ideal post-retirement situation, and the likelihood that an employee in a cooperative organization engages in retirement planning. We make several contributions to the literature on retirement planning (Hershey et al. 2007b; van Rooij et al. 2011, 2012). First, we examine the impact of retirement concerns on the propensity to plan for retirement based on a sample of cooperative members involved in the Dutch healthcare sector. Second, we examine individual preferences regarding ideal post-retirement situations. In particular, we focus on perceptions of the ideal financial situation and the impact of retirement concerns on such perceptions. We consider concerns regarding an individual’s post-retirement financial situation, living situation, care provision, health condition, and loneliness. This study thus aims to further our understanding of these five factors and their role in retirement planning and in shaping individuals’ post-retirement preferences. Finally, we examine the retirement concerns of members/employees, and we offer suggestions for policymakers to help people better prepare for future retirement challenges. This paper is structured as follows. In the following section, we briefly describe the Dutch pension system and present information regarding pension arrangements and people’s expectations for retirement. Next, we provide a brief review of the literature on the relationship between retirement concerns and retirement planning, and we discuss related findings. We then present the data and methods that we employed in conducting this study, and the subsequent section reports the regression results and analyses. The discussion of the implications of our results is presented in the final section.",
53,4,Business Economics,29 October 2018,https://link.springer.com/article/10.1057/s11369-018-0098-9,Oil and the economy: evolution not revolution,October 2018,Mine Yücel,,,Female,Unknown,Unknown,Female,"Let me first start with an overview of the energy industry. OPEC is practically synonymous with oil today, but OPEC came into existence only in 1960. We had an OPEC of our own in the U.S.; it was the Texas Railroad Commission, which started regulating oil and gas in 1919 and established quotas for producers in 1930. From 1931 to the late 1950s, the TRC controlled more than 40% of U.S. production. The U.S. produced 66% of global production in 1945, for example, which meant the TRC effectively controlled 30% of world oil production. If we look at historical oil prices (Fig. 1), oil prices were “relatively” stable until the 1970s. This is when OPEC started flexing its muscles. When it was formed, OPEC’s objective, ostensibly, was to have fair and stable prices for producers and an economic and regular supply of oil to consumers. Source: BP statistical review
 World oil prices. Prices have not been too stable, though. Geopolitical issues that came to the fore in the 1970s led to volatile oil prices. You may remember (or have read about) oil prices more than tripling in the early 70s—from near $3.50 in 1973 to $11.50 in 1974—during the OPEC oil embargo in response to the U.S. providing arms to Israel in the Arab–Israeli war. With the Iranian revolution, oil prices again tripled, to $37 per barrel in 1981. Of course, prices plunged to the teens in 1986 when Saudi Arabia decided to stop being the swing producer. Decades later, with the growth of China and increased demand, we again saw prices move up (until the Great Recession, of course). High oil prices were the catalyst for the U.S. shale revolution. U.S. shale production started coming on strong after 2008 and really surged through early 2015. U.S. output rose by 4.5 mb/day from 2008 to April 2015. This was basically equivalent to adding another Iraq to the global oil market. The growth of U.S. shale oil threatened OPEC’s hegemony in the market, and the cartel decided in 2014 that it would not constrain output to keep oil prices high since high oil prices help sustained the U.S. shale oil industry. Some OPEC members wanted to cut production, but the Saudis wanted to protect their market share and not have it erode with increased U.S. production. In the end, OPEC decided not to cut production, but rather let the market sort things out—i.e., let oil prices fall and see what happened to U.S. shale. OPEC production rose by 3.5 mb/day from February 2014 to October 2016. I was at a conference in February 2016 when the then-Saudi oil minister Ali al-Naimi said, “We are leaving it to the market as the most efficient way to rebalance supply and demand. It is a simple case of letting the market work.” The next day newspapers were saying, “Naimi declares price war on U.S. shale.” By then, prices had collapsed to $30 per barrel. The U.S. oil industry went through another bust. Production declined by 1 mb/day. Real private investment in mining fell 64%. And all major oil producing states in the U.S., with the exception of Texas, fell into recession.Footnote 1
 However, the decline in oil prices was also quite detrimental to OPEC revenues. OPEC export revenues were more than halved, declining from $1.1 trillion in 2013 to $533 billion in 2015. They declined further to $450 billion in 2016. Given that the Saudis fund most of their government expenses with oil revenues, they depend on a certain oil price and volume of production to make ends meet. The IMF estimates that the fiscal breakeven in 2018 for Saudi Arabia is $88 per barrel. In response, in October 2016, OPEC joined with Russia and a couple of other producers and agreed to cut oil production by 1.8 mb/day. Oil prices started moving back up. U.S. shale producers, meanwhile cut costs, became more efficient and increased production. U.S. production is now near 11 mb/day, almost 2.5 mb/day higher than in mid-2016. The U.S. has now become the second-largest oil producer in the world after Russia. The U.S. oil and gas industry only makes up a small share of total GDP and employment. Of course this share rises and falls with oil prices. At the height of the oil boom in the 1980s, the sector was about 2.6% of Gross Domestic Product (GDP) and 0.8% of total employment. Now, with output higher than anytime in U.S. history, the shares are 1.1% of GDP and 0.3% of employment. Compared with the early 1980s, the industry has become leaner and more efficient. Even though the share of the oil and gas industry is less than half of what it was in the 1980s, oil output today is 2.5 mb/day or about 30% greater (10.96 mb/day vs 8.5 mb/day). If oil is such a small part of the economy, why has it played such a big role? What are the channels by which an oil price shock affects the economy?",2
53,4,Business Economics,13 July 2018,https://link.springer.com/article/10.1057/s11369-018-0088-y,Dani Rodrik: Straight talk on trade—ideas for a Sane World Economy,October 2018,Stuart P. M. Mackintosh,,,Male,Unknown,Unknown,Male,,
53,4,Business Economics,24 September 2018,https://link.springer.com/article/10.1057/s11369-018-0096-y,Douglas A. Irwin: Clashing over commerce: a history of U.S. trade policy,October 2018,David Beckworth,,,Male,Unknown,Unknown,Male,"The revenue regime ran from 1763 to 1865 and was motivated by a desire to raise revenue to pay for the federal government. Tariffs were the only practical way to raise tax revenue during this period and, as Irwin notes, was one of the key motivations for the Constitutional Convention of 1787 as the document gave the federal government the power for the first time to raise revenues on its own. There are some popular trade myths surrounding this period that Irwin convincingly debunks. One myth is that Alexander Hamilton was a protectionist, a champion of raising tariffs to protect domestic industries. Irwin shows, however, that Hamilton was worried first and foremost about the tax base. He wanted to pay off the debts from Revolutionary War and fund the federal government. Consequently, his goal was to have tariffs low enough to encourage trade but high enough to preserve the credit standing of the U.S. government. At the most, he wanted subsidies, not tariffs, for domestic industries. Another myth is that the South fought the Civil War, in part, over tariffs. Irwin acknowledges that there were strong regional differences in tariff preferences. The industrial North wanted higher tariffs to protect its industries, while the textile-exporting South wanted freer trade. These differences led to the so-called Tariff of Abominations in 1828 that raised tariffs so high that it led to threats of secession by South Carolina. The crisis was resolved with the Compromise of 1833 that lowered tariffs to the level desired by the South. Though contentious, the South won the battle over tariffs leading up to the Civil War. Trade policy, then, continued to be used for tax-revenue purposes rather than protectionism up until the Civil War.",
53,4,Business Economics,04 September 2018,https://link.springer.com/article/10.1057/s11369-018-0094-0,U can’t touch this! The intangible revolution,October 2018,Daniel E. Sichel,,,Male,Unknown,Unknown,Male,,1
54,1,Business Economics,04 March 2019,https://link.springer.com/article/10.1057/s11369-018-00114-3,From the editor,January 2019,,,,Unknown,Unknown,Unknown,Unknown,,
54,1,Business Economics,07 February 2019,https://link.springer.com/article/10.1057/s11369-018-00113-4,Financial crises: past and future,January 2019,Carmen M. Reinhart,,,Female,Unknown,Unknown,Female,"This article takes a selective global tour of some of the prominent economic and financial risks in advanced, emerging, and low-income developing economies. The primary emphasis is on near-term risks. The discussion covers areas where vulnerabilities have either already become manifest, such as the situation faced by many emerging markets (EM) in the past year, or those where risks are mounting but have not yet sounded a glaring alarm, including the recent surge in collateralized lending obligations (CLOs) in the United States and Europe and the deterioration in Italy’s fiscal position. A brief discussion of some persistent medium-to-long-term concerns about the rising levels of US public debt and the tensions that arise from internal economic objectives and the external pressures associated with the US dollar’s role as the world’s principal reserve currency completes the discussion. The starting point in the tour is an assessment of the 2008–2009 global financial crisis’ (GFC) recovery.",1
54,1,Business Economics,13 August 2018,https://link.springer.com/article/10.1057/s11369-018-0091-3,Real options? Labor contracts in an uncertain world,January 2019,Julie Byrne,Margaret Hurley,Rowena A. Pecchenino,Female,Female,Female,Female,"How many workers does a firm need to operate profitably? When the “firm” is a manufacturer, say General Motors, the answer to that question may have been relatively straightforward. This is not the case in retail. A “firm”, here more Starbucks than General Motors, can be open from 5:30 in the morning to midnight, but the number of workers it requires to operate profitably over the course of the day, from day to day, and from outlet to outlet can vary hugely. Optimal staffing requires split shifts, short hours, and on-call arrangements tailored to each outlet’s specific needs. This flexible staffing cannot be achieved via a “standard labor contract”,Footnote 1 the union-negotiated, permanent contract. For the standard contract, the unit of analysis was the worker, but the concern was the wellbeing of the worker’s family throughout life, extending to well after the employment relationship had ceased. Such contracts would have been offered to a GM worker although not to the barista at Starbucks. While the institutional and legal legacy of the standard contract remains, the standard labor contract itself is more and more a thing of the past. What is replacing it is a wide array of non-standard contracts, all of which provide the firm with a real option. That real option gives the firm the right, but not the obligation, to flexibly adjust and optimally time the use of its worker’s human capital assets. The provision of the social contract component of the standard contract now falls onto the worker or the state. The demise of the standard contract can be linked, in part, to the short expected lifespans of firms today. At less than two decades, firm longevity is less than a worker’s average work life. Even the GMs of the world must be nimble to survive. Now, non-standard employment relationships, the so-called alternative work arrangements, are replacing standard ones (Katz and Krueger 2016). In contrast to the standard contracts, alternative work arrangements are flexible and diverse, with supervision and employment relationships often divorced from the place of work and for whom the work is done. Alternative work arrangements include part-time contracts, temporary agency contracts, short-term contracts, contingent work, such as zero-hour or on-call contracts, self-employment and/or independent contracting. Many of these contracts have real option characteristics since they give the firm the needed flexibility to make contingent decisions about real labor assets (Sick 1989). Standard contracts are desirable to workers because of the social contract under which they were established and the law and custom supporting them. They provide job, income and social security benefits, such as healthcare and pensions. These features make them costly to firms (Kalleberg 2003). Alternative work arrangements, in contrast, provide flexibility (Berg et al. 2014; Drache et al. 2015; Wilthagen and Tros 2004). They allow firms to control costs, to improve efficiency, and to match their just-in-time inventory systems or the peaks and troughs of retail foot traffic with just-in-time labor input (Kalleberg 2003), potentially shifting demand and cost risk onto their workers. They have brought about changes in internal firm structures (Guidetti and Pedrini 2013; Lindbeck and Snower 1988; Piori 1986) and how work is organised (Broschak and Davis-Blake 2006; Davis-Blake, et al. 2003; Kalleberg 2003). Alternative work arrangements, unlike standard contracts, are not governed by a risk-hedging social contract. The worker is on his or her own or dependent on the state for what was once, for some, employer provided. In this paper, we examine the optimal labor contracting decisions of a firm not obligated by union agreement or custom and tradition to offer a “standard contract”. Its goal is to maximize its market value. The firm faces uncertainty over demand for its product or service and its costs of production. Maximizing value requires that it minimizes the cost of this uncertainty. To set the scene, we begin in Sect. 2 by discussing a firm’s labor demand decision when its goal is maximizing owner wealth. We discuss the different types of labor contracts available to the firm, determining when non-standard contracts generate real option value. In Sect. 3, we characterise the firm’s problem under a variety of assumptions regarding the demand and cost uncertainty it faces and on the flexibility of the human capital/labor contracts it can offer.Footnote 2 We determine when non-standard contracts, the alternative work arrangements, have real option value to the firm and interpret the strike condition, the value at which the non-standard contract dominates the standard contract for new hires. We discuss what this means to the worker offered a non-standard rather than a standard contract. In Sect. 4, we apply our analysis to case studies that mirror our contracting structures. Here we assess the cost and benefits of these contracts to the workers depending on the type of contracts the firm provided and social benefits made available to them. In Sect. 5, we offer some concluding recommendations on how the option value of flexibility can be shared between the firm and the worker to the benefit of both and the economy as a whole.",
54,1,Business Economics,05 December 2018,https://link.springer.com/article/10.1057/s11369-018-00106-3,"Heigh Ho, Heigh Ho: flexible labor contracts with real option characteristics",January 2019,Julie Byrne,Rowena A. Pecchenino,,Female,Female,Unknown,Female,"The issue of standard—inflexible—versus alternative—flexible—work arrangements is not new. Both workers (Anderson et al. 2003; Euwals 2001; Blau and Shdyvko 2011; Gielen 2009; Drago et al. 2009; Skinner and Pocock 2010; Waterhouse and Colley 2010) and firms (Askenazy 2004; Connolly and Gallagher 2004; May et al. 2013; Hopkins and Fairfoul 2014; Berg et al. 2014) desire or require flexibility. Flexibility in any decision has value. Quantifying that value, whether from the perspective of the prospective McDonald’s worker considering a zero-hours contract or McDonald’s considering offering or withdrawing such contracts, remains a challenge. However, in many cases it can be done. Real option analysis, as developed in the finance discipline, provides a framework to evaluate the strategic impact flexibility has on the individual’s labor supply or the firm’s labor demand decisions. In this paper, we contribute to the labor economics literature by using a real options framework to understand a worker’s labor supply decision. To the extent that real option contracts in the labor market have been considered, it has been from the firm perspective, where human capital is viewed as an asset. Since the goal of a firm is wealth maximization, the firm takes its investment decisions to achieve this goal. Defining human capital as an asset rather than a person makes identifying the real option value in labor contracts to the firm relatively straightforward. For a non-standard employment contract to have real option value, it must both lead to future choices and enable advantageous access to future opportunities. Such contracts are valuable to firms when they give firms the flexibility to change decisions as new information reveals itself. Unlike the firm, individual workers do not consider themselves as assets whose value is to be maximized. Rather, they seek to maximize their overall utility by choosing the optimal combination of work and leisure. The concept of real options provides a useful framework for assessing the value of labor contract components to achieve this utility goal. Using the real options framework, we argue that in many cases non-standard work contracts have real option value embedded in them for both the worker and the firm. The value is contingent on the type of non-standard labor contract being implemented. The reason there can be real option value to both worker and firm is due to the difference of the overall goal of the counterparties to the non-standard work contract. Employers will issue non-standard employment contracts if they allow them to increase efficiency, limit losses and/or maximize gains, which is all in line with their wealth maximization goal. Workers may accept non-standard contracts if they enable them to choose work hours, where the work is done, the rate of pay, and/or when the contract is terminated. For such workers, these contract features can be far more valuable to their overall goal of utility maximization than a rigid fixed employment contract, despite the benefits traditionally associated with such contracts. We examine a series of case studies to illustrate our point and to identify who gets the real option value—the worker, the firm, or both. The paper proceeds as follows. In Sect. 2, we explore labor contracting from a real options perspective, showing how a real options framework can reveal implicit value, and then examine the evolution of labor contract flexibility from the supply side and the demand side. In Sect. 3, we examine the individual’s labor supply choice when, given a fixed market wage, contracts are perfectly flexible, when contracts are perfectly inflexible, and when the wage is a random variable as in many gig economy settings, respectively. In Sect. 4, using our framework in the context of case studies, we characterize the option value of flexibility from the worker’s as well as the firm’s perspective. In Sect. 5, we address the difficulties inherent in empirically establishing a specific monetary value for the real option characteristics of flexible labor contracts and propose an indirect alternative. Section 6 concludes.",1
54,1,Business Economics,06 February 2019,https://link.springer.com/article/10.1057/s11369-018-00111-6,Business strategy and firm location decisions: testing traditional and modern methods,January 2019,Patrick L. Anderson,,,Male,Unknown,Unknown,Male,"For nearly a century, economists, managers, and lawmakers have relied upon the neoclassical principle of a “profit-maximizing firm.” This principle underlies decades of business school teaching, numerous aspects of finance, and a panoply of models for inventory management, yield management, and pricing policies.Footnote 1 Indeed, the principle of a firm choosing a location to maximize profits dates at least as far back as the landmark works of David Ricardo and Johann Heinrich von Thünen, predating the emergence of the neoclassical school of economics by a half century.Footnote 2 Even the “new economic geography” that emerged in the last years of the 20th century relies upon an updated version of the profit-maximizing firm.Footnote 3 However ubiquitous the principle, the practice appears quite different: Corporate managers commonly state that they focus on “shareholder value,” and explain costly investment decisions, risky acquisitions, and speculative research expenditures on that basis. Excessive focus on maximizing profit has even been characterized negatively as “short-termism.” American legal doctrine, which at one time was firmly supportive of the profit-maximizing goal of the corporation, has shifted toward both “shareholder value” and the concept of businesses pursuing larger societal goals.Footnote 4 Machine learning methods, often supported by “big data” collection and storage, increasingly drive advertising decisions, planned maintenance, yield management, and real-time pricing policies. The focus of these methods is often maximizing metrics other than profits, such as page views, purchases, number of users, and “clicks.” The standard curriculum in graduate programs for business and economics now recognizes the value of managerial flexibility, the importance of “real options,” and the use of hedging and other strategies to minimize risk.Footnote 5 The “world is flat” meme questions whether location decisions (and, implicitly, the costs associated with doing business in any one place) really matter in the 21st century. While not quite a theory, this idea has generated wide currency in the popular press and heated intellectual debate.Footnote 6 A number of widely recognized corporations have risen to extremely large market capitalizations without profits; some very large and highly profitable firms have made an explicit policy of not distributing dividends to their stockholders.Footnote 7 None of these phenomena are consistent with the principle of maximizing profits.Footnote 8 What best explains important business decisions, if not profit maximization? We use a natural experiment, and a very large dataset covering multiple categories of information, to examine this question. We focus on the following important business decision: where do medium- and large-sized firms choose to locate? We examine four approaches, two traditional and two modern, to model such an important business decision: 1. A traditional income model of the firm. This approach is based on the profit-maximizing principle from neoclassical theory, and relies upon financial statement analysis and cost-minimization methods that are commonly taught in business and management courses. Both professional judgement and analysis of financial statement data are used in this model, which requires only rudimentary calculations. 2. An index of key variables based on professional judgment. This approach is representative of the notion that wisdom, experience, and judgment are superior to purely financial models. It uses the skills that business economists have traditionally employed. We have examples from three different groups of economists for this approach. 3. A suite of machinelearning models. This approach represents the dramatic growth of machine learning and “big data” methods in the business world. It also represents a fundamental challenge to the notion of structured business decisions, suggesting that assembling a large amount of data, and “letting the data decide,” is often superior to traditional managerial or economic methods. Among these approaches, this makes the widest use of the large amount of data, albeit with a computational burden that is exponentially larger than with the traditional approaches. We used dozens of variations of such models for the comparison in this article. 4. A recursive decision model of a value-maximizing firm. This novel approach, which has become practical only recently, makes use of advances in control theory and computational methods. It replaces the neoclassical principle of profit maximization with a new one of value maximizing. This approach relies upon the same financial statement analysis as a traditional income model, but takes into account management options such as the ability to change course in the future. To solve such a model requires the use of functional equations and recursive methods. The amount of professional judgment on variables is somewhat higher than with the traditional approaches, as is the computational burden. To establish a benchmark, we also developed “coin flip” and “educated guess” models. These represent the predictive results that could be expected without any expensive investment in quantitative methods. Each of these is described in Sect. 1.2. Additional technical information is included in the “Methodology Appendix.” The data needed to use four different approaches are both large and varied. We include data on all of the following: Financial statement data on Amazon, Inc., as reported in their periodic financial disclosures. We supplement this company-specific data with industry and economy-wide data relevant to large retail and technology firms. These data informed our income model of the firm. The population, location, and airport service characteristics of large cities and populous metropolitan areas in North America. These data were used to identify large cities that were candidates for a major investment for a large-sized firm. A wide variety of quantitative variables, covering over 50 indicators across 56 cities. These data extend far beyond those traditionally used in business economics, and include the following: Traditional cost variables, such that of real estate, and wage and benefit costs for specific categories of workers. Size of the workforce, the number of graduates of higher education institutions in the area, and immigrants to the region with advanced degrees. Amenity indicators, such as “walkability,” availability of parks, and number of sunny days each year. Sentiment variables, such as the share of votes for both major political parties in two different election years. Business tax burdens. Availability of an international airport with regular flights to certain cities. Transit and transportation use indicators, including hours lost to congestion, and number of trips on mass transit. Indices of economic freedom, representing policies related to worker and employer liberty. Indicators of disparity in income and racial inclusion. The share of households speaking English only. The number of major league sports teams. It is important to note that these variables were not selected for a specific structural model. Furthermore, the indicators are of varying quality, overlap conceptually, and in some cases are produced by entities with explicit policy agendas. The use of a large number of variables covering multiple topics is characteristic of the machine learning approach, and allows for the data to partially determine both the parameters and the structure of the model. Information on the competition among cities for the Amazon, Inc. “HQ2” that began in 2017. This includes the original RFP document outlining the intended HQ2 facility and its operation; the factors they considered important for their selection; the identity of many of the over 200 cities that submitted responses; and the 20 that were selected in January 2018. A set of three predictions made in the Fall of 2017 by business economists, all of which were published after the Amazon HQ2 announcement in September 2017 but before their selection of candidate cities in January 2018. These included: The “HQ2 Index” assembled by a group of business economists at Anderson Economic Group, which included costs, workforce, higher education, tax burden, and transit variables for 35 metropolitan areas within the US. This dataset was later augmented by disaggregating the cities that were within the large metropolitan areas surrounding New York and Washington DC, and by adding cities (including Toronto, Canada, and Columbus, Ohio) that were originally excluded. A prediction of 20 MSAs made by the Brookings Institution, based on a published rationale. A prediction of 10 MSAs made by Moody’s Analytics, which included a list of factors and an economic rationale for their use. This large and varied set of data allowed us to calculate predictions, and compare the accuracy of predictions, for a wide range of models that fit into four different approaches. These data are further described in the “Data Appendix.” Exploratory data analyses of variables used in the HQ2 Index are shown in Fig. 1 and Fig. 2.  Exploratory Data Analysis of Cost Data Exploratory Data Analysis of Workforce and Transportation Data",3
54,1,Business Economics,02 November 2018,https://link.springer.com/article/10.1057/s11369-018-0100-6,Are yield-curve/monetary cycles’ approaches enough to predict recessions?,January 2019,Azhar Iqbal,Sam Bullard,John Silvia,,,Male,Mix,,
54,1,Business Economics,21 January 2019,https://link.springer.com/article/10.1057/s11369-018-00108-1,When will the longest expansion end?,January 2019,Robert J. Gordon,,,Male,Unknown,Unknown,Male,,
54,1,Business Economics,03 January 2019,https://link.springer.com/article/10.1057/s11369-018-00110-7,The business cycle is alive and well,January 2019,James H. Stock,,,Male,Unknown,Unknown,Male,,3
54,1,Business Economics,10 December 2018,https://link.springer.com/article/10.1057/s11369-018-00105-4,A look back at a luck-filled career,January 2019,John Silvia,,,Male,Unknown,Unknown,Male,"One question I am often asked is, how specifically did you manage and communicate with people who have different levels of education in economics and skill sets? Thanks to word of mouth by some friends, I was offered the position of chief economist on the U.S. Senate Banking Committee under Chairman Senator Phil Gramm (R-TX) who also has a Ph.D. in economics. My fellow staffers did not need a monthly forecast nor a lesson in econometrics. Here, though my formal economics education exceeded everyone’s but Senator Gramm’s, my legislative experience was nil. What my staff needed from me was a straightforward, honest assessment of the economic, not political, implications of many proposals. Communication came through our discussions, very short memos, and the notion that I was available to attend meetings with senators on both sides of the issue. There were no indicators or weekly reports, nor a monthly outlook. They did not need a lecture. They needed a discussion—both pros and cons (yes, a two-armed economist). This sixth principle I took away from my experience on Capitol Hill. When making a presentation, do not lecture. Present your materials as a discussion. When your colleagues are less familiar with economics than you, discuss and simplify, do not pontificate on complex principles. From my Capitol Hill experience, I had two more takeaways. On expertise, committee staff are experts in their respective fields, even without the academic Ph.D. In addition, I learned to appreciate the knowledge of issues that came from lobbyists. That may sound crazy to someone who never worked on the Hill. While I may not have agreed with some of their views, lobbyists certainly knew their issues. They sharpened my input of the issues to my staff director and the senators. Second, I kept up my friendships with staff members from both sides of the aisle after I left. It may have been a very different world then in terms of the political environment than today, but the lesson is that when you have a culture of getting things done, no matter what the institution, friendships on both sides are essential. Focus on what the client or co-worker needs. One challenge in our profession is that there is a tendency to tell all we know (especially in overly lengthy PowerPoint presentations).",
54,1,Business Economics,21 November 2018,https://link.springer.com/article/10.1057/s11369-018-0101-5,Adaptive markets: financial evolution at the speed of thought by Andrew Lo,January 2019,Richard Berner,,,Male,Unknown,Unknown,Male,,1
54,1,Business Economics,23 August 2018,https://link.springer.com/article/10.1057/s11369-018-0093-1,Susan Wharton Gates: Days of Slaughter,January 2019,Frank E. Nothaft,,,Male,Unknown,Unknown,Male,,
54,1,Business Economics,27 June 2018,https://link.springer.com/article/10.1057/s11369-018-0087-z,Book review for “The High Cost of Good Intentions”,January 2019,Bill Dupor,,,Male,Unknown,Unknown,Male,,
54,1,Business Economics,01 January 2019,https://link.springer.com/article/10.1057/s11369-018-00112-5,"Review of the fed and Lehman brothers: setting the record straight on a financial disaster, by Laurence Ball",January 2019,John A. Weinberg,,,Male,Unknown,Unknown,Male,,
54,1,Business Economics,12 December 2018,https://link.springer.com/article/10.1057/s11369-018-00109-0,Review of Economic indicators for professionals by Charles Steindel,January 2019,Diane Coyle,,,Female,Unknown,Unknown,Female,,
54,2,Business Economics,06 May 2019,https://link.springer.com/article/10.1057/s11369-019-00127-6,From the editor,April 2019,Charles Steindel,,,Male,Unknown,Unknown,Male,,
54,2,Business Economics,29 April 2019,https://link.springer.com/article/10.1057/s11369-019-00125-8,Forces engendering the long-term economic outlook,April 2019,Alan Greenspan,,,Male,Unknown,Unknown,Male,,
54,2,Business Economics,19 March 2019,https://link.springer.com/article/10.1057/s11369-019-00120-z,The third post-world war II wealth bubble,April 2019,Eugene Steuerle,,,Male,Unknown,Unknown,Male,"At the end of the 3rd quarter of 2018, based on the Financial Accounts of the United States as estimated by the Board of Governors of the Federal Reserve System, the ratio of net worth (here both “net worth” and “wealth” refer to assets less liabilities) of US households to GDP hit an all-time high 5.3 based on measures going back to the end of World War II (Fig. 1).Footnote 3,Footnote 4 As thorough as these data are, their limitations still restrict interpretation of the causes of these bubbles. In the ideal, one would have available data on worldwide measures of wealth and income, as bubbles in the US and abroad can be negatively correlated, as when a burst abroad leads to a further flight to safety in US assets, or positively correlated to common worldwide factors, such as aggregate worldwide saving (demand) relative to new investment opportunities (supply). Also, since my focus is on all asset markets, not just the stock market, I would like to have had a reliable and time-consistent measure of total returns to capital, including from household real estate and the capital portion, if it could be measured, of returns to partnerships, Subchapter S corporations, farms, and self-employment. With such a parsing, it might have been possible to separate further how much bubbling was due to changes in the earnings share of all capital for the economy, new investment, and other factors, such as movement of capital from farms to corporations to pass-through businesses. Source Federal Reserve Z101 Tables Household Net Worth as a Percentage of GDP, 1946–2018. Households include nonprofit organizations. Household real estate excludes business assets. Just before the Tech Bubble Recession and the Great Recession, all-time highs were reached, at 4.5 at the end of the first quarter of 2000, and 4.9 at the end of the first quarter of 2007. Based on end-of-quarter data, the drop, peak to trough, was about 10% in the first case (by the third quarter of 2002) and 17% in the second (by the first quarter of 2009). In both cases, the drop was to almost the same level, at about 4 times GDP. Yet, even this ratio represented a new “high low” since, by comparison, in no single quarter before 1998 had the household net worth-to-GDP ratio ever reached 4.0 or higher. In effect, to the extent that monetary and fiscal policy limited the impact of these recessions by curtailing the fall-off in wealth values and the further downward economic spiral likely to follow, they still left household wealth-to-income ratio on a higher-than-normal plateau.",
54,2,Business Economics,29 April 2019,https://link.springer.com/article/10.1057/s11369-019-00130-x,What is going right in manufacturing?,April 2019,Mark Vitner,Azhar Iqbal,,Male,,Unknown,Mix,,
54,2,Business Economics,10 December 2018,https://link.springer.com/article/10.1057/s11369-018-00107-2,From Keynesianism to the knowledge economy: the rise and fall of growth regimes,April 2019,Peter A. Hall,,,Male,Unknown,Unknown,Male,,3
54,2,Business Economics,02 January 2019,https://link.springer.com/article/10.1057/s11369-018-00104-5,Nudging: a very short guide,April 2019,Cass R. Sunstein,,,,Unknown,Unknown,Mix,,
54,2,Business Economics,30 January 2019,https://link.springer.com/article/10.1057/s11369-019-00117-8,Correction to: Nudging: a very short guide,April 2019,Cass R. Sunstein,,,,Unknown,Unknown,Mix,,
54,2,Business Economics,17 January 2019,https://link.springer.com/article/10.1057/s11369-018-00115-2,Our troubling fiscal situation,April 2019,Douglas W. Elmendorf,,,Male,Unknown,Unknown,Male,"
Federal debt is on an unsustainable path. This is a very large long-term problem. I don’t think it’s a very large short- or even medium-term problem, because interest rates are quite low. I don’t mean just that short-term rates are quite low right now; they are, but they’ll continue to rise given what is happening in the economy. What I mean is that interest rates, in general, have been declining in this country for a few decades. That precedes the financial crisis. It has not been overturned by the sharp run-up in US debt during the crisis and the recession. It’s actually common to a number of other countries as well. What those lower interest rates are telling us is that people have greater willingness to supply money to our government and other governments with a very low return because they value the safety, the liquidity, the convenience or some other feature. Now, that could go away, and it could go away unexpectedly. But that low interest rate environment didn’t occur all at once. It really has occurred over a few decades, even though I and many other economists did not fully grasp it until the research that has been done in the last few years. That environment buys us time. We don’t have to narrow the deficit for next year or the year after, or so on. But we are still in this country, as you may know, in the process of raising the full retirement age for Social Security through legislation that was passed in 1983, 35 years ago. If we want to set in motion changes in the budget, certainly changes in spending programs for older Americans, we want to give them some advance warning. Therefore, we need to get those changes started now, and use the extra time that we have from low interest rates to phase out those changes gradually rather than having to make any dramatic changes overnight. The reason why we want to move a little slowly, in terms of the actual impact on the deficit numbers, is partly because we also have a problem of not investing enough. With low interest rates, the signal that is sent is that we should probably do more public physical and human investment than otherwise. However, in fact, federal investment in infrastructure and education now, as traditionally measured in the nondiscretionary appropriations part of the budget, has been the smallest percentage of GDP ever in my lifetime. So we’re going the wrong way in investment and are following a misguided approach to reducing deficits. We have to invest more and, then, set in motion longer-term budgetary changes.",
54,2,Business Economics,15 April 2019,https://link.springer.com/article/10.1057/s11369-019-00126-7,How federal deficits could hurt state and local governments,April 2019,Kim Rueben,,,,Unknown,Unknown,Mix,,
54,2,Business Economics,13 February 2019,https://link.springer.com/article/10.1057/s11369-019-00118-7,"Fundraising, social media and tourism in American symphony orchestras and opera houses",April 2019,Angela Besana,Annamaria Esposito,,Female,Female,Unknown,Female,"Meaningful players in the performance arts industry, American symphony orchestras and opera houses have been facing keen competition for resources since the beginning of the global financial crisis in 2007. Turbulent times stimulated marketing officers and fundraisers to exploit willingness-to-pay and willingness-to-donate. Their strategies have included single tickets, flexible subscription series, friendships, memberships, planned giving and donations, so that revenues have been diversified and maximised.Footnote 1 Today, audience development concentrates mainly on local communities. Orchestras and opera houses supply their local communities with education programs: performances, musical activities and other events that deepen the experience of orchestral music and music education for communities.Footnote 2 Music tourists are the next frontier of marketing. Music tourists can be attracted by single events, festivals, and special events with stars, whether instrumentalists or singers. Music tourism is about performances and festivals with backgrounds and traditions of music genres and myths, memories of famous concerts, bands, divas and interpreters. It encompasses wide ranges of interests, advocacy and emotions.Footnote 3 Fundraising still represents the main source of revenue for orchestras and opera houses.Footnote 4 International patrons are the frontier of fundraising, whose pursuit can correspond with marketers’ goals, when the tourist becomes the international patron. Social media are revealing themselves as leading and innovative tools, by engaging new marketing-oriented and fundraising-oriented targets and segmenting them.Footnote 5 By using a normal mixture cluster analysis of revenues and expenses in 2015, and considering a cultural tourism ratio, economic performances and targets of social media, this paper segments a sample of some of the largest American symphony orchestras and opera houses, as determined by 2015 total income.Footnote 6",4
54,2,Business Economics,07 March 2019,https://link.springer.com/article/10.1057/s11369-018-0102-4,"Big is Beautiful: debunking the myth of small business, by Robert D. Atkinson and Michael Lind",April 2019,Ryan A. Decker,,,,Unknown,Unknown,Mix,,
54,2,Business Economics,28 January 2019,https://link.springer.com/article/10.1057/s11369-019-00116-9,"Unelected Power, the quest for legitimacy in central banking and the regulatory state, by Paul Tucker",April 2019,Catherine L. Mann,,,Female,Unknown,Unknown,Female,,
54,2,Business Economics,23 April 2019,https://link.springer.com/article/10.1057/s11369-019-00129-4,Volcker and de Larosière: the end of giants?,April 2019,Stuart P. M. Mackintosh,,,Male,Unknown,Unknown,Male,"Paul Volcker’s roots are made of sturdy German stock. His father set the tone. As the town manager in Teaneck, New Jersey during the Great Depression, Volcker’s father balanced the books, cut taxes, and always did what was he felt prudent and right. Appearances always mattered. Because of this, the young Paul could not take a municipal job—even an internship in the summer. It would just not do; it would smack of favoritism. Clearly Volcker’s father was instrumental in influencing not only young Paul’s commitment to public service, but also his views on the proper and prudent behavior required—nay demanded—of all public officials. de Larosière’s roots go back to provincial France, with a series of town councilors, and soldiers in his family tree, including his father who was a naval officer in the First World War, and a diplomat at the onset of the Second World War. de Larosière states movingly, “I have nothing but happy memories of my parents. They surrounded me with love and affection, and also gave us a sense of work and duty.” Both men’s lives could be subtitled “do your duty.”",
54,3,Business Economics,22 July 2019,https://link.springer.com/article/10.1057/s11369-019-00136-5,From the Editor,July 2019,Charles Steindel,,,Male,Unknown,Unknown,Male,,
54,3,Business Economics,15 April 2019,https://link.springer.com/article/10.1057/s11369-019-00121-y,Perspectives on debt and deficits,July 2019,Paul Krugman,,,Male,Unknown,Unknown,Male,,7
54,3,Business Economics,28 March 2019,https://link.springer.com/article/10.1057/s11369-019-00122-x,Financial stability and monetary policy,July 2019,William C. Dudley,,,Male,Unknown,Unknown,Male,,1
54,3,Business Economics,11 April 2019,https://link.springer.com/article/10.1057/s11369-019-00123-w,Financial stability and monetary policy,July 2019,Nellie Liang,,,Female,Unknown,Unknown,Female,,1
54,3,Business Economics,13 May 2019,https://link.springer.com/article/10.1057/s11369-019-00131-w,Does China change the game?,July 2019,Adam S. Posen,,,Male,Unknown,Unknown,Male,,
54,3,Business Economics,09 April 2019,https://link.springer.com/article/10.1057/s11369-019-00124-9,U.S. economic security and the challenge from China,July 2019,Peter Morici,,,Male,Unknown,Unknown,Male,,1
54,3,Business Economics,22 May 2019,https://link.springer.com/article/10.1057/s11369-019-00132-9,The economic and fiscal consequences of immigration: highlights from the National Academies report,July 2019,Francine D. Blau,Jennifer Hunt,,Female,Female,Unknown,Female,"The report documents current trends in immigration, and characteristics and outcomes of immigrants relative to natives. While in 1995, only 9% of the U.S. population was an immigrant (foreign-born), by 2014 this figure had risen to 13% (and by 2017 to 13.7%, according to the Migration Policy Institute (MPI)Footnote 2). The increasing immigration rate over some decades has led to nearly one in four American residents being either an immigrant or child of an immigrant in 2014. The unauthorized population also grew over the period, from an estimated 5.7 million in 1995 to 11.1 million by 2014, but the growth was uneven: the unauthorized numbers shrank between 2007 and 2009, and have since leveled off (MPI’s estimate for 2016 is 11.3 million). Because immigrants are younger on average than natives, and more likely than natives to be of working age, immigration has slowed the aging of the American population and immigrants have increased faster as a share of the labor force than of the population: up from 11 to 16% in the 20 years preceding the report (and reaching 17.1% by 2017 according to MPI). Immigrants and their children account for the vast majority of current and future labor force growth. Over time, the immigrant population has grown more educated, albeit at a slower rate than the native-born population. Figure 1 shows that the average education of recent immigrants rose from 10.2 years in 1970 to 12.6 years in 2012 (right axis), as the share of immigrants not having completed high school declined from 51 to 26% and the share with a Bachelor’s degrees and above increased from 20 to 36%. Another important change is that, since the 1990s, the immigrant population has shifted away from traditional gateway cities, in California and New York, in particular, and dispersed to states and communities with historically few immigrants. Nevertheless, over half of the foreign-born population is concentrated in just ten metropolitan areas. Source Analysis of 1970, 1980, 1990 and 2000 Decennial Census data, and 2010–2012 ACS data Education attainment of recent immigrants (those who entered in the 5 years prior), 1979–2012.",4
54,3,Business Economics,02 July 2019,https://link.springer.com/article/10.1057/s11369-019-00135-6,The title insurance industry: infusing innovation and competition,July 2019,Thomas A. Hemphill,,,Male,Unknown,Unknown,Male,"“Title insurance is an almost completely unnecessary scam perpetrated on the public by an oligopoly with the help of regulators”, says Jonathan Tepper and Denise Hearn in their recent book, The Myth of Capitalism: Monopolies and the Death of Capitalism (2019, p. 135). Strong words, indeed, for a product that generally flies below most American consumers’ personal radar—until they are purchasing a home. Predominantly found in the U.S., title insurance is a form of indemnity insurance against financial loss from certain defects, e.g., construction, creditor, and tax liens, court judgments, etc., in title to real property, as well as from the invalidity or unenforceability of mortgage loans (Hayes 2019b). Most of the industrialized world, however, uses land registration systems—called Torrens title—for the transfer of land titles or interests in them (Arrunada and Garoupa 2005). Under these land registration systems, the government determines title ownership and encumbrances using its land registration, and with few exceptions, the government’s determination is conclusive (Arrunada and Garoupa 2005). There are two types of title insurance: owner and lender (Hayes 2019a). For owners, either the property owner or seller may buy an owner’s policy (National Association of Insurance Commissioners 2018). However, in many jurisdictions, a property seller pays for owner title policies as a part of their obligation in the transfer of title to the property purchaser (National Association of Insurance Commissioners 2018). If a mortgage is required to purchase a property, nearly all lenders require that the property buyer purchase the lender’s title insurance policy (as part of the loan collateral secured by real estate) for an amount equal to the mortgage loan (National Association of Insurance Commissioners 2018). In the U.S., state recorders of deeds generally do not guarantee indefensible title to recorded titles; thus, title insurance will defend against litigation. i.e., cover legal fees, brought against the title or reimburse the insured for the monetary loss incurred up to the dollar amount of insurance provided by the policy (National Association of Insurance Commissioners 2018). The cost of title insurance consists of a one-time premium charge and service fee, and in some states, such as California, can cost $2000 or more per home purchase (Andrews 2018; Hayes 2019a). Unlike non-labor intensive property and liability policies, a title policy might require the transcription of a complex legal description unique to the insured property, along with enumeration of often equally complex and unique terms of easements or other special property rights (A. M. Best 2019). In property and liability lines of insurance, agents’ commissions generally are in the range of 10% to 25% of the premium on the policies that agents write, while in title insurance, the agent retains a much larger proportion of the amount charged, typically in the range of 60% to 90% of the premium (A. M. Best 2019). Regulation of the title insurance industry is a primary responsibility of state government. Title insurance rates, generally structured as dollars per $1000 worth of mortgage debt, differ from state-to-state, and state rate regulation systems vary (Andrews 2018). For example, twenty states use a “file and use” system, where title insurers choose their own rates, and the state government has the authority to reject the title insurer’s choice (Andrews 2018). Likewise, sixteen states have title insurers seek prior approval from a regulatory agency for the rates they charge, while ten states have no direct rate regulation (Andrews 2018). In all but a few states, laws restrict major insurance companies from offering title insurance, thus largely eliminating a source of potential competition. Title insurance agents/companies search public records to develop and document the chain of ownership of a property. If any liens or encumbrances are uncovered, the title company might require that the property owner take steps to eliminate them before issuing a title policy. Title insurance companies historically have had low loss ratios (between 4 and 12% of premium dollars on claims) because title underwriters perform extensive underwriting research on subject properties before issuing a policy (A.M. Best Special Report 2010). Research costs, the need to fund long-tail loss reserves, and high allocation claims expenses when a claim does occur, cause title insurance expense ratios to be higher than other property/casualty lines of insurance (Keleher 2012). Allegedly, this “pre-underwriting” enables the issuing company to avoid issuing coverage on a property with a “questionable” title history.",2
54,3,Business Economics,15 March 2019,https://link.springer.com/article/10.1057/s11369-019-00119-6,Richard Collier and Joseph Andrus: transfer pricing and the arm’s length principle after BEPS,July 2019,William J. Seeger,,,Male,Unknown,Unknown,Male,"In 2013, the Organization for Economic Cooperation and Development (OECD) and the Group of 20 (G20) governments embarked on the most significant revision of the international tax rules in a century. The Base Erosion and Profit Shifting initiative (BEPS), launched during the 2007 Financial Crisis, had one goal: revise international tax rules to align them with the major commercial developments in the global economy. This revision explicitly considered the continuing viability of the Arm’s Length Principle (ALP) as the worldwide tax standard in the allocation of income among members of a controlled multinational group. With the recent conclusion of the BEPS project, now is the ideal time to step back and reflect on the accomplishments and limitations of the BEPS program, as well as the work remaining to address ongoing challenges. In this regard, the authors have done a splendid job; they are uniquely qualified to assess the accomplishments and limitations of this historic measure. Richard Collier, PhD, is an Associate Fellow at the Centre for Business Taxation at Oxford University, and Joseph L Andrus, JD, was the Head of the Transfer Pricing Unit at the OECD and was responsible for directing the OECD work on BEPS transfer pricing.",
54,3,Business Economics,19 April 2019,https://link.springer.com/article/10.1057/s11369-019-00128-5,"Review of AI Superpowers: China, Silicon Valley and the New World Order, by Kai-Fu Lee",July 2019,R. Preston McAfee,,,Unknown,Unknown,Unknown,Unknown,,
54,3,Business Economics,19 June 2019,https://link.springer.com/article/10.1057/s11369-019-00133-8,"Jesse Norman: Adam Smith: what he thought, and why it matters",July 2019,Thomas Kevin Swift,,,Male,Unknown,Unknown,Male,,
54,3,Business Economics,12 July 2019,https://link.springer.com/article/10.1057/s11369-019-00134-7,"Ben S. Bernanke, Timothy F. Geithner, and Henry M. Paulson Jr.: Firefighting: The financial crisis and its lessons",July 2019,Laurence Ball,,,Female,Unknown,Unknown,Female,,
54,4,Business Economics,26 November 2019,https://link.springer.com/article/10.1057/s11369-019-00149-0,From the Editor,October 2019,Charles Steindel,,,Male,Unknown,Unknown,Male,,
54,4,Business Economics,18 November 2019,https://link.springer.com/article/10.1057/s11369-019-00148-1,Consumer expectations: a new paradigm,October 2019,Richard Curtin,,,Male,Unknown,Unknown,Male,"I had the opportunity in 2007 to participate in an international project on how consumers assessed the importance of, and how they used, the economic data produced by federal statistical agencies. The initial findings were a disappointment. Those results showed widespread ignorance, so a few years later, in 2009, I repeated those same questions with the hypothesis that consumers would be more knowledgeable about economic statistics when the economy was in recession. Consumers reported in the midst of the Great Recession an unusually high degree of interest in economic data, and the mass media provided an unusually high degree of coverage. The question sequence was as follows: each question first briefly explained the concept—the unemployment rate, the consumer price index, and gross domestic product—then asked respondents if they knew the latest figure announced by the agency, and if not, if they could recall hearing a past announcement, and finally if they had even heard of the economic statistic or the federal agency (Table 2). My hypothesis that people would be more knowledgeable during the Great Recession was not sustained. The only significant difference was that more people could cite the current unemployment rate. Nonetheless, the errors among those who thought they knew the latest statistic were quite large, and more importantly, the errors were larger in 2009 than in 2007. These results were stunning. How could so many people avoid the information on these common economic statistics, given the saturated coverage in the mass media, especially in the midst of the worst recession in decades? The critical assumption made by most economists is that the media reports the quantitative figure for the unemployment rate, the CPI, and GDP. To determine what information was made available by the national media, I undertook a careful analysis of media reports from January 2006 to April 2007 and from January 2008 to April 2009 (Table 3). I sought to determine how frequently the federal statistics were cited in a numeric format. Qualitative reports, such as “unemployment rose to new heights” or “the economy sank to new lows” that offered no quantitative information were ignored. Of the top five TV networks, mentions of the quantitative unemployment rate fell to 60% in 2009 from 83% in 2007, despite sharp increases in the unemployment rate. For the CPI, quantitative reports remained at about one-third, and for GDP, quantitative reports rose slightly to 51% in 2009. The same lower coverage of quantitative reports was found in the top 22 newspapers, although quantitative reports on the unemployment rate reached 75% in 2009, up from 51% in 2007. The only newspaper to publish the numeric information every time was the Washington Post, although the Wall Street Journal and the New York Times were close. Note that the wire services, UPI and AP, published the quantitative statistics for every release, implying that the quantitative information was available, but many in the media preferred to only publish qualitative descriptions. The much more common practice was to summarize the release of the latest statistic by using subjective phrases, such as economic growth had improved or worsened, inflation rose or declined sharply, unemployment surged or plunged, and so forth. It was also apparent that stories on economic statistics were likely to be communicated with facial expression and body language. Most reports on rising unemployment, for example, were accompanied by an interview with someone who had suffered the loss of a job, vividly showing a distraught worker. While the media’s motivation may have simply been to expand their audience, neuroscientists discovered that people have a special ability to understand facial and bodily expressions as though they had personally experienced that same events. The impact is even greater when the person enjoys the credibility of belonging to the same social groups. I will return to this important finding that emotions cannot be separated from rational deliberations, often referred to as “Descartes’s Error” (Damasio 1994).",6
54,4,Business Economics,13 August 2019,https://link.springer.com/article/10.1057/s11369-019-00138-3,"Deregulation, tax policy, and certainty: foundations of the U.S. recovery",October 2019,Michael J. Chow,William C. Dunkelberg,,Male,Male,Unknown,Male," Since the 2016 election, economic activity and performance in the small business sector of the economy has soared to record levels as documented by the National Federation of Independent Business’s (NFIB) Small Business Economic Trends survey, which captures the economic performance and sentiments of NFIB’s 300,000 member businesses. The Tax Cuts and Jobs Act and deregulation appear to have been critical to energizing this important sector of the economy. NFIB’s data suggest a strong correlation between implementing pro-growth policies (and eliminating government impediments to wealth creation) and job creation, investment, wage growth, and profits. In addition to examining the relationship between actual changes in policy and economic performance, we also explore the impact that expectations and certainty can have on economic activity. In a wealthy economy, there is substantial discretion about the timing and magnitude of private spending. Even small changes in spending and saving by hundreds of millions of economic agents can move the dial on economic growth. While these changes may be induced by altering important parameters like tax rates, they can also be triggered by simple changes in expectations for the future which can prompt the use of “pools of discretion” (savings, the savings rate, debt capacity, asset conversion to liquidity, cash, etc.) to finance changes in spending. The resolution of uncertainty is supportive of economic growth, illustrated by the immediate post-election reactions of economic agents, whose optimism about the future surged and ignited spending and hiring that took growth to a higher level. We provide specific evidence that such was certainly the case in the small business sector, reinforcing the canonical thinking that economic agents are indeed forward-looking and underscoring the importance that expectations and certainty have for economic performance.",
54,4,Business Economics,25 October 2019,https://link.springer.com/article/10.1057/s11369-019-00142-7,A look at the historical growth of the U.S. internet sector,October 2019,Christopher Hooton,Sera Crasta,,Male,Unknown,Unknown,Male,"In Q1 2012, the United States economy entered into a new era, quietly and with essentially no notice from analysts, policymakers, or other stakeholders. The U.S. Bureau of Economic Analysis’s (BEA) 5-year update to the North American Industrial Classification System (NAICS) resulted ‘overnight’ in nearly a doubling of total revenue from the U.S. internet sector in national accounts. The shift upward of the curve to a new plateau reflected more accurate measurement of the increasing amount of online economic activity conducted by businesses. The shift went unnoticed by many at the time, because of the lack of research and understanding on the internet as an economic industry, particularly related to how it is identified and measured. Research from the past decade has indeed estimated the size of the internet sector, things such as jobs and value-added contributions. However, the earlier research only produced single observations—typically one-year measurements based on varying methodologies. For example, estimates by Boston Consulting Group typically put the internet sector’s GDP contributions in 2012 between 3 and 12% in large and developed economies (Dean et al. 2012; Hooton 2017). Economists Incorporated estimated the internet industry accounted for approximately 6% of U.S. gross domestic product (GDP) and approximately 3 million jobs in 2014 (Siwek 2015). In other words, we have had various snapshots of the internet sector’s economic footprint, but not repeated estimates using the same methodology, which would show the long-term historical development of the internet as an economic sector. The purposes of the paper are to first clarify how the internet sector is conceptualized as an economic entity, and then to provide estimates of how it has evolved over time. It is the first research, to the extent of the authors’ knowledge, to provide time series estimates of the internet sector’s economic footprint for multiple standard metrics. To accomplish this, we apply the internet sector’s own (self) identification methodology developed by Siwek (2015) to identify and examine a variety of data sources from government accounts, individual companies, and web-scraping.Footnote 1 We argue that the paper provides a novel and important overview of the history of the internet industry. Annual estimates of internet sector value-added start in 1963 and show a sharp rise beginning in the mid-1990s. Estimates are also provided for the industry’s aggregate revenue, employment, establishments, market capitalization, and capital expenditures for the last 5–15 years (the length of the series varies by metric). Together, these estimates provide some initial evidence on how the internet sector has developed historically. They also add some evidence of nuance in sector performance, specifically signs of potential seasonality in revenue, and a recent leveling of the number of internet sector establishments despite continued growth in employment. Finally, they help to identify some potential future areas of research. Section 2 clarifies key concepts, notably by defining the formal internet sector definition, based on NAICS codes. We also detail the methodology for our data collection process. Section 3 provides a brief literature review on previous internet sector research. Section 4 presents the paper’s results. We conclude in Sect. 5.",
54,4,Business Economics,25 July 2019,https://link.springer.com/article/10.1057/s11369-019-00137-4,Diehard or delicate? Violence and young firm performance in a developing country,October 2019,Ummad Mazhar,Fahd Rehman,,Unknown,Male,Unknown,Male,"Firm age is an important determinant of its performance. However, little is known about the performance of young firms in a developing country context (Coad 2018 surveys the literature). This is surprising as many firms in developing countries are part of the supply chains of global corporations. Understandably, firms in Western economies incur transaction costs and face risks when they enter into contract with developing countries’ suppliers (Siems 2005). These costs could be search and investigation ones, and there is a need to identify the potential local suppliers. Information on their reliability, credit worthiness, cost of internal operations, and nature of local risks is required. This study shows the viability of very young firms under a weak political environment in a developing country like Pakistan. With China’s Belt and Road Initiative (BRI), the economies in the Asia-Pacific region are offering opportunities to developed country corporations to take advantage of the growth potential of the region, its favorable demography, and large labor market (O’Trakoun 2018). In this context, Pakistan is uniquely positioned to play the role of an economic corridor linking China with other countries in West Asia, Central Asia, the Persian Gulf, and Africa. Being a part of China’s BRI, and a member of the South Asian nuclear club, the business environment in the country can affect the regional geopolitical risk environment. This paper looks beyond the usual indexes of business conditions to identify political violence as one weakness that can hamper performance of young firms in Pakistan. We use a novel empirical strategy to estimate the link between weak political environment and the performance of young firms. Specifically, we categorize young firms into two main groups. The first group (labeled very young) comprises firms of ages 2 or 3 years, while the second group (labeled young) consists of firms with ages 4 to 7 years. The category of very young firms is chosen to focus on years generally considered most turbulent in a firm’s life (Coad et al. 2018a, b; Calvino et al. 2015; Hiatt and Sine 2014). The main empirical model explains the performance of a firm as a function of terrorism, firm-specific controls, and external influences, using pooled cross-section data. The core findings pinpoint that the very young group of firms bears the greatest burden of security risks, while the effect on old firms is significant but lesser in magnitude. The unique data set specific to this paper merges Global Terrorism Database (GTD 2016) and World Bank Enterprise Surveys (ES) information to gather disaggregated evidence on the link between terrorism and performance of young firms in Pakistan. In the robustness analysis, we have also employed a general measure of political violence as developed by De Mesquita et al. (2015), which is broader in its coverage of violent incidents and is Pakistan-specific. In terms of quantitative magnitude, the estimates suggest a decrease of 1.5 percentage points in the employment growth experienced by very young firms, with each additional increase in terrorism equaling its average rate of growth over the sample period of the study. For the old firms’ category, the same analysis suggests a decrease in job creation rate of 1 percentage point per annum. These findings withstand changes in model specifications and different measures of political violence, and endogeneity concerns. The paper is structured as follows: The second section overviews the relevant literature, followed by data description and empirical analysis, including major findings and their robustness. The final section concludes the study.",2
54,4,Business Economics,21 October 2019,https://link.springer.com/article/10.1057/s11369-019-00141-8,Decoupling from China: an economic analysis of the impact on the U.S. economy of a permanent tariff on Chinese imports,October 2019,Steven Byers,Jeff Ferry,,Male,Male,Unknown,Male,"In 2018, the United States initiated the first broad-based tariffs on imports in 87 years. The results of this real-world economic experiment have accorded with economists’ expectations in some ways but defied them in others. For example, with a small number of exceptions, U.S. tariffs have not led to price increases in consumer goods as was widely expected. The effects of tariffs in today’s interconnected global economy are not well known due to limited recent experience. In this study, we build an economic model of U.S.-China trade to study the effects of a permanent tariff on U.S. imports from China. Our model is based on a combination of observed experience and economic theory. We employ a novel methodology that combines sector-by-sector analysis of 142 industries that make up U.S. imports from China with a standard macroeconomic forecasting model of the U.S. economy. The industry-specific model enables us to forecast the rate at which production in China of goods for export to the U.S. migrates out of China to other locations. Plentiful experience in the past 2 years has shown that U.S. multinationals are able to relocate production more aggressively and more quickly than many thought possible before 2017. We are interested in the impact of permanent tariffs on China, because this is one potential path to decouple the U.S. economy from the Chinese economy. Further, the impact of a permanent tariff would be stronger than today’s temporary tariffs, which are subject to repeal at any moment. Permanent tariffs would disadvantage China as a site for U.S. companies’ production for the long term, and to a lesser extent for all production for global markets. As the world’s largest consumer market and largest importer in many categories of finished goods, the U.S. has a large impact on world markets and business location decisions. We model the effects of a permanent, across-the-board 25% tariff (henceforth PATB-25) on all U.S. imports from China. We also run a non-tariff economic forecast using our macroeconomic model to provide a baseline comparison. We then compare the two cases to quantify the impact of the PATB-25 tariff. Our production location model estimates that by 2024, 45% of U.S. imports from China leave for third countries. Using independent sources on international manufacturing cost comparisons, we find that the average cost of U.S. imports falls 4.3% in 2024 as compared to the baseline forecast. We then insert the new profile of U.S. imports into our macroeconomic model, using the well-known REMI PI + model. The U.S. economy grows slightly faster as a result of the tariff, due to two factors. First, the lower average cost of imports stimulates consumer spending. Secondly, a small portion, amounting to some 10.6% of Chinese export production, migrates back to the U.S. This production gain provides a direct stimulus to the U.S. economy. This paper is organized as follows: Introduction, (1.0) REMI baseline model description, (2.0) Model of migration out of China, 2.1) Destination of production that leaves China, (2.2) Production returning to U.S., (2.3) BCG Cost-Competitiveness and relative cost differentials, (2.4) Adjusting relative cost-competitiveness differentials to account for the fixed costs of moving production from China, (2.5) Calculating the tariffed relative cost-competitiveness, Index, (3.0) Simulation Results, (4.0) Concluding remarks.",
54,4,Business Economics,02 October 2019,https://link.springer.com/article/10.1057/s11369-019-00140-9,"Review of Crashed: How a Decade of Financial Crises Changed the World by Adam Tooze. London: Penguin UK, 2018",October 2019,David Miles,,,Male,Unknown,Unknown,Male,,1
54,4,Business Economics,12 August 2019,https://link.springer.com/article/10.1057/s11369-019-00139-2,"Review of Fighting Financial Crises: Learning from the Past by Gary B. Gorton and Ellis W. Tallman. Chicago: University of Chicago Press, 2018",October 2019,David C. Wheelock,,,Male,Unknown,Unknown,Male,,
54,4,Business Economics,13 November 2019,https://link.springer.com/article/10.1057/s11369-019-00147-2,The State Strikes Back: the end of economic reform in China? by Nicholas R. Lardy,October 2019,John O’Trakoun,,,Male,Unknown,Unknown,Male,,
54,4,Business Economics,06 November 2019,https://link.springer.com/article/10.1057/s11369-019-00145-4,"William G. Gale, Fiscal therapy: curing America’s debt addiction and investing in the future",October 2019,Robert C. Fry Jr.,,,Male,Unknown,Unknown,Male,,
55,1,Business Economics,21 January 2020,https://link.springer.com/article/10.1057/s11369-020-00160-w,From the Editor,January 2020,Charles Steindel,,,Male,Unknown,Unknown,Male,,
55,1,Business Economics,06 January 2020,https://link.springer.com/article/10.1057/s11369-019-00151-6,Alan’s approach to research and his colleagues,January 2020,Erica L. Groshen,,,Female,Unknown,Unknown,Female,,
55,1,Business Economics,10 December 2019,https://link.springer.com/article/10.1057/s11369-019-00152-5,Alan Krueger mattered,January 2020,Jonathan Gruber,,,Male,Unknown,Unknown,Male,,
55,1,Business Economics,09 January 2020,https://link.springer.com/article/10.1057/s11369-019-00156-1,From time use to contingent work to labor supply: thoughts on the contributions of Alan Kreuger,January 2020,Michael Horrigan,,,Male,Unknown,Unknown,Male,,
55,1,Business Economics,06 December 2019,https://link.springer.com/article/10.1057/s11369-019-00155-2,MMT: assume a can opener,January 2020,Laurence Meyer,,,Female,Unknown,Unknown,Female,,1
55,1,Business Economics,10 December 2019,https://link.springer.com/article/10.1057/s11369-019-00153-4,"Some observations on MMT: what’s right, not right, and what’s too simplistic",January 2020,Catherine L. Mann,,,Female,Unknown,Unknown,Female,,3
55,1,Business Economics,13 January 2020,https://link.springer.com/article/10.1057/s11369-019-00158-z,Giving MMT the credit it is due,January 2020,Julia Coronado,,,Female,Unknown,Unknown,Female,,1
55,1,Business Economics,14 January 2020,https://link.springer.com/article/10.1057/s11369-019-00159-y,Blue smoke and seers: measuring latent demand for cannabis products in a partially criminalized market,January 2020,Patrick L. Anderson,,,Male,Unknown,Unknown,Male,"Estimating demand for products for which there are no reliable sales data requires either cleverness or clairvoyance. Both approaches have been tried. Baroque claims made by marijuana opponents—including the specter of madness—played a key role in convincing the United States to prohibit sales in the 1930s. Those expecting prohibition to erase consumer demand were mistaken. During the eight decades of this era, durable consumer demand led to a vast underground economy, expensive interdiction efforts, and the institutionalization of organized crime across multiple countries. Most Americans did not consume marijuana during this era, but a substantial number did. Government statistics regarding actual use are hugely inconsistent, with interdiction-based estimates bizarrely implying that only a small fraction of Americans consume marijuana, alongside public health statistics suggesting that one out of eight are regular consumers. We reject both these approaches as unreliable. Instead, we construct estimates of latent demand—demand that cannot be directly observed—from observed consumption of products we theorize are substitutes for cannabis, including alcoholic beverages. This method incorporates state-by-state variations in consumer taste and allows for calibration using actual purchase data for the tiny fraction of the population for which such data are reported. From these estimates for all 50 states, we construct a monthly index of demand for both medical and adult-use products in the United States. Using these data, we find that: Demand for cannabis products, including both medical and recreational, has increased by approximately 20% over the past year, and nearly 40% since November 2016 (Anderson 2016). State governments have frequently relied upon forecasts of tax revenue and sales that substantially exceed estimates using the method we outline. In states such as California and Michigan, this has resulted in serious shortfalls in expected tax revenue. Consumers substitute cannabis for other products, such as alcoholic beverages and tobacco. Moreover, self-medicating consumers substitute cannabis products for pharmaceuticals, particularly in specific categories such as palliative care. We observe that both proponents and governments have routinely ignored this cannibalization-by-cannabis effect. As with any consumer product, we observe the beginnings of product differentiation, the emergence of brands, and trends in consumer tastes. State laws that accompany legalization have often imposed serious burdens on producers, including double taxation and defective implementation of licensing rules. With a rational regulatory structure, consumer demand would be higher in these states, as would tax revenue. Furthermore, the underground market—which continues to function in states that have legalized—would be smaller. Over the last decade, the trickle of states that began to legalize medical-use marijuana in the late 1990s has turned into a small flood. Voters in states across the country—including both coasts and the Midwest—have now voted to allow possession and use by adults. It would be inaccurate, however, to describe these states as true open markets, or even rationally-regulated markets. Among the many defects in the functioning of these markets are the following: In some states, possession is legal, but sale is not. In some states, sellers of cannabis can only possess a limited amount at any one time. Federal regulation of banking has resulted in the forced practice of dealing in cash. This makes marijuana businesses and their customers nearly the only portion of the legal economy without a secure payment system, forcing legal businesses to operate in a manner commonly associated with drug cartels and gambling dens. Federal law continues to include marijuana as a Schedule I narcotic under the Controlled Substance Act. Licensing in states that voted to legalize has been erratic, and in some cases punitive and inexplicable. Compounding this, some states have already changed their regulatory and tax structures at least twice. Although a nominal listing of states that have voted to legalize adult use would suggest otherwise, actual sales or tax revenue is currently reported on a regular basis by only a tiny handful of states. Overlapping tax systems are the norm, rather than the exception. Section 280E of the Internal Revenue Code arguably applies the highest income tax rate in the history of the United States on legal cannabis businesses. Many states have excise taxes that apply twice on the same product, and some have income or other business taxes that also overlap. Contrary to an apparent presumption by many proponents of legalization, nothing in state laws forces underground-economy sellers, or buyers, to abandon established and reliable supply lines. The definition of “medical marijuana,” and the allowable uses and requirements to become a medical user, are at best mutable and at worst comical. As we enter the third decade of this century, we find ourselves without reliable sales statistics in the vast majority of states. Even in states with some reported sales, data reflect a market that (1) is only partially decriminalized, (2) is subject to contradictory regulation, (3) is frequently subject to double taxation, (4) sells a product with a mutable character, and (5) does not allow normal payments and banking. Under such conditions, we must recognize that a substantial part of the demand remains unobservable. Furthermore, we should expect that some consumers and suppliers remain in the underground economy, where conditions are arguably more stable than in partially-legal markets. Governments seeking tax revenue, businesses seeking reliable market indicators, and consumers wanting reliable suppliers all have an interest in filling the existing gap in knowledge about the legal marijuana market. Estimating demand for products for which there are no reliable sales data requires either cleverness or clairvoyance. Both approaches have been tried. Opponents of marijuana unblushingly relied on their clairvoyance regarding marijuana users in the 1930s, evoking baroque claims of the likely result of allowing it to contaminate American youth—including the specter of madness. Although easy to overlook today, this effort played a key role in convincing the United States to prohibit sales in the 1930s. It is also important in understanding consumer opinion about this substance, as we summarize in the following section. Another prominent approach to estimating demand relied heavily on a perspective common to proponents of the “war on drugs,” as the concerted effort by military and civilian authorities to end the drug trade into the United States in the late twentieth century is commonly called. Borrowing a page from the same book that opponents were reading from in the 1930s, this approach relied on criminal justice statistics, viewed the substance as equivalent to other prohibited narcotics (such as heroin and cocaine), and considered any use to be aberrant and criminal. Ironically, economists played a unique role in calling this approach into question, with many influential economic analyses of the war on drugs arising from straightforward economic reasoning about consumer demand and the benefits of a regulated, but open market compared to the black market. While we find that the markets are not yet actually open, we follow these scholars in approaching the question from a perspective of fundamental economics. The methodology we present here is firmly grounded in the fundamental economics of consumer demand, as well as the willingness and ability of private businesses to supply that demand. These economic fundamentals should, of course, recognize the considerable demand for marijuana among Americans, which has persisted even through eight decades of prohibition. At the same time, it should also take into account tax and regulatory burdens, business uncertainties, and consumer preferences. Those preferences include outright antipathy from a large segment of the population based on health risks, association with crime, the untested and non-standardized nature of the product, and the persisting stigma from the Reefer Madness era described below. Moreover, economic fundamentals require taking consumer budget constraints into account, implying that consumer expenditures on cannabis products will crowd out purchases of other products. This dynamic, we note further below, has been almost entirely missing from government analyses of likely demand and tax revenue. This approach was first outlined in a presentation to a conference of forensic economists in Amsterdam in 2015, at a time when usable adult-use sales data were available from only one state, and for only a short period of time (Anderson 2015). Comments from economists at that conference, and at a subsequent conference presentation in 2016, were unusually rich: they reflected both expected methodological suggestions and decidedly unexpected anecdotes regarding use among conference attendees. In an underground economy, such anecdotes represent market observations and have not been ignored. The opening of a handful of other state markets has produced a trickle of additional reliable sales data, which we now have for both medical and adult-use demand. That trickle has allowed testing of the fundamental approach, as well as refining of the methodology and improvements in the parameters. This article therefore includes both an improved methodology beyond what was presented in 2015 and 2016, and analysis of data that was not possible until 2019.",1
55,1,Business Economics,29 November 2019,https://link.springer.com/article/10.1057/s11369-019-00150-7,Reflections on economic policy,January 2020,Arthur B. Laffer,,,Male,Unknown,Unknown,Male,"The Laffer curve is my profile. I do mathematical economics. I developed pedagogic devices to explain the math to my students. And one of them was the Laffer curve. At 100% tax rates, you know, you’ll not work, and therefore, there’ll be no revenue. At a zero tax rate, even though you work a lot and earn a lot, there’ll be no revenues either. What you got is a curve shape, in a pedagogic form. And Marty Feldstein—I’m very sorry he passed—said it’s something you can show a Congressman and then have that Congressman talk about it for 6 months. The “Whip Inflation Now” program, if any of you remember, proposed a 5% tax surcharge. The budget had it raising 5% more revenues. I said, “Look, it will not raise 5% more revenues. It might raise 4% more, 3%, 2%. But it also might lose revenues.” I was trying to explain it. I showed my little pedagogic device, the curve, to Don Rumsfeld and Dick Cheney, who were my classmates at Yale. And there was a reporter named Jude Wanniski there for the Wall Street Journal. And it was like it caught fire. And I don’t know whether I actually drew it on a cloth napkin or not. My mom would be terribly offended if I did. But whatever, that’s the story, and I’m stickin’ to it. They’ve got a little copy of it, supposedly, at the Smithsonian. But just for us, I did that 2 years later. Some people think that the audience might be a little surprised that I worked on the campaigns of Gary Hart and Jerry Brown. I designed Jerry Brown’s flat tax, if any of you know the 1992 race. It’s my ideal tax. I wrote this paper in 1981, I think it was, called “The Complete Flat Tax.” It was the backing behind Kemp-Kasten. And it was also the backing behind Bradley-Gephardt and the Tax Reform Act of 1986, which was, I think, the best tax bill that’s ever been done in America. But what we did was we got rid of all federal taxes, all of them, all federal taxes except sin taxes. The reason we kept sin taxes is because their purpose is not so much to raise revenues as it is to change behavior. I jokingly refer to it as we Americans don’t like drunk people smoking while we shoot each other. We proposed getting rid of all those taxes and put in two flat rate taxes, one on business net sales, if you’re a Republican, and if you’re a Democrat, on value added, and on personal unadjusted gross income from the first dollar to the last dollar. We made it static revenue neutral, which came out to a 12% rate. Jerry Brown bumped it to 13% because he wanted a little more revenue. We went from eighth in the race to second in the race. We got the second most number of delegates in the Democratic primaries. I still dream. My dream is to get a flat tax that’s really a complete flat tax from the first dollar to the last dollar. You don’t need an IRS. If a company owes you 100 bucks, they send you 87 and send 13 in to the government. If you buy something you have a 13% sales tax or whatever it would be on it, no individual tax filings or anything. Now, if you mow your neighbor’s lawn for ten bucks, yeah, you do have to send in $1.30. But, 95% of all tax collections would be done at the business level, which is a very efficient way to collect taxes. That’s my dream of what the tax system should look like. I voted for Clinton twice. I just thought he was a great president. You know, he cut government spending as a share of GDP. You all know government spending is taxation. That was just amazing. He got rid of the retirement test on Social Security, and reformed welfare such that you actually have to look for a job to get welfare. I started my career, at least in politics, in 1970. I was the first chief economist at the OMB when it was formed. I only talk about economics, just so you know. I think Trump and Reagan are very similar. If you look at economics, there are five pillars of prosperity: taxes, government spending, monetary policy, regulatory policy, and trade. And if you could look at the bill by Trump in the first term, my view was that the 2017 bill is probably the single best tax bill in the first term of any administration ever. I just think it’s beautiful. President Trump said to me, “Why’d you say first term, Art?” And I said, “Well, because the ‘86 Tax Act was even better. But that was a second term.” If you look at spending, this administration has done nothing on spending. Neither have the prior two. On monetary policy, I think Jerome Powell’s doing a fine job as Chairman of the Fed. I think he’s a lot better than Bernanke or Yellen was. As for deregulation, this administration is the best ever.",1
55,1,Business Economics,13 January 2020,https://link.springer.com/article/10.1057/s11369-019-00146-3,Modernizing data collection for the Consumer Price Index,January 2020,Crystal Konny,,,Female,Unknown,Unknown,Female,"The Consumer Price Index (CPI) is a measure of the average change over time in the prices paid by urban consumers for a representative basket of consumer goods and services, including everything from food items to automobiles to rent. It measures inflation as experienced by consumers in their day-to-day living expenses. The CPI is a complex measure that combines economic theory with sampling and other statistical techniques and uses data from several surveys to produce a timely and accurate measure of average price change for the consumption sector of the American economy. Surveys used to provide input data to the CPI rely on the voluntary cooperation of many people and establishments throughout the country who, without compulsion or compensation, supply data to the government’s data collection staff. As part of the process to continually improve these surveys, BLS is working with corporations to collect retail price data in a new way while maintaining the accuracy of the CPI. The CPI affects nearly all Americans because of the many ways it is used, for example: As a means of adjusting dollar values. The CPI is often used to adjust consumers’ income payments (for example, Social Security), to adjust income eligibility levels for government assistance, and to automatically provide cost-of-living wage adjustments to millions of American workers. As a result of statutory action, the CPI affects the income of millions of Americans. Over 68 million Social Security beneficiaries and military and Federal Civil Service retirees have cost-of-living adjustments tied to the CPI. Another example of how dollar values may be adjusted is the use of the CPI to adjust the Federal income tax structure. These adjustments prevent inflation-induced increases in tax rates. In addition, eligibility criteria for millions of Supplemental Nutrition Assistance Program recipients, and children who eat lunch at school, are affected by changes in the CPI. Many collective bargaining agreements also tie wage increases to the CPI. As an economic indicator. The CPI is a widely used measure of inflation and is sometimes viewed as an indicator of the effectiveness of government economic policy. It provides information about price changes in the nation’s economy to government, business, labor, and private citizens, and is used as a guide to making economic decisions. In addition, the President, Congress, and the Federal Reserve Board use trends in the CPI to aid in formulating fiscal and monetary policies. As a deflator of other economic series. The CPI and its components are used to adjust other economic series for price changes and to translate these series into inflation-free dollars. Examples of series adjusted by the CPI include retail sales, hourly and weekly earnings, and components of the National Income and Product Accounts, in particular, in estimation of the Personal Consumption Expenditures Price Index. As an estimate of purchasing power over time. The CPI is also used as a deflator of the value of the consumer’s dollar to find its purchasing power. The purchasing power of the consumer’s dollar measures the value to the consumer of goods and services that a dollar will buy at different dates. While the CPI is constructed using a set of interlocking surveys, it is fundamentally a measure of price change (Bureau of Labor Statistics 2018). Household spending weights used in the estimation of the CPI are derived from the Consumer Expenditure (CE) Survey which furnishes data on both item category household purchases used to draw the CPI item sample and retail outlets where households purchase commodities and services used to draw the CPI outlet sample. Weights are derived from the reciprocal of the probabilities of selection. The monthly movement in the CPI derives from weighted averages of the price changes of the items in its sample. A sample item’s price change is the ratio of its price at the current time to its price in a previous time. A sample item’s weight in this average is the share of total consumer spending that it represents. The CPI uses the geometric mean index number formula, which approximates a COLI (Cost-of-Living Index) under the restrictive assumption of Cobb–Douglas utility, to average price change within most item categories. BLS has not had access on a real-time basis to the expenditure information necessary to produce superlative indexes, the preferred class of index formulas for COLI estimation, for the lower level component indexes that feed all CPI outputs. All CPI outputs use the same input price data, and either use different upper level weights or a different upper level aggregation formula. BLS currently only uses a superlative index formula at the upper level of aggregation to produce the Chained CPI-U, and it is subject to revision with final data posted 10-12 months after initial release (Bureau of Labor Statistics 2019). The headline CPI for All Urban Consumers (CPI-U) and the CPI for Wage Earners (CPI-W) uses a Laspeyres formula to average price changes across categories of items, and published numbers are not subject to revision. CPI data are published monthly, with the index value representing an estimate of the price level for the month as a whole, rather than a specific date. The CPI geographic sample is based on the 2010 Decennial Census and is selected to be representative of the demographics of the USA. The sample consists of 75 urban areas, called primary sampling units (PSUs). PSUs are defined using Office of Management and Budget (OMB) Core Based Statistical Area (CBSA) definitions. Pricing information in the current CPI is primarily based on two surveys: Commodities and Services (C&S) and Housing. A description of the data collection paradigm and its challenges explains why BLS is interested in working with corporations to collect data in a different way. The CPI draws a representative sample of commodities and services and follows the prices of these items for 4 years. Establishment and item samples are on average refreshed every 4 years on a rotating basis. BLS data collectors visit (in person or on the web) or call thousands of retail stores and service establishments all over the USA to obtain information on the prices of the thousands of items used to calculate the CPI. Prices used to compute the CPI are collected during the entire month. BLS data collectors attempt to record the prices of about 90,000 items each month, representing a probability-based sample of the prices paid by consumers for goods and services purchased. During each call or visit, the data collector collects price data on a specific good or service that was precisely defined during an earlier visit. If the selected item is no longer available, or if there have been changes in the quality or quantity (for example, a 64-ounce container has been replaced by a 59-ounce container) of the good or service since the last time prices were collected, a new item is selected or the quality change in the current item is recorded. Pricing information is then sent electronically to our national office, where specialists who have detailed knowledge about the particular goods or services check the data for accuracy and consistency, and make any necessary corrections or adjustments. Adjustments made by the specialists are designed to prevent changes in the quality of items from affecting the CPI’s measurement of price change and may include an adjustment for a change in the size or quantity of a packaged item to more complex adjustments based on statistical analysis of the value of an item’s features or quality. The CPI attempts collection of about 7,000 rental housing unit rents each month to compute the indexes for rent and owners’ equivalent rent (an estimate of the rent owner-occupants would have to pay if they were renting their homes). The housing sample is divided into six panels, with the units in each panel priced twice per year. Rents are collected by personal visit, email, or telephone, and can be collected at any time during the month. The housing sample is refreshed on a continuous basis over a six-year period. Housing specialists review the data for accuracy. Several challenges arise in calculating the CPI using traditional data collection. First, the CPI is based on samples rather than a census of all purchases, which can introduce sampling error. If possible, price indexes would be calculated using in-scope sales of all goods and services purchased by US consumers in all retail establishments. Second, BLS is often only able to collect offer prices that do not reflect all of the discounts applied to a transaction—the data collector records the offer price minus any readily available discounts offered to the general public for that particular item. A discount offered to an individual customer on an entire transaction, such as ten percent off an entire purchase or the use of some individual coupons, would not be included. One price on one day is collected for each sample item in the collection month. In addition, because the CPI aims to measure constant-quality price change over time, when a unique item is no longer sold, a replacement item must be selected, and any quality change between the original and replacement items must be estimated and removed to reflect pure price change in the index. The process of adjusting for quality change is not always straightforward and often requires additional information. And new goods entering the marketplace must be accounted for in a timely manner with the appropriate weight, and that is not easy to achieve with sample refreshments occurring only every 4 years on a rotating basis. In terms of survey operations, data collected by BLS through pricing surveys are costly and more difficult to collect. Metropolitan areas have generally increased in size, which causes a corresponding increase in travel costs. The increase in the number of chain stores has increased the time required to obtain corporate approval to collect data, which BLS always receives prior to collecting data. Response rates are declining as the result of many factors: new privacy concerns, increasing number of surveys, increasing distrust of government, data security concerns, and less confidence in the accuracy of the CPI. Tradeoffs and compromises are made at every level, from eliminating physical establishments in foreign countries to collecting a sample of consumer purchases rather than a census to collecting list prices when transaction prices are unavailable. BLS is continually looking for improvements to CPI collection and estimation methodologies with the goal being to produce the most accurate and objective measure of price change for US consumers that is possible within the BLS budget. Alternative data are a hopeful avenue to explore for potential remedies to these tradeoffs and data collection challenges.",3
55,1,Business Economics,12 December 2019,https://link.springer.com/article/10.1057/s11369-019-00157-0,"The business aviation industry: growth, contraction and consolidation",January 2020,Jesse Jacobs,Brad Goebel,,Male,Male,Unknown,Male,"Business aviation has a history of being a workhorse for corporate organizations, whether to move employees around the world on short notice or to meet face to face for essential meetings. Nevertheless, some view it as a flashy, excessive, asset that is listed first on the chopping block in the event of a recessionary downturn. Historically, as corporate profits rebound, investment in new aircraft returns. However, the last “Great Recession” was not typical, and the relationship between business jet deliveries and corporate profits made a divergence from prior history. Further insight into the industry helps analysts and economists take a broader view of the strength behind corporate profits and determine whether a relationship with corporate aviation still exists. The Transportation Security Administration breaks down general aviation activities that primarily involve business/corporate aviation, personal/recreational, instruction/flight training, aerial application/crop dusting, aerial observation/search, rescue, law enforcement surveillance, air tours/air taxis, air medical/emergency services and skydiving/parachute operations. Since most aircraft activities fall within the business/corporate aviation grouping, analysts tend to refer to all the jets within a particular weight/size classification as “Business Aviation.” This paper focuses on the models agreed upon by industry analysts who participate in the Transportation Research Board (TRB) Subcommittee on Business Aviation. Business jets range from the smallest entry-level jets, like the Embraer Phenom 100EV, to the largest “airliner”-sized jets, like the Airbus ACJ318, used for business purposes. For safety and other numerous reasons, Federal Aviation Regulations (FARs) distinguish aviation based on the usage of the aircraft. Business aviation falls within two primary groups. Part 91 outlines the general operating and flight rules for aircraft flying for private purpose (non-commercial). Part 91 is more lenient and has less rigorous maintenance standards. Part 135 outlines rules for air taxi and on-demand private jet charter operations. Because Part 135 allows operators to be compensated “for hire,” the restrictions are more restrictive and rigorous to ensure safety and professionalism (Wieand 2011).",2
55,1,Business Economics,21 December 2019,https://link.springer.com/article/10.1057/s11369-019-00154-3,Raghuram Rajan: The Third Pillar—How Markets and the State Leave the Community Behind,January 2020,Beth Ann Bovino,,,Female,Unknown,Unknown,Female,,
55,2,Business Economics,21 April 2020,https://link.springer.com/article/10.1057/s11369-020-00170-8,From the Editor,April 2020,Charles Steindel,,,Male,Unknown,Unknown,Male,,
55,2,Business Economics,03 April 2020,https://link.springer.com/article/10.1057/s11369-020-00166-4,Martin Feldstein,April 2020,James M. Poterba,,,Male,Unknown,Unknown,Male,,
55,2,Business Economics,05 February 2020,https://link.springer.com/article/10.1057/s11369-020-00163-7,The impacts of the U.S.–China trade war,April 2020,Gordon H. Hanson,,,Male,Unknown,Unknown,Male,,7
55,2,Business Economics,03 April 2020,https://link.springer.com/article/10.1057/s11369-020-00169-1,Housing finance reform: the future of Fannie Mae and Freddie Mac,April 2020,Laurie S. Goodman,,,Female,Unknown,Unknown,Female,,
55,2,Business Economics,05 February 2020,https://link.springer.com/article/10.1057/s11369-020-00162-8,Brazil’s economic reform roads,April 2020,Diogo Ramos Coelho,,,Male,Unknown,Unknown,Male,,3
55,2,Business Economics,09 March 2020,https://link.springer.com/article/10.1057/s11369-020-00167-3,"Stefano Battilossi, Youssef Cassis, and Kazuhiko Yago (eds.): Review of Handbook of the history of money and currency",April 2020,Ronnie J. Phillips,,,,Unknown,Unknown,Mix,,
55,2,Business Economics,09 March 2020,https://link.springer.com/article/10.1057/s11369-020-00165-5,Book Review for Not Working: Where Have All the Good Jobs Gone?,April 2020,David Wiczer,,,Male,Unknown,Unknown,Male,,
55,2,Business Economics,27 February 2020,https://link.springer.com/article/10.1057/s11369-020-00164-6,M. Ayhan Kose and Franziska Ohnsorge (eds.): Review of A decade after the global recession: lessons and challenges for emerging and developing economies,April 2020,Amalia Estenssoro,,,Female,Unknown,Unknown,Female,"There are three main messages the authors try to convey in this book. First, EMDEs have learned from past shocks and, during the most recent crisis, were far better prepared to deal with it, including the ability to implement, for the first time, “large-scale countercyclical fiscal and monetary policies during the global recession.” This is because the 2002–2007 prerecession period produced EMDE growth averaging 6.7% per year in the face of a benign external scenario in an increasing globalized world. The prerecession period saw higher exports, growth, and commodity prices that eventually lowered (relative to GDP) government debt levels as well as current account and fiscal deficits, and increased foreign reserves in EMDEs. These improvements were the policy “buffers” the EMDEs relied upon during the crisis. Second, EMDEs have been accumulating worrying levels of debt since the global recession, while experiencing weaker commodity prices and slower growth, which, together with more recent trade disputes, were described in the book as “chipping away” at important engines of EMDE growth. Third, policy frameworks and structural reforms matter. Since the 1997–1998 Asian crisis, EMDEs have polished their policy toolkits to include fiscal rules, floating exchange rates, and inflation targeting monetary policy regimes, among other reforms to improve their resiliency in the face of external shocks. However, the impetus for structural reforms has stalled after the 2010 recovery. The authors then proceed to answer a very specific question: Are EMDEs now well prepared to face yet another global downturn? Their answer, quite simply, is No! This view embodies what the authors describe as the main “lessons” from the 2009 global recession: the “ability to minimize the effects of adverse shocks” using “policy room” at the time of the shock, and subsequently rebuilding policy buffers to face future shocks. All of this amounts to very sensible advice on how EMDEs can avoid the capital-flow-driven boom-and-bust cycles that historically have plagued emerging economies. But the overriding assumption remains that EMDEs outperform AEs in a globalized world of relatively-free capital mobility, which allows the building of buffers in good times. The question the reader cannot escape asking, is what happens if any of these assumption were to be challenged in a changing post global recession world. For example, what would be the effects on EMDEs if China succeeds in changing its growth model? How would a persistent “de-globalization” trend affect capital flows? Rather than facing a global downturn, EMDEs might be facing a far different, and slower growing, world economy.",
55,2,Business Economics,22 January 2020,https://link.springer.com/article/10.1057/s11369-020-00161-9,Review of Reflections on Allan H. Meltzer’s Contributions to Monetary Economics and Public Policy,April 2020,Stuart G. Hoffman,,,Male,Unknown,Unknown,Male,,
55,3,Business Economics,23 June 2020,https://link.springer.com/article/10.1057/s11369-020-00182-4,From the editor,July 2020,Charles Steindel,,,Male,Unknown,Unknown,Male,,
55,3,Business Economics,22 June 2020,https://link.springer.com/article/10.1057/s11369-020-00178-0,"Solving policy problems, at the Federal Reserve and elsewhere",July 2020,Roger W. Ferguson Jr.,,,Male,Unknown,Unknown,Male,,2
55,3,Business Economics,24 May 2020,https://link.springer.com/article/10.1057/s11369-020-00177-1,Fiscal policy responses to economic inequality,July 2020,Jason Furman,Douglas Holtz-Eakin,,Male,Male,Unknown,Male,,1
55,3,Business Economics,24 June 2020,https://link.springer.com/article/10.1057/s11369-020-00179-z,A strong dollar: help or harm?,July 2020,David Dollar,Nathan Sheets,,Male,Male,Unknown,Male,,
55,3,Business Economics,22 June 2020,https://link.springer.com/article/10.1057/s11369-020-00180-6,Rising student loan burdens and what to do about them,July 2020,Karen Dynan,,,Female,Unknown,Unknown,Female,"Data on the repayment status of the Federal Student Loan Portfolio provide concrete evidence of the struggles that some borrowers are having paying off their student debt. Figure 2 shows that 17% of individuals with federal student debt were in default as of mid-2019. However, traditional default rates greatly understate the challenges facing these individuals. A better denominator for the calculation would remove the 21% of borrowers that are still in school or in the 6-month grace period that follows leaving school (represented by the lighter gray bars in the figure) as those borrowers are not expected to be paying off their loans. One should also recognize that borrowers facing hardship may be put into deferment or forbearance. About 42% of the federal student loan program borrowers that have entered the period when they are scheduled to be repaying their loans (represented by the darker blue bars in the figure) were in default, forbearance, or deferral as of mid-2019. Repayment status of federal student loans in 2019:Q2 Moreover, some student borrowers who are repaying are only able to do so because they are in the government’s income-driven-repayment (IDR) program. The IDR program allows people having trouble managing their full loan payments to pay just a given fraction of their income (typically 10%). Nearly one-third of student loan borrowers are in IDR. While borrowers in IDR are avoiding the negative consequences of default, the reduced payments result in extended loan terms and the accumulation of more interest over time. Various trends among young adults have raised concerns about the degree to which higher student debt burdens may be impairing their economic circumstances. Figure 3 shows the share of individuals between ages 25 and 34 (older than the age of the typical college student) who are living with their parents. The figure shows pronounced uptrends for both men (the darker line) and women (the lighter line) since the early 2000s, with almost 20% of young men in this age group now living with their parents. What is especially striking is that trends did not turn around with the strong labor market of the late 2010s. Individuals 25–34 living with parents Figure 4 shows median inflation-adjusted net worth for households in the same age group. As of 2016, their net worth was significantly behind that of their counterparts a generation ago. In a mechanical sense, the greater student debt of the more recent cohort is clearly weighing on its net worth. Of course, the greater education facilitated by the student debt is likely to raise the incomes of many within the cohort, leading to higher net worth growth over time than would otherwise be the case. Still, their lower net worth early in life may hinder other types of economic opportunities. Median net worth for households 25–34 The potential for student debt to suppress economic mobility is of particular concern for some groups. A long literature documents persistent racial wealth disparities in the USA (see, for example, Gale et al. 2019), and patterns of student borrowing by race raise questions about whether student loans may be one mechanism through which these disparities are transmitted from one generation to the next. Figure 5 shows results from Scott-Clayton and Li (2016) documenting that blacks are more likely to have student debt than individuals in other race groups; the same study showed that average student debt per borrower is also much higher for Blacks, at around $53,000, compared with less than $30,000 for other groups. Share of 2008 grads with student debt in 2012 Of course, rigorous academic research is needed to definitively link greater student borrowing to trends in household formation, wealth accumulation, economic mobility, or any other economic outcome. The existing body of research of this type is fairly small but the available studies have, if anything, yielded reassuring results. For example, a particularly well-identified study by Mezza et al. (2020) finds that having student debt delays homeownership but that the magnitude of the delay is small—about 4 months on average. Likewise, Cooper and Luengo-Prado (2018) do not find greater student debt to be among the main factors explaining why more recent cohorts of young adults are less likely to form households than earlier cohorts; they conclude that most of the change is explained by different demographics, higher housing costs, and macroeconomic conditions.Footnote 1 Important caveats apply, however. The first is that research on the many ways in which student debt might affect economic outcomes has been limited by data availability and identification struggles. To date, the research has focused on questions that can be tackled because the right data exist. There are all sorts of ways that student debt may be affecting individuals and the economy, and, for many of these channels, researchers have found it hard to parse out the role of student loans versus other factors. The other major caveat is that the problems associated with greater student debt load may take years or decades to fully emerge. Factors that weigh on the economic opportunities in ways that are not very evident when a person is young can compound to be a much bigger deal later on in life. For example, a student borrower prevented from starting a business because of insufficient capital (and access to credit) may see little difference income at first but might have substantially lower income over her lifetime because of the constraint.",
55,3,Business Economics,01 July 2020,https://link.springer.com/article/10.1057/s11369-020-00181-5,Student debt overhang: imprint on homeownership and the economy,July 2020,Kamila Sommer,,,Female,Unknown,Unknown,Female,,1
55,3,Business Economics,01 July 2020,https://link.springer.com/article/10.1057/s11369-020-00183-3,Public pension shortfalls and state economic growth: a preliminary examination,July 2020,Charles Steindel,,,Male,Unknown,Unknown,Male,,1
55,3,Business Economics,26 May 2020,https://link.springer.com/article/10.1057/s11369-020-00175-3,The small-dollar loan industry: a new era of regulatory reform—and emerging competition?,July 2020,Thomas A. Hemphill,,,Male,Unknown,Unknown,Male,"In 2018, the Federal Reserve reported that about 40% of U.S. adults surveyed said that if confronted with a $400 unexpected expense, they would either not be able to pay it or would do so by selling a possession or borrowing money (Board of Governors of the Federal Reserve System 2018). Annually, U.S. consumers borrow nearly $90 billion in short-term, small-dollar loans (Wilson and Wolkowitz 2017). These consist of different financial services products, including pawn loans, online payday loans, storefront payday loans, installment loans, and marketplace personal loans (Wilson and Wolkowitz 2017). The “small-dollar loan” or “payday loan” industry, often consisting of non-bank and non-credit union financial service providers, offers a small, short-term, unsecured lump-sum loan (usually $500 or less, although ranging up to $5000) to “banked” borrowers having a checking account of a verifiable source of income,Footnote 1 who promise to repay the loan out of their subsequent paycheck or regular income payment (Federal Deposit Insurance Corporation 2015). The Consumer Financial Protection Bureau (2013) estimates that the median size payday loan is $350, and the Pew Charitable Trusts (2013, pp. 12–16) reports that approximately 12 million Americans use payday loans annually, spending an average of $520 on fees to repeatedly borrow $375. The small-dollar loan (SDL) is a financial product in contrast to a traditional installment loan, in which the borrower, after successfully meeting the lending institution’s underwriting requirements, repays the loan with equal payments over time. Traditional lenders—such as banks and credit unions—who make installment loans thus “underwrite” their loans (underwriting helps the lender to predict whether borrowers can afford the payments), examining the potential borrower’s cash inflows and outflows (Miller Jr. 2016). Because of the underwriting review process, from 40% to 60% of traditional installment loan applicants fail approval for credit assistance (Miller Jr. 2016). Nevertheless, these rejected applicants still have a demand for credit—in many cases, a demand only met by the non-traditional SDL industry. SDL industry revenue increased by more than $2 billion between 2012 and 2016 (Wellstein 2018), with the payday lending and money transfers industry reporting industry sales of $17.2 billion in 2018 (Market Research.com 2019). According to the non-profit Center for Financial Services Innovation, industry small-dollar lenders generated $5.3 billion in fees and interest in 2017, down from $9.2 billion in 2012 (Wilson and Wolkowitz 2017; Rappaport 2018). Industry researchers have estimated (in 2018) that there are 15,000 storefront payday lenders in the U.S. (down from an industry peak of about 24,000 in 2007) (ISBS World 2018)—and that does not include online nationwide payday lenders, both those licensed and unlicensed (Allen 2016). Factoring into this decline in storefront small-dollar lenders has been the increase in the number of SDLs made on the Internet, along with the use of electronic payment systems transferring funds and making payments convenient and faster. In 2010, online small-dollar lending accounted for less than 1% of U.S. personal loan balances; in 2018, it made up more than 38% of such balances while contributing about half of the revenues (TransUnion 2019). Moreover the online lending industry has doubled in revenue size each year since 2010, and is expected to increase to $90 billion by 2020 (U.S. Department of the Treasury 2016).",
55,3,Business Economics,26 April 2020,https://link.springer.com/article/10.1057/s11369-020-00171-7,"First responders: inside the US strategy for fighting the 2008–2009 global financial crisis Ben S. Bernanke, Timothy F. Geithner, and Henry M. Paulson, Yale University Press",July 2020,Garry Gorton,,,Male,Unknown,Unknown,Male,,
55,3,Business Economics,20 April 2020,https://link.springer.com/article/10.1057/s11369-020-00173-5,Jump-starting America: how breakthrough science can revive economic growth and the American dream,July 2020,Jesse M. Abraham,,,Male,Unknown,Unknown,Male,,
55,3,Business Economics,16 April 2020,https://link.springer.com/article/10.1057/s11369-020-00172-6,Janek Wasserman: The Marginal Revolutionaries: how Austrian economists fought the war of ideas,July 2020,Alex Tabarrok,,,Male,Unknown,Unknown,Male,,
55,3,Business Economics,04 May 2020,https://link.springer.com/article/10.1057/s11369-020-00168-2,Burying the Lede: A Review of The Economists’ Hour by Binyamin Appelbaum,July 2020,Vincent Reinhart,,,Male,Unknown,Unknown,Male,,
55,3,Business Economics,31 May 2020,https://link.springer.com/article/10.1057/s11369-020-00176-2,Thomas Piketty: Capital and Ideology (translated by Arthur Goldhammer),July 2020,Charles Steindel,,,Male,Unknown,Unknown,Male,,1
55,3,Business Economics,28 April 2020,https://link.springer.com/article/10.1057/s11369-020-00174-4,Otmar Issing: The Long Journey of Central Bank Communication,July 2020,Mark A. Wynne,,,Male,Unknown,Unknown,Male,,
55,4,Business Economics,03 November 2020,https://link.springer.com/article/10.1057/s11369-020-00194-0,From the editor,October 2020,Charles Steindel,,,Male,Unknown,Unknown,Male,,
55,4,Business Economics,24 September 2020,https://link.springer.com/article/10.1057/s11369-020-00184-2,The Covid-19 economic crisis: dangerously unique,October 2020,Claudio Borio,,,Male,Unknown,Unknown,Male,"Much has rightly been said about the uniqueness of this crisis. The crisis has resulted from a policy to tackle a health emergency through containment measures. Hence characterisations such as “putting the global economy into an induced coma” or “into hibernation”. And it has induced contractions in output and employment that have been even steeper than those during the Great Depression. Hence characterisations such as “a global sudden stop” (Fig. 1). A global sudden stop All this means that, in contrast to the Great Financial Crisis (GFC) of 2007–2009, the present crisis has three key features. It is truly exogenous, not the result of the unravelling of previous financial imbalances—the typical recession trigger since the mid-1980s. It is truly uncertain, in the specific sense that the wide range of possibilities depends on unpredictable non-economic factors. And it is truly global: despite how the 2007–2009 crisis is generally portrayed, many countries did not actually experience it, not least in Asia. A unique crisis calls for a unique response. The response has been unique in terms of objectives: not so much to boost aggregate demand so as to elicit increases in supply—home confinement has made the two highly unresponsive to traditional macroeconomic stimulus—but to offer a lifeline to firms and households during lockdowns, by providing the necessary bridge financing and resources. It has been unique in terms of scope: there has been unprecedented coordination between monetary, fiscal and prudential policies. And it has been unique in terms of the characteristics of the response in each of the policy areas. Take monetary, prudential and fiscal policy in turn. Monetary policy has relied less on interest rate cuts than on its time-honoured lender of last resort function. To be sure, cuts have been implemented; but more to instil confidence than to boost demand through the usual channels. For its part, the lender of last resort function has hardly followed standard script, as it has been adapted to the nature of the shock and the evolving structure of the financial system. This adaptation deserves particular attention. In one respect, speed and scope aside (Table 1), the adaptation has simply extended the evolution already seen during the GFC because of the rapid growth of market-based finance relative to bank finance (Fig. 2). Central banks have acted more as dealers or, strictly speaking, buyers of last resort than just lenders of last resort. Hence, their large-scale purchases of both private and public sector securities in an effort to stabilise markets. Indeed, for the first time, central banks in emerging market economies (EMEs) have done the same, by intervening in their now better developed domestic currency bond markets, where foreign investor participation has greatly increased (Arslan et al. 2020). This is testimony to EMEs’ much stronger and more credible macroeconomic frameworks, which have also allowed central banks to cut, rather than raise, policy rates. Share of bank loans in firms’ financing has fallen1 (in percent) In another respect, monetary policy has broken new ground. Central banks have gone one step further relative to the past, seeking to cover “the last mile” to reach businesses directly, including small and medium-sized enterprises. They have done this through backstops for bank funding. For example, think of the Fed’s Main Street Lending Program and its direct purchases of corporate securities. In the process, central banks have gone down the credit scale more than ever before, including taking on risk below investment grade (or the equivalent when companies are unrated). Prudential policy has taken an unprecedented direction (Borio 2020; Borio and Restoy 2020). Rather than encouraging banks to shore up their balance sheets and retrench, it has actually encouraged them to partly draw down the capital buffers accumulated since the GFC in order to keep credit flowing. By “capital buffer”, I mean the amount of capital above regulatory minima. To that effect, prudential authorities around the world have used the available flexibility to: ease both capital and liquidity requirements; impose blanket distribution restrictions, such as on dividends; and ease both the classification of exposures, such as non-performing loans, and the regulatory treatment of accounting losses—specifically, the new expected credit loss provisioning standard (Fig. 3). Countries taking easing prudential measures (in per cent) This fundamental change in approach reflects three factors. The first is the sense that everyone had to play their part to tackle the emergency. The second is the post-GFC change in perspective from a purely microprudential (MiP) approach—focused on the safety of individual banks considered in isolation—to a more macroprudential (MaP) approach, which considers them as part of a system (Borio 2018). Hence the notion of the “fallacy of composition”: it may be rational and indeed compelling for each institution to retrench and cut lending as the outlook deteriorates. But, if all do so collectively, they may actually end up worse off because of the spillbacks from the real economy. This is an instance of the excessive procyclicality of the financial system. Finally, the unique response reflects the fact that the banking system was much better capitalised going into the crisis, largely thanks to the post-GFC financial reforms (Fig. 4). As a result, policymakers could look upon banks as part of the solution, rather than as part of the problem. Banks entered the crisis in a strong position (in percent) In its own way, fiscal policy, too, has broken new ground. Huge size aside (Fig. 5), it has responded with a speed that is in all likelihood unprecedented. And it has adjusted the response to the nature of the shock. Hence the heavy reliance on furlough schemes designed to keep employees attached to their firms, and on guarantees extended either to borrowers, thereby providing banks with essential incentives to keep lending, or to the central bank, thereby leveraging its firing power. Prompt and forceful fiscal response (as a percentage of GDP) So far, the concerted policy response seems to have worked. Financial markets have stabilised—if anything, “too much”, in the sense that risky asset prices appear to have run ahead of a realistic assessment of the economic outlook (Fig. 6). Credit has kept flowing: bank credit has increased, while it had contracted during the GFC (Fig. 7). In part, this reflects the fact that, as firms drew on their credit lines, banks did not cut other forms of lending, at least to the same extent, and the economy has withstood the shock. Granted, the drop in activity has been dramatic. But activity has begun to rebound since the easing of containment measures (Fig. 1), and the drop would surely have been much bigger without such a vigorous policy response. Policies have stabilised markets Credit expanded considerably more during this crisis than during the GFC (in percentage points) Still, near-term and longer-term challenges remain, and they crucially depend on the uncertain evolution of the pandemic.",82
55,4,Business Economics,13 November 2020,https://link.springer.com/article/10.1057/s11369-020-00189-x,Central bank responses to COVID-19,October 2020,Patricia C. Mosser,,,Female,Unknown,Unknown,Female,,18
55,4,Business Economics,12 October 2020,https://link.springer.com/article/10.1057/s11369-020-00188-y,The effects of the COVID pandemic on the federal budget outlook,October 2020,Alan J. Auerbach,William Gale,,Male,Male,Unknown,Male,"COVID-19 has created significant changes in almost all aspects of the economy. In this paper, we examine the impact on the federal budget outlook, with five main results. First, we document that the pandemic and the policy responses to it rapidly and substantially raised federal deficits.Footnote 1 This increase is temporary, however. Spending and revenues are projected to return to pre-COVID baseline values relatively quickly. Second, the long-term fiscal outlook through 2050 has deteriorated somewhat. Under the Congressional Budget Office’s (CBO 2020f) assumptions for GDP growth and interest rates, we project that the debt-to-GDP ratio, currently 98%, will rise to 190% in 2050 under current law, compared to a pre-COVID baseline projection of 180%. CBO (2020f) obtains a similar projection—195%—using a slightly different set of assumptions about taxes and spending programs. Third, although the economic downturn and COVID-related legislation raise debt permanently, sharply lower projections of interest rates for the next dozen years help moderate future debt accumulation. Nevertheless, even during the period when interest rates are projected to be low, the projected debt-to-GDP ratio rises due to substantial and rising primary deficits, driven largely by rising outlays on health-related programs and Social Security. As the economy grows and debt accumulates, interest rates are projected to rise and to exceed the nominal GDP growth rate by increasing amounts starting in the early 2040s. Fourth, under a “current policy” projection that allows temporary tax provisions—such as those in the Tax Cut and Jobs Act of 2017—to be made permanent, the debt-to-GDP ratio would rise to 222% by 2050 and would continuing rising thereafter. Fifth, the long-term projections are sensitive to interest rates. If interest rates remain low (that is, at their projected level for 2025), rather than rising as in the CBO projections, the debt-to-GDP ratio would equal 157% in 2050 under current policy. We discuss several aspects of these results—including how the current episode compares to past debt changes, the role of historically low interest rates, and recent Federal Reserve Board policies. Because of the macro-stabilization effects of fiscal tightening, and because low interest rates create “breathing room” for fiscal policy,Footnote 2 we do not see the large, short-run debt accumulation resulting from the current pandemic as necessitating any immediate offsetting response. But the long-term projections show that significant fiscal imbalances remain and will eventually require attention.",2
55,4,Business Economics,13 November 2020,https://link.springer.com/article/10.1057/s11369-020-00193-1,COVID-19’s impact on the U.S. labor market as of September 2020,October 2020,Erica L. Groshen,,,Female,Unknown,Unknown,Female,"Perhaps only students of pandemics were not surprised by the rapid and massive impact of COVID-19 on the national and global economy. Certainly, few economists have analyzed policy and behavioral shocks of the breadth and depth that the U.S. has sustained since February 2020. The goal of this paper is to summarize, as of mid-September 2020, the impact of COVID-19 on a significant part of the U.S. economy, its labor market. The labor market is the largest and most complex market in the country. Furthermore, its outcomes affect the living standards of virtually all Americans. Thus, by examining the labor market, we can begin to assess the impact of the pandemic beyond the direct suffering it has caused through illness, deaths, disability and more. This article proceeds as follows. Section 2 lays the groundwork by reviewing the measurement of labor market conditions, primarily from the monthly Bureau of Labor Statistics’ Employment Situation releases. The main body addresses three big questions that help frame the discussion. The first question is what has happened so far to overall labor market conditions? To answer this, I look at measures of depth and abruptness, as well as the dimensions of the recovery since April, by looking at payroll jobs (Sect. 3) and the unemployment rate along with a broader measure of job disruptions (Sect. 4). The second question is who has been affected most, by race and sex (Sect. 5). Third, how does the information help inform expectations going forward? The conclusion (Sect. 6) summarizes the previous sections and then discusses the forces that will drive labor market outcomes in the coming months and some implications for official labor market statistics.",23
55,4,Business Economics,13 November 2020,https://link.springer.com/article/10.1057/s11369-020-00190-4,Measuring employment during COVID-19: challenges and opportunities,October 2020,Gerald D. Cohen,,,Male,Unknown,Unknown,Male,,10
55,4,Business Economics,20 October 2020,https://link.springer.com/article/10.1057/s11369-020-00185-1,In recovery mode: manufacturers try to bounce back after COVID-19 disruptions,October 2020,Chad  Moutray,,,Male,Unknown,Unknown,Male,,7
55,4,Business Economics,23 November 2020,https://link.springer.com/article/10.1057/s11369-020-00191-3,The coronavirus crisis and the technology sector,October 2020,Carolyn Evans,,,Female,Unknown,Unknown,Female,"The coronavirus crisis has created a profound shift in how people interact and economies function. Policy mandates and fears of becoming infected or infecting others have impelled populations to shelter at home, socially distance and otherwise reduce direct, in-person interactions with others. By enabling people to conduct many regular activities remotely, including working, learning, shopping and receiving medical services, technology has allowed the continuation of some semblance of a normal lifestyle in this new environment. Within this context, the IT sector has provided the tools and resources required to support these remote activities, as well as to address the pandemic more directly through efforts such as supporting contact tracing and providing high-performance computing resources for COVID-19-related research. This paper discusses in more detail this impact of the pandemic on the technology industry, and its response to this shock, by addressing three elements: accelerated digital transformation, increased importance of technology in the economy and society, and prevalence of inequalities in access to and the use of technology.",11
55,4,Business Economics,09 November 2020,https://link.springer.com/article/10.1057/s11369-020-00195-z,Tracking the U.S. health sector: the impact of the COVID-19 pandemic,October 2020,Corwin Rhyan,Ani Turner,George Miller,Male,Female,Male,Mix,,
55,4,Business Economics,28 October 2020,https://link.springer.com/article/10.1057/s11369-020-00186-0,Economists tackle the challenges of a pandemic,October 2020,Diane Swonk,Lisa D. Cook,Ellen Zentner,Female,Female,Female,Female,"For this special edition of the Economics at Work section, I tapped a broad spectrum of economists to share how the COVID-19 pandemic has reshaped what we do. I hope you will find the insights and inspiration that I have in reading and working with these extraordinary individuals. They warned us that it was the course of the virus, not lockdowns alone, that would determine how the economy weathered the storm unleashed by COVID-19. They saw the role that fear played in our collective decision-making and the lingering effects that uncertainty could have on our ability to move past the initial shock. They revealed the inequality that COVID-19 magnified and exacerbated, not least in our own profession. They showed how systemic biases undermine our potential to grow, not just as individuals but as an economy. They taught us that what we do requires heavy doses of humility and humanity to execute effectively.",1
56,1,Business Economics,08 February 2021,https://link.springer.com/article/10.1057/s11369-020-00207-y,From the editor,January 2021,Charles Steindel,,,Male,Unknown,Unknown,Male,,
56,1,Business Economics,04 January 2021,https://link.springer.com/article/10.1057/s11369-020-00197-x,What does the new long-run monetary policy framework imply for the path ahead?,January 2021,Charles L. Evans,,,Male,Unknown,Unknown,Male,"Thank you very much for inviting me to offer my views on the economy and monetary policy. It’s a real honor to be the kickoff speaker at this year’s NABE (National Association for Business Economics) Annual Meeting. It’s not every day you get to be the opening act for all of the rock stars you have later in the program. I’ve got my amp all set to go up to 11. And before I get started, there is the usual disclaimer that these are my own views and not necessarily those of the Federal Reserve System or the Federal Open Market Committee (FOMC). As 2020 opened, the fundamentals for the U.S. economy were solid, with unemployment at 3.5% and strong consumer spending. Monetary policy had been repositioned, moving from a tightening cycle to being on hold because of 18 months of rising uncertainties related to difficult and erratic foreign trade negotiations and their effect on business sentiment and spending. Then the Covid-19 virus hit and activity plummeted, as we and much of the rest of the world locked down in order to fight the pandemic. Unemployment soared, and inflation fell. Today, the economy is forging its way back, in spite of virus flare-ups and a horrific death toll of over 200,000 in the U.S. The performance of the economy has been surprisingly more resilient than I would have expected under these health circumstances. Much can be attributed to the fact that large portions of the business sector have successfully adapted to operating safely in the current environment and that individuals have taken to wearing masks and social distancing in response to the outbreaks. Still, the U.S. economy has a long way to go. The unemployment rate is just under 8%; about 12–1/2 million people are unemployed; and inflation continues to be well below our 2% goal. My forecast is very much in line with the latest Summary of Economic Projections (SEP) median.Footnote 1 Now that the reopening surge is behind us and given the extent of the lost ground we need to make up, I don’t expect activity will return to its pre-pandemic level until late in 2021. My outlook has the unemployment rate moving down steadily, but it is still 2023 before it reaches 4%. I expect inflation to slowly improve, reaching 2% on a persistent basis in 2023 and then moderately overshooting 2% over the following few years. My forecast assumes that additional federal fiscal policy actions are coming. I expect such support will play an enormously powerful role for providing adequate emergency relief until the Covid-19 health crisis is better contained. Still-struggling households and businesses need more aid, and help for state and local governments is especially important. Without adequate fiscal support before too long, I am concerned that recessionary dynamics will gain more traction and lead to a slower trajectory back to maximum employment. Of course, monetary policy also has an important role to play in these unusual and uncertain times. The Federal Reserve is prepared to use its full range of tools in order to support the U.S. economy’s attainment of maximum employment and inflation that averages 2% over time. A key element of this support will be the effective execution of the FOMC’s new long-run monetary policy framework that Chair Powell announced in August and that is articulated in the Committee’s revised “Consensus statement on longer-run goals and monetary policy strategy.”Footnote 2 The recent September FOMC statement followed through on the new strategic principles, in part by providing strong, outcome-based forward guidance about the future path for interest rates.Footnote 3 These commitments reaffirm the Federal Reserve’s resolve to provide the monetary accommodation needed to achieve our policy goals in as timely a fashion as possible. Today I would like to discuss some of my views on the messages in our new long-run framework and the September policy statement. I also will explore how the new consensus statement might have affected what some commentators have described as the Fed’s mistaken decisions to raise rates as soon and as high as we did over the 2015–2018 rate cycle. I hope to be more informative on alternative policy considerations than simply defensive, but I’ll let the listener be the judge.",
56,1,Business Economics,11 January 2021,https://link.springer.com/article/10.1057/s11369-020-00199-9,Outlook for democracy and democratic institutions,January 2021,Roger B. Myerson,,,Male,Unknown,Unknown,Male,,
56,1,Business Economics,29 January 2021,https://link.springer.com/article/10.1057/s11369-020-00200-5,Covid’s economic reset: making the quixotic quotidian,January 2021,Constance L. Hunter,,,Female,Unknown,Unknown,Female,,3
56,1,Business Economics,04 January 2021,https://link.springer.com/article/10.1057/s11369-020-00198-w,The legacy of Paul Volcker,January 2021,Sheila Bair,Donald Kohn,John Taylor,Female,Male,Male,Mix,,
56,1,Business Economics,07 December 2020,https://link.springer.com/article/10.1057/s11369-020-00192-2,"COVID-19, labor demand, and government responses: evidence from job posting data",January 2021,Xiaobing Shuai,Christine Chmura,James Stinchcomb,Unknown,Female,Male,Mix,,
56,1,Business Economics,07 October 2020,https://link.springer.com/article/10.1057/s11369-020-00187-z,Women empowerment and insecurity: firm-level evidence,January 2021,Ummad Mazhar,,,Unknown,Unknown,Unknown,Unknown,,
56,1,Business Economics,27 November 2020,https://link.springer.com/article/10.1057/s11369-020-00196-y,"Anjan V. Thakor, The purpose of banking: transforming banking for stability and economic growth",January 2021,W. Scott Frame,,,Unknown,Unknown,Unknown,Unknown,,
56,2,Business Economics,25 April 2021,https://link.springer.com/article/10.1057/s11369-021-00216-5,From the Editor,April 2021,Charles Steindel,,,Male,Unknown,Unknown,Male,,
56,2,Business Economics,30 March 2021,https://link.springer.com/article/10.1057/s11369-021-00215-6,Can addressing inequality unleash economic growth?,April 2021,Lisa D. Cook,Nela Richardson,Jim Tankersley,Female,Female,Male,Mix,,
56,2,Business Economics,28 January 2021,https://link.springer.com/article/10.1057/s11369-021-00208-5,Is it a gender representation issue or a gender pay gap issue? A study of the replaced executives in the USA,April 2021,Rama K. Malladi,Joshua D. Mean,,,Male,Unknown,Mix,,
56,2,Business Economics,26 March 2021,https://link.springer.com/article/10.1057/s11369-021-00214-7,Pre-retirement use of 401(k) funds,April 2021,David Bernstein,,,Male,Unknown,Unknown,Male,"Current tax law encourages people to save for retirement by providing substantial incentives for workers to contribute to their 401(k) plans.Footnote 1 Contributions to a conventional 401(k) plan are not part of adjusted gross income in the year; they accumulate earnings and are not taxed until disbursed. Contributions to a Roth 401(k) plan are fully taxed during the contribution year, but are untaxed when disbursed after age 59 ½. Tax law allows workers to use funds in a 401(k) plan prior to retirement. All plan participants are allowed to cash out their entire account at any time subject to payment of tax and penalty. Plan participants can also obtain funds through a 401(k) loan or a hardship distribution. Failure to repay 401(k) loans can lead to a tax and a 10% penalty on the unpaid balance. Hardship distributions are subject to tax as ordinary income, but are not subject to a penalty. The IRS has, in response to natural disasters, loosened restrictions on pre-retirement use of 401(k) funds.Footnote 2 The recently enacted CARES Act made even larger changes for workers experiencing economic hardship from the pandemic. The CARES Act allows workers impacted by the pandemic to withdrawal of up to $100,000 without penalty and to avoid tax on the distribution if funds are repaid in three years.Footnote 3 Intuitively, the pre-retirement use of 401(k) funds will be most pronounced for people with little or no financial slack. The use of 401(k) funds prior to retirement by households with weak balance sheets will likely increase the number of households entering retirement with inadequate financial resources. This paper uses data from a 2018 survey to provide a benchmark, pre-COVID, description of the relationship between pre-retirement use of 401(k) funds and household finances. The findings provide insight on the likelihood many households that will retire with insufficient income and wealth and the need for changes to rules governing the pre-retirement use of 401(k) assets.",
56,2,Business Economics,01 April 2021,https://link.springer.com/article/10.1057/s11369-021-00213-8,The market for acquiring card payments from small and medium-sized Canadian merchants,April 2021,Angelika Welte,Jozsef Molnar,,Female,Unknown,Unknown,Female,"Globally, card payments and other electronic payment methods are displacing cash at the point of sale (POS). Countries with mature card payment systems still experience growth in card payments outpacing growth in personal consumption expenditure; examples include Canada, the United States, Australia, and a number of EU member states, as well as countries in the Asia–Pacific region. The Covid-19 pandemic has also highlighted the significance of non-POS or e-POS payments where non-cash payments are already dominant (Chen et al. 2020; Huynh et al. 2020). Given these changes, the economics of the card ecosystem has become a focus for researchers and business experts around the world. The participants in the card ecosystem are the card networks, cardholders, issuers, merchants, and acquirers. In this article, we use industry data and merchant-level data from a survey of Canadian businesses to explore the relationship between acquirers and small and medium-sized merchants. Acquirers are payment service providers who offer merchants access to the card networks; thus, they play a key role in card payments. To fix terminology, we use the term acquirer here for the company that facilitates this access. Some acquirers have direct access to the card network (full-service acquirers), while others act as a third-party agent (TPA), or also processor (TPP), between the merchant and full-service acquirers. TPAs may provide merchant services as payment facilitators or independent sales organizations (ISO) for full-service acquirers or offer partial services to merchants who have an agreement with another acquirer. Although not directly connected, all major credit card networks such as Visa (2020) and MasterCard (2020) require TPAs to register with them. In 2018, acquirers provided debit card services to almost 500,000 merchants and credit card services to about 1 million merchants in Canada.Footnote 1 In the United States, the number of card-accepting merchants in 2018 was around 10.6 million (The Nilson Report). Although the literature (Rochet and Tirole 2002) and industry experts (Van Duynhoven 2010) seems to imply that the Canadian acquiring market is efficient and even competitive, empirical work on fees merchants pay to acquirers and on market structure in Canada is lacking. A report by the European Commission (2006) looks at the acquiring market in the EU-25 group of countries before the creation of the Single Euro Payments Area. Kjos (2007) discusses the US acquiring market, and Ho et al. (2020), using data on Chinese merchants, study a monopolistic acquiring market. Kosse et al. (2017) compute the resource costs of providing cash and card payments in Canada. They use a representative sample of small and medium-sized businesses (SMBs) that responded to the 2015 Retailer Survey on the Cost of Payment Methods (RSCPM). They assume that acquirers’ costs are equal to the fees they charge merchants. These fees blend fees remitted to other payment system participants with the acquirers’ own fees, however. In subsequent work, Fung et al. (2018) find heterogeneity in merchant fees. These studies motivate us to analyze acquirer prices by size of business. Three main findings contribute to our understanding of the Canadian acquiring market: Based on the representative sample of SMBs in Kosse et al. (2017), we find that smaller merchants pay their acquirer more for every dollar of card payment than larger merchants. Monthly fixed fees paid to the acquirer are proportionally more significant for merchants with lower card sales. Four large acquirers process about 85 percent of card transactions in Canada. These four acquirers have direct access to networks. While we discuss potential drivers of the observed market structure, further research and empirical evidence, which are beyond the scope of this article, are needed to assess the efficiency of the Canadian acquiring market.",1
56,2,Business Economics,12 January 2021,https://link.springer.com/article/10.1057/s11369-020-00201-4,John H. Cochrane and John B. Taylor (editors): Strategies for monetary policy,April 2021,David Wilcox,,,Male,Unknown,Unknown,Male,,
56,2,Business Economics,17 January 2021,https://link.springer.com/article/10.1057/s11369-020-00205-0,"Edward Nelson: Milton Friedman and economic debate in the United States, 1932–1972 (volumes 1 and 2)",April 2021,Peter N. Ireland,,,Male,Unknown,Unknown,Male,,
56,2,Business Economics,12 January 2021,https://link.springer.com/article/10.1057/s11369-020-00204-1,John Kay and Mervyn King: Radical uncertainty: decision-making beyond the numbers,April 2021,Paul Mizen,,,Male,Unknown,Unknown,Male,,
56,2,Business Economics,25 January 2021,https://link.springer.com/article/10.1057/s11369-020-00206-z,Robert Shiller: Narrative Economics: How Stories Go Viral and Drive Major Economic Events,April 2021,Stuart P. M. Mackintosh,,,Male,Unknown,Unknown,Male,,1
56,2,Business Economics,26 January 2021,https://link.springer.com/article/10.1057/s11369-020-00203-2,"Charles Goodhart and Manon Pradhan: The great demographic reversal: ageing societies, waning inequality, and an inflation revival",April 2021,Robert Eisenbeis,,,Male,Unknown,Unknown,Male,,
56,2,Business Economics,20 April 2021,https://link.springer.com/article/10.1057/s11369-021-00219-2,"Correction to: Charles Goodhart and Manon Pradhan: The great demographic reversal: ageing societies, waning inequality, and an inflation revival",April 2021,Robert Eisenbeis,,,Male,Unknown,Unknown,Male,"The article “Charles Goodhart and Manon Pradhan: The great demographic reversal: ageing societies, waning inequality, and an inflation revival”, written by Robert Eisenbeis, was originally published online on the publisher’s internet portal on 26 January 2021 with Open Access under a Creative Commons Attribution (CC BY) license 4.0. With the author’s/authors’ decision to cancel Open Access the copyright of the article changed on 07 April 2021 to © National Association for Business Economics 2021.",
56,3,Business Economics,09 June 2021,https://link.springer.com/article/10.1057/s11369-021-00231-6,From the Editor,July 2021,Charles Steindel,,,Male,Unknown,Unknown,Male,,
56,3,Business Economics,05 July 2021,https://link.springer.com/article/10.1057/s11369-021-00233-4,Correction to: From the Editor,July 2021,Charles Steindel,,,Male,Unknown,Unknown,Male,,
56,3,Business Economics,02 June 2021,https://link.springer.com/article/10.1057/s11369-021-00227-2,What do price equations say about future inflation?,July 2021,Ray C. Fair,,,,Unknown,Unknown,Mix,,
56,3,Business Economics,22 March 2021,https://link.springer.com/article/10.1057/s11369-021-00212-9,The impact of Hurricanes on the value of commercial real estate,July 2021,Jeffrey D. Fisher,Sara R. Rutledge,,Male,Female,Unknown,Mix,,
56,3,Business Economics,10 May 2021,https://link.springer.com/article/10.1057/s11369-021-00220-9,What influences entrepreneurship among skilled immigrants in the USA? Evidence from micro-data,July 2021,Nabamita Dutta,Saibal Kar,Russell S. Sobel,Unknown,Unknown,Male,Male,"Immigration continues to be a politically and economically challenging issue.Footnote 1 One of the persistent observations in large migrant-receiving developed countries is that the rates of self-employment, business ownership, and entrepreneurship are significantly higher among immigrants than native-born individuals, and this effect is equally prevalent in the USA, United Kingdom, Canada, Sweden, and Australia (Borjas 1986, 2000; Lofstrom 2017; Clark and Drinkwater 2000; Fairlie et al. 2012; Fairlie (1996, 2002); Blanchflower 2000; Ohlsson et al. 2010). While some of this may be due to ‘pull’ effects, such as immigrants placing higher value on independence or being more willing to take on risk, much is likely to have been caused by ‘push’ factors, due to the barriers immigrants face in finding wage employment, including, but not limited to, language barriers, lack of social networks, or discrimination.Footnote 2 While the literature regularly reports higher rates of self-employment for immigrants, the most convincing reasons behind this differential are still elusive. Another stylized fact from the literature is that there are persistent pay and job disparities between immigrant and native-born workers in traditional wage employment. For the USA, estimates of the income gap range from 17 to 20% (Borjas 1994; Card 2005).Footnote 3 For Canada, Reitz et al. (2014) report that under-utilization of immigrants’ skills results in a loss of $11.37 billion to the Canadian economy. One reasons for the gaps in wage employment that has received attention in the theoretical literature is the asymmetric information problem between domestic employers and immigrants regarding the quality of the human capital attained by immigrants in their home countries (Katz and Stark 1984; Chau and Stark 1999; Barrett et al. 2012; Beladi and Kar 2015; etc.). When domestic employers have difficulty in differentiating between high-quality and low-quality foreign educational credentials or human capital, the equilibrium wage offer (based on the expected or average quality) will lie below the true value of high-quality human capital, but above the true value of low-quality human capital. This wage offer will make employment highly attractive for those with lower quality human capital, but unattractive for those with higher quality human capital, leading to lower quality human capital types dominating the pool of applicants.Footnote 4 Worsening the situation, immigrants with lower quality human capital have an incentive to pose as high-quality, strategically clouding valuable signals to employers. The net result is lower equilibrium wages and firms hiring fewer immigrants, an effect the literature terms as ‘statistical discrimination’.Footnote 5 For our purposes, however, the important point is that immigrants with higher quality education and human capital will be undervalued in the domestic wage labor market, and the degree of this undervaluation is based on educational attainment. We hypothesize this asymmetric information effect as a possible causal explanation for some of the higher rates of self-employment among immigrants. Our hypothesis has a clear testable implication, that the differential in the likelihood of self-employment between immigrants and natives is not just a constant effect, as has been modeled in the prior literature, but rather the differential should grow with the level of human capital. This is because the domestic market wage differential (relative to the true value of the immigrant’s human capital) grows with the level of education, the incentive to seek self-employment as an alternative will be stronger for immigrants with higher levels of human capital. Low-skilled immigrants seeking mostly manual labor-intensive occupations should not suffer from as high a differential, because their educational credentials are not as big a factor in the hiring decision.Footnote 6 Thus, while prior literature has found higher generalized rates of self-employment for immigrants, our hypothesis leads us to specifically test whether this differential depends on the level of education. To test our hypothesis, we employ micro-level data for over one million individuals from the Integrated Public Use Microdata Series (IPUMS) database for the USA published by the Minnesota Population Center at the University of Minnesota. To describe the results briefly, we find that the probability of self-employment increases at a significantly higher rate with education for immigrants when compared to native-born individuals. Based on our model estimates for highly educated immigrants this effect accounts for almost 60 percent of the differential in the rate of self-employment between immigrants and natives. Consistent with available literature we also find important effects of age, gender, family size, and marital status on the likelihood of self-employment. Section 2 of the paper describes the data and develops the empirical specification. Section 3 discusses the results. Section 4 concludes by discussing the policy implications, avenues for future research, and potential limitations of our analysis.",2
56,3,Business Economics,01 July 2021,https://link.springer.com/article/10.1057/s11369-021-00229-0,High-frequency data from the U.S. Census Bureau during the COVID-19 pandemic: small vs. new businesses,July 2021,Catherine Buffington,Daniel Chapman,John Haltiwanger,Female,Male,Male,Mix,,
56,3,Business Economics,08 July 2021,https://link.springer.com/article/10.1057/s11369-021-00230-7,Who are the essential and frontline workers?,July 2021,Francine D. Blau,Josefine Koebe,Pamela A. Meyerhofer,Female,Female,Female,Female,"The COVID-19 pandemic has required the identification of essential workers, who are vital for the core functioning of societal infrastructure. Formation of policies to protect and meet the needs of these essential workers and to allocate scarce resources like personal protective equipment (PPE) and vaccines depends on knowing their composition and characteristics. However, identifying essential workers is not straightforward. The definition of essential work may differ by state, or even locality, and change rapidly over time. Moreover, the risk essential workers face is influenced by whether they are frontline workers who must provide their labor in person or whether they can work from home. As some industries, even those deemed essential, may at times be mostly shut down or facing steep decreases in demand, who is really at work also depends on the current shut down or demand status of their industry. We address these data issues to provide information on the characteristics of essential workers and, more specifically, frontline workers. We begin by applying the official industry guidelines issued by the Department of Homeland Security (DHS) Cybersecurity and Infrastructure Security Agency (CISA) in March 2020 to microdata from the 2018 and 2019 American Community Survey (ACS) to identify the broader group of essential workers.Footnote 1 We then use data on the feasibility of work from home in the worker’s occupation group (Dingel and Neiman 2020) to identify those most likely to be frontline workers. We find that the broader group of essential workers comprises a large share of the labor force and tends to mirror its demographic characteristics. In contrast, frontline workers are a less educated, lower wage, group, with a higher representation of men, disadvantaged minorities, especially Hispanics, and immigrants, on average. Both conclusions remain unchanged when excluding industries that were considered shut down/diminished demand during the early stages of the COVID crisis (Vavra 2020). Results for essential and frontline workers are similar when accounting for changes in the federal guidelines over time by using the December 2020 DHS guidelines which include a few additional groups of workers, including workers in the education sector.",33
56,3,Business Economics,01 February 2021,https://link.springer.com/article/10.1057/s11369-021-00209-4,Three cheers (sort of) for carbon taxes: a review of recent books on climate change economics and policy,July 2021,Yoram Bauman,,,Male,Unknown,Unknown,Male,,
56,3,Business Economics,18 February 2021,https://link.springer.com/article/10.1057/s11369-020-00202-3,Mitchel Y. Abolafia: Stewards of the Market: How the Federal Reserve Made Sense of the Financial Crisis,July 2021,Harvey Rosenblum,,,Male,Unknown,Unknown,Male,"This is a great book. It describes in granular detail the “Surprise; Confusion; and Groping Forward” of Fed Chairman Ben Bernanke and his fellow participants on the Federal Open Market Committee as they dealt with the economic paradigm that was seemingly shifting beneath their feet during the Great Financial Crisis (“GFC”). The reader gets to witness an almost live view of economic history through the eyes and perspective of a sociologist/cultural anthropologist, a refreshing alternative to the viewpoints of journalists, economists, historians, and policymakers themselves. From our vantagepoint in 2020, we know the stories of what happened and how the Fed stabilized the financial system and the macro economy. We also grasp that the Coronavirus Pandemic of 2020 is changing our economic paradigm from the post-GFC paradigm that we were beginning to feel accustomed to. Against this backdrop, the lessons of how economic policymakers, especially the Fed, cope, and adapt to rapid changes in the economic environment, are especially relevant.",
56,3,Business Economics,24 February 2021,https://link.springer.com/article/10.1057/s11369-021-00210-x,Anne Case and Angus Deaton: Deaths of despair and the future of capitalism,July 2021,Pia Orrenius,,,Female,Unknown,Unknown,Female,,
56,3,Business Economics,07 April 2021,https://link.springer.com/article/10.1057/s11369-021-00217-4,Michael Strain: The American dream is not dead (but populism could kill it),July 2021,Diane Lim,,,Female,Unknown,Unknown,Female,"In this book, Michael Strain attempts to make the case that the “Dream” is still “alive” and well, given the economic circumstances of the typical American, and suggests that populism—by leading Americans to blame each other for their remaining economic hardships—is undermining it. Strain tackles this challenge as any well-respected, well-trained economist would. But I think the trouble is that the economics discipline views social and even economic circumstances through lenses of our own design, and the issue of economic opportunity, outcomes, and “well-being” reaches far beyond that which can be studied and measured using only our traditional theoretical constructs and research methods. This review may suffer from my own personal biases as an Asian, first-generation-American, female economist, but I hope it will provide a useful, different perspective from those quoted in the “advance praise” or “dissenting points of view”—who are all men (and nearly all white men). I also have the advantage of writing this review post-pandemic, while Strain wrote his book pre-pandemic. The world has certainly dramatically changed—not just in circumstances, but in attitudes toward this very issue of economic opportunity and the closely related one of racism and other forms of discrimination. Consider the vivid reckoning about racism we experienced in the spring and summer of 2020 with George Floyd’s death and the Black Lives Matter movement, which coincided with the earliest peak of the pandemic crisis, and the racist verbal and physical attacks against Asians that surged at the start of the pandemic that persists to this day (President Biden condemned it at the beginning of his March 2021 address to the nation about the American Rescue Plan). Strain acknowledges from the start that he is not trying to define or evaluate the entirety of what people might mean by “The American Dream” but is focused on the economic component—which he introduces in Chapter 1 (“Defining the Dream”) as indicated by two criteria: Having a “successful career” and Having a “better quality of life than their parents”. Strain attempts to define these aspirations and evaluate how well they have been achieved based on some objective economic statistics, peering through his (white male) economist lens from a good distance (more than 6 feet!) away. The problem is that both these components of the “Dream” are rather subjective; “success” and “quality of life” are in the eye of the beholder and are likely impossible to quantify in any objectively standardized way. And these are even within just the economic components of “the American Dream!” Which is another way of asking if economists are well qualified to answer any part of the much larger question about what is “the American Dream” and whether it is indeed alive and well. After a brief set-up of “Today’s Message: The Dream Is Dead” (the pessimism of populism) in Chapter 2, Strain acknowledges some “real challenges” facing the American economy in Chapter 3: Not enough labor force participation (declining for prime-age men, stagnating for prime-age women); Human skills not keeping up with, or adapting to, technological advances/automation; Decline of business “dynamism” and entrepreneurship; All of the above directly feed into the social challenges of the opioid crisis and “deaths of despair.” 
Strain is silent on what I feel should be an opening (and huge) qualifier to the premise of his book: that it is the inherent biases and inequities in our social fabric that lead to seemingly insurmountable economic challenges facing large segments of the American population—rather than the other way around. It is naïve for economists to ignore the role of racism in generating disparities in economic outcomes and to strain (and for Strain!) to explain economic inequities by analyzing data which overwhelmingly ignore (do not adequately measure) minority populations. The aggregated nature of the main economic statistics that Strain relies on means they tell us mostly about white people (who still make up over three-fourths of the population) and to a marked extent about rich white people (given the skewness in the income distribution). Relying on these means we cannot “see” the people and the aspects of our society and economy we don’t seek out. So all of Strain’s case for the American Dream being alive and well needs to be hugely caveated with a label: “based on the data about mostly white people, using conventional economic research methods and measures.” In Chapter 4 Strain outlines the ways in which he will make the case in subsequent chapters that “The American Dream Is Not Dead”: “Today’s economy is delivering for American workers. Wages and incomes have not been stagnant for typical workers over the past three decades. The broader quality of life has improved significantly for typical households over the past several decades. Middle-income jobs have been “hollowed out,” but a new middle of the labor market seems to be forming. And, in the aggregate, hollowing out is a story of middle-skill employment being replaced by high-skill employment. America is still broadly characterized by upward economic mobility.” 
Let me now comment on each of the above conclusions and the case Strain presents. Chapter 5—“Today’s Economy Is Delivering.” Strain makes this claim based on the record-low unemployment numbers pre-pandemic. Even if it could optimistically be said the economy was “delivering” pre-pandemic (with the fastest growth in jobs in the leisure/hospitality and health and education services sectors), the parts of the economy that were growing fastest are now the most challenged in the midst of, and emerging from, the pandemic. And while those parts of the economy were thriving in job numbers, they had not been delivering in terms of pay. Service-providing, human-intensive jobs are (and always have been) disproportionately held by younger, female, minority, and immigrant workers—and those groups earn lower average pay, whether justifiable or not. Chapter 6—“Incomes Are Growing.” Strain substantiates this claim based on average real (hourly) wages; median and bottom quintile real cumulative growth in after-tax-and-transfer incomes; and inequality measured by the Gini coefficient (a single summary statistic). In all cases Strain is picking out the statistical evidence that most supports his claim, but those measures/metrics are not well suited for the question at hand about the “typical” American and his or her “dream”—or even this person’s situation relative to that person’s. Even those over-generalized statistics are quantitatively weak in terms of supporting his conclusions. For example, should we ignore that real market incomes for lower- and median-income households have failed to grow in the past decade and just take comfort that government safety-net policies have made up for it? (Without government policies, incomes at the bottom and median grew back in the 1990s but have not grown since.) Chapter 7—“Quality of Life Has Clearly Improved.” Based on what metrics, over what time period? For such a weighty assertion, Strain remarks “I don’t want to take up too much space demonstrating the obvious in this short book.” The whole chapter is less than 3.5 pages long, and Strain lists some examples that seem randomly picked from two other (white male) authors’ books instead of synthesizing into his own most compelling case. Chapter 8—“’Hollowing Out’ Won’t Be the End of the Story.” In this chapter Strain focuses on the change in the mix of jobs in the economy, largely caused by automation (but I’d add, also, offshoring). He finds that some “high-skill” jobs have been growing to replace “middle-skill” jobs that have gone away. But these job categories are based on ranking according to average wages paid by industry (which obscures the wide variance of jobs within each industry), and the growing vs. shrinking jobs basically follow the (decades-long) shift from goods-producing to service-providing industries. The distribution of occupations by industry doesn’t tell us anything about the experience of a typical worker working in that industry or even moving across industries. The mix of skills that are required for certain jobs in certain industries is constantly changing with advances in technology and changes in consumer tastes/demands, and with that, the nature and quality of jobs and what they pay will change as well. We shouldn’t care which jobs go away and which jobs come anew, as long as the people in those jobs have continued opportunities to advance in their careers and earn higher incomes. But it’s impossible to say how good or bad these career opportunities are for typical Americans by simply looking at aggregate changes in the numbers of jobs by industry. (For that matter, it is hard to understand the “future of work” by looking only at the “past “of work—i.e., historical, aggregated data.) Chapter 9—“America Is an Upwardly Mobile Society.” Strain gives the American economy at least a passing grade on the “mobility” text based on analyzing income categories within the Panel Study of Income Dynamics (PSID) and his subjective judgment about how much mobility is “good” enough. Even ignoring the fact that the PSID is far from a representative sample of American households, I can’t see how he interprets 6.6% of people raised in a bottom-quintile family ending up in the top quintile as a “good” amount of upward mobility. In his concluding chapter (Chapter 10, “Advancing the Dream”) Strain brings it back to the subtitle of his book, arguing that populism is undermining the American Dream. I think Strain means to blame polarized politics, and the “us vs. them” mentality, not populism. In my view, populism, properly understood, is about letting the experiences of everyone be heard and considered, and it seems to me ironic to have Strain blame it for trying to kill the American Dream, while he demonstrates the Dream is alive and well with his aggregate and average measures, all without talking with a single individual person trying to live it. To summarize my critique of Strain’s analytical approach: He assumes chosen categorizations (by annual income levels in one’s or one’s parents’ 40 s, occupational categories) are good (enough) representations of individual experiences and identities; He assumes that static, realized/observed outcomes tell us something about whether “dreams” were fulfilled—and that not only do average outcomes characterize the full distribution of outcomes decently well, but that the disaggregated, differentiated dynamics of the paths different people took to get from their points A to (higher) points B don’t matter. He assumes “success” when the objective quantitative measure is subjectively judged (by Strain) to be a “significant” amount (e.g., 6.6% of people raised in a bottom-quintile family ending up in the top quintile as adults).  In evaluating “the American Dream”—even from a purely economic perspective–one really cannot get away with ignoring the diversity of circumstances and experiences among all Americans. Within any of Strain’s income or occupational categories, there are huge disparities in economic opportunity, and the following identities (and various intersections among them) are typically disadvantaged relative to straight white males: Women People of color (particularly blacks and Hispanics) Immigrants People with disabilities The LGBTQ population  Even the PSID which Strain relies on for his analysis of mobility is based on a very non-representative sample of households: everyone in the PSID sample is associated with an original household surveyed in 1968 when the PSID started. (Thus, the sample has hardly any immigrants and does not have much racial diversity, especially lacking in Hispanics and Asians.) In conclusion, while I appreciate Michael Strain’s attempt to provide an optimistic view of the state of the American Dream, I find that he fails to provide a definitive assessment, given what I see as his (typical economist’s) far-off, blurry, and impersonal vantage point. How is it possible to use his book to motivate a call for policymakers to do better and more to help more Americans achieve whatever their own definition of their American Dream is? We cannot rely on the conventional research methodologies of economists, which summarize patterns by aggregate and average measurable outcomes, to reveal meaningful insights on how well the American Dream—even just the economic component of it—is doing among real Americans. Economists need to join forces with other social scientists and do more survey research and actually talk with real people who represent different intersections of identities. We first must better understand the vast diversity of different definitions of “successful” and “quality of life” that exist among (the vast diversity of) us—before we can better uncover and understand where the real barriers are to achieving those dreams and how new or reformed public policies could help.",
56,3,Business Economics,19 April 2021,https://link.springer.com/article/10.1057/s11369-021-00218-3,Michael Luca and Max H. Bazerman: The Power of Experiments: Decision-Making in a Data-Driven World,July 2021,Sara R. Rutledge,,,Female,Unknown,Unknown,Female,,
56,4,Business Economics,29 October 2021,https://link.springer.com/article/10.1057/s11369-021-00242-3,From the editor,October 2021,Charles Steindel,,,Male,Unknown,Unknown,Male,,
56,4,Business Economics,29 October 2021,https://link.springer.com/article/10.1057/s11369-021-00243-2,Economics at Google,October 2021,Hal Varian,,,Male,Unknown,Unknown,Male,,
56,4,Business Economics,18 October 2021,https://link.springer.com/article/10.1057/s11369-021-00241-4,"Housing preferences during the pandemic: effect on home price, rent, and inflation measurement",October 2021,Molly Boesel,Shu Chen,Frank E. Nothaft,Female,,Male,Mix,,
56,4,Business Economics,12 October 2021,https://link.springer.com/article/10.1057/s11369-021-00240-5,Estimating the economic cost of the COVID-19 pandemic,October 2021,Azhar Iqbal,Sam Bullard,,,,Unknown,Mix,,
56,4,Business Economics,22 September 2021,https://link.springer.com/article/10.1057/s11369-021-00224-5,"The power of prediction: predictive analytics, workplace complements, and business performance",October 2021,Erik Brynjolfsson,Wang Jin,Kristina McElheran,Male,,Female,Mix,,
56,4,Business Economics,24 August 2021,https://link.springer.com/article/10.1057/s11369-021-00237-0,Labor market outcomes under digital platform business models in the sharing economy: the case of the taxi services industry,October 2021,Sanae Tashiro,Stephen Choi,,,Male,Unknown,Mix,,
56,4,Business Economics,18 March 2021,https://link.springer.com/article/10.1057/s11369-021-00211-w,Stephanie Kelton: The Deficit Myth: Modern Monetary Theory and the Birth of the People’s Economy,October 2021,R. W. Hafer,,,Unknown,Unknown,Unknown,Unknown,,
56,4,Business Economics,10 May 2021,https://link.springer.com/article/10.1057/s11369-021-00223-6,"Harold James, Making a modern central bank: the Bank of England 1979–2003",October 2021,John Calverley,,,Male,Unknown,Unknown,Male,,
56,4,Business Economics,17 May 2021,https://link.springer.com/article/10.1057/s11369-021-00225-4,William Quinn and John D. Turner: Boom and bust: a global history of financial bubbles,October 2021,Christopher M. Meissner,,,Male,Unknown,Unknown,Male,,
56,4,Business Economics,18 May 2021,https://link.springer.com/article/10.1057/s11369-021-00221-8,"Charles Camic: Veblen, The Making of an Economist Who Unmade Economics",October 2021,Charles Steindel,,,Male,Unknown,Unknown,Male,,
56,4,Business Economics,21 May 2021,https://link.springer.com/article/10.1057/s11369-021-00222-7,Viral V. Acharya: Quest for restoring financial stability in India,October 2021,Rajeev Dhawan,,,Male,Unknown,Unknown,Male,"This book has a preface, which is the longest I have ever seen in my career, of 35 pages! And it was a delightful read. It is actually titled “Fiscal Dominance—A Theory of Everything in India” and is based upon a talk the author gave at NYU in January 2020. It spells out his main thesis, as the title itself entails, by telling you about the state of fiscal dominance in India. Fiscal dominance, in a nutshell, is funding of fiscal deficits by the central bank and managing that debt. In the Indian case, according to the author, it also means dominance of all financial sector policies and regulations (emphasis author’s). How this affects India’s financial stability is a very interesting read for anybody who wants to understand the working of an emerging economy’s financial system when they sit eight thousand miles away from it. The preface shows how this dominance leads to financial crises and low productivity traps. My assertion that one can understand firing (or gentle easing out) of central bank chiefs comes from understanding the fact that in order to clean up the shaky balance sheet of publicly-owned banks the government has to issue more debt. This is in addition to making the politically infeasible actions of closing zombie firms (and even public sector enterprises). The government is always loath to do this as it impinges on populist spending needs. As that takes precedence, it kicks the can down the road—unlike the formation of the Resolution Trust Corporation here in early 90’s to clean up the savings and loan mess from the prior decade. The country then relies even more on foreign capital inflows to meet these financing needs, as the corporate sector is also a net borrower, and domestic savings are insufficient. Consequently, when the central bank wants to fight inflation arising due to government’s ever-rising fiscal spending, fueled by populism, or from an unexpected oil price shock, or a sudden exchange rate weakening when foreign capital inflows reverse as world interest rates (read U.S.) rise, its only option is to hike the policy rate, which further deteriorates the banking sector’s papered-over financials. That has an immediate fiscal impact, as more capital now has to be pumped into the system by the government, competing with other spending programs. The end result is fiscal dominance of the central bank as it is now busy monetizing this new debt. Rate hikes are never popular, even in the developed world (if you remember our experience just a few years ago), but are outrightly detested in emerging markets by the powers that be. This then sets the stage for all kinds of negative outcomes for central bankers in emerging economies such as Turkey and Argentina.",
56,4,Business Economics,15 June 2021,https://link.springer.com/article/10.1057/s11369-021-00232-5,Correction to: Can addressing inequality unleash economic growth?,October 2021,Lisa D. Cook,Nela Richardson,Jim Tankersley,Female,Female,Male,Mix,,
56,4,Business Economics,17 June 2021,https://link.springer.com/article/10.1057/s11369-021-00226-3,"Correction to: Charles Camic: Veblen, The Making of an Economist Who Unmade Economics",October 2021,Charles Steindel,,,Male,Unknown,Unknown,Male,"The article Charles Camic: Veblen, The Making of an Economist Who Unmade Economics, written by Charles Steindel, has been updated. The subtitle was incorrect and has been corrected to Harvard University Press. No other changes to the article have been made.",
57,1,Business Economics,07 February 2022,https://link.springer.com/article/10.1057/s11369-022-00253-8,From the Editor,January 2022,Charles Steindel,,,Male,Unknown,Unknown,Male,,
57,1,Business Economics,31 January 2022,https://link.springer.com/article/10.1057/s11369-022-00252-9,The multiple hats of a Global Business Economist,January 2022,Manuel Balmaseda,,,Male,Unknown,Unknown,Male,,
57,1,Business Economics,20 January 2022,https://link.springer.com/article/10.1057/s11369-021-00246-z,Is there a labor shortage?,January 2022,Michael Horrigan,Misty Heggeness,Michael R. Strain,Male,,Male,Mix,,
57,1,Business Economics,12 July 2021,https://link.springer.com/article/10.1057/s11369-021-00234-3,"Carol Corrado, Jonathan Haskel, Javier Miranda, and Daniel Sichel (eds.): Measuring and accounting for innovation in the twenty-first century (NBER studies in income and wealth, volume 78)",January 2022,Diane Coyle,,,Female,Unknown,Unknown,Female,,3
57,1,Business Economics,03 August 2021,https://link.springer.com/article/10.1057/s11369-021-00235-2,"Daniel Yergin: The new map: energy, climate, and the clash of nations",January 2022,Tim Mullaly,,,Male,Unknown,Unknown,Male,,
57,1,Business Economics,12 August 2021,https://link.springer.com/article/10.1057/s11369-021-00236-1,"Fabian Schär and Aleksander Berentsen: Bitcoin, Blockchain, and Cryptoassets: A Comprehensive Introduction",January 2022,Rodney Garratt,,,Male,Unknown,Unknown,Male,,
57,1,Business Economics,26 August 2021,https://link.springer.com/article/10.1057/s11369-021-00239-y,"Stephen E. Koonin: unsettled: what climate science tells us, what it doesn’t, and why it matters",January 2022,David K. Levine,,,Male,Unknown,Unknown,Male,,
57,1,Business Economics,25 October 2021,https://link.springer.com/article/10.1057/s11369-021-00245-0,Timothy J. Bartik: Making Sense of Incentives: Taming Business Incentives to Promote Prosperity,January 2022,Cletus C. Coughlin,,,Male,Unknown,Unknown,Male,,
57,1,Business Economics,26 August 2021,https://link.springer.com/article/10.1057/s11369-021-00238-z,Matthew Yglesias: One Billion Americans: The Case for Thinking Bigger,January 2022,Ken Simonson,,,Male,Unknown,Unknown,Male,,
57,2,Business Economics,31 May 2022,https://link.springer.com/article/10.1057/s11369-022-00264-5,From the Editor,April 2022,Charles Steindel,,,Male,Unknown,Unknown,Male,,
57,2,Business Economics,11 May 2022,https://link.springer.com/article/10.1057/s11369-022-00259-2,Climate policy is macro policy: 2022 Volcker lecture at NABE conference,April 2022,Mark Carney,,,Male,Unknown,Unknown,Male,"It is a great honour to deliver the 2022 Paul Volcker lecture. Paul’s accomplishments and the lessons he bestowed are legion. He is best remembered for his success as Chair of the Federal Reserve in eradicating the high and volatile inflation that plagued America in the 1970s and early 1980s. From that singular achievement, we learned the value of operational independence of monetary policy and the importance of credible and forceful actions to quell high inflation and anchor inflation expectations. Many of Paul Volcker’s other contributions sprang from his deep understanding of the interdependencies within the economic and financial systems. Throughout his career, he was a tireless advocate of improving international cooperation to promote monetary and financial stability. From his time at the helm of US Treasury and the Federal Reserve Board, Volcker understood how the relationship between monetary and fiscal policies could be virtuous or vicious. From his spells at the NY Fed and in private finance, he knew first-hand how financial markets could anticipate and reinforce credible policies. Paul understood that financial markets need consistent, comparable, and decision-useful disclosure to do their job. That’s why he became the inaugural Chair of IFRS Foundation Trustees and shepherded in the first uniform global accounting standard. And finally, Paul Volcker foresaw the enormous threats from climate change. Never one to avoid tough choices, he was an early and vocal advocate of a carbon tax (https://www.ft.com/content/e9fd0472-19de-11e9-9e64-d150b3105d21). Today, I will draw on this legacy to consider the macro-economic challenges of the next decades. Economic history is rhyming with alarming force and frequency. The economic environment is now very different from that which reigned since the global financial crisis. Deficient demand and divine coincidence are out, trade-off inducing supply shocks and malign coincidence are in. I will argue that the policy responses to these developments cannot be considered in isolation of climate change. Indeed, whether it is addressed or ignored, climate change is now macro critical, and climate policy has become the third pillar of macro policy. The conduct of climate policy will directly impact the efficacy of fiscal and monetary policies, and its interactions with the financial system will heavily influence the pace of job and wealth creation. Like the rate of inflation, the degree of climate change is a choice, one that affects the prosperity and welfare of all. On climate change, as with the Volcker disinflation, now is not the time for half measures.",2
57,2,Business Economics,06 April 2022,https://link.springer.com/article/10.1057/s11369-022-00254-7,A note on the fed’s power to lower inflation,April 2022,Ray C. Fair,,,,Unknown,Unknown,Mix,,
57,2,Business Economics,23 February 2022,https://link.springer.com/article/10.1057/s11369-021-00250-3,Blending data to understand the economic impact of COVID-19,April 2022,Scott A. Brave,Rebecca Hutchinson,Ron Jarmin,Male,Female,Male,Mix,,
57,2,Business Economics,16 November 2021,https://link.springer.com/article/10.1057/s11369-021-00244-1,"Peter J. Boettke, Alexander William Salter, and Daniel J. Smith: Money and the rule of law: generality and predictability in monetary institutions",April 2022,Carl E. Walsh,,,Male,Unknown,Unknown,Male,,
57,2,Business Economics,26 November 2021,https://link.springer.com/article/10.1057/s11369-021-00247-y,Robert E. Gallman and Paul W. Rhode: Capital in the Nineteenth Century,April 2022,Alexander J. Field,,,Male,Unknown,Unknown,Male,,
57,2,Business Economics,03 February 2022,https://link.springer.com/article/10.1057/s11369-021-00249-w,"Jeffrey E. Garten: Three days at Camp David: how a secret meeting in 1971 transformed the global economy HarperCollins, 2021",April 2022,Edwin M. Truman,,,Male,Unknown,Unknown,Male,,
57,2,Business Economics,07 February 2022,https://link.springer.com/article/10.1057/s11369-022-00251-w,"Daniel Kahneman, Olivier Sibony and Cass R. Sunstein: Noise: a flaw in human judgment",April 2022,John Kay,,,Male,Unknown,Unknown,Male,,1
57,2,Business Economics,12 April 2022,https://link.springer.com/article/10.1057/s11369-022-00256-5,"Marie Springer: The politics of Ponzi schemes: history, theory, and policy",April 2022,Robert E. Wright,,,Male,Unknown,Unknown,Male,,
57,3,Business Economics,15 July 2022,https://link.springer.com/article/10.1057/s11369-022-00273-4,From the Editor,July 2022,Charles Steindel,,,Male,Unknown,Unknown,Male,,
57,3,Business Economics,11 June 2022,https://link.springer.com/article/10.1057/s11369-022-00267-2,Business forecasting during the pandemic,July 2022,John O’Trakoun,,,Male,Unknown,Unknown,Male,"The COVID-19 pandemic shock represented a once-in-a-generation challenge to both the global economy and business forecasting, and contributes to elevated uncertainty through the present day. In addition to the severity of the shock itself—the 31.2% annualized drop in GDP in Q2 2020 was the largest in data stretching back to 1947, and the 14.7% unemployment rate recorded in April 2020 was the largest in monthly data dating back to January 1948—the policy response to the shock upended a number of empirical regularities observed in pre-pandemic cycles. For example, via multiple rounds of fiscal support, real disposable personal income rose significantly during the pandemic recession, in contrast to experience of prior recessions (see Fig. 1). Additionally, the Federal Reserve implemented significant monetary policy accommodation to support the economy during the pandemic, including lowering the federal funds rate to its effective lower bound and expanding the asset side of its balance sheet from $4 trillion prior to the pandemic to nearly $9 trillion by March 2022. The unprecedented volatility in the economy left businesses scrambling to adjust their operations, and business economists scrambling to recalibrate forecasts and understand the tremendous shifts in the data. Real disposable personal income (SAAR, Bil. Chained 2012$) Ho (2021a, b) surveys various approaches to forecasting during COVID. During the episode, forecasters faced key dilemmas, including how to model the nature of the COVID shock (for example, as a period of high volatility versus a structural break in relationships between macroeconomic variables) and how to think about the persistence of the shock (for example, forecasters debated whether the subsequent recovery would be “U-shaped,” “V-shaped,” “L-shaped,” etc.). Research in this field is active and ongoing (Lenza and Primiceri 2020, Primiceri and Tambalotti 2020, Foroni et al. 2020; Ng 2021). In this article, we take the perspective of an applied business economist. We perform a retrospective evaluation of some of the workhorse statistical models used by business forecasters to see which approaches were most resilient during the early stages of the pandemic shock. We find projection-based approaches were more resilient to the pandemic shock than iteration-based forecasts in the cases we studied. We also find that the pandemic induced high variation in forecast performance among the models which incorporate macroeconomic data. The reliability of such models is sensitive to the extent to which the outcome variable in the forecaster’s industry is representative of broader economic trends. Given the volatility in standard data during the pandemic, many economists and forecasters turned toward nonstandard, high-frequency data to glean insights about the economy (Ryssdal and Hollnhorst 2021; McCracken 2020). We find that simply incorporating alternative high-frequency data into standard models did not necessarily improve forecast performance, however more research is needed to assess the extent to which these indicators improved business planning. Our results are in line with those of Schorfheide and Song (2021) who find mixed results when incorporating the data into their forecast models, with the baseline model performing poorly during the trough of the pandemic. The remainder of the paper proceeds as follows. In the next section, we discuss the data and various forecasting approaches that we compare. Section 3 presents results of the assessment, and Sect. 4 concludes.",
57,3,Business Economics,07 June 2022,https://link.springer.com/article/10.1057/s11369-022-00262-7,Technology and productivity growth,July 2022,Lucia Foster,Alex He,,Female,Male,Unknown,Mix,,
57,3,Business Economics,27 July 2022,https://link.springer.com/article/10.1057/s11369-022-00275-2,Correction: Technology and productivity growth,July 2022,Lucia Foster,Alex He,,Female,Male,Unknown,Mix,,
57,3,Business Economics,22 June 2022,https://link.springer.com/article/10.1057/s11369-022-00268-1,The impact of a U.S.–U.K. free trade agreement on workers: a CGE model with worker displacement,July 2022,Jeff Ferry,Badri Narayanan Gopalakrishnan,Amanda Mayoral,Male,Male,Female,Mix,,
57,3,Business Economics,25 July 2022,https://link.springer.com/article/10.1057/s11369-022-00276-1,Correction to: The impact of a U.S.–U.K. free trade agreement on workers: a CGE model with worker displacement,July 2022,Jeff Ferry,Badri Narayanan Gopalakrishnan,Amanda Mayoral,Male,Male,Female,Mix,,
57,3,Business Economics,13 June 2022,https://link.springer.com/article/10.1057/s11369-022-00257-4,Christopher Leonard: The Lords of Easy Money: how the federal reserve broke the American economy,July 2022,William Poole,,,Male,Unknown,Unknown,Male,"Suppose the Fed is doing a good job keeping the economy on a good track using its policy instruments. It would be best to say, “instrument” singular, rather than “instruments” plural. I assert that the only instrument the Fed has to affect the economy as a whole—the macro or aggregate economy—is the rate of interest. We used to talk of money growth and/or bank credit growth as instruments but Fed policy has long focused on interest rates. Putting aside extensive debates about these topics, most economists have agreed that the Fed can pursue a successful policy using its interest-rate instrument. What about quantitative easing and the Fed’s balance sheet? I’ll take up that subject shortly. Economists widely accept the proposition that monetary policy—with exceptions to be discussed—does not affect real variables in the long run. The Fed cannot keep the unemployment rate below the natural rate, or “non-accelerating inflation rate of unemployment” (NAIRU), as some prefer to put it. The Fed has no instruments to affect population growth, productivity growth, immigration, the distribution of income, and other real variables. The Fed cannot affect the number of people who are vaccinated. This argument concerning monetary effects on real variables needs to be modified to reflect the fact that inflation itself affects many real variables. Economists differ as to exactly how and why, but we know that inflation does affect real variables; therefore, Fed policy does affect real variables via inflation. Inflation above the Fed’s target of 2% per year may create inefficiencies. Also, inequities. So also might inflation below 2%; Variable inflation is a special concern because it is certain to be inaccurately anticipated by economic agents—consumers and producers and government officials at all levels. Variable inflation also tends to create financial instability. What do we mean by “financial instability""?It is normal in a market economy that asset prices fluctuate and that individuals and firms sometimes realize losses on their investments. What we must mean is something along the line of damaged market functioning. Examples would include the upset in the commercial paper market when Penn-Central Railroad declared bankruptcy in 1970. Same, at much larger scale, when Lehman Brothers declared bankruptcy in 2008. When markets are functioning properly, one firm can default on its obligations without affecting access to the market that other firms have. The Lehman failure created fear that other weakly capitalized firms holding real estate assets, and other assets of uncertain value, would also fail. The result was a stampede into safe assets. Damaged market functioning is especially likely when a large bank fails, as so many did in the horrible slide from 1929 to the bottom of the Great Depression in March 1933. When market functioning becomes disrupted, the Fed can step in to provide support in several different ways.Footnote 3 However, doing so risks creating longer-run problems as market expectations evolve to expect Federal Reserve rescues—the problem of moral hazard. Banking regulators do have another instrument available—bank capital standards. Unfortunately, capital standards have been too low and remain too low. Evidence that capital standards were too low in 2008 is that too many banks failed and that the standards have since been increased. In 2008, many market professionals did not trust the regulatory process.Footnote 4 There is another instrument that the Federal Government could and should use. An amendment to the corporate income tax code to eliminate or scale back the deductibility of interest would reduce the use of excessive amounts of debt by corporations. Another approach would be to permit corporations to deduct dividend payments in calculating corporate income subject to tax. Unfortunately, when discussing Rexnord neither Mr. Leonard nor his reviewers mention these options, long discussed in the public finance literature. Nor do they discuss higher bank capital requirements. These are changes that would make the financial environment more stable. Application of additional policy instruments could accomplish objectives monetary policy alone could not achieve. It is also true, unfortunately, that Federal Reserve officials have not argued vigorously for these reforms. As the U.S. economy recovered from the 1981–1982 recession, starting in November 1982, the inflation rate was much lower than it had been. Years of substantial growth followed, up to the crisis of 2008. During the Volcker-Greenspan years we saw Fed policy adjustments that helped the economy to get past hiccups of various sorts. Iraq’s takeover of Kuwait triggered an upset in the oil markets and a relatively mild recession in the United States starting in July 1990. The Fed lowered interest rates and maintained an environment of price stability. Before 2008, there were several other mild upsets. The Fed’s forecasts were not always accurate, nor its policy responses perfectly timed. Nonetheless, the analysis was good enough to bring the economy back to an even keel after each hiccups and growth continued. This experience demonstrates that in an environment of price stability and expectations of continuing price stability the Fed’s policy adjustments can contribute to a relatively stable and growing economy. Except for brief periods measured in quarters, the Fed did not affect real variables. The environment of economic stability surely contributed to enormous advances in computer technology and adoption of that technology by producers and consumers. Provided inflation expectations are firmly held, the Fed can simulate or restrain the real economy by adjusting its fed funds rate instrument down or up a bit. Thus, this single instrument can be used not only to stabilize inflation and inflation expectations but also can offset, at least in part, temporary conditions that would otherwise push the real economy off track. Mr. Leonard and his reviewers have completely failed to understand that the Fed has only one instrument—the short-term interest rate—it can use to guide the macro economy. Using that instrument for other purposes guarantees that it will not be used to guide the economy appropriately. Where We Are Today. Mr. Leonard and his reviewers discuss the effects of excessive Fed activism in growing its portfolio. What they do not examine sufficiently is the instability created by the huge growth in the Fed’s portfolio over a period of years. They focus on distributional issues and ignore the really big issue—potential to create or enable inflation. The inflation that began in early 2021 will have negative effects on income and wealth distribution. Inflation is punishing frugal households that invested conservatively. Inflation will reward households that borrowed on 30-year mortgages to buy houses they could just barely afford. They will look back and wonder why they ever had a doubt about buying those expensive houses. Even though residential property prices are today (January 2022) almost 20% higher than a year ago, market demand is still strong as families kick themselves for not buying sooner. These observations are consistent with Mr. Hoenig’s observations about what happened as a consequence of the 1970s inflation. The Fed’s problem, as of early 2022, is that it has lost its reverse gear and has no tugboats to call. By the end of 2021, the 12-month inflation rate was higher than it had been for 40 years. Tom Hoenig worried in 2010 because he observed the enormous growth in the Fed’s balance sheet without any apparent concern as to how to reverse the growth when the time came. My complaint about 2010—the year of Mr. Hoenig’s repeated dissents—and later years is very simple. Why did the Fed continue with massive asset purchases without figuring out why they were not working? A memo prepared for the March 2010 FOMC meeting is entitled, “Large-Scale Asset Purchases by the Federal Reserve: Did They Work?”Footnote 5 The authors say, “we discuss the economic mechanisms through which LSAPs may be expected to stimulate the economy and present some empirical evidence on those effects.” In fact, they provide extensive analysis supporting the proposition that the asset purchases reduced long-term interest rates but no analysis of whether that effect increased business investment or increased real GDP in some other way. Then, in later years the Fed continued to expand its LSAP program without any evidence as to why earlier efforts were unsuccessful. A physician who continued to prescribe the same drug without understanding why earlier dosages did not work would be guilty of medical malpractice. My review of FOMC materials available through 2016 does not identify any convincing studies of why the program was not working. I did some research on the investment question in 2011. In December of that year I presented a paper at a Philadelphia Fed conference entitled, “Where is the Investment Boom?” In the paper I tracked regulatory decisions of the Federal Energy Regulatory Commission (FERC) with regard to licensing pumped storage energy projects. I summarized my findings in a 2014 opinion piece in Forbes Magazine. “The federal government is not ‘permitting’ the economy to grow. Yes, that’s ‘permitting’ as in Environmental Protection Agency permits, Federal Energy Regulatory Commission (FERC) permits, Department of Transportation permits, and the list goes on and on. Federal regulatory agencies are not granting permits for private firms to build infrastructure, and U.S. investment and employment growth are hurting because of it.” In 2014, Fed staff members Eugenio Pinto and Stacey Tevlin wrote a memo for the FOMC, “Perspectives on the Recent Weakness in Business Investment.” They said nothing about regulation, instead basing their analysis on regression studies. Their regressions picked up the vigorous recovery in business investment after the 1980–1981 recession. That was an era characterized by the Reagan administration’s emphasis on reducing regulation and the investment incentives in the Economic Recovery Tax Act of 1981. In contrast, the recovery after 2009 was marked by heightened regulatory scrutiny and opposition of environmental groups to all investment that—as I like to put it—involved moving earth. The Keystone pipeline was victim of this mentality. Given the publicity over Keystone, my view was hardly obscure. Nonetheless, as best I can tell, the Federal Reserve never investigated the regulatory environment carefully. Wasn’t my hypothesis at least viable enough to deserve careful consideration? Along with Tom Hoenig, I feared the inflation consequences of the Fed’s asset purchases. My timing was far off—no question about that. That said, I wonder what the Fed would have found if it had done a thorough job investigating regulatory constraints. The investment boom in 1983–1984, which followed the deep recession of 1981–1982, reflected lower inflation, the investment tax credit in the 1981 Economic Recovery Tax Act, the Reagan administration’s emphasis on less regulation and the military build-up. Defense procurement is generally on a cost-plus basis and is difficult to model correctly. Positive conditions like these were absent after 2009; that, I believe, helps to explain the slow recovery of investment and economic activity after the financial crisis. My explanation is almost surely partial. The research is not yet in hand to finish the story.",
57,3,Business Economics,16 April 2022,https://link.springer.com/article/10.1057/s11369-022-00255-6,Nicholas Wapshott: Samuelson Friedman: The Battle Over the Free Market,July 2022,Richard G. Anderson,,,Male,Unknown,Unknown,Male,,
57,3,Business Economics,15 April 2022,https://link.springer.com/article/10.1057/s11369-022-00258-3,Rebecca Henderson: Reimagining capitalism in a world on fire,July 2022,Douglas J. Lamdin,,,Male,Unknown,Unknown,Male,,
57,3,Business Economics,02 June 2022,https://link.springer.com/article/10.1057/s11369-022-00266-3,Christopher Mims: Arriving today: from factory to front door—why everything has changed about how and what we buy,July 2022,Kenneth D. Simonson,,,Male,Unknown,Unknown,Male,,
57,3,Business Economics,04 June 2022,https://link.springer.com/article/10.1057/s11369-022-00265-4,Edward Glaeser and David Cutler: Survival of the City: Living and Thriving in an Age of Isolation,July 2022,Jeffrey P. Cohen,,,Male,Unknown,Unknown,Male,,
57,3,Business Economics,07 June 2022,https://link.springer.com/article/10.1057/s11369-022-00263-6,"Diane Coyle: Cogs and Monsters: what economics is, and what it should be Princeton University Press, 2021",July 2022,Rachel Soloveichik,,,Female,Unknown,Unknown,Female,,1
57,4,Business Economics,03 December 2022,https://link.springer.com/article/10.1057/s11369-022-00296-x,From the Editor,October 2022,Charles Steindel,,,Male,Unknown,Unknown,Male,,
57,4,Business Economics,06 August 2022,https://link.springer.com/article/10.1057/s11369-022-00277-0,"The great divide: education, despair, and death",October 2022,Angus Deaton,,,Male,Unknown,Unknown,Male,,
57,4,Business Economics,23 November 2022,https://link.springer.com/article/10.1057/s11369-022-00289-w,Why I am proud to be an economist,October 2022,David Altig,,,Male,Unknown,Unknown,Male,,
57,4,Business Economics,26 July 2022,https://link.springer.com/article/10.1057/s11369-022-00261-8,Is this full employment? assessing global labor force dynamics,October 2022,Dana M. Peterson,,,Female,Unknown,Unknown,Female,,
57,4,Business Economics,03 August 2022,https://link.springer.com/article/10.1057/s11369-022-00274-3,Measuring race in US economic statistics: what do we know?,October 2022,Sonya Ravindranath Waddell,John M. Abowd,Mark Hugo Lopez,Female,Male,Male,Mix,,
57,4,Business Economics,05 November 2022,https://link.springer.com/article/10.1057/s11369-022-00287-y,ROC approach to forecasting recessions using daily yield spreads,October 2022,Kajal Lahiri,Cheng Yang,,Female,,Unknown,Mix,,
57,4,Business Economics,21 October 2022,https://link.springer.com/article/10.1057/s11369-022-00282-3,The price elasticity of senior housing demand: is it a necessity or a luxury?,October 2022,Daniel G. Lindberg,,,Male,Unknown,Unknown,Male,"By 2030, one-in-five people will be of retirement age in the United States, every Baby Boomer will be at least 65 years of age, and approximately 10,000 people will turn 85 each day (US Census Bureau 2020). For this population, numerous life transitions impact their housing needs. Many of these transitions are positive: an emptied nest, retirement, grandchildren, travel, and leisure. Households may want to downsize or relocate near their children or grandchildren. Other transitions, like increased frailty, healthcare episodes, the death of a spouse or caregiver, or memory loss are more serious and require some form of personal care or support. In addition to more traditional options like single- or multi-family housing, seniors have tailored housing and care options available to them. Properties marketing active adult, senior apartments, independent living, assisted living, and memory care abound and discussions around senior housing demand have reemerged. In a recent white paper, the National Investment Center for the Seniors Housing and Care Industry (NIC), projects an additional 80,000–140,000 new senior housing units will be needed to meet the demands of aging Baby Boomers by the late 2030s, an investment that could reach 100 billion dollars (National Investment Center for the Seniors Housing and Care Industry 2019). Because the senior housing property sector emerged from needs-based settings, assessment of new market opportunities or demand traditionally assumed some percentage of need subject to an income qualification. When senior housing was a relatively new housing segment, demand exceeded supply and feasibility methods showing unmet need were naturally reinforced. Suppose a new development opportunity was evaluated in the late 1990s, constructed in 2000, opened in 2001, and became fully occupied by 2003? The needs-based methods used to assess new markets were a part of a positive feedback loop contributing to oversupply. Our approach estimates a simple model where demand is a function of price. This allows for an estimate of price elasticities, which inform several decisions for industry stakeholders: pricing, new market selection, renovation decisions, and competitive strategy. Moreover, it allows us to examine whether senior housing is considered a necessity or luxury by its consumers. We focus on independent living units separately and combine assisted living and memory care units since both services are often combined on the same campus. Section two provides an industry background, section three reviews the relevant literature, section four discusses the methods and data, section five presents results, and section six concludes.",
57,4,Business Economics,30 November 2022,https://link.springer.com/article/10.1057/s11369-022-00295-y,Subjective inflation expectations of households,October 2022,Michael Weber,,,Male,Unknown,Unknown,Male,"The expectations of households and firms determine virtually all forward-looking choices actual decision makers do. Inflation expectations take a special role, because they shape households’ consumption and savings decisions (D’Acunto et al. 2022a), households’ wage bargaining and labor supply (D’Acunto et al. 2022b), and also investment and leverage choices (Hackethal et al. 2022a, b). On the firm side, inflation expectations shape managers’ investment, hiring, and price-setting decisions (Weber et al. 2022a, b). A leading explanation for realized inflation dynamics, the New Keynesian Philipps Curve, also prescribes an important role to inflation expectations. Hence, it is not surprising that policymakers watch them closely, and Jerome Powell (2021) recently argued, “Inflation expectations are terribly important. We spend a lot of time watching them.” Yet, for many decades after the rational expectations revolution, academic economists had lost interest in studying how actual decision-makers form expectations, because the model directly implied the expectations of the representative agent. Moreover, traditionally, central banks typically focus on the inflation expectations of professional forecasters and financial markets. However, it is households and firms in our models whose decisions central banks aim to influence and empirically, inflation expectations are dispersed, upward biased relative to ex-post realized inflation, and systematically related to the characteristics of households and firms (D’Acunto et al. 2021a,b, forthcoming). In this article, I review the recent growing body of work that documents stylized facts on the formation of subjective inflation expectations, their determinants, and how they shape real decisions. I will focus on households but argue at the end that most points apply equally to firms.",
57,4,Business Economics,25 June 2022,https://link.springer.com/article/10.1057/s11369-022-00269-0,Gregory Zuckerman: A shot to save the world: the inside story of the life-or-death race for a COVID-19 vaccine,October 2022,Catherine L. Troisi,,,Female,Unknown,Unknown,Female,,
57,4,Business Economics,02 August 2022,https://link.springer.com/article/10.1057/s11369-022-00272-5,Ben S. Bernanke: 21st century monetary policy: the federal reserve from the great inflation to COVID-19,October 2022,Charles Goodhart,,,Male,Unknown,Unknown,Male,,
57,4,Business Economics,25 August 2022,https://link.springer.com/article/10.1057/s11369-022-00278-z,"Scott Sumner: The money illusion: market monetarism, the great recession, and the future of monetary policy",October 2022,Donald Kohn,,,Male,Unknown,Unknown,Male,,1
57,4,Business Economics,22 September 2022,https://link.springer.com/article/10.1057/s11369-022-00281-4,"Masaaki Shirakawa: Tumultuous times: central banking in an era of crisis Yale University Press, 2021",October 2022,Takeo Hoshi,,,Male,Unknown,Unknown,Male,,
57,4,Business Economics,27 October 2022,https://link.springer.com/article/10.1057/s11369-022-00288-x,Lev Menand: The Fed Unbound: Central Banking in a Time of Crisis,October 2022,Patrick M. Parkinson,,,Male,Unknown,Unknown,Male,,
57,4,Business Economics,17 November 2022,https://link.springer.com/article/10.1057/s11369-022-00291-2,Nick Timiraos: Trillion Dollar Triage: How Jay Powell and the Fed Battled a President and a Pandemic—And Prevented Economic Disaster,October 2022,Ellen E. Meade,,,Female,Unknown,Unknown,Female,,
57,4,Business Economics,15 November 2022,https://link.springer.com/article/10.1057/s11369-022-00293-0,Jon Hilsenrath: Yellen: The Trailblazing Economist Who Navigated an Era of Upheaval,October 2022,Charles Steindel,,,Male,Unknown,Unknown,Male,,
58,1,Business Economics,02 February 2023,https://link.springer.com/article/10.1057/s11369-023-00306-6,From the Editor,January 2023,Charles Steindel,,,Male,Unknown,Unknown,Male,,
58,1,Business Economics,04 January 2023,https://link.springer.com/article/10.1057/s11369-022-00298-9,The Voltage Effect,January 2023,John A. List,,,Male,Unknown,Unknown,Male,"I am beyond flattered by David Altig. I am actually floored that my work might be useful to someone of his stature in this group. Dana Suskind, my wife, and I were just flying home from London. I started to think about how important this award is. It is really important to me for sort of three reasons, and it really starts with David. That David is a scholar, and I read David's work in the '90s without him even knowing it. The work with Charles Carlstrom and others. When you receive an award like this from somebody who selects you, this is something that is beyond humbling, and I just want to say, David, thanks for your great work. Also, for your impeccable preferences in choosing good books. Second, I think this is a big win for field experiments. I want to talk a little bit about field experiments because a lot of you may not even know this, but field experiments are something I've been doing now for over 30 years. It is essentially using the world as my lab. Let me make this a little bit of a two-way street, and let me ask you, have you, in the last 5 years, used an Uber or Lyft? Have you voted in at least one of the past two presidential elections? About half of you are lying, my research would suggest. How many of you have flown United Airlines, or have used Google to search, in the past 3 years? Has anyone not said yes at least once? All of you have been a subject in at least one of my field experiments. It's not creepy in that I cannot pinpoint your name. I respect privacy. What I can say is I know what are good incentives to get you to give to a charitable cause. I know what are good incentives to keep you committed to a charitable cause. Why? Because we've been using field experiments now for 25 years in the economics of charity. I can talk about what motivates people to go and vote. Again, without compromising privacy. So a big reason why I received this award is because of all of you, helping me learn about the real world through economics. Finally, for me, to receive this award, when I looked at the other Adam Smith awardees, it really is a “Who’s who?” in economics. Many of my heroes, as an undergrad, and as a grad student, are on the list of Adam Smith awardees. So I want to say to David, thank you so much and to all of NABE, thank you so much. Now let’s talk about The Voltage Effect (List, 2022). The Voltage Effect (List, 2022) is essentially about scaling. It’s about the second half of my career. We talk a little bit about the first half, which is using field experiments to learn about the world. The second half, or understanding scaling, is actually how you change the world using science. I think, as social scientists, we have let down the broader community by asking and answering the wrong questions. That’s what I think we will learn today and throughout the first several chapters of The Voltage Effect Now, where I want to start is pre-K, how did we get to the voltage effect? I want to give you a little bit of background about the roots of the book. Of course, a book like The Voltage Effect has many roots, but one very important root happened in Chicago Heights. How many of you have heard of the community called Chicago Heights? It's a community that is about 30 miles straight south of here. This is a community wherein 95% of households are on federal food stamps. This is a community that the manufacturing jobs have left behind. They called me in 2007, and they said, ""John, we need your help."" As a humane human, it's not really about answering that question, ""yes"" or ""no."" The question really is, where do we start? Now, where we started was building our own pre-K. We wanted to start with early childhood, and we wanted to start because we believe, and I think most of us in this room believe, that the opportunity gap is something that continues to stifle not only equity but efficiency. You might be wondering right now, ""What in the world does an economist know about building a pre-K?"" Nothing. Zero. I don't know how to hire teachers. I don't know about bus lines. I don't know about federally subsidized lunches or milk. I do now, but I didn't then. What I had was a community that wanted to open up and needed help. Right away, I realized they don't have resources. So I went to Ken and Anne Griffin, and their foundation gave us $20 million to start the Chicago Heights Early Childhood Center. We started building in 2008, and we opened in 2010. Three, four, and five-year-olds, thousands of them. I had three goals. One, I wanted to help Chicago Heights kids. Two, I wanted to learn about the education production function, using randomization to explore executive function—cognitive—skills. I wanted to write academic papers to teach the rest of the world about what we were learning about human capital skill formation. Then three, I wanted to create a curriculum that we could scale, so every child in America and the rest of the world could use our scientifically-created curriculum. Data set after data set came in 2011, 2012, 2013, and 2014. We’re killing it. We are moving kids from the South Side of Chicago to the North Side of Chicago within 6 months. Looking at cognitive scores from a battery of tests, one called Woodcock-Johnson, looking at executive function skills. Many were created by us because it is a wild west in the executive function skill space. We were moving kids from the 20th percentile to the 60th within 6 months. Through CHECC, we're helping Chicago Heights kids. These kids are now sophomores and juniors in high school, and we're still tracking them. Second goal: write academic papers. Yes. We were writing dozens and dozens of academic papers about what we’d learned. The most recent one just came out in the journal of political economy on how social preferences are affected by our early childhood program. Now we get to the third goal. What do you think happens at the third goal? Remember, the third goal was, I want to scale that program to change the world. Here’s what happens: the slap in the face. “John, your program had an impressive benefit profile, but don’t expect it to happen at scale.” This was around 2014 or 2015. At that point, I had been doing field experiments for close to 25 years. I had never been met with that criticism. Ever. I started doing field experiments at baseball card conventions. It'd be in big halls, where people would walk in and buy, sell, and trade. In this case, people say, ""Well, those are weird people. Your results will never generalize to other people."" That was the argument. It was never about scaling. So I pushed back and said, ""Why?"" And they said, ""Your idea does not have the silver bullet."" What in the world does that mean? It doesn't have ""the silver bullet""? Does anything? Maybe, I thought, they were talking about ""Well, it doesn't have the Lebron James, or the Michael Jordan”, but it felt like art to me. So I pushed them. I said, ""I just don't understand."" They said, ""Look, all the experts come to us and say they have great interventions. And the ones that we choose, we scale those, and it's never close to what they promise."" Now that right there caused me to pause. Chicago Heights is my petri dish, and I want to scale it up. What is the science behind the expectation to scale? What is the model that people are thinking about when they go from the small to the large? That's when Dana comes in. Dana and I were sitting right down the street from Uber in 2016. I was chief economist. Dana had also been thinking about scaling a lot. Dana works in early childhood. She's a surgeon. Dana is telling me, ""Look, John. You need to take on this scaling thing. We need to add science to scaling.” We sat there and talked at length about the work that was out there. I talked to a lot of businesses and a lot of governments. Where I ended up was, this was a state of play in scaling. You had some great idea, the innovation—right? ""The pearls behind the swine,"" if you're into Biblical phrases. The innovation is great. Then, we move fast and break things. We throw spaghetti against the wall, and whatever sticks, you kick it. It's a gut feeling. Fake it till you make it. It's all art. This is simply art. I started to think, ""Oh, my gosh, what if we started to work on an economic model, develop the economic theory, add data, and throw the economic science at the problem of scaling?” So that's what we did. We started to write academic papers. Dana and I, and several other co-authors. We've now published dozens. I’m guessing not many have read one of our academic papers on scaling. When we write academic papers. We're lucky if two people read them. The editor and one of the three referees, and then we win. We wrote these academic papers, which is great. That's what we should be doing as academics. The problem is, the number of secrets and wisdom locked in academic journals is enormous. We need more translators. We have to unlock our ideas and knowledge, and our stuff, you don't even want to read it. It's a bunch of math. It's a bunch of jargon. It's economese. If you don't speak economese, you can't read economese, and it's all wrapped up in a parochial paper that is written for a very narrow audience. There's too much of that. There is too much wisdom locked up, so I decided to write the book. I decided to translate our work to an audience that hopefully will read it and understand it. The hardest part about the book business is getting people to open the book up. My worst enemy is a person who walks up and says, ""I love your book. It's sitting next to me on my nightstand."" And I say, ""What chapter are you on?"" ""Oh, I'm about halfway through chapter one."" And I say, ""Well, when did you put it down?"" ""Oh, about 3 weeks ago."" It's done. I'm done. There will be a new book that replaces mine, and they’ll get halfway through the intro. It’s still a tough business, but that's why I decided to write The Voltage Effect (List, 2022). So, let's talk about The Voltage Effect (List, 2022) now. Let's ask a question; when the policymakers said, ""Look, all these interventionists come to us and say they have something great,"" what do you think? Were they right or wrong? Well, that's what I call the voltage effect. It's the first law of scaling. The value proposition, whether it's the benefit–cost profile—however you want to define it, will change whenever you move from the small to the big, and in a predictable way. This is where the engineers might be mad at me right now for my language. They want me to title my book, ""The Wattage Effect,"" so then, instead of two people reading it, one would. My dad. Nobody wants to buy a book called ""The Wattage Effect."" So give me some artistic liberty. Here's a little bone for the engineers: high voltage is when you scale something up, and you reach a lot of different people, a lot of different situations. That's how I think about high voltage. What about that ""silver bullet"" thing? Were they correct there? They got the first one right. They get the second one exactly wrong. This is not a best-shot problem. It's not a, have Michael Jordan, and you're good. This is an Anna Karenina Problem: scalable ideas are all alike. Each un-scalable idea is un-scalable in its own way. The first half of the book documents the five vital signs, which really represents the DNA of ideas that have a shot to scale. I'm not talking about execution yet. I'm talking about, does your idea have the DNA to even have a chance? In the second half of the book, we'll talk about four little behavioral economic secrets about execution. Let me quickly go over these five vital signs. Vital sign number one, and it's amazing how many people fall prey to this. It was simply a false positive. This is ubiquitous among governments. I start off chapter one talking about Nancy Reagan, and her DARE program, which is one, big fat, false positive. Today, I want to talk about Tommy Lasorda. Remember Tommy Lasorda? Tommy Lasorda was the CEO of Chrysler. Tommy came to us at the University of Chicago and said, ""We have a weight problem amongst our line workers. And it's costing us money: health care expenses, presenteeism, and absenteeism."" He said, ""Can you help us?"" So, we put together a weight-loss program and we tested it in one plant. We killed it. Tommy immediately wanted to scale to the other 31 plants. Just like we all do when we get an initial set of results and we're really excited about it. We said, ""Tom, let's just wait. Let's try it again in that plant. Let's replicate it, and then let's go to three new plants."" Guess what happened when we did that? No result, no result, no result. It was simply a false positive. Two months later, Tommy got fired from Chrysler, and we ended up scaling a different plan with the new CEO. That's false positives. Vital sign number two: know your audience. Do you know who Commander Spock is? What's his genetic makeup? Half Vulcan, half NABE economist. He never gets it wrong. Ever. Let’s put him on the side for a minute, and discuss smart thermostats. Engineers promised us that, if every household had a smart thermostat installed, we would take a big chunk out of the climate change problem. That's what their model said. So we gave it a go. We went to California and several thousand households signed up for a free, smart thermostat. We sent half of them the smart thermostat and kept the other half back. Then we observed them for free months, 6 months, 9 months, and 12 months. Guess how much energy they saved? Zero. The paper just came out 2 weeks ago in the NBER. What happened? The engineers assumed the end-user was Commander Spock. Folks, the end user is more like Homer Simpson. The person gets the smart thermostat, goes in, and fiddles with the pre-sets. They fiddle with the defaults and exactly undo all of the good stuff. As Dana knows, this is exactly what I did to ours. Dana threw me the 28-page manual. I got it. I looked at it, and when you get my age, it seems like they write smaller and smaller. I looked at it and threw it back to her, and said, ""Honey, I have this. Let me take care of it."" What did I do? I undid all the presets. Know your audience. Chapter three. Is it the chef or is it the ingredients? I think this chapter contains the richest part of the first half of the book because it combines horizontal scaling with vertical scaling. It talks about understanding, in the petri dish, what are the most important features, or the non-negotiable features, that your idea has to have at scale? After thermostats, I start out talking about restaurants. We all know a restaurant that kills it. It has $1 million in EBITDA. Then, they say, ""Why not have ten or 20? We'll have ten million or 12 million of EBITDA, plus we'll make even more due to economies of scale."" So they give it a go. I'm here to tell you today that, if that initial success was due to a unique chef, the idea will never scale. Unique humans don't scale. We try to teach unique skills. Then it doesn't work. If you can systematize it, you have a shot, but it's hard to systematize it. Think about Uber and Lyft. If we needed Danica Patrick, Jeff Gordon, or Michael Schumacher type of drivers, how's that going to work? It's not going to work until we systematize it. We often do efficacy tests in the petri dish, because we want to give our idea its best shot. As academics, we are rewarded for big treatment effects. You get published in big journals and written up in the New York Times if you have big results. We do efficacy tests, and then we forget to tell everyone else that it was an efficacy test when we write it up. We are asking and answering the wrong question if we want to change the world with our science. We're asking the question “Can I create a program that can work in the petri dish, with the best inputs and a souped-up sample?” We really should be asking, “Can I create policy-based evidence?” What that means, if I'm talking about CHECC, is that I found out that I need good teachers at CHECC, in Chicago Heights. I only had to hire 30 teachers for my program. What would happen if I had to hire 30,000 good teachers, and kept the budget the same? What would happen to teacher quality? I didn't test for that. I did not oversample marginal teachers. I did not test the inputs that I needed to employ at scale. In chapter three, I call that policy-based evidence. I think it's a very important reason why we've been working on development economics, inner-city schools, and discrimination for decades. We are asking and answering the wrong question. For business types, really what I'm talking about is situationally-congruent evidence. What will you face at scale? Chapter four: unintended consequences and spillovers. Do you remember Ralph Nader? He wrote a popular book in '65. What was he complaining about? Highway safety. He complains so much and had such a deep impact that, by the time 1968 rolled around, the federal government mandated that every new automobile had to have a seat belt installed. Now young people are rolling their eyes, because they're, like, ""Oh, my goodness, that was back in the cave-person days. What do they mean, no seat belt?"" That's what I'm talking about. No seat belt. My colleague, Sam Peltzman, did an economic analysis that was published in 1975 that assessed how many lives were saved by that seat belt law. Guess what Sam estimated in his 1975 paper? Zero. What happened? People wearing seat belts drove more aggressively. Even though they died a little bit less often, they hit people without seat belts, and they died a little bit more often. That's a kind of spillover. At the individual level. Another kind of spillover happened to me when I was the chief economist at Uber. Turn back the clock to January 27th, 2017. President Trump issues an executive order on immigration. Remember that night? People went nuts. Taxi cab drivers around JFK went on strike. Uber, whenever something like that happens in a market, a market disruption, you know what Uber does? They turn off surge charges. They don't want to be viewed as price gougers. So, they turned off surge charges. Nevertheless, a taxi cab driver thought that Uber turned off surge charges to try to break up the strike. He went on a Twitter rampage that ended with, the hashtag #deleteuber. And overnight, the market share that Lyft had—remember, the main competitor to Uber is Lyft–Lyft had 5% market share Saturday morning. Sunday evening, they had 30% market share. Overnight, the hashtag, #deleteuber, gave Lyft a lifeline. Travis Kalanick came to me and my team. Travis said, ""Look, John, your team is responsible for getting the drivers back."" What does an economist do when somebody says, ""Your job is to get the drivers back."" What do you think my big idea was? Cash. Pay them more. I said, ""Let's introduce tipping."" Drivers want tipping. A lot of you might not remember this, but back then, there were tin cups in a lot of people's backseats. Customers didn't like it. Drivers thought they were not receiving the tips that they wanted. Everyone wanted tipping. All the executives said, no, right away. That was in early February 2017. I went door to door, saying, ""We need tipping. All the drivers want tipping."" I eventually won that battle. When you win a battle like that at Uber, guess what the booty is? The booty is, your team gets to roll out tipping. A field experimentalist being told, ""You get to roll out tipping."" so what do I do? I do it as a massive field experiment where I started working in the summer of 2017 on little pilots. I did one right here in Chicago, where I took 5% of the drivers in the market and said, ""You get to receive tips. The other 95% don't."" Then I observe them. They made more money. They worked more. Win, win, but when we rolled it out that October to all drivers in Chicago, guess what happened? All the drivers worked more. New drivers came in. They came in so much, and they worked so much more, that the wage effect that I observed in the summer went away completely. The market came to a new equilibrium where drivers would work more but drove around with empty cars more often. Their hourly wage was exactly the same as it was, pre-tipping. That's a market-wide spillover, that sometimes our ideas have. So, chapter four is about spillovers. Chapter five is going to be left behind closed doors. I am an economist after all, and I understand incentives. So I don't want to give everything away. Let me quickly talk about the last part of the book. For those that said yes to using Uber and/or Lyft, let me ask another question. How many of you tip your driver every time on Uber or Lyft? Guess what? Only 1% of customers tip on every trip. One percent. Three out of five people never, ever tip. Ever. But, guess what happens when I take those three out of five people who never tip on Uber or Lyft, and put them in a yellow cab, where they have to settle up afterward, face to face? Now guess how many of them tip? Ninety-five percent. Three out of five tip zero. Take those three out, they always tip face-to-face. Those are non-financial incentives—social pressure, social image, and social norms. These types of incentives are great incentives that scale. I talk about those and other behavioral economic incentives in chapter six. Chapter seven hearkened me back to the classroom. We always tell our students, ""Think on the margin. Be marginal thinkers. Not average thinkers."" They see how to do it mathematically, but when they go out to the real world, they have no idea how to apply it. In this chapter marginal thinking. Let me tell you a little bit about what happened at Lyft. I was the chief economist at Lyft for 4 years. The driver acquisition team came to me and said, ""Look, Logan Green, the CEO and founder of Lyft, has given us money to bring in new drivers."" I said, ""Okay. Tell me what you have."" They said, ""Well when we place ads on Facebook, it costs us $300 on average to bring in the last thousand drivers."" Okay. Then they said, the last thousand drivers using Google ads actually cost $400 each. I said, ""What do you plan on doing?"" They said, ""Well, of course, we're going to place the ads on Facebook."" I said, ""Well, let's take a little thinner slice of the data. Tell me a little bit about, say, the last 25 drivers."" They said, ""Well, we don't have that just yet."" I said, ""Send it to me tonight."" Here's what they sent me. They said, ""Professor, the last 25 drivers on Facebook ads actually cost us $1000 each, and on Google, it cost us $500 each."" They said, ""We get your point. We wish we could go back in time and take money from Facebook to Google. Going forward now, for a while, we're going to use Google ads."" Big data is great for precision, but when you add big data from a different regime, it causes you to think on the margin less often. Chapter eight: winners quit. There are a lot of old-schoolers in the room that may have liked me up until now, but I'm telling you, we do not quit enough. We don't quit enough for two reasons: one, society tells us that quitting is repugnant. I was born and raised in the land of Vince Lombardi. ""Winners never quit, quitters never win."" Type in, ""Quitting inspirational posters"" in Google, and you will have enough posters to fill every museum in the world. Right? We don't quit. It's repugnant. Another reason why we don't quit enough is because we neglect our opportunity cost of time. It's just human nature. Now that's a lot of economese, so let me quickly spell that out for you. I did a big survey on recent job quitters. I said, ""Why did you quit your job?"" Reason number one: I lost the meaning of work. Reason number two: I didn't get the promotion I deserved. Reason number three: I didn't get the pay raise I thought I deserved. Dot, dot, dot, all the way down to, ""I didn't like my cubicle anymore. Every reason was, my current lot in life got soiled. It was never, my opportunity set got better. That's why I left Lyft. I love Lyft. But my opportunity set got better. Walmart came to me, so I left. There's science in this chapter. The inspirational quotes on posters? That's art. Chapter nine is about scaling culture. This was fun because I've been working on the gender pay gap for a long time. I've worked on inclusiveness, equity, et cetera. I learned a lot at Uber. As you might imagine, a lot of stuff went down at Uber. When I moved to Lyft, I really didn't appreciate culture until I lived it. I saw Uber's culture. I saw Lyft's culture. Now, I see a very different culture at Walmart. It all starts at the top. The people who are working day to day, doing data science, or marketing, or management, or what have you, they're following the cues and the leadership from the group of executives. It happens in selection. So I talk here a lot about selection. Once you hire a rotten apple, it's really hard to change a rotten apple to a good apple. I talk about incentives to try to do that, but a lot of times, if you can't contract everything, the rotten apple's still rotten. So, that's chapter nine. That's The Voltage Effect. Thank you so much for your attention.
",1
58,1,Business Economics,19 January 2023,https://link.springer.com/article/10.1057/s11369-023-00301-x,Do looks matter in supply chain contracting? An experimental study,January 2023,Lyudmyla Starostyuk,Kay-Yut Chen,Edmund L. Prater,Female,Unknown,Male,Mix,,
58,1,Business Economics,28 December 2022,https://link.springer.com/article/10.1057/s11369-022-00299-8,Public sentiment and opinion regarding the CARES Act,January 2023,Maliha Singh,,,Female,Unknown,Unknown,Female,"When shutdowns began in March 2020 due to the COVID-19 pandemic, the US labor market experienced unprecedented job loss for the first two months accompanied by great economic uncertainty for an even longer period. According to the National Bureau of Economic Research, the contraction lasted only two months until April 2020. The COVID-19 recession was not only one of the deepest, but also the shortest in US history. The economy lost twenty million jobs rapidly after the COVID-19 shutdowns started, with the unemployment rate peaking at 14.8 percent in April 2020 (Center on Budget and Policy Priorities). To counter the adverse effects of COVID-19 shutdown on the economy, the federal government enacted five laws at a total fiscal cost of $3.4 trillion through Fiscal Year 2021. On March 27, 2020, the third law, the Coronavirus Aid, Relief, and Economic Security (CARES) Act was passed, which provided $1.8 trillion in direct aid to individuals and businesses, making it the largest stimulus package in US history. The massive emergency spending bill promised to deliver a tidal wave of cash to individual Americans, businesses, and healthcare facilities all reeling from the coronavirus pandemic. Studies using real-time data to understand the effect of COVID-19 shutdowns and stimulus cheques found that the economic collapse was mostly triggered by reduced spending by high-income individuals. Despite the stimulus checks being effective at increasing spending by lower-income individuals, the spending was not directed at the sectors most affected by the collapse in demand. The studies concluded that addressing the virus itself and restoring consumer confidence were essential to economic recovery (Chetty et al. 2020; Baker et al. 2020). The announcement of the CARES Act and the first round of stimulus checks, besides providing direct payments to individuals to spend on goods and services, might have also boosted consumer confidence in public policy. Besides the direct payments increasing spending through the fiscal multiplier effect, the increase in consumer confidence from the CARES Act and stimulus checks might have acted as an important fuel for economic recovery. To obtain a fuller evaluation of the CARES Act, it is essential to look into how the announcement of the CARES Act impacted consumer sentiment using real-time data. This paper analyzes the impact of the CARES Act on public sentiment and highlights public opinion regarding the CARES Act. Data for the analyses in this paper come from the popular social media platform, Twitter, and comments on the online news portal, New York Times (NYT). The sentiment analysis results from this paper indicate a statistically significant improvement in public sentiment on Twitter following the announcement of the CARES Act. The improvement faded away after a week, but again showed up for another week as the date of receiving the first round of stimulus checos approaches. To analyze public opinion surrounding the CARES Act, I implemented the Latent Dirichlet Allocation (LDA) algorithm. The LDA algorithm is an unsupervised learning algorithm that is commonly used to discover topics shared by observations in a dataset. In the LDA algorithm, each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics. The topic modeling results of the LDA model inform us that stimulus checks were needed to pay rent, food, emergency and mortgage bills, and support children and families. The results also indicate public support for the paycheck protection program, as well as aid for small businesses affected by the pandemic, healthcare systems and workers, retirement provision funds, and distiller businesses (who provided essential services to combat the sanitizer crisis). It also highlights the importance of tax credits by the Internal Revenue Service (IRS), webinars for emergency grant application information, policies designed to help workers, additional support for healthcare workers to combat health emergencies in communities, and the need for payment plans for debt and mortgage.",
58,1,Business Economics,21 January 2023,https://link.springer.com/article/10.1057/s11369-023-00303-9,Science of price experimentation at Amazon,January 2023,Joe Cooprider,Shima Nassiri,,Male,Female,Unknown,Mix,,
58,1,Business Economics,06 February 2023,https://link.springer.com/article/10.1057/s11369-023-00302-w,Disruption in the meat industry: new technologies in nonmeat substitutes,January 2023,Christopher Swann,Mary Kelly,,Male,,Unknown,Mix,,
58,1,Business Economics,02 August 2022,https://link.springer.com/article/10.1057/s11369-022-00271-6,"Mark Carney: Value(s)—Building a Better World for All Public Affairs, Hatchett Book Group, 2021",January 2023,Dennis Lockhart,,,Male,Unknown,Unknown,Male,,
58,1,Business Economics,21 September 2022,https://link.springer.com/article/10.1057/s11369-022-00279-y,"David Autor, David A. Mindell, and Elisabeth B. Reynolds: The Work of the Future—Building Better Jobs in an Age of Intelligent Machines",January 2023,Amanda Michaud,,,Female,Unknown,Unknown,Female,,
58,1,Business Economics,22 September 2022,https://link.springer.com/article/10.1057/s11369-022-00280-5,"Laurence J. Kotlikoff: An economist’s secrets to more money, less risk, and a better life",January 2023,Alicia H. Munnell,,,Female,Unknown,Unknown,Female,,
58,1,Business Economics,21 October 2022,https://link.springer.com/article/10.1057/s11369-022-00284-1,Paul Oyer: An economist goes to the game: how to throw away $580 million and other surprising insights from the economics of sports,January 2023,Robert Butler,,,Male,Unknown,Unknown,Male,,
58,1,Business Economics,21 October 2022,https://link.springer.com/article/10.1057/s11369-022-00286-z,"Michael Keen and Joel Slemrod: review of Rebellion, rascals, and revenues: tax follies and wisdom through the ages",January 2023,Jane G. Gravelle,,,Female,Unknown,Unknown,Female,,
58,1,Business Economics,26 October 2022,https://link.springer.com/article/10.1057/s11369-022-00285-0,Modernizing the Consumer Price Index for the 21st Century,January 2023,David Lebow,,,Male,Unknown,Unknown,Male,,
58,2,Business Economics,16 May 2023,https://link.springer.com/article/10.1057/s11369-023-00322-6,From the Editor,April 2023,Charles Steindel,,,Male,Unknown,Unknown,Male,,
58,2,Business Economics,11 May 2023,https://link.springer.com/article/10.1057/s11369-023-00318-2,"Bailouts, low rates, and their impact on financial stability",April 2023,Sheila Bair,,,Female,Unknown,Unknown,Female,,
58,2,Business Economics,20 April 2023,https://link.springer.com/article/10.1057/s11369-023-00311-9,Shift or shock? Long-term balance of power in labor markets,April 2023,Betsey Stevenson,Diane Swonk,Michael Horrigan,Unknown,Female,Male,Mix,,
58,2,Business Economics,07 May 2023,https://link.springer.com/article/10.1057/s11369-023-00319-1,Estimating the impact of NABE member characteristics on compensation,April 2023,Christopher Swann,Lilianna Ruby,,Male,Female,Unknown,Mix,,
58,2,Business Economics,18 April 2023,https://link.springer.com/article/10.1057/s11369-023-00316-4,Perspectives from an accidental economist,April 2023,Michelle Robinson,,,Female,Unknown,Unknown,Female,,
58,2,Business Economics,18 November 2022,https://link.springer.com/article/10.1057/s11369-022-00292-1,Andrew W. Lo and Stephen R. Foerster: In Pursuit of the Perfect Portfolio,April 2023,Steven A. Sharpe,,,Male,Unknown,Unknown,Male,,
58,2,Business Economics,19 November 2022,https://link.springer.com/article/10.1057/s11369-022-00290-3,"Thomas Moser and Marcel Savioz, Editors: Karl Brunner and Monetarism",April 2023,Boris Hofmann,,,Male,Unknown,Unknown,Male,,
58,2,Business Economics,22 November 2022,https://link.springer.com/article/10.1057/s11369-022-00294-z,Roger Lowenstein: Ways and Means: Lincoln and His Cabinet and the Financing of the Civil War,April 2023,Richard Sylla,,,Male,Unknown,Unknown,Male,,
58,2,Business Economics,29 December 2022,https://link.springer.com/article/10.1057/s11369-022-00300-4,Nicholas Mulder: The economic weapon: the rise of sanctions as a tool of modern war,April 2023,Gary Clyde Hufbauer,,,Male,Unknown,Unknown,Male,,
58,2,Business Economics,16 January 2023,https://link.springer.com/article/10.1057/s11369-022-00297-w,R. Douglas Arnold: Fixing Social Security: The Politics of Reform in a Polarized Age,April 2023,Peter Diamond,,,Male,Unknown,Unknown,Male,,
58,2,Business Economics,03 February 2023,https://link.springer.com/article/10.1057/s11369-023-00305-7,Jonathan Haskel and Stian Westlake: Restarting the Future: How to Fix the Intangible Economy,April 2023,Andrea L. Eisfeldt,,,Female,Unknown,Unknown,Female,,
58,2,Business Economics,06 March 2023,https://link.springer.com/article/10.1057/s11369-023-00309-3,Correction to Disruption in the meat industry: new technologies in nonmeat substitutes,April 2023,Christopher Swann,Mary Kelly,,Male,,Unknown,Mix,,
58,3,Business Economics,26 July 2023,https://link.springer.com/article/10.1057/s11369-023-00331-5,From the editor,July 2023,Charles Steindel,,,Male,Unknown,Unknown,Male,,
58,3,Business Economics,24 July 2023,https://link.springer.com/article/10.1057/s11369-023-00330-6,Financial stability,July 2023,Janet L. Yellen,,,Female,Unknown,Unknown,Female,,
58,3,Business Economics,05 July 2023,https://link.springer.com/article/10.1057/s11369-023-00324-4,The future of family-friendly policies,July 2023,Melissa Boteach,Chelsea Follett,Isabel V. Sawhill,Female,Female,Female,Female,,
58,3,Business Economics,08 July 2023,https://link.springer.com/article/10.1057/s11369-023-00328-0,"Housing market trends: analyzing housing dynamics, evaluating mortgage risk, and understanding the impact of filtering on affordability",July 2023,Edward J. Pinto,,,Male,Unknown,Unknown,Male,,
58,3,Business Economics,15 June 2023,https://link.springer.com/article/10.1057/s11369-023-00326-2,Confronting the housing supply shortage: policy options,July 2023,Laurie S. Goodman,,,Female,Unknown,Unknown,Female,,
58,3,Business Economics,09 June 2023,https://link.springer.com/article/10.1057/s11369-023-00323-5,Marvin Goodfriend: economist and central banker,July 2023,David A. Marshall,,,Male,Unknown,Unknown,Male,,
58,3,Business Economics,27 February 2023,https://link.springer.com/article/10.1057/s11369-023-00307-5,Jacob Soll: Free market: the history of an idea,July 2023,Benjamin M. Friedman,,,Male,Unknown,Unknown,Male,,
58,3,Business Economics,06 March 2023,https://link.springer.com/article/10.1057/s11369-023-00308-4,"Mary Childs: The bond king—how one man made a market, built an empire, and lost it all",July 2023,William R. Emmons,,,Male,Unknown,Unknown,Male,,
58,3,Business Economics,21 March 2023,https://link.springer.com/article/10.1057/s11369-023-00310-w,"Edward Chancellor: The Price of Time: The Real Story of Interest Atlantic Monthly Press, 2022",July 2023,Paul Schmelzing,,,Male,Unknown,Unknown,Male,,
58,3,Business Economics,27 March 2023,https://link.springer.com/article/10.1057/s11369-023-00312-8,"J. Bradford DeLong: Slouching towards Utopia: an economic history of the twentieth century Hachette Book Group, 2022",July 2023,Ralf R. Meisenzahl,,,Male,Unknown,Unknown,Male,,
58,3,Business Economics,27 March 2023,https://link.springer.com/article/10.1057/s11369-023-00313-7,"Alan S. Blinder: A monetary and fiscal history of the United States, 1961–2021",July 2023,Robert L. Hetzel,,,Male,Unknown,Unknown,Male,,
58,3,Business Economics,04 April 2023,https://link.springer.com/article/10.1057/s11369-023-00314-6,Daron Acemoglu and Simon Johnson: Power and progress: our thousand-year struggle over technology and prosperity,July 2023,Nicolas L. Ziebarth,,,Male,Unknown,Unknown,Male,,

Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.3,Welcoming the Evolutionary and Institutional Economics Review,November 2004,Geoffrey M. Hodgson,,,Male,Unknown,Unknown,Male,,
1.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.5,Evolutionary Economics in the 21st Century: A Manifesto,November 2004,Yoshinori Shiozawa,,,Male,Unknown,Unknown,Male,,23
1.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.49,Keynesian Economics—An Evolutionist Manifesto,November 2004,Masaaki Yoshida,,,Male,Unknown,Unknown,Male,,
1.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.57,Adam Smith and Competitive Equilibrium,November 2004,Ramesh Chandra,,,Male,Unknown,Unknown,Male,,7
1.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.85,Bottleneck Monopolies and Network Externalities in Network Industries,November 2004,Takanori Ida,,,Male,Unknown,Unknown,Male,,
1.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.107,Management Model for Technological Change and Sustainable Growth,November 2004,Shungo Sakaki,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.127,"Population Thinking, Price’s Equation and the Analysis of Economic Evolution",November 2004,Esben Sloth Andersen,,,Male,Unknown,Unknown,Male,,53
1.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.155,Why is Environmental Policy not Market-Based?,March 2005,Tosihiro Oka,,,Unknown,Unknown,Unknown,Unknown,,
1.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.177,Co-evolution of Accounting Rules and Creative Accounting Instruments—The Case of a Rulesbased Approach to Accounting Standard Setting,March 2005,Norio Sawabe,,,Male,Unknown,Unknown,Male,,10
1.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.197,A Study on the Consistency between Empirical Studies and Growth Models with Demand Satiation and Structural Change,March 2005,Tatsuyoshi Matsumae,,,Unknown,Unknown,Unknown,Unknown,,
1.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.1.221,Some Comments on the Methodological Principles of Nelson and Winter’s Evolutionary Theory,March 2005,Patrick Eparvier,,,Male,Unknown,Unknown,Male,,2
2.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.1,The Socio-economics of Institutions and Evolution,October 2005,Makoto Nishibe,Hiroyasu Uemura,,,Male,Unknown,Mix,,
2.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.7,The Nature of Institutional Economics,October 2005,Tony Lawson,,,Male,Unknown,Unknown,Male,,27
2.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.21,Can Self-interest Explain Cooperation?,October 2005,Samuel Bowles,Herbert Gintis,,Male,Male,Unknown,Male,,15
2.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.43,"Coherence, Diversity, and the Evolution of Capitalisms—The Institutional Complementarity Hypothesis",October 2005,Robert Boyer,,,Male,Unknown,Unknown,Male,,79
2.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.81,Rules and Knowledge,October 2005,Carlos M. Parra,,,Male,Unknown,Unknown,Male,,11
2.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.113,Economics as an Evolutionary System—Psychological Development and Economic Behavior,October 2005,Bernard Lietaer,Stefan Brunnhuber,,Male,Male,Unknown,Male,,5
2.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.145,An Evolutionary Theory of Economic Interaction—Introduction to Socio- and Econo-Physics,March 2006,Yuji Aruka,Jürgen Mimkes,,Male,Male,Unknown,Male,,5
2.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.161,Intentions and Principles of Sociodynamics,March 2006,Wolfgang Weidlich,,,Male,Unknown,Unknown,Male,,6
2.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.167,Real Estate Price Peaks—A Comparative Overview,March 2006,Bertrand M. Roehner,,,Male,Unknown,Unknown,Male,,13
2.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.183,Re-examination of the Size Distribution of Firms,March 2006,Taisei Kaizoji,Hiroshi Iyetomi,Yuichi Ikeda,Unknown,Male,Male,Male,,14
2.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.199,The Monetary Circuit —A “Mathematical Institutional” Interpretation,March 2006,Romar Correa,,,Unknown,Unknown,Unknown,Unknown,,
2.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.2.213,Characterizing Institutional and Heterodox Economics—A Reply to Tony Lawson,March 2006,Geoffrey M. Hodgson,,,Male,Unknown,Unknown,Male,,7
3.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.3,Redefining Evolutionary Economics,June 2006,Makoto Nishibe,,,,Unknown,Unknown,Mix,,
3.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.27,Evolutionary Linguistics and Evolutionary Economics,June 2006,Takashi Hashimoto,,,Male,Unknown,Unknown,Male,,2
3.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.47,A Present Appreciation of Evolutionary Economics — A Historical Characterization of the Alternative Thoughts of Economics in the Light of Evolutionism,June 2006,Susumu Egashira,,,Male,Unknown,Unknown,Male,,2
3.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.71,What Can Economics Learn from Post-Darwinist Developments in Biological Evolutionary Theory Represented by S. Kauffman and L. Margulis? — An Application Study in Asian Economic Systems,June 2006,Kenji Tominomori,,,Male,Unknown,Unknown,Male,,
3.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.89,Trust as a Governance Mechanism in Inter-Firm Relations — Conceptual Considerations,June 2006,Teemu Kautonen,,,Male,Unknown,Unknown,Male,,7
3.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.109,Co-evolution of Firm Strategies and Institutional Setting in Firm-based Late Industrialization — The Case of the Japanese Commercial Aircraft Industry,June 2006,Seishi Kimura,,,,Unknown,Unknown,Mix,,
3.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.141,"A New Construction of Ricardian Trade Theory—A Many-country, Many-commodity Case with Intermediate Goods and Choice of Production Techniques—",September 2007,Yoshinori Shiozawa,,,Male,Unknown,Unknown,Male,,39
3.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.189,Evolutionary Reading of Max Weber’s Economic Sociology—A Reappraisal of ‘Marx-Weber Problem’,September 2007,Kiichiro Yagi,,,Unknown,Unknown,Unknown,Unknown,,
3.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.209,Deliberative Trade Policy,September 2007,Carsten Herrmann-Pillath,,,Male,Unknown,Unknown,Male,,10
3.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.239,“S-shaped” curves in economic growth. A theoretical contribution and an application,September 2007,Gloria Jarne,Julio Sanchez-Choliz,Francisco Fatas-Villafranca,Female,Male,Male,Mix,,
3.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.261,Why Most (but not all) Churches Hate Sex,September 2007,Guido Ortona,,,Male,Unknown,Unknown,Male,,
3.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.275,Myrdal’s Theory of Cumulative Causation,September 2007,Nanako Fujita,,,Female,Unknown,Unknown,Female,,18
3.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.3.285,"Samuel Bowles, Microeconomics: Behavior, Institutions, and Evolution, Princeton University Press, 2004.",September 2007,Akinori Isogai,Hiroyasu Uemura,,Male,Male,Unknown,Male,,
4.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.1,The Evolution of Institutions and Organizations,December 2007,Hiroyasu Uemura,Akinori Isogai,,Male,Male,Unknown,Male,,
4.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.7,Evolutionary and Institutional Economics as the New Mainstream?,December 2007,Geoffrey M. Hodgson,,,Male,Unknown,Unknown,Male,,88
4.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.27,Karl Marx after New Institutional Economics,December 2007,Ugo Pagano,,,Male,Unknown,Unknown,Male,,11
4.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.55,Architecture-Based Comparative Advantage — A Design Information View of Manufacturing,December 2007,Takahiro Fujimoto,,,Male,Unknown,Unknown,Male,,65
4.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.113,The Diversity of Capitalism and Heterogeneity of Firms—A Case Study of Japan during the Lost Decade,December 2007,Sébastien Lechevalier,,,Male,Unknown,Unknown,Male,,13
4.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.143,Econo-physics: A Perspective of Matching Two Sciences,December 2007,Yuri Yegorov,,,Male,Unknown,Unknown,Male,,8
4.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.171,A Study on Changes in the Composition of Output—An Alternative Consumption Theory in Terms of Multiple-self,December 2007,Satoshi Yoshii,,,Male,Unknown,Unknown,Male,,
4.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.181,Productivity and the Complementary Nature of the Internal Institutions of the Firm,December 2007,Zhu Mei,,,,Unknown,Unknown,Mix,,
4.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.195,"Philippe Batifoulier (ed), Théorie des Conventions, Economica, Paris, 2001, 328 pages",December 2007,Fumiaki Suda,Akira Ebizuka,,Male,,Unknown,Mix,,
4.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.207,"Masaaki Hirooka, Innovation Dynamism and Economic Growth. A Nonlinear Perspective, Edward Elgar, Cheltenham, UK and Northampton, MA, USA, 2006, 448 pages",December 2007,Takeshi Sakade,,,Male,Unknown,Unknown,Male,,5
4.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.217,The Evolution of Moral Science: Economic Rationality in the Complex Social System,February 2008,Yuji Aruka,,,Male,Unknown,Unknown,Male,,1
4.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.239,The General Pattern of Marshallian Evolution,February 2008,Tiziano Raffaelli,,,Male,Unknown,Unknown,Male,,4
4.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.251,Econophysics: Challenges and Promises —An Observation-based Approach,February 2008,Bertrand M. Roehner,,,Male,Unknown,Unknown,Male,,3
4.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.267,Network Analyses of the Circulation Flow of Community Currency,February 2008,Nozomi Kichiji,Makoto Nishibe,,Female,,Unknown,Mix,,
4.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.301,Rate of Profit and Disproportionate Productivity Growth under a Constant Profit Share,February 2008,Hiroaki Sasaki,,,Male,Unknown,Unknown,Male,,
4.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.4.313,"Masanao Aoki and Hiroshi Yoshikawa, Reconstructing Macroeconomics—A Perspective from Statistical Physics and Combinatorial Stochastic Processes, Cambridge University Press, 2007, 352 pages",February 2008,Yoshi Fujiwara,,,Male,Unknown,Unknown,Male,,3
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.1,What is the U-Mart Project?,September 2008,Kazuhisa Taniguchi,,,Male,Unknown,Unknown,Male,,1
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.5,Where and Why Does the Zaraba Method Have Advantages over the Itayose Method? — Comparison of the Zaraba Method and the Itayose Method by Using the U-Mart System—,September 2008,Kazuhisa Taniguchi,Isao Ono,Naoki Mori,Male,Male,Male,Male,,4
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.21,Artificial Market Study as Interdisciplinary Research,September 2008,Hajime Kita,,,Male,Unknown,Unknown,Male,,1
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.29,Strategy Experiments in an Artificial Futures Market,September 2008,Takashi Yamada,Yuhsuke Koyama,Takao Terano,Male,Unknown,Male,Male,,2
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.53,U-Mart as a New Generation Artificial Market,September 2008,Yuhsuke Koyama,,,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.63,U-Mart System: A Market Simulator for Analyzing and Designing Institutions,September 2008,Isao Ono,Hiroshi Sato,Hajime Kita,Male,Male,Male,Male,,6
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.81,"Equilibrium Illusion, Economic Complexity and Evolutionary Foundation in Economic Analysis",September 2008,Ping Chen,,,,Unknown,Unknown,Mix,,
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.129,Consilience and the Naturalistic Foundations of Evolutionary Economics,September 2008,Carsten Herrmann-Pillath,,,Male,Unknown,Unknown,Male,,2
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.163,Tipping Points: Managing Complex Systems for Economic Development Success,September 2008,Hans-Peter Brunner,,,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.189,Toward an Incentive Alignment Theory of Nonprofit Organization,September 2008,Vladislav Valentinov,,,Male,Unknown,Unknown,Male,,6
5.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.197,"Y. Shiozawa, Y. Nakajima, H. Matsui, Y. Koyama, K. Taniguchi, and F. Hashimoto, Artificial Market Experiments with the U-Mart System, Springer ABSS Series Vol. 4, 2008, Springer-Verlag",September 2008,Takao Terano,,,Male,Unknown,Unknown,Male,,
5.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.205,Foundations and Microfoundations of Dynamic Capabilities,March 2009,Victor Pelaez,Ruth Hofmann,Dayani Aquino,Male,Female,Unknown,Mix,,
5.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.225,Where Do We Stand on the Intellectual Property Rights System between “Open Access and Reinforcing Private Ownership”?,March 2009,Shungo Sakaki,,,Unknown,Unknown,Unknown,Unknown,,
5.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.259,An Evolutionary Perspective on the Management of Stability and Change,March 2009,Carl Henning Reschke,Sascha Kraus,,Male,,Unknown,Mix,,
5.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.279,Can Chocolate be Money as a Medium of Exchange? Belief Learning vs. Reinforcement Learning,March 2009,Toshiji Kawagoe,,,Male,Unknown,Unknown,Male,,5
5.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.293,A Note on the Institution as a Nested Reasoning Structure in Terms of Bounded Cognition,March 2009,Tetsuya Kawamura,,,Male,Unknown,Unknown,Male,,2
5.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.5.307,"Klaus Mainzer, Der kreative Zufall: Wie das Neue in die Welt kommt (The Creative Chance. How Novelty Comes into the World, (In German)), C.H. Beck, München, 2007, 283 pages",March 2009,Yuji Aruka,,,Male,Unknown,Unknown,Male,,3
6.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.1,Challenges of Complexity in the 21st Century,September 2009,Klaus Mainzer,,,Male,Unknown,Unknown,Male,,5
6.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.23,On the Theory of Economic Evolution,September 2009,Kurt Dopfer,Jason Potts,,Male,Male,Unknown,Male,,33
6.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.45,Agent-based and “History-Friendly” Models for Explaining Industrial Evolution,September 2009,Minho Yoon,Keun Lee,,Unknown,,Unknown,Mix,,
6.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.71,A Transformational Conception of Evolutionary Processes,September 2009,Nuno Martins,,,Male,Unknown,Unknown,Male,,4
6.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.103,Two Propositions on the Theory of the Firm,September 2009,Francesco Bogliacino,,,Male,Unknown,Unknown,Male,,
6.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.121,Have Foreign Shareholders Helped Japanese Firms Demand Changes in the Number of their Employees?,September 2009,Jun Fukuda,,,,Unknown,Unknown,Mix,,
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.143,Simulation Analysis of Global Orders Based on the Concept of Global Public Goods,March 2010,Kazuo Yoshida,Makoto Sejima,Shigeru Fujimoto,Male,,Male,Mix,,
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.159,A Simulation Environment to Support Simulation Studies on Global Public Goods,March 2010,Hirofumi Yamaki,Muneyoshi Saito,Kazuo Yoshida,Male,Unknown,Male,Male,,
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.185,Coalition and Dilemma in a Three-person Game,March 2010,Takashi Hashimoto,Yasuhiro Uehara,,Male,Male,Unknown,Male,,
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.201,Alliance Formation and Better-Shot Global Public Goods: Theory and Simulation,March 2010,Shintaro Nakagawa,Makoto Sejima,Shigeru Fujimoto,Male,,Male,Mix,,
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.227,An Evolutionary Route to the Formation of Coordination in the Iterated Leader’ s Game with Errors,March 2010,Eizo Akiyama,,,Male,Unknown,Unknown,Male,,
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.245,Two-country Negotiation Game by Players Presuming the Opponent’ s Payoff Structure,March 2010,Susumu Egashira,,,Male,Unknown,Unknown,Male,,
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.259,Of Ants and Men—the Role of Complexity in Social Change,March 2010,David G. Green,Tania G. Leishman,Gary D. Leishman,Male,Female,Male,Mix,,
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.277,Multilevel Selection Processes in Economics: Theory and Methods,March 2010,Natalia Zinovyeva,,,Female,Unknown,Unknown,Female,,1
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.299,"A Comparison of Non-regular Employment in Korea and Japan: Nature, Difference, and its Possible Reasons",March 2010,Joon-Young Kim,,,,Unknown,Unknown,Mix,,
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.329,Normal and Critical Evolution in Sociodynamics,March 2010,Wolfgang Weidlich,,,Male,Unknown,Unknown,Male,,1
6.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.6.337,Torahiko Terada (1878–1935): Father of the science of complex systems,March 2010,Mitsugu Matsushita,,,Male,Unknown,Unknown,Male,,1
7.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.1,Bounded Rationality and Institutional Change,September 2010,Nikolas Kyriazis,Theodore Metaxas,,Male,Male,Unknown,Male,,13
7.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.21,"Scope and Limits of Predictions by Social Dynamic Models: Crisis, Innovation, Decision Making",September 2010,Péter Érdi,,,Male,Unknown,Unknown,Male,,2
7.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.43,Cohort Effects in Food Consumption: What They Are and How They Are Formed,September 2010,Hiroshi Mori,Yoshiharu Saegusa,,Male,Male,Unknown,Male,,5
7.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.65,A Doubly Structural Network Model: Bifurcation Analysis on the Emergence of Money,September 2010,Masaaki Kunigami,Masato Kobayashi,Takao Terano,Male,Male,Male,Male,,7
7.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.89,Analysis of Realized Volatility in Superstatistics,September 2010,Tetsuya Takaishi,,,Male,Unknown,Unknown,Male,,9
7.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.101,Analysis of Institutional Evolution in Circuit Breakers Using the Concepts of Replicator and Interactor,September 2010,Shigeto Kobayashi,Takashi Hashimoto,,Male,Male,Unknown,Male,,1
7.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.113,A Study on the Effectiveness of Short-selling Regulation using Artificial Markets,September 2010,Isao Yagi,Takanobu Mizuta,Kiyoshi Izumi,Male,Unknown,Male,Male,,20
7.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.133,Influence of Investors’ Loss-cut Behavior to Artificial Market,September 2010,Fei Zhai,Kenji Takahashi,Eisuke Kita,,Male,Male,Mix,,
7.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.155,"Institutional Hierarchy Hypothesis, Multilayered Adjustment, and Macroeconomic Performance: A Post-Keynesian Dynamic Approach",September 2010,Hiroshi Nishi,,,Male,Unknown,Unknown,Male,,3
7.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.177,The Place of National Systems of Social Protection and Political Representation in Socio-Economic Regulation: A Morphogenetic Structuralist View on Institutional Change in Comparative Perspective with Special References to Japan and France,March 2011,Bruno Théret,,,Male,Unknown,Unknown,Male,,7
7.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.209,An Analysis of the Japanese Credit Network,March 2011,Giulia De Masi,Yoshi Fujiwara,Joseph E. Stiglitz,Female,Male,Male,Mix,,
7.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.233,Elements of Structural Economics,March 2011,Yuri Yegorov,,,Male,Unknown,Unknown,Male,,1
7.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.261,A Model of Credit Rationing without Asymmetric Information: An Inquiry into the Credit Market during an Economic Depression,March 2011,Daiki Asanuma,,,Male,Unknown,Unknown,Male,,2
7.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.279,The Explanation of Incomplete Contracts in Mainstream Contract Theory: A Critique of the Distinction between “Observable” and “Verifiable”,March 2011,Hans Lind,Johan Nyström,,Male,Male,Unknown,Male,,
7.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.295,Labour Decisions and Industrial Dynamics in an Evolutionary Model: A Neglected Modelling Approach,March 2011,Sandra T. Silva,Aurora A. C. Teixeira,,Female,Female,Unknown,Female,,
7.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.333,Competitiveness and Industrial Evolution: The Case of the Ceramics Industry,March 2011,Elisabeth T. Pereira,António J. Fernandes,Henrique M. M. Diz,Female,Male,Male,Mix,,
7.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.355,Benefits and Limits of Circuit Breaker: Institutional Design Using Artificial Futures Market,March 2011,Shigeto Kobayashi,Takashi Hashimoto,,Male,Male,Unknown,Male,,7
7.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.7.373,"Institutions, Firms and Consumers’ Choice: Extending Neoschumpeterian Competition to Consumption",March 2011,Felipe Almeida,Huascar Pessali,,Male,Unknown,Unknown,Male,,1
8.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.1,Inter and Intra Company Competition in the Age of Global Competition: A Micro and Macro Interpretation of Ricardian Trade Theory,June 2011,Takahiro Fujimoto,Yoshinori Shiozawa,,Male,Male,Unknown,Male,,3
8.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.39,Globalization and the Emerging Economies: East Asia’s Structural Shift from the NIEs to Potentially Bigger Market Economies (PoBMEs),June 2011,Hitoshi Hirakawa,Than Than Aung,,Male,,Unknown,Mix,,
8.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.65,Hunting for a Bogeyman? In Search of Statistical Evidence of Direct Competition between Firms,June 2011,Alex Coad,Marco Valente,,Male,Male,Unknown,Male,,2
8.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.87,Eigenvalue Distribution and the Production Price-Profit Rate Relationship: Theory and Empirical Evidence,June 2011,Theodore Mariolis,Lefteris Tsoulfidis,,Male,Unknown,Unknown,Male,,23
8.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.123,A Concept of Transaction-Based Economics: A System of National Accounts Based on Corporate Transactions,June 2011,Shungo Sakaki,,,Unknown,Unknown,Unknown,Unknown,,
8.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.159,Overcoming Cournot’ s Dilemma on Increasing Returns and Competition through an Integrated Perspective on the Firm,June 2011,Mario Morroni,,,Male,Unknown,Unknown,Male,,4
8.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.177,Evolutionary Aspects of Coasean Economics,June 2011,Masahiro Mikami,,,Male,Unknown,Unknown,Male,,2
8.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.193,Inter and Intra Company Competition in the Age of Global Competition: A Micro and Macro Interpretation of Ricardian Trade Theory,March 2012,Takahiro Fujimoto,Yoshinori Shiozawa,,Male,Male,Unknown,Male,,16
8.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.233,Money Manager Capitalism and the New Diseases of Capitalism,March 2012,Shigeyuki Hattori,,,Male,Unknown,Unknown,Male,,2
8.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.253,Export of Deindustrialization and the Anti-Balassa-Samuelson Effect: The Consequences of Productivity Growth Differential,March 2012,Hiroaki Sasaki,,,Male,Unknown,Unknown,Male,,
8.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.279,Is Cooperation Stimulated in a Prisoner’s Dilemma by Establishing an Efficient Outcome in a Simultaneously Played Coordination Game?: An Experimental Analysis,March 2012,Tetsuya Kawamura,Kazuhito Ogawa,Sobei H. Oda,Male,Male,Unknown,Male,,
8.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.8.297,Reflective Exploitation of Economic Descriptors: Non-Normative Rationality,March 2012,Mohamed Ihab Kira,,,Male,Unknown,Unknown,Male,,
9.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.O2012001,Evolution of Firms and Industries,December 2012,Takahiro Fujimoto,,,Male,Unknown,Unknown,Male,,6
9.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2012002,Estimating Optimal Product Variety of Firms,December 2012,Yoshinori Shiozawa,,,Male,Unknown,Unknown,Male,,1
9.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2012003,The Evolution of the Home Video Game Software Industry in Japan: An Empirical Study on Factors in the Industry’s Evolution,December 2012,Fumihiko Ikuine,,,Male,Unknown,Unknown,Male,,4
9.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2012004,An Economic Analysis of Architecture and Coordination: Applying Ricardian Comparative Advantage to Design Costs and Locations,December 2012,Takahiro Fujimoto,,,Male,Unknown,Unknown,Male,,12
9.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2012005,The Predictable Outcome of Speculative House Price Peaks,December 2012,Peter Richmond,Bertrand Roehner,,Male,Male,Unknown,Male,,3
9.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2012006,An Analytical Framework for the Relationship between Environmental Measures and Economic Growth Based on the Régulation Theory: Key Concepts and a Simple Model,December 2012,Kazuhiro Okuma,,,Male,Unknown,Unknown,Male,,13
9.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2012007,The Effects of Working Hours Schemes on Overtime Working Hours in Japan,December 2012,Jun Fukuda,,,,Unknown,Unknown,Mix,,
9.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.N2012008,Investigating Entrepreneurial Spirit with the Rule Approach: Why Self-employment is on the Decline in Japan,December 2012,Georg D. Blind,,,Male,Unknown,Unknown,Male,,11
10.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.O2013001,Kondratiev Gold Medal to Professor Masaaki Hirooka,June 2013,Koichi Shimizu,,,Male,Unknown,Unknown,Male,,1
10.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.D2013002,"Economics 2.0: The Natural Step towards a Self-Regulating, Participatory Market Society",June 2013,Dirk Helbing,,,Male,Unknown,Unknown,Male,,38
10.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.D2013003,A Study on “Economic Network and Stability” for Sustainable Socio-Economic Ecosystem,June 2013,Duk Hee Lee,Joohyun Kim,,,Unknown,Unknown,Mix,,
10.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013004,Learning Curve for Collective Behavior of Zero-Intelligence Agents in Successive Job-Hunting Processes with a Diversity of Jaynes-Shannon’s MaxEnt Principle,June 2013,He Chen,Jun-ichi Inoue,,,Unknown,Unknown,Mix,,
10.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013005,Market-Wide Price Co-movement around Crashes in the Tokyo Stock Exchange,June 2013,Jun-ichi Maskawa,Joshin Murai,Koji Kuroda,Unknown,Unknown,Male,Male,,4
10.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013006,A Stochastic Model for Order Book Dynamics in Online Product Markets,June 2013,Takayuki Mizuno,Makoto Nirei,Tsutomu Watanabe,Male,,Male,Mix,,
10.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013007,Is Mainstream Economics a Slave Morality?: An Analysis of the Longstanding Anomalies of Mainstream Economics within the Evolutionary Framework of Multiple Moralities,June 2013,Zhixiong Jiang,Lili Bao,Hanqing Liu,Unknown,Female,Unknown,Female,,
10.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013008,Goodwin’s Growth Cycle Model with the Bhaduri-Marglin Accumulation Function,June 2013,Theodore Mariolis,,,Male,Unknown,Unknown,Male,,8
10.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013009,"Exploring Schumpeterian Dynamics: Innovation, Adaptation and Growth",December 2013,Ian Steedman,Stan Metcalfe,,Male,Male,Unknown,Male,,4
10.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013010,Keynes After the Economics of Conventions,December 2013,Olivier Favereau,,,Male,Unknown,Unknown,Male,,12
10.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013011,Energy and War in the 21st Century,December 2013,Anastasia Lekka,Nicholas Kyriazes,,Female,Male,Unknown,Mix,,
10.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013012,Can Hybrid Organizations—Based on the Combination of Long-term Employment and Performance-related Pay—Operate Effectively in Japan?,December 2013,Mitsuharu Miyamoto,Hiroatsu Nohara,,Male,Unknown,Unknown,Male,,5
10.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013013,The Theory of Sovereignty in the Institutional Economics of John R. Commons from the Perspective of Constituent Power,December 2013,Kota Kitagawa,,,Male,Unknown,Unknown,Male,,1
10.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013014,A Theory of Profit and Competition,December 2013,Alberto Battistini,,,Male,Unknown,Unknown,Male,,1
10.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013015,Aspiration-Based Learning in a Cournot Duopoly Model,December 2013,Manahan Siallagan,Hiroshi Deguchi,Manabu Ichikawa,Unknown,Male,Male,Male,,2
10.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.A2013016,Direct Evidence for Synchronization in Japanese Business Cycles,December 2013,Yuichi Ikeda,Hideaki Aoyama,Hiroshi Yoshikawa,Male,Male,Male,Male,,9
11.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110101,Cyclical Patterns in Japanese Manufacturing Firms: Analyzing Three Post- Keynesian Models,June 2014,Taro Abe,,,Male,Unknown,Unknown,Male,,
11.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110102,An Institutional Economic Approach to the Low-Cost Production System: The Wage-Labour Nexus in Hyundai Motor Group,June 2014,WooJin Kim,,,Unknown,Unknown,Unknown,Unknown,,
11.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110103,Rent Seeking in Successive Monopoly: The Case of Casino Gambling in New Orleans,June 2014,Franklin G. Mixon Jr.,Rand W. Ressler,,Male,Female,Unknown,Mix,,
11.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110104,Multilayer Rules and Governance in Fiji Coastal Communities: A Case Study of Veivatuloa Village,June 2014,Jokim Kitolelei,Satoru Nishimura,Toru Kobari,Unknown,Male,Male,Male,,
11.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110105,The Role of Human Beings in Social Evolution: Interactor or Agent,June 2014,Yoshikazu Tomizuka,,,Male,Unknown,Unknown,Male,,
11.0,1.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110106,Diversity and Transformations of Asian Capitalisms,June 2014,Norio Tokomaro,,,Male,Unknown,Unknown,Male,,
11.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110201,Our Move to a New Springer Series from the Present Form of EIER: A Retrospective on the Past Decade,December 2014,Yuji Aruka,,,Male,Unknown,Unknown,Male,,1
11.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110203,Introduction to Corporate Leaders Analytics and Network System (CLANS) in China and Its Data Management Mechanism,December 2014,Yuanyuan Man,Shuai Wang,Irwin King,Unknown,Unknown,Male,Male,,
11.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110202,Proactive News Article Summarization Service Using Personal Intention Models,December 2014,In Seok Oh,Ji Eun Lee,Kyung Joong Kim,,,,Mix,,
11.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110204,The Report to the Union of National Economic Association in Japan on JAFEE and EIER (2004),December 2014,Yuji Aruka,,,Male,Unknown,Unknown,Male,,
11.0,2.0,Evolutionary and Institutional Economics Review,09 April 2015,https://link.springer.com/article/10.14441/eier.110205,The Report to the Union of National Economic Association in Japan on JAFEE and EIER (2010),December 2014,Yuji Aruka,,,Male,Unknown,Unknown,Male,,
12.0,1.0,Evolutionary and Institutional Economics Review,05 June 2015,https://link.springer.com/article/10.1007/s40844-015-0010-5,A new stage of the Evolutionary and Institutional Economics Review,June 2015,Kiichiro Yagi,Yuji Aruka,Takahiro Fujimoto,Unknown,Male,Male,Male,,1
12.0,1.0,Evolutionary and Institutional Economics Review,14 May 2015,https://link.springer.com/article/10.1007/s40844-015-0003-4,The rise of pure economics under a new form of scholasticism in view of the present socio-economic system,June 2015,Yuji Aruka,,,Male,Unknown,Unknown,Male,"Economic scientists often work apart from pure economists. They usually come from physics, informatics, and systems science backgrounds. Why have economists convinced themselves that the economic system could be an independent system purely separate from other social systems and/or institutions? In contrast, it holds that economic scientists prefer to address the socio-economic system rather than simply the economic system when they discuss economic phenomena. In the same way that we cannot live without minding our health, we must also be aware of our economic activities. To this end, several analytical tools with multi-potency that are applicable to various wider fields are thriving. It may be easier to adopt some of these tools for economic analysis than other. It is not unusual to encounter scientists who are interested in and seeking to cultivate new economic analyses. Here, pure economists should recognize that their prescriptions naturally cannot give other scientists or the public reason to believe their opinions because pure economics may never guarantee a practical result. Currently, the use of algorithms dominates the financial market and other socio-economic systems. How should we re-examine the economics of human nature without referring to artificial intelligence? In the era of algorithm dominance by artificial intelligence, it must be the case that our society no longer holds under the principle of human nature only. We are nevertheless exposed to the so-called worldwide standardization of economic disciplines. In general, however, any compulsion to follow doctrine would likely be futile in either religion or science.",1
12.0,1.0,Evolutionary and Institutional Economics Review,03 June 2015,https://link.springer.com/article/10.1007/s40844-015-0005-2,Globalization: evolution of capitalist market economy through “Internalization of the Market”,June 2015,Makoto Nishibe,,,,Unknown,Unknown,Mix,,
12.0,1.0,Evolutionary and Institutional Economics Review,09 May 2015,https://link.springer.com/article/10.1007/s40844-015-0002-5,Large-scale empirical study on pairs trading for all possible pairs of stocks listed in the first section of the Tokyo Stock Exchange,June 2015,Mitsuaki Murota,Jun-ichi Inoue,,Male,Unknown,Unknown,Male,"Cross-correlations often provide us very useful information about financial markets to figure out various non-trivial and complicated structures behind the stocks as multivariate time series (Bouchaud and Potters 2009). Actually, the use of the cross-correlation can visualize collective behavior of stocks during the crisis. As such examples, we visualized the collective movement of the stocks by means of the so-called multidimensional scaling (MDS) during the earthquake in Japan on March 2011 (Ibuki et al. 2012a, b, 2013). We have also constructed a prediction procedure for several stocks simultaneously by means of multi-layer Ising model having mutual correlations through the mean fields in each layer (Ibuki et al. 2012a, b, 2013; Murota and Inoue 2013). Usually, we need information about the trend of each stock to predict the price for, you might say, ‘single trading’ (Murota and Inoue 2013; Kaizoji 2000; Bouchaud 2012). However, it sometimes requires us a lot of unlearnable ‘craftsperson’s techniques’ to make a profit. Hence, it is reasonable for us to use the procedure without any trend-forecasting-type way to manage the asset with a small risk. From the view point of time series prediction, Elliot et al. (2005) made a model for the spread and tried to estimate the state variables (spread) as hidden variables from observations by means of Kalman filter. They also estimated the hyper-parameters appearing in the model using EM algorithm (Expectation and Maximization algorithm) which has been used in the field of computer science. As an example of constructing optimal pairs, (Mudchanatongsuk 2008) regarded pair prices as Ornstein–Uhlenbeck process, and they proposed a portfolio optimization for the pair by means of stochastic control. For the managing of assets, the so-called pairs trading (Vidyamurthy 2004; Whistler 2004; Gatev et al. 2006) has attracted trader’s attention. The pairs trading is based on the assumption that the spread between highly correlated two stocks might shrink eventually even if the two prices of the stocks temporally exhibit ‘mis-pricing’ leading up to a large spread. It has been believed that the pairs trading is almost ‘risk-free’ procedure; however, there are only a few extensive studies (Perlin 2009; Do and Faff 2010) so far to examine the conjecture in terms of big-data scientific approach. Of course, several purely theoretical approaches based on probabilistic theory have been reported. For instance, the so-called arbitrage pricing theory (APT) Gatev et al. (2006) in the research field of econometrics has suggested that the pairs trading works effectively if the linear combination of two stocks, each of which is non-stationary time series, becomes stationary. Namely, the pair of two stocks showing the properties of the so-called co-integration (Engle and Granger 1987; Stock and Watson 1988) might be a suitable pair. However, it might cost us a large computational time to check the stationarity of the co-integration for all possible pairs in a market, whereas it might be quite relevant issue to clarify whether the pairs trading is actually safer than the conventional ‘single trading’ [see for instance (Murota and Inoue 2013)] to manage the asset, or to what extent the return from the pairs trading would be expected etc. With these central issues in mind, here we construct a platform to carry out and to investigate the pairs trading which has been recognized an effective procedure for some kind of ‘risk-hedge’ in asset management. We propose an effective algorithm (procedure) to check the amount of profit from the pair trading easily and automatically. We apply our algorithm to daily data of stocks in the first section of the Tokyo Stock Exchange, which is now available at the Yahoo! finance web site http://finance.yahoo.co.jp. In the algorithm, three distinct conditions, namely, starting (\(\theta \)), profit-taking (\(\varepsilon \)) and stop-loss (\(\Omega \)) conditions of transaction are automatically built into the system by evaluating the spread (gap) between the prices of two stocks for a given pair. Namely, we shall introduce three essential conditions to inform us when we should start the trading, when the spread between the stock prices satisfies the profit-taking conditions, etc. by making use of a very simple way. Numerical evaluations of the algorithm for the empirical data set are carried out for all possible pairs by changing the starting, profit-taking and stop-loss conditions to look for the best possible combination of the conditions. This paper is organized as follows. In the next Sect. 2, we introduce several descriptions for the mathematical modeling of pairs trading and set-up for the empirical data analysis by defining various variables and quantities. Here we also mention that the pairs trading is described by a first-passage process (Redner 2001), and explain the difference between our study and arbitrage pricing theory (APT) (Gatev et al. 2006) which has highly developed in the research field of econometrics. In Sect. 3, we introduce several rules of the game for the trading. We define two relevant measurements to quantify the usefulness of pairs trading, namely, winning probability and profit rate. The concrete algorithm to carry out pairs trading automatically is also given in this section explicitly. The results of empirical data analysis are reported and argued in Sect. 4. The last section is devoted to summary.",1
12.0,1.0,Evolutionary and Institutional Economics Review,25 April 2015,https://link.springer.com/article/10.1007/s40844-015-0001-6,On the problem of scale: a general theory of morphogenesis and normative policy signals for economic evolution,June 2015,Benjamen F. Gussen,,,Unknown,Unknown,Unknown,Unknown,,
12.0,1.0,Evolutionary and Institutional Economics Review,12 May 2015,https://link.springer.com/article/10.1007/s40844-015-0004-3,Goodwin’s growth cycle model with the Bhaduri–Marglin accumulation function: a note on the C.E.S. case,June 2015,Nikolaos Rodousakis,,,Male,Unknown,Unknown,Male,"In a recent paper in this Journal, Mariolis (2013) has shown that the incorporation of the Bhaduri–Marglin accumulation function in Goodwin’s (1967) growth cycle model amounts to the introduction of a sign-variable friction coefficient into the equations of motion and, hence, can considerably modify the system’s behaviour.Footnote 1 More specifically, the local dynamic properties of that extended system depend crucially on the elasticity of the degree of capacity utilization—share of profits in total income (or IS)-curve, which in its turn depends on the form of the accumulation function. As is well known, van der Ploeg (1985) has shown, using a constant elasticity of substitution (C.E.S.) production function that the substitution possibilities between the ‘factors’ of production affect crucially the dynamics of Goodwin’s original model: it gives rise to trajectories that converge to equilibrium. In fact, “firms substitute [labour] by firing workers and installing some extra capital over the course of the conflict cycle. It is this additional factor of ‘competition within the species’ that destroys the conservative nature of the Goodwin system.” (van der Ploeg, 1985, pp. 228–229).Footnote 2
 The purpose of this note is to incorporate a C.E.S. production function in the above extended Goodwin’s model and to explore the stability properties of it, i.e. if the substitution possibilities lead to different dynamic behaviours to that obtained by Mariolis (2013). The remainder of the paper is organized as follows. Section 2 presents the analytic framework. Section 3 provides the results of the investigation. Section 4 concludes.",4
12.0,1.0,Evolutionary and Institutional Economics Review,03 June 2015,https://link.springer.com/article/10.1007/s40844-015-0009-y,Preface for the special feature,June 2015,Alan Kirman,,,Male,Unknown,Unknown,Male,,
12.0,1.0,Evolutionary and Institutional Economics Review,13 June 2015,https://link.springer.com/article/10.1007/s40844-015-0011-4,Preface for the special feature,June 2015,Stefano Marmi,,,Male,Unknown,Unknown,Male,,
12.0,1.0,Evolutionary and Institutional Economics Review,23 June 2015,https://link.springer.com/article/10.1007/s40844-015-0015-0,Note for the special issue,June 2015,Eva-Maria Feichtner,Simona Settepanella,,Unknown,Female,Unknown,Female,,
12.0,1.0,Evolutionary and Institutional Economics Review,09 June 2015,https://link.springer.com/article/10.1007/s40844-015-0008-z,Social science puzzles: a systems analysis challenge,June 2015,Donald G. Saari,,,Male,Unknown,Unknown,Male,"All of those mysteries and intricacies about systems in the social sciences that have been uncovered with mathematics are a delight. These puzzles range from the complexities of trying to relate microeconomic conclusions to macroeconomic challenges, the Sonnenschein (1972), Mantel (1974), Debreu (1974) result, which, by identifying all possible aggregate excess demand functions [and my extension Saari (1992) finding all possible collections of such functions over all subsets of two or more commodities] essentially asserts that there exist many settings for which Adam Smith’s “Invisible Hand” story, or even certain revealed preference assertions, cannot hold, to all of those impossibility assertions serving as roadblocks for progress in social choice theory. A first objective is to identify what causes these obstacles. Surprisingly, as indicated here [and in my companion article Saari (2015)], these impediments to progress share a common systems analysis source. Identifying why problems arise provides insight about how to convert negative assertions into positive conclusions. Sen (1970) “Minimal Liberalism” result, for instance, commonly is interpreted as reflecting a conflict between individuals who are just making innocuous personal decisions and the same group’s unanimous conclusions. But by identifying what actually causes this conflict, a radically different interpretation of Sen’s result emerges from which resolutions follow. It is interesting how these resolutions resemble commonly adopted approaches that are used in practice to combat “Tragedy of the Commons” difficulties. A systems analysis challenge for the social sciences is to discover positive answers for wider classes of concerns. A related challenge is to identify an appropriate choice from among many options. For instance, all sorts of voting methods can be used to identify the voters’ aggregate decision. But, which rule can be trusted to give appropriate outcomes? To illustrate with a simple example, suppose a group of 17 is to elect one of \(A, B\), or \(C\). The preferences are The problem is not to find a voting method; there are many of them. For instance 
\(A\) wins with the commonly used plurality vote (“vote-for-one”), with the \(A\succ C\succ B\) outcome and 7:6:4 tally. 
\(B\) wins with “vote-for-two” method with the opposite election ranking of \(B\succ C\succ A\) and tally 13:12:9. 
\(C\) wins with the Borda Count, where ballots are tallied by assigning 2 and 1 points, respectively, to a ballot’s top and second positioned candidates. The election ranking is \(C\succ B\succ A\) with tally 18:17:16. A concern is that, rather than reflecting the voters’ views, each candidate “wins” with an appropriately selected election rule.Footnote 1 The choice of the “winner” can have consequences, which underscores the importance of identifying an appropriate voting rule. Incidentally, pairwise majority votes fail to help with this example because they generate the inconclusive \(A\succ C, \, C\succ B, \, B\succ A\) cycle. An election outcome, then, can more accurately reflect properties of the voting method rather than the views of the voters. But voting rules are aggregation methods, which raises a general concern: social science conclusions tend to be involve aggregation methods, so do the outcomes faithfully reflect the data, or the peculiarities of the particular rule?",3
12.0,1.0,Evolutionary and Institutional Economics Review,13 May 2015,https://link.springer.com/article/10.1007/s40844-015-0006-1,Decidability in complex social choices,June 2015,Gennaro Amendola,Luigi Marengo,Akimichi Takemura,Male,Male,Unknown,Male,"Social choice theory usually assumes that agents are confronted with a set of exogenously given and mutually exclusive alternatives. These alternatives are given in the sense that the pre-choice process through which they are constructed is not analyzed. Moreover, these alternatives are “simple”, in the sense that they are one-dimensional objects or, even when they are multidimensional, they are simply points in some portion of the homogeneous \({\mathbb {R}}^n\) space and they lack any internal structure that limits the set of possible alternatives. Many choices in real-life situations depart substantially from this simple setting. Choices are often made among bundles of interdependent elements. Those bundles may be formed in a variety of ways, which in turn affect the selection process of a social outcome. Examples could be a group of friends deciding how to spend the evening: alternatives like go to a movie, concert, restaurant, etc. are labels for bundles of elements (e.g., with whom, where, when, movie genre, director, type of food, etc.) and everyone’s preference is unlikely to be expressed before these labels are specified in their constituting elements. Moving on, to more serious examples, candidates and parties in political elections stand for complex bundles of interdependent policies and personality traits. Committees and boards are called to decide upon packages of policies, e.g., a recruitment package that a university governing board has to approve. In principle, any combination of elements (subject to a budget or some other feasibility constraint) could be considered and compared (e.g., through majority voting) with any other, but in reality only a relatively small number of packages undergo examination. Typically, the bundling of elements serves the purpose of reducing the number of alternatives to be examined, by decomposing the whole space of alternatives into smaller subspaces. In Marengo and Pasquali (2011), Marengo and Settepanella (2012), Amendola and Settepanella (2012), some of us presented a model of social choice among bundles of elements, called objects. This model turns out to be a generalization of the one introduced and developed in Lang (2007), Conitzer et al. (2009, 2011). In this paper, we provide a more general set of results and, among others, the proof of a result conjectured in Conitzer et al. (2011). We exploit a probabilistic approach and develop some numerical calculations that allow us to show that the introduction of objects strictly increases the probability to get a social optimum and that an authority who has the power to construct objects (and choose how elements are described) may obtain a desired outcome even when the latter is freely chosen in a democratic process. The paper is organized as follows. In Sect. 2, we briefly discuss the similarities and differences between our approach and those already existing in the literature. In Sect. 3, we provide a simplified algebraic and geometric version of the the mathematical model introduced in Marengo and Settepanella (2012), Amendola and Settepanella (2012) that we also illustrate by means of a series of examples. Sections 4 and 5 present our novel results obtained, respectively, with numerical and probabilistic approaches. Notably, Sect. 4 introduces a series of numerical simulations that show how decidability is greatly enhanced in our model, as new kind of social optima (that we call local and u-local) tend to appear also in case in which no Condorcet winner exists in the classical model. Section 5, presents a further elaboration by means of probabilistic tools. Here we prove that the probability to obtain at least a local optimum when each element (called feature) has two possible outcomes (the yes/no or \(0/1\) case) is always greater than 60 %, i.e., the decidability in this case is always very high (that is the Conitzer et al. 2011 conjecture). From the methodological point of view, we believe that an important contribution of our paper is to show how algebraic (graphs), geometric (hyperplanes arrangements), numerical (combinatorial) and probabilistic approaches can converge in a general framework. Finally, in Sect. 6, we draw some conclusions.",2
12.0,1.0,Evolutionary and Institutional Economics Review,17 June 2015,https://link.springer.com/article/10.1007/s40844-015-0013-2,An invitation to tropical geometry,June 2015,Eva Maria Feichtner,,,Female,Unknown,Unknown,Female,"Though tropical mathematics appeared in various guises over about half a century—in connection with automata theory in the 70s (Simon 1978) and under the term idempotent analysis or Maslov dequantization in the 80s (Kolokoltsov and Maslov 1997)—it gained renewed and vivid interest in the beginning of the last decade (Sturmfels 2002; Richter-Gebert et al. 2005). Over the past 15 years and nourished by the variety of possible approaches to the matter, tropical geometry has developed into a subject on its own right. Applications to as varied areas as enumerative algebraic geometry, evolutionary biology, and notably economics, contribute to its fame. Approaches are many, but even the general viewpoint on the subject and its main directions is at least twofold: One way to think about tropical geometry is to view it as an algebraic geometry over the real numbers extended by infinity with tropical arithmetic, i.e., the operation of taking the minimum replaces addition, and addition replaces multiplication. The main goal is to build a general theory inspired by classical algebraic geometry, i.e., to develop meaningful concepts of lines, hypersurfaces, varieties, intersection theory, (co)homology, and the like, in tropical terms. Another way to think about tropical geometry is to view tropicalization as a mechanism to convert complex algebraic varieties into polyhedral complexes and thereby retaining as much of the algebro-geometric information of the original variety as possible. This amounts to studying a “polyhedral shadow” instead of the original variety, a simplification that potentially is crucial for solving problems in classical algebraic geometry. Instances like Mikhalkin’s Correspondence Theorem (Mikhalkin 2005) have fueled a rapid development of tropical techniques. We here give a short introduction to the notions and techniques of tropical geometry for the mathematically inclined reader.Footnote 1 A very informative text with a similar flavor is Speyer and Sturmfels (2009). Expert readers will find comprehensive sources, e.g., the recently published textbook by Maclagan and Sturmfels (2015).",
12.0,1.0,Evolutionary and Institutional Economics Review,26 June 2015,https://link.springer.com/article/10.1007/s40844-015-0012-3,International trade theory and exotic algebras,June 2015,Yoshinori Shiozawa,,,Male,Unknown,Unknown,Male,"International trade has fascinated economists because of its pure theory and its policy implications. Approximately 200 years ago, David Ricardo first presented his famous numerical example (Ricardo 1817, Chapter 7). Since then, economic theories have developed tremendously, and yet, the logical structure of international trade theory has not been well explored. Thus, it was surprising to find that an exotic algebra lies behind the Ricardian trade theory. Many traditional problems have been translated into the language of an exotic algebra, including the determination of specialization patterns, price determination, combinatorial questions, and graph theory. Exotic algebras, which include tropical algebra, is a relatively new topic, even in mathematics. Major topics in tropical algebra, and its associated geometry, have resulted from research in this area during the past two decades. Tropical algebra has become one of the most productive fields of applied mathematics (e.g., timed event graphs), and provides indispensable tools for pure mathematical studies, such as algebraic geometry (Sturmfels 1994). Because they have a rich body of literature, trade economists have many ready-made theories that provide us with concrete information on combinatorics and the geometry of production possibility sets. Here, topics cover theories such as tropical-oriented matroids (Ardila and Develin 2009), the number of vertices and facets, the Minkowski sum, zonotopes (Huber et al. 2000), and cephoids (Pallaschke and Rosenmüller 2006). These studies provide knowledge on the geometry and combinatorics of the production possibility set of a Ricardian trade economy. Such knowledge was unattainable prior to these theories. Thus, they have resulted in significant gains in terms of Ricardian trade theory. On the other hand, Ricardian trade theory provides mathematicians with interesting mathematical entities that connect various tricks in tropical geometry, such as the tropical matroid, Minkowski sum, permanents, and transportation polytopes (De Loera et al. 2009). For example, the Cayley trick, hitherto studied from a topological point of view, now has a new, quantitative interpretation. In this sense, tropical algebra and classic polytope theory are new mathematical objects with a rich structure that need to be studied. Note this paper serves more as an introductory text on trade theory and exotic algebra. A book is currently being written that provides a thorough discussion, including details of all formulations and proofs (Shiozawa to appear). To this end, I focus on three-country, three-commodity cases, or smaller models, although the theory is applicable to any number of countries and commodities. Sections 2 and 3 introduce exotic algebras and Ricardian trade theory, respectively, two fields that have thus far been treated separately. Section 4 offers a graphical representation of a Ricardian economy. Section 5 describes the conjugate relation between wage rate and prices as a first application of subtropical matrix operations. Section 6 prepares the mathematical part of our examination. Note that Sects. 4, 5 and 6 are only concerned with wage rates and prices. Then, Sect. 7 introduces production. The world production possibility set is defined as a Minkowski sum of each country’s production possibility sets. Section 8 gives an almost complete description of possible types of specialization, and relates these to the faces of the maximal frontier of the world production set. Section 9 describes a little-known result on a modified McKenzie diagram. The new diagram and its construction illustrate the intrinsic connections between various entities in tropical geometry. Lastly, Sect. 10 discusses Jones’ classic results and compares them with the new formulations. Richter-Gebert et al. (2003) and Joswig (2014) give introductory knowledge on tropical geometry, whereas Ziegler (1995) gives basic knowledge on convex polytopes.",16
12.0,1.0,Evolutionary and Institutional Economics Review,05 June 2015,https://link.springer.com/article/10.1007/s40844-015-0007-0,The PerronRank family: a brief review,June 2015,Ngoc Mai Tran,,,,Unknown,Unknown,Mix,,
12.0,1.0,Evolutionary and Institutional Economics Review,27 June 2015,https://link.springer.com/article/10.1007/s40844-015-0014-1,A discrete geometric approach to heterogeneity and production theory,June 2015,Simona Settepanella,Giovanni Dosi,Federico Ponchio,Female,Male,Male,Mix,,
12.0,2.0,Evolutionary and Institutional Economics Review,13 January 2016,https://link.springer.com/article/10.1007/s40844-015-0021-2,"A eulogy on the late Prof. Yuichi Shionoya, 1932–2015",December 2015,,,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Evolutionary and Institutional Economics Review,21 January 2016,https://link.springer.com/article/10.1007/s40844-015-0027-9,A tribute to professor Masahiko Aoki,December 2015,,,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Evolutionary and Institutional Economics Review,19 January 2016,https://link.springer.com/article/10.1007/s40844-015-0026-x,In memoriam: Masahiko Aoki (1938–2015),December 2015,,,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Evolutionary and Institutional Economics Review,07 July 2015,https://link.springer.com/article/10.1007/s40844-015-0016-z,Relationship between socioeconomic flows and social stocks: case study on Japanese air transportation,December 2015,Aki-Hiro Sato,Hidefumi Sawai,,Unknown,Male,Unknown,Male,"A complex network of socioeconomic systems has been actively studied in interdisciplinary research fields. Data-driven investigation of socioeconomic systems is intensely studied due to development of data environment. 
Guimerà and Amaral (2004) have shown small-world properties of the air transportation network. The cumulative degree distribution in world-wide airport network shows truncated power law with the exponent 1.0. Woolley et al. (2011) investigated world-wide air transportation and global cargo-ship movements. Brockmann and Helbing (2013) investigated contagion phenomena on the world-wide air transportation network. People flows spread culture and habits as well as epidemics, and both social stocks and socioeconomic flows have been constructed through interactive co-evolution. One of current issues in Japan is to develop rural areas other than a few urban areas where people and firms concentrate. A few rich places gain more social resources and others are drained of them. Why does such an inequality happen? This is a fundamental question of our study. We hypothesise that there are several factors to create motivation potentials of migration. In order to understand such mechanisms and obtain fundamental principles to design our socioeconomic systems, it is worth grasping the past and present structure and mobility of transportation. Why do people and freight travel from one place to another? The migration processes have been intensively studied in the context of socioeconomic dynamics. Haag and Weidlich (1984) proposed a master equation description with transition probabilities depending on both regional-dependent and time-dependent utility and mobility in order to describe collective tendency of agent decision in migration opportunity. In the context of tourism management, there is a model that views motivation of the two dimensions of “push” and “pull” factors, and this model has been generally accepted (Cha et al. 1995). We understand that socioeconomic flows (passenger, freight, monetary and information flows) and social stocks (properties, such as buildings, cars and houses, infrastructure, such as roads, railways and pipelines, human capital, such as population and workers, and organisations, such as firms, schools, hospitals and so on) have a positive correlation. In this study, we attempt to characterise the relationship between socioeconomic flows and social stocks from an empirical point of view based on governmental statistics data. According to Zhang (2010) and Bettencourt (2007), there are allometric relationships between extensive macroscopic variables X and population M in cities or countries. This is described as a power-law relationship: where \(\alpha\) is a power-law exponent. If \(\alpha > 1\), the property of X has a super-linear relation with respect to M. \(\alpha < 1\) means that X has a sub-linear relation with respect to M. \(\alpha \approx 1\) shows a linear relation between X and M. Therefore, many extensive macroscopic variables to characterise cities or areas can be described as a power-law function of M. Therefore, we adopt population of people, workers and firms as macroscopic variables that characterise a value of an airport. If we use correlated variables as explanatory variables for multiple regression analysis, we face the multicollinearity problem. Moreover, to model the number of passengers and the weight of freight, several microscopic models are proposed (Isard 1975). Traffic around airports, roads and the capacity of hotels can be used as parameters. However, it is not easy to collect all the data needed to grasp these parameters of microscopic models and estimate all the parameters in a rigorous manner. To avoid these two issues, we select a simple model with population and distance as explanatory variables. Therefore, the relationship between population and air traffic flows on a Japanese airline network is investigated under an assumption of a gravity model. The gravity model is studied in the context of transportation (Zipf 1946; Isard 1975; Eriander 1990; Mikkonen 1999; Rodrigue 2009; Jung et al. 2009; Sivrikaya and Tunç 2013). Originally, Zipf assumed a gravity model to explain movement. Currently, this is used to infer population movement (Barthélemy 2010), cargo shipping volume (Expert et al. 2011) and bilateral trade flows between nations (Pöyhönen 1963). The simple gravity model assumes that a socioeconomic flow between two places is a function of the multiplication of populations of these places (Zipf 1946). The flow is also inversely proportional to a function in terms of geodesic distance between two places. More generally, it is suggested that the traffic flow \(F_{ij}\) between airports i and j is proportional to \(x_i^{\beta _1} x_j^{\beta _2}/f(D_{ij}),\) where \(x_i\) represents the population of city i and the geodesic distance \(D_{ij}\) between two airports, i and j. \(\beta _1\) and \(\beta _2\) are adjustable exponents and the deterrence function \(f(D_{ij})\) is chosen to fit the empirical data. Is the gravity model assumption empirically validated in the Japanese domestic air transportation? This is a main question of this study. In addition, how do we determine an adequate area to compute social stocks related to airports. This is the second question of this study. To identify an effective area where people who use the airport act, we introduce a circle extending from the centre of an airport and compute the population of people, workers and firms within a certain radius. To answer these questions, we need a holistic approach to socioeconomic flows and social stocks based on a sufficient amount of data. In this study, we use data from a 1-km grid square statistics from the population census in 2010, economic census (the number of workers and firms) in 2011 and Japanese air transportation statistics in order to estimate the relationship between socioeconomic flows (people and freight flows) and social stocks. The people flows may be determined by the economic behaviour and economic behaviour may also be determined by the people flows. How much people do travel through a Japanese domestic air transportation system? Specifically, if we want to launch new connections between two airports of which do not have any flights, it is worth grasping potential mobility and inferring the potential number of passengers in advance. To do so, we need to construct the following steps: Establish an adequate model to describe a system. Estimate parameters of the model. Use the model with parameter estimates in order to infer an unknown portion of the system. It is expected that the people flow through a Japanese domestic air transport system is dominated by Japanese population and economic activities. In this paper, we attempt to compute parameters of the gravity model based on high-resolution population census data in order to infer the potential number of passengers between airports of which have no flight and predict the potential number of passengers for airports having no connections. This is our strategy adopted in this study.",4
12.0,2.0,Evolutionary and Institutional Economics Review,22 August 2015,https://link.springer.com/article/10.1007/s40844-015-0018-x,Schumpeter as a diffusionist: a new interpretation of Schumpeter’s theory of socio-cultural evolution,December 2015,Daisuke Kobayashi,,,Male,Unknown,Unknown,Male,"Many scholars have linked the theory of socio-cultural evolution, propounded by Joseph Alois Schumpeter, to several strands of thought. These include Karl Marx’s theory of class struggle, the well-known Methodenstreit (the debate on methods) in economics that took place between the German Historical School and the Austrian school, the German sociological context, American economic theorists of the time, elite theory, and business cycle theory (Allen 1994, Andersen 2011a, b, März 1991 and Shionoya 1997). While it is reasonable to suggest that Schumpeter’s theory of evolution reflects all of these strands, another essential influence on his work, namely anthropology,Footnote 1 has been overlooked. This paper offers a complementary interpretation that aims to support and deepen understanding of the development of Schumpeter’s evolutionary theory. Throughout the nineteenth century, anthropology played an important role in both complementing and endorsing theories within jurisprudence, arguably the first social science, that were derived from Enlightenment philosophy and social contract theory based on “natural law”. Early disputes within anthropology concerned “the family versus the social contract” (Barnard 2000, 30), with the outcomes of these disputes culminating in kinship theory, one of the key areas of study in the field. Thereafter, anthropology gradually extended its focus to problems of private property, marriage, religion, and totemism. Anthropological findings also came to be used to validate those of other social sciences such as sociology and economics up to the late nineteenth century.Footnote 2 At the time, scholars within various fields of social science frequently used the comparative method to compare people of their civilization with those of primitive cultures to estimate the sequential order of stages in the developmental path toward civilization. Continuing the legacy of the Enlightenment, these scholars sought to identify a definite and continuous order of socio-cultural developmental stages, termed unilinear evolution, that could be applied to every culture and society throughout the world. These anthropologists were known as evolutionists. However, from the 1890s to the 1910s, many theories were formulated, mainly by German scholars, that aimed to disprove unilinear evolution. During the early 1900s, this movement against unilinear evolution spread among anthropologists within both the United Kingdom and the United States. This emerging new school of anthropology essentially supported the idea of diffusion and sought to explain similarities found between many cultures in geographically diverse areas as a result of transfer and immigration, that is, diffusion, and not of unilinear evolution. Consequently, these anthropologists were known as diffusionists. It is reasonable to speculate that Schumpeter, who obtained a doctorate in law from the University of Vienna, would have been familiar with the methods used in jurisprudence to empirically prove facts. Moreover, he must have been well acquainted with anthropological findings as jurisprudence, at the time, relied heavily on the results of anthropological research.Footnote 3 Moreover, the period when diffusionism came into vogue coincided with the period of Schumpeter’s study of jurisprudence and economics at the University of Vienna (1901–1906), and of economics and the social sciences, including anthropology, in the United Kingdom (from the winter of 1906–1907) (Yagi 1993, 73–74). It was also during this period that Schumpeter developed his vision of socio-cultural evolution articulated in his second book, Theorie der wirtschaftlichen Entwicklung (Schumpeter 1912). This paper suggests that Schumpeter first became aware of concepts within diffusionism sometime between 1901 and 1907 and subsequently borrowed ideas from this school in developing his own theory of socio-cultural evolution. A clarification of the relation between Schumpeter and the school of diffusionism should resolve many of the mysteries associated with his writings such as his separation of invention and innovation, his concept of discontinuity, and his initial hesitation to use the terms evolution.",2
12.0,2.0,Evolutionary and Institutional Economics Review,02 September 2015,https://link.springer.com/article/10.1007/s40844-015-0019-9,Justice-seeking in the perspective of rent-seeking,December 2015,Yasushi Suzuki,Md. Dulal Miah,,Male,Unknown,Unknown,Male,"Justice is considered the basic building block of a society. It helps maintain social relationship among different actors as well as establish peace and stability. Social institutions such as rules, regulations, customs, and traditions govern the distribution of rights and liberties of individuals. These institutions are formed focusing on the fundamental principles of justice with the motto that everyone in a society should be equally treated or should be fairly treated. Fairness is not a static state but rather a dynamic concept which can be interpreted and evaluated from several different dimensions. For instance, Rawls (1971) theory of justice is primarily concerned with distributive justice. Rawls proposes that ‘all social values—liberty and opportunity, income and wealth, and the bases of self-respect—are to be distributed equally unless an unequal distribution of any, or all, of these values is to everyone’s advantage’ (Rawls 1971:62). Rawls calls this principle ‘justice as fairness’. This theorization of distributive justice in the perspective of homo economicus is justified with reference to political values and should not be presented as part of moral, religious and philosophical doctrine (Rawls 1985). Unlike homo economicus, which deals with the end result of distributive justice, procedural fairness focuses on the use of a fair distributive procedure regardless of the outcome. Such homo socialis ideology views fairness as a group value model of procedural fairness in which people more often seek to satisfy relational motives such as need to feel valued, respected and included in social groups important to them (Lind and Tyler 1988). As such, the perception of fairness evolves through persistent influence of procedural treatment as a group. Skitka et al. (2008) however, offer another perspective of fairness stating that the principle of justice can simply be viewed as maintaining human innate morality. Or more precisely, human beings vow to maintain social cooperation if a society rewards those who behave morally and punishes those who do not. This homo moralis theory of justice emphasizes on the idea that human morality is preoccupied with the need to be cared instead of fulfilling material self-interest embedded with homo economicus or taking care of belongingness need peculiar to homo socialis. It follows that justice is fair if it appreciates only moral behavior of individuals in a society. The above illustration leads to an inference that justice is a very elusive concept and is difficult to define due to its subjective nature of interpretation. What is just or unjust depends on actors’ perceived view of fairness. Brighouse (2004:2) thus, rightly mentions that justice is systematically ambiguous, and it is clear only from the context. Everyone in a society expects to be equally or fairly treated within their self-defined contextual sense of fairness. In case of unfair treatment, the person deprived might feel indignation, grievance, anger etc. which would prompt him/her seeking fair treatment. In this sense, justice-seeking refers to the quest for receiving fair treatment. Fairness can be judged in any of the above discussed context. The consequences of justice-seeking however, depend on a host of critical factors including the nature and process of seeking justice. Collier and Hoeffier (1999) for instance, explain civil war from the perspective of justice seeking. They analyze data comprising of various civil wars and find that people engage in war because they ask for justice to alleviate grievances caused by unfair treatment. At the same time, it can also be the result of peoples’ desire for loot driven by greed (Collier and Hoeffier 1999). Their model provides a useful conceptual distinction between greed and grievance in understanding the motivation for civil war. However, the causality between their selected proxies as variables and the onset of civil wars in the model are quite vague. The arbitrariness of the variables is also criticized as ‘frightful inadequacy of most of the statistics’ (Nathan 2005). We, in this study however, do not intend to argue about the viability of their model but rather wish to expand rent-seeking model to justice-seeking in explaining the net effects of justice-seeking in a society. Rent-seeking results changes in institutions for re-distribution of rights towards rent-seekers. This requires costs in monetary or non-monetary forms whereas the resulting changes in institutions can be welfare enhancing or reducing for the society. As opposed to the general conception that rent-seeking always yields negative results, advanced rent-seeking model analyzes the net effect of rent-seeking taking the calculus of rent-seeking cost and benefit into account (Khan 2000). Similarly, justice-seeking involves with various costs (civil war for instance is costly) and the end result can be welfare enhancing (greater good through changing towards better social institutions) or welfare damaging (further chaos and social disorder). Therefore, justice-seeking can also be analyzed taking both justice-seeking cost and outcome into account. This is what the paper aspires to accomplish. In the broader perspective of rent-seeking model, an analytical framework for justice-seeking is developed based on cost-benefit analysis. Under this broader framework, some episodes are illustrated showing the nature of justice-seeking cost and the eventual result.",3
12.0,2.0,Evolutionary and Institutional Economics Review,20 August 2015,https://link.springer.com/article/10.1007/s40844-015-0017-y,The role of social capital for the governance of hybrid forms in agribusiness: an analysis of Brazilian beef alliances,December 2015,Luís Otávio Bau Macedo,,,Male,Unknown,Unknown,Male,"The coordination in agro-food chains based on complex organizational forms is a strategy to increase competitiveness in agribusiness systems. Agriculture markets face increasing requirements for players to follow production practices that deliver quality standards, healthy products, and social and environmental commitments. The supply of those attributes by agribusiness systems requires the formation of dense networks of commercial relations among input suppliers, agriculture producers, industry processors, wholesale and retail operators. The capability to coordinate transactions to provide the right incentives to deliver cooperation among players is therefore of key importance to agribusiness systems (Zylbersztajn and Farina 2010; Saes and Silveira 2014). The main explicative perspective for the governance of agribusiness systems is based on the study of economic transactions. That approach applies the ideas of TCE—Transaction Cost Economics to the analysis of governance structures in agribusiness systems. The main trait of that theoretical approach is that minimizing transactions costs entails the choice of the governance structures in a spectrum that varies from firm to hybrid and market forms. The main premises are that individuals operate with bounded rationality in markets in which uncertainty and opportunism are pervasive (Zylbersztajn and Farina 1999; Zybersztajn 1995). Nonetheless the importance and reliability of the TCE contributions, the diversity of governance structures perceived in real agribusiness organizations is greater than the one it predicts. The so-called “plural forms” designate diverse governance structures that deal with similar transaction costs. In recent years, there was a rush in research to explain organizational forms that do not fit into the theoretical framework of TCE (Ménard et al. 2014). A promising explicative agenda is related to social attributes that induce trust and cooperation. Collective action faces the challenge of building governance arrangements that entail coordination and mutual dependence among players without reducing their individual autonomy. That requires players to abide by cooperative behaviors based on economic incentives and institutional norms and sanctions that strengthen relational density. That challenge is not a trivial one and is solved in accordance with the idiosyncratic characteristics of each agribusiness system and so it requires specific empirical studies (Ménard 2013; Ménard et al. 2014; Miranda and Chaddad 2014). Regarding to relational density in the last decades there were increasing efforts to the understanding of the non-economic resources that induce agents to cooperate. That field of study is multidisciplinary and has a focus on the analysis of social capital embedded in relationships based on trust and cooperation (Coleman 1988; Putman 2000). In accordance with Sporleder and Wu (2006), the link between social capital and its prevalent role in agro-food chains is related to its capability to strengthen the generalized trust that emerges from knowledge about members’ cooperative patterns along time. Social capital plays a crucial role in facilitating generalized trust that turns relational contracts feasible to govern transactions. Facing the diversity of organizational forms perceived in agribusiness in nowadays, the fundamental idea developed in this paper is that the governance of hybrid forms is based not only on transaction costs as predicted by Williamson (1985, 1991), but also on the features of the relational density that foster coordination along the supply chain. The choice of the governance structure depends upon the features of social capital embedded among players that provide the relational density required to foster trust and cooperative behavior. When that is not the case it is necessary to deliver coordination by the use of controls and incentives to strengthen alignment with organizational objectives. Brazilian beef agribusiness system provides interesting clues to applying the social capital concept to the analysis of agri-food supply chains. Brazilian beef agribusiness system has an institutional environment in which transactions have low coordination. Transactions are mainly on cash for fed cattle, and there is a high degree of uncertainty about the financial fragility of packers. Also, there are recurrent complaints among cattle breeders about the lack of reliability of carcasses’ post-slaughter classifications. On the other hand, quality certification programs that aim to strengthen coordination have low adherence among breeders and packers (Caleman et al. 2003). To face that challenges, new commercialization schemes have arisen: (1) cattle associations to pool together input acquisition and to supply fed animals to slaughter; (2) alliances among breeders, packers and retailers to marketing brand quality cuts; (3) electronic auction arrangements to marketing animals; (4) exports of live cattle to slaughter abroad (Macedo and de Moraes 2009). Facing the diversity of organizational arrangements envisioned in agribusiness to deliver coordination, a blended analytical framework is proposed in the paper. The theoretical background is rooted in the concepts of transaction costs and social capital to explain the choice of the governance form (horizontal or vertical). The study provides an illustration based on four case studies of Brazilian beef alliances that operate in accordance to distinctive models of governance to assess their comparative performances.",4
12.0,2.0,Evolutionary and Institutional Economics Review,08 January 2016,https://link.springer.com/article/10.1007/s40844-015-0023-0,Preface to the special feature,December 2015,Akira Namatame,,,,Unknown,Unknown,Mix,,
12.0,2.0,Evolutionary and Institutional Economics Review,11 December 2015,https://link.springer.com/article/10.1007/s40844-015-0022-1,Towards capturing heterogeneity of supply network structures and their temporal transitions: an investigation of supply relationships in the Japanese automobile industry,December 2015,Tomomi Kito,Steve New,,,Male,Unknown,Mix,,
12.0,2.0,Evolutionary and Institutional Economics Review,08 January 2016,https://link.springer.com/article/10.1007/s40844-015-0025-y,Worldwide aviation network vulnerability analysis: a complex network approach,December 2015,Q. H. Anh Tran,Akira Namatame,,Unknown,,Unknown,Mix,,
12.0,2.0,Evolutionary and Institutional Economics Review,18 November 2015,https://link.springer.com/article/10.1007/s40844-015-0020-3,Effects of dark pools on financial markets’ efficiency and price discovery function: an investigation by multi-agent simulations,December 2015,Takanobu Mizuta,Shintaro Kosugi,Kiyoshi Izumi,Unknown,Male,Male,Male,"In financial stock markets, dark pools, in which order books or quotes are not provided, are becoming widely used, especially by institutional investors (SEC 2010). In dark pools, investors can trade a large block of stock without market impacts because investors need not show their orders to anyone else, and reducing market impacts by such investors may make markets more efficient (Johnson 2010). However, increasing the use of dark pools would raise regulatory concerns as it may ultimately affect the quality of the price discovery function in the lit markets, which are normal markets in which all order books are provided to investors. This may destabilize a market and heighten financial systemic risk (European Commission 2010; Ye 2012). Therefore, for example in Europe, regulators are discussing introducing a volume cap regulation for dark pools, specifically a 8 % limit for the trading volume for each stock (Bowley 2014). Dark pools are very difficult to discuss by only using results of empirical studies. Because so many factors cause price formation in actual markets, an empirical study cannot isolate the pure contribution of existing new types of markets such as dark pools to price formation. Furthermore, empirical studies cannot investigate situations that have never occurred before in real financial markets. We usually discuss whether new types of markets should be spread or not on the basis of their effects on price formation. An artificial market, which is a kind of a multi-agent simulation, can isolate the pure contribution of these new types of markets to the price formation and can treat situations that have never occurred (LeBaron 2006; Chen et al. 2012; Cristelli 2014). These are strong points of the artificial market simulation study. Many studies have investigated the effects of several new regulations and effects of new types of markets by using artificial market simulations, for example, investigating effects of short selling regulations (Yagi et al. 2010), transaction taxes (Westerhoff 2008), financial leverages (Thurner et al. 2012), circuit breakers (Kobayashi and Hashimoto 2011), price variations (Yeh and Yang 2010; Mizuta et al. 2015a), tick sizes (Mizuta et al. 2013), speedup of exchange computer system (Mizuta et al. 2015c), and dark pools (Mo and Yang 2013; Mizuta et al. 2015b). Indeed, Mo and Yang (2013) investigated dark pools by using artificial market simulations. However, they have not investigated situations that have never occurred before such as usage rates of dark pools that are much higher than those at present because they also use real historical stock prices. Mizuta et al. (2015b) investigated whether dark pools reduce market impacts or not by using artificial market simulations. However, they neither discuss market efficiency nor markets’ price discovery function, and their market selection model was not realistic. Therefore, in this study, we investigated effects of a dark pool on financial markets’ efficiency and price discovery function by using an artificial market model. This is a very important investigation into financial systemic risk because making a market inefficient and losing the price discovery function may make the market unstable and increase financial systemic risk. In this study, we additionally implemented smart order routing (SOR) to the model of Mizuta et al. (2015b) to treat actual market selection of investors. We discussed quantitatively how the spreading of dark pools beyond our experience could affect the price discovery function and aimed to clarify the mechanism of dark pools that makes a market efficient or inefficient.",9
12.0,2.0,Evolutionary and Institutional Economics Review,14 January 2016,https://link.springer.com/article/10.1007/s40844-015-0024-z,Shock transfer by arbitrage trading: analysis using multi-asset artificial market,December 2015,Takuma Torii,Kiyoshi Izumi,Kenta Yamada,Male,Male,Male,Male,"Most traders simultaneously watch and trade a large variety of assets including stocks, currencies, bonds, and options (Senft 2013). New trading technologies, such as high-frequency and algorithmic trading, promote simultaneous trading of multiple assets. Simultaneous trading is often regarded as a major cause of complicated relationships among assets. For instance, a number of stocks tend to mimic the price movements of each other just before a financial crisis (Harmon et al. 2010). Another example is the Flash Crash on May 6, 2010, a sharp decline and rise of the U.S. stock market within minutes. The shock was initiated by a significant drop in a futures market, and the drop eventually propagated throughout the stock market (US 2010). Artificial markets (Izumi et al. 2007; Leitner and Wall 2013), e.g., agent-based simulations of financial markets, are useful ways to study market dynamics and acquire information to devise market rules. For example, the NASDAQ, a major stock market in the USA, conducted multiagent simulations when they were contemplating the change in the market tick size, the smallest price unit of a stock (Darley and Outkin 2007). Moreover, the Tokyo Stock Exchange used an artificial market to test the effect of tick size reduction on competition between stock exchanges (Mizuta et al. 2013). Although there are several approaches to using artificial market simulations, e.g., involving news text mining (Izumi et al. 2007) or trader-network characteristics (Kodia et al. 2009), most of the existing artificial market models handle only a single asset and thus are not applicable to analyses of the relationships among multiple assets. A few artificial market models involving not only multiple agents but also multiple assets have been developed (Westerhoff 2004; Chiarella et al. 2007; Wah and Wellman 2013; Kawakubo et al. 2014; Xu et al. 2014). Regarding agent-based simulations, an option market model based on the Black–Scholes formula was studied by Kawakubo et al. (2014). Xu et al. (2014) studied multiple assets with spot-futures arbitrageurs in an attempt to reproduce the statistical properties (volatility, order-flow, etc) of real Chinese markets. Wah and Wellman (2013) study securities traded on two markets (exchanges) involving information delay and latency arbitrageurs. Our goal is to use a multiagent framework to make suggestions for market regulation. In this study, we constructed a multi-asset artificial market in which various trader agents sell or buy multiple stocks and/or index futures. To find the conditions under which a sudden price drop in one stock affects the whole market, we conducted exhaustive simulations on combinations of various agent types. Then, we analyzed the effect of arbitrage and a circuit breaker on the propagation of the price decline. Arbitrage is a style of trading that attempts to profit from market imbalances, i.e., price differences among different financial instruments that must be equally valued, by applying the simple rule: buy cheap ones and sell expensive ones simultaneously. Today, arbitrage is in the form of high-frequency and algorithmic trading. In our model, a group of agents called arbitrageurs profit from the differences between stock and index futures prices. A circuit breaker is a trading regulation that stops trading for a while in response to a substantial drop or rise in market price to reduce market volatility. In the case of a single-asset market, arbitrage and the circuit breaker are considered able to suppress market fluctuations. Arbitrage provides a mechanism to ensure that a price does not deviate substantially from its fair value or fundamental price—a rational and unbiased estimate of the potential true market price of an asset given all the information about economic activities, costs, risks, and so on. Since trading by arbitrageurs can result in a reduction in price deviations among multiple assets, it works to prevent irrational price fluctuations. Proponents say circuit breakers can prevent panic selling in a market and, so, avert large volatilities of the market. In multi-asset cases, unlike single-asset cases, the effects of arbitrage and circuit breakers are still not well understood because of the complicated dynamics of real markets. To understand the interactions among multiple assets, this study focuses on shock propagation from one asset to another and the effects of both arbitrage and circuit breakers in a multi-asset artificial market.",24
13.0,1.0,Evolutionary and Institutional Economics Review,24 May 2016,https://link.springer.com/article/10.1007/s40844-016-0039-0,J. M. Keynes on probability versus F. H. Knight on uncertainty: reflections on the miracle year of 1921,June 2016,Yasuhiro Sakai,,,Male,Unknown,Unknown,Male,"There is no doubt that J. M. Keynes and F. H. Knight are two towering figures in the history of the economics of risk and uncertainty. Knight’s contribution on risk and uncertainty is well known to many economists: in fact, his book Risk, Uncertainty and Profit (1921) is now regarded as a classic in the economics profession. In contrast to this, Keynes’ accomplishments on risk and uncertainty have been rather underestimated in the dark shadow of his most famous book The General Theory of Employment, Interest and Money (1936). The main purpose of this paper is to mend such an unfortunate tendency by focusing on his earlier yet equally important book A Treatise on Probability (1921). It is interesting to see that the two economic giants published apparently similar books on risk, probability and uncertainty in the same year. Therefore, 1921 may be referred to as a sort of miracle year in the long history of economic thought. The relation between Keynes and Knight, the two giants in the history of economic thought, is so delicate and complicated that cannot be described by a simple passage. It is true that they were contemporaries and lived through the two world wars, the First World War (1914–18) and the Second World War (1939–45). It is noted, however, that they were poles apart in origins. Keynes (1883–46) was born with a silver spoon in the United Kingdom: he spent his young days at a rich Victorian house, in a peaceful Cambridge district. During all his proactive career in the fields of academics and practical affairs, he could cheerfully return to this house, full of nice memories, and to his beloved parents. His roots were deep in Harvey road, which maintained the traditional values of the British society. He spent a very colorful life until his untimely death in 1946, first as a university instructor, then as a high government officer, and sometimes as an art collector. His academic accomplishments in monetary and macroeconomic theories were so novel and revolutionary that he was regarded by most people as the greatest economist in the 20th century. Unlike Keynes, Knight (1885–72) was born with a wooden spoon in the United States: his career began from rather unpromising roots in Maclean County, Illinois. He was the eldest of eleven children, being raised by parents who strongly believed in Christian fundamentalism. He received general education in small colleges in rural Tennessee and proceeded to the University of Tennessee. From there, he moved on to the Graduate School of Economics, Cornell University. His concentration and hard work resulted in a Ph. D. thesis entitled A Theory of Business Profit, written during the period from 1905 to 1915, and later published with some revisions in 1921 under the revised title Risk, Uncertainty and Profit. It was in 1927 that he finally appointed Professor of Economics at the University of Chicago, and continued to live there as the “Grandpa of Chicago” until he finished his long life in 1972. In short, Keynes and Knight have very different backgrounds in many ways. First of all, comparison of these two men might be characterized as a silver spoon versus a wooden spoon. Or alternatively, they could be portrayed as an elite in the Old World versus an ordinary man in the New World, or perhaps as a colorful “practical man” with many talents versus a colorless professor living quietly at the ivory tower. Considering such personal and educational differences between the two, it might be understandable to see that very few books and papers on Keynes versus Knight have ever been published in academic circles around the world. In this paper, however, I intend to break such an unfortunate tendency by carefully comparing the two great economists from an angle of probability and uncertainty. The contents of this paper are as follows. Session 2 will focus on the way how Keynes has dealt with the concepts of probability and uncertainty. The exact meaning of probability and Keynes’ strange chart of probability will carefully be discussed. Session 3 will attempt to compare Keynes and Knight in terms of risk, probability, and uncertainty. It will be shown that the figures of multiple rings are quite helpful for such a comparison. Session 4 will turn attention to J. R. Hicks, still another great economist in the 20th century, who has regarded economics on the verges of both sciences and history. It will be seen that Hicks’ approach is more or less influenced by Keynes and Knight. Some final remarks will be made in the final session.",19
13.0,1.0,Evolutionary and Institutional Economics Review,09 February 2016,https://link.springer.com/article/10.1007/s40844-016-0029-2,"Financial structure, financial instability, and inflation targeting",June 2016,Kenshiro Ninomiya,,,Unknown,Unknown,Unknown,Unknown,,
13.0,1.0,Evolutionary and Institutional Economics Review,09 May 2016,https://link.springer.com/article/10.1007/s40844-016-0038-1,"Net worth ratio, bank lending and financial instability",June 2016,Toshio Watanabe,,,Male,Unknown,Unknown,Male,"Recently, financial factors have increasingly come to affect the real economy. For example, the financial crisis in 2007, which was caused by sub-prime loan problems, symbolizes the influence of the instability of financial markets on real economies. However, it is hardly new to discuss the interactions between financial markets and the real economy. Previously, Keynes (1936) rejected classical economics and emphasized the effects of instability in the financial markets on the real economy. The discussion on instability in the economy has been mainly taken up by the post-Keynesians. Steindl (1952) focused on the “gearing ratio” which is the ratio of the stock of owned capital to total capital, and considered the interaction of real and financial factors in the capitalist economy. He investigated the short-run and long-run behavior of the economy in which firms have monopoly power and finance a part of their investment out of retained earnings. Dutt (1995) and Lavoie (1995) developed a simple formulation of Steindl’s model. Dutt analyzed the sources of the instability in the economy, and examined the implications of financial factors regarding the effects of a rise in monopoly power on the growth rate of the economy. However, although both models involve some financial factors, these treat the interest rate as exogenous and do not explicitly represent the financial markets.Footnote 1
 On the other hand, Minsky (1975) developed his own ideas on financial crises based on his interpretations of Keynes’s theory. He provided a new theory called the financial instability hypothesis. Minsky focused on financial intermediaries and emphasized that economic fluctuations enlarge through the expansion or contraction of bank credit. He criticized the standard IS-LM model, because it does not include these matters. We note the following remarks made by Minsky: “The higher leverage ratio of banks was part of the process that moved the economy towards financial fragility, because it facilitated an increase in short-term borrowing (and leverage) by bank customers” (Minsky 1986, p. 238). Minsky’s ideas led to the development of various mathematical models. Taylor and O’Connell (1985) is one such representative model. It takes into account long-run expectations and proves that an economy will fall into a financial crisis if the decline in the expected profit rate worsens firms’ financial condition and increases household preference for liquidity. In recent studies, Ryoo (2010, 2013) contributes to this literature by incorporating the active role of a profit-seeking bank. The model comprises dynamic equations of both net worth ratio of the bank and firms’ debt ratio, and shows the long waves through the interaction between firms’ and banks’ financial practices. Our approach is similar to Ryoo and has two distinctive features. One of the purposes of this paper is to build a simple macro model on the net worth ratio of the firm in line with Steindl’s ideas.Footnote 2 Also, we focus on the importance of the bank as financial intermediaries in accelerating boom and crisis in line with Minsky’s ideas.Footnote 3 We refer to the loanable fund theory and explicitly represent the bank lending market. We endogenize the interest rate and investigate financial factors which give rise to instability in the economy.Footnote 4
 The second purpose of this paper is to analyze the effectiveness of monetary policy. We first construct a static model and then extend it to a dynamic model that incorporates equations of both net worth ratio and interest rate. We investigate the effects of monetary policy on the short-run and long-run equilibrium. We consider the stabilizing effect of monetary policy when the steady state is unstable. Our results demonstrate that the high level of the propensity of consumption to the asset income constitutes to stability of the steady state following Lavoie (1995) and Hein (2007). On the other hand, we show that the economy can be unstable when the dividend rate is low and the bank’s lending reaction is elastic with respect to the net worth ratio of the firm. Especially, it can be said that the latter result accords with Minsky’s ideas which emphasize the effects of financial intermediaries. When the steady state is a saddle point, the change in the discount rate is likely to shift the economy from an unstable path to a convergence path.Footnote 5 We can see that monetary policy may have a stabilizing effect in the long run as well as a positive effect in the short run. This paper is organized in the following manner. Section 2 presents an overview of the model introduced in this paper. Section 3 discusses the respective behavior of firms, banks, and households. We derive the investment function, the bank lending function, and the saving function. In Sect. 4, we consider the equilibrium of the commodity market and bank lending market, respectively, and analyze the equilibrium of the short-run economy. Section 5 investigates instability in the economy. We construct a dynamic system and consider the stabilizing effect of monetary policy. Finally, Sect. 6 summarizes the results.",
13.0,1.0,Evolutionary and Institutional Economics Review,27 April 2016,https://link.springer.com/article/10.1007/s40844-016-0036-3,A comparative analysis of export growth in Turkey and China through macroeconomic and institutional factors,June 2016,Emre Ünal,,,Male,Unknown,Unknown,Male,"Turkey experienced a structural economic crisis in 2000–2001 that influenced the entire economy. The lira depreciated significantly, the GDP fell, and the productivity growth of export goods decreased dramatically.Footnote 1 As part of a new program called the transition to the strong economy, new reforms were produced and implemented to stabilize the Turkish economy, with the main goals of stabilizing the lira’s value and decreasing inflation. The new reforms created an outstanding institutional change in the Turkish exchange rate system. Turkey adopted the floating exchange rate system. Through institutional changes in wage–labor relations that aimed to link wage growth to the productivity growth under the inflation targeting policy system,Footnote 2 decreasing wage growth slowed the change rate in purchasing power parity (PPP),Footnote 3 compared with the pre-2000–2001 economic crisis period. During the 2000s, relatively decreasing labor costs in production helped Turkey to increase its exports of goods. Wage growth fell more under the control of the government. Privatization increased, and unproductive industries were privatized in large numbers. To perform a comparative analysis, we chose China and Turkey to compare their macroeconomic factors because these countries have similar export goods structures and are both developing countries. Both countries experienced a parallel development in their industrial production. For instance, both countries were based on low- and medium-tech industries, such as the textile and textile products industry, in the 1990s; however, in the 2000s, both countries experienced a technological change to medium- and high-tech goods industries. Thus, the machinery industry and the transport equipment industry became more important (see Table 3). Although China reflects a larger scale of economy compared with Turkey, its economy is quite proportional to that of Turkey. For example, China’s GDP is nearly 10 times larger than Turkey’s. However, in terms of GDP per capita, these countries are more similar to each other than to other countries in East Asia, such as Japan and South Korea. In 2014, the GDP per capita was $10515 in Turkey and $7590 in China.Footnote 4 Agriculture employment is still a large portion of the employment in Turkey and China, carrying development potential and labor supply for manufacturing industries, whereas South Korea and Japan are developed countries with a lower ratio of agriculture employment to total employment. The ratio of agriculture employment to total employment was 33.8 % in China, 25.5 % in Turkey, 6.7 % in South Korea and 4.8 % in Japan in 2011.Footnote 5 Although China and Turkey had different growth types, at the beginning of the 1990s, they were associated with similar trends, with high inflation and low privatizations. Although the Turkish economy is supported by international organizations such as the International Monetary Fund (IMF),Footnote 6 the government is dominant in its economic policies to create new institutional changes, which is similar to the Chinese government, but not because of their political systems. Additionally, although the two countries are separated in terms of ideology and their geographic positions in Europe and East Asia, both countries have remained closed economies for a large part of their history and have experienced the same trends for privatizations, attracting Foreign Direct Investment (FDI) inflows and gaining a new position in their areas. Most importantly, contrary to developed East Asian countries such as South Korea and Japan,Footnote 7 China has become a more important country because it is a developing country like Turkey, and it is one of the largest trade partners of Turkey that has caused a large amount of trade deficit, whereas South Korea and Japan are not even among the top 10 trade partners with respect to trade volume. Therefore, the competitive position of Turkey against China gains more importance to influence its macroeconomic factors in comparative analysis. A comparative analysis between Turkey and China will show more clearly how their growth strategies differ from each other. In addition, we could explain why the Turkish economy is less competitive than the Chinese economy. Usually, the Chinese economy has been analyzed regarding export-led growth by including the natural exchange rate or ULCP between 1990 and 2002, such as Uni (2007 and 2012). China’s export growth between 1994 and 2003 was considered by McKinnon (2005). The Chinese economy was examined regarding the effects of linkages between 1992 and 2002 under export-led growth (Yan 2012, pp. 166–170). In this work, we considered the Chinese economy between 1990 and 2011 to compare it with the Turkish economy. There have been no satisfactory research on the Turkish economy under the export-led growth model, considering macroeconomic and, most importantly, institutional factors by implementing input–output tables. We analyzed the Turkish economy intensively from 1973 to 2011 for macroeconomic and institutional factors and compared it to the Chinese economy using the export-led growth model described in “Sect. 2”. During the 1990s, China became a leading export country in a short period of time. It is currently the world’s leading exporting country. Its GDP increased significantly and it became the world’s second largest economy. Investment and the export of goods from China outstandingly stimulated its development. Turkey experienced a similar situation during the 2000s, but compared with China, its macroeconomic factors could not make it a strong economic power in such a short period of time. Another goal of this study is to show why China could be a strong export country and why Turkey could not create a similar situation. To research more deeply, we analyze several macroeconomic factors: the productivity growth of non-tradable goods and export goods; wage growth; inflation; exchange rate; and PPP which is derived from the markup rate, wage rate, vertically integrated labor input coefficient of export goods and imported material cost. Growth in exports can speed up for several reasons, such as an undervalued currency, low labor costs, technological developments and new economic organizations. Macroeconomic factors can be transformed by institutional changes, which are stimulated by key roles of governments. In this paper, we considered two institutional factors. The first is the wage–labor relations related to wages, productivity, ULC and PPP, which are influenced by deregulation policies. The second is the exchange rate system, which played significant roles in the economic development of the Turkish and Chinese economies regarding their rates of change compared with PPP. To make a clear sectoral distinction, it is better to use the non-tradable goods sector and export goods sector.Footnote 8 In this manner, we can explain the productivity growth of non-tradable goods and export goods, and we can distinguish their price levels. To calculate the productivity growth of non-tradable and export goods and to estimate PPP, we used input–output tables (for detailed sources of the input–output tables and employment see the “Appendix”). To conduct a comparative analysis in terms of macroeconomic factors, we compared two periods, which became the main parts of this analysis. These are the 1985–2003 and 2003–2011 periods.Footnote 9
 We explain the main concepts and methods in “Sect. 2”. We analyze the macroeconomic factors between 1985 and 2011 in “Sect. 3”. In “Sect. 4”, we intensified this research by considering institutional changes in wage–labor relations and the exchange rate system, which played significant roles among the macroeconomic factors. First, we analyze the Turkish economic transformation from import substitution industrialization to open economic policies in the mid-1980s, which increased its productivity growth of export goods. In addition, by focusing particularly on the Turkish economy, we perform a comparative analysis with the Chinese economy in terms of institutional changes. Finally, we assumed the type of institutional changes that must be implemented in the Turkish economy to increase its competitive power and to eliminate its trade deficit.",12
13.0,1.0,Evolutionary and Institutional Economics Review,18 January 2016,https://link.springer.com/article/10.1007/s40844-015-0028-8,The emergence and the development of the Achaean federation: lessons and institutional proposals for modern societies,June 2016,Emmanouil Marios L. Economou,Nicholas Kyriazis,,Male,Male,Unknown,Male,"It is generally accepted that direct democracy emerged by the end of the sixth century BCE in Classical Greece, the first fully developed example being Athens after the reforms of Cleisthenes of 510–507 and the fall of tyranny. This development was the result of a preceding framework of values, norms, customs, institutions and ideas that evolved in different fields of human activity, such as war, religion, athletics, and the city-state, which, combined, were unique in eighth to sixth century BCE Greece (Ober 2008, 2015; Pritchard 2013; Kyriazis and Economou 2015). The emergence of democracy was preceded by a previous political stage, that of isonomia, meaning equality of citizens before the law (Birgalias 2009; Meier 2011)—in most cases according to property criteria, and linked to that, to those able to afford the expensive hoplite military equipment, financed through their own means (Kyriazis and Paparrigopoulos 2011; Pritchard 2013). Democracy was not static but evolutionary. Democracy during the fourth century BCE was more developed institutionally and also more “moderate” (in the sense of not being too radical) than its fifth century predecessor (Hansen 1999; Kyriazis and Metaxas 2010; Halkos and Kyriazis 2010). What is less known generally, but very important due to the early modern and contemporary developments, is that within the same democratic culture the idea of voluntary federations of democratic city-states also emerged and was practiced. Leagues and alliances were of course well known during the sixth and fifth centuries, and even much earlier, when the Mycenaean Kingdoms that fought the Trojan War (during the late Thirteenth or early twelfth century BCE) were an ad-hoc alliance under the “supreme military commander” king Agamemnon of Mycenae. But the concept of federations of free democratic city-states that unite voluntarily to evolve into a specific political unit with an appropriate institutional structure was completely novel in its width and depth.Footnote 1 Federations, like the Boeotian one, and many more, are attested already with certainty during the seventh century, but what was new with the great fourth century federations was their democratic basis, both on a participating city-state and federal level, as well as their elaborate political and economic structure.Footnote 2
 In the present essay we analyse briefly, first, the historical setting of the Achaean federation (389-146 BCE). Then (Sect. 2), we proceed to their political and economic institutional organization and next (Sect. 3) we focus on the motives which made feasible the creation, the development and the functionality of the Achaean federation for two consecutive centuries. Finally, (Sect. 4) we offer arguments as to why the Achaean federation emerged and proved successful and we offer some ideas as to how the Achaean federation could face a series of current global issues, such as the European Union (EU) integration, global terrorism, etc.",12
13.0,1.0,Evolutionary and Institutional Economics Review,16 April 2016,https://link.springer.com/article/10.1007/s40844-016-0035-4,Spiteful behavior can make everybody better off,June 2016,Robert Philipowski,,,Male,Unknown,Unknown,Male,,2
13.0,1.0,Evolutionary and Institutional Economics Review,02 June 2016,https://link.springer.com/article/10.1007/s40844-016-0042-5,"Special feature: evolving diversity of firms and industries and dynamics of economic structures: the régulationist and institutionalist approach to Japan, South Korea, and Taiwan",June 2016,Hiroyasu Uemura,Yuji Harada,,Male,Male,Unknown,Male,,
13.0,1.0,Evolutionary and Institutional Economics Review,19 March 2016,https://link.springer.com/article/10.1007/s40844-016-0030-9,Diversification of Japanese firms: how hybrid organizations evolved through corporate governance reform,June 2016,Mitsuharu Miyamoto,,,Male,Unknown,Unknown,Male,"Confronted with the difficult business conditions as outcomes of the collapse of the economic bubble in the early 1990s, particularly the critical situations in the late 1990s, Japanese firms undertook two areas of corporate reform: corporate governance and human-resource (HR) management reform. The former advocated the shareholder-oriented corporate governance through the introduction of a US-style governance structure, and the latter proposed market-oriented HR management through the introduction of performance-related pay (PRP). Beyond these changes, when the practice of long-term employment (LTE) is abandoned, Japanese firms appeared to move more strongly to US firms. However, as Jacoby (2005) pointed out, US firms have also moved in a more market-based direction and, thus, the differences between US and Japanese firms still remained up to the present. In fact, there are very few Japanese firms that adopt a US-style corporate governance structure: the committee system wherein the majority of the board is composed of independent outside directors. Similarly, at least until now, many Japanese firms have maintained LTE practice for regular workers, although this number has fallen in favor of increasingly employing non-regular workers. It is also apparent that Japanese firms are far less concerned than US firms with stressing shareholder interests, boosting labor mobility, and widening individual pay gap according to the short-term performance appraisals. Therefore, Japanese and US firms are not likely to converge despite these recent changes in Japan. This evidence supports the VoC (varieties of capitalism) perspectives that highlight the national characteristics of market economies based on each country’s institutional framework (Hall and Soskice 2001). However, as Streeck and Thelen (2005) pointed out, the VoC approach are prone to stress the institutional continuity in a one-sided manner; thus tending to neglect the actual changes that occur. Therefore, while several authors mention the resilience of the Japanese system by seeing the continuity of LTE practices even under recent corporate governance reform, they tend to coincidently address the disappointment in the failure of Japanese firms to change, contrary to initial expectations (Whittaker and Deakin 2009). This discontent also corresponds to popular arguments in Japan that Japanese firms lag behind US firms because of the resistance to change; in particular, resistance from Japanese managers to protect managerial autonomy, and from Japanese workers to protect employment stability, both of which run contrary to the US-style corporation. As long as the US-style corporation is regarded as the standard model, Japanese firms are usually deemed to be delayed or insufficient with regard to institutional change. However, these arguments seem to have lost confidence from the perspective of ‘gradual institutional change’ presented by Streeck and Thelen (2005) and Mahoney and Thelen (2010). They point out the fallacy to advocate overall structural change, such as a change from organization to market-based corporations, and mention that actual changes are incremental and cumulative. Consequently, the existing institutional framework changes gradually. As such, we focus on the managerial reform through the introduction of the corporate executive officer system, an incremental change as compared to the adoption of US-style committee system, and demonstrate how the cumulative changes result in a new type of Japanese firms. In addition, from the perspective of the ‘evolving diversity in the corporate system’ presented by Aoki (2010), we focus on the hybrid type of organization composed of heterogeneous institutions, and examine how such an organization evolves from the traditional Japanese firms. From these points of view, this study investigates the ongoing changes in corporate governance, and examines their effects on HR practices. In particular, we investigate the diversification of Japanese firms in terms of the combination of LTE and PRP. While the traditional type of Japanese firms is characterized by a combination of LTE and seniority-based wages (non-PRP), we reveal a new type of Japanese firms that maintains LTE and introduces PRP. Moreover, we have discovered another type of Japanese firms that restricts or abandons LTE and introduces PRP, which is rather opposite to the traditional type. Further, we have found a degenerate type of Japanese firms that neither maintains LTE nor introduces PRP. While Jackson and Miyajima (2007) demonstrated similar diversification, classified into six types, it still remains in the typology. In contrast, we investigate the diversification as an evolutionary process of change in Japanese firms, using two data sets obtained from surveys conducted by the Japan Institute for Labor Policy and Training (JILPT) in 2004 and 2008. This paper is organized as follows. Section 2 overviews corporate reform beginning in the late 1990s, from the view of two driving forces such as banking and managerial crises, and examines the actual changes in corporate governance and HR practice. Section 3 reports the findings from the two surveys, and presents four types of Japanese firms in terms of different combinations of LTE and PRP. Section 4 estimates the effects of corporate governance reform on the diversification of Japanese firms, while Sect. 5 offers a concluding discussion.",4
13.0,1.0,Evolutionary and Institutional Economics Review,06 April 2016,https://link.springer.com/article/10.1007/s40844-016-0032-7,The collapse of Japanese companyist regulation and survival of the upstream industry: developing East Asian production linkage,June 2016,Mayumi Tabata,,,Female,Unknown,Unknown,Female,"Beginning in about 2000, the Taiwanese electronics firms began to outpace their competitors from Japan in the thin film transistor liquid crystal display (TFT-LCD) industry (Tabata 2012, 2014) but only in the manufacturing process. Taiwanese manufacturers still rely on the Japanese upstream industry for electronic components, key materials, and manufacturing equipment. Consequently, Japanese upstream firms have extended their business activities in East Asian production linkage and have maintained considerable control despite the catch-up phenomenon in South Korean, Taiwanese, and Chinese firms. In this paper, I focus on the diversity of East Asian capitalisms (Boyer et al. 2012) using concepts from the literature on regulation theory (Boyer 1986) to track the transformation of interdependence between the Japanese and Taiwanese TFT-LCD industries. Using data from in-depth interviews with senior engineers, business managers, and national government bureaucrats in Japan and Taiwan, along with secondary analysis, I explore the mechanisms of interdependence between the Japanese and Taiwanese TFT-LCD industries and the impact of the catch-up phenomenon in Taiwanese and South Korean firms on the strategies and systems of innovation in Japanese firms. According to Amable and Petit (2003) definition, an innovation system, rather than being viewed as a simple process of individual decision-making, is a concept encompassing the entire institutional environment (i.e., a whole set of rules, forms of organizations). Innovation is inspired by interactions among actors (firms, researchers, universities, laboratories) and their institutions. Amable and Petit mention that these institutions are involved in scientific activities, including scientific systems, research laboratories, scientific institutions in technology and universities, and institutes of higher education. Because innovation is also an accumulation of knowledge, the training system should be viewed as an important constituent of an innovation system (Amable and Petit 2003, 207). From this point of view, as I discuss later, the long-term and sustainable training system of engineers in Japan, which has been supported by stable life-time employment, has played an extremely important role in the Japanese social systems of innovation since the end of World War II. Amable and Petit also focus on the diversity of capitalism, noting that regional or national economies are affected by institutions’ diversity, which is classified as “institutional complementarities.” The idea of institutional complementarities sheds light on the interactions between institutions. For example, in a non-market-based economy, a stable labor market can enhance long-term training in the labor force and the accumulation of knowledge, and close relationships between banks and firms can facilitate sustainable equipment investment projects. On the contrary, in a market-based economy, though a flexible labor market facilitates employee mobility, employees lose their bargaining power in union-management relations. Deregulation of the financial system triggers both good and bad outcomes: for example, flexibility in raising funds and the collapse of long-term stable investment, respectively. As we shall see in the next section, the Japanese companyist regime is rooted in a non-market-based economy, whereas Taiwanese institutional complementarities are oriented toward a market-based economy. The difference between them brings diverse characteristics to their innovation systems. In the manufacturing process of TFT-LCD, Taiwanese manufacturers are no longer learning technology from Japanese firms. However, in terms of components, materials, and TFT-LCD manufacturing equipment, Taiwanese firms still rely on the technology of their Japanese counterparts. What is the main problem with systems of innovation in the Taiwanese TFT-LCD industry? How does the strength of the Japanese upstream industry fight against the catch-up of South Korean, Taiwanese, and Chinese firms in the East Asian production linkage? These are the main research questions I address in this article. Finally, I try to integrate the historical process of the transformation of interdependence between the Japanese and Taiwanese TFT-LCD industry and elaborate on the regulation mode (Lipietz 1987) in the diversity of East Asian capitalisms.",
13.0,1.0,Evolutionary and Institutional Economics Review,21 May 2016,https://link.springer.com/article/10.1007/s40844-016-0040-7,The Korean exception: service outsourcing by manufacturing firms and the role of institutions,June 2016,Wooseok Ok,,,Unknown,Unknown,Unknown,Unknown,,
13.0,1.0,Evolutionary and Institutional Economics Review,25 April 2016,https://link.springer.com/article/10.1007/s40844-016-0034-5,Structural change and transformation of growth regime in the Japanese economy,June 2016,Hiroshi Nishi,,,Male,Unknown,Unknown,Male,"This study examines the relationship between economic growth and structural change in the Japanese economy. We show how structural changes in inputs and output in the various sectors of the Japanese economy affect its economic growth and stagnation. The structural change in this study mainly refers to change in the sectoral composition (share) of an aggregate economy. In this study, therefore, we use a disaggregation approach and divide the macroeconomy into sectors on the basis of the Japan Industrial Productivity (JIP) database 2014 compiled by the Research Institute of Economy, Trade and Industry (RIETI). The disaggregation approach is important because, as we see below, sectoral performances, such as growth of labour productivity and value added, are not always uniform among sectors and thus, sectoral heterogeneity is evolving in Japan. This study is based on the growth regime analysis of the régulationists and post-Keynesians. In addition, we attempt to empirically expand these theories from a multi-sectoral perspective. The régulationists and post-Keynesians have analysed the economic growth regime of a country from a macroeconomic perspective (Boyer and Yamada 2000; Boyer et al. 2011; Lavoie and Stockhammer 2013; Hein 2014). They reduced the dynamic relationship between productivity growth and demand growth into two equations, namely, the demand regime and productivity regime, and revealed different constellations of these regimes by country and period. Since such macroeconomic analysis is conducted at the aggregate level, they assume no sectorally heterogeneous configuration and structural change. Therefore, they cannot capture the relationship between macroeconomic performance and industrial structure. Work by Uemura and Tahara (2014) is a recent contribution that overcomes such issues in aggregate analysis. Their multi-sectoral approach integrally analyses the growth regime as well as the de-industrialization mechanism in Japan. The study shows that the Japanese economy has undergone both positive and negative de-industrialization, depending especially on the performance of the export-core manufacturing sectors over the past 30 years. The relationship between structural change and aggregate economic growth has been an important subject so far.Footnote 1 One of the most important research lines originates from the study by Baumol (1967). According to this study, there is negative feedback on economic growth in the process of tertiarization, in what is well known as ‘Baumol’s disease’. Studies after Baumol have attempted to see whether expansion in a non-progressive sector has a negative impact on overall economic growth. These studies consider expansion of the tertiary sector, also classified as a non-progressive low-productivity sector, as structural change. While some studies support Baumol’s prediction (Baumol et al. 1985; Peneder 2003; Nordhaus 2008; Hartwig 1998, 2012), others do not (Maroto-Sánchez and Cuadrado-Roura 2009; Dietrich 2012). While the existing literature focuses mainly on the EU and US, recent studies on Japan (Fukao and Miyagawa 2008; Ito and Lechevalier 2009; Fukao 2012; Morikawa 2014) focus on the industrial structure and attempt to find the cause and consequence of different performances at the industrial and firm levels. They focus on the supply side, with the investigation of total factor productivity (TFP) growth certainly being the most popular. Studies in Fukao and Miyagawa (2008) indicate that the TFP growth rate differs at the industrial and firm levels. Fukao (2012) emphasizes that the cause of the slowdown in TFP growth is Japan lagging behind in ICT investment. Morikawa (2014) intensively investigates the economic performance of firms in the services sector and finds that their productivity is not necessarily low. He points out that there are a number of high-productivity firms in this sector. Ito and Lechevalier (2009) focus on the dispersion of productivity growth across heterogeneous firms. They find evidence that internationalization has a significant and positive impact on productivity dispersion. Almost all of these studies point out the heterogeneity of industries and firms. This means that by examining the variables at an average or aggregate level, we cannot correctly understand the overall economic performance. Instead, we need to observe the sectoral distribution that creates the aggregate outcomes. Even though the aggregation approach is useful to analyse the growth process of the 1970s and 1980s, it cannot explain the uneven growth process after the 1990s, as we show in this study. The uneven growth process after the 1990s is not satisfactorily explained even by Uemura and Tahara (2014), because they do not consider the distribution of the sectors contributing to economic growth. In this context, we introduce the multi-sectoral perspective to the growth regime analysis of the post-Keynesians and régulationists. In their theory, the growth regime is examined by the interaction of growth in output and labour productivity. The current study does not focus on their interaction, but instead, it empirically explores the industrial foundation that creates the growth in output and labour productivity in the Japanese economy. We believe that such an attempt as ours has three novelties for growth regime analysis. First, this study clarifies the sectoral distribution behind economic growth. In other words, we attempt to investigate the industrial foundation of the growth regime. Second, to examine Japanese economic growth and stagnation, we introduce structural change in various sectors. In doing so, we investigate the change in the sectoral structure of inputs and output in relation to economic growth. To be more precise, the structural change in output means a change in sectoral composition of value added, which is a proxy for structural change on the demand side. The structural change in inputs means change in sectoral share of factors of production, such as labour and capital, which is a proxy for structural change on the supply side. Thus, we also consider the demand side of sectoral performance, which the existing literature has not examined adequately. Hence, the third novelty of this study is that we empirically detect the kind of structural change required to promote economic growth. The remainder of this paper is organized as follows. Section 2 introduces sectoral classification based on the JIP database 2014. This section also presents the indices used to measure the degree of structural change in this paper. Section 3 considers Japanese sectoral performances and macroeconomic growth. This section illustrates, first, the structural change in inputs and output, and second the sectoral contribution to growth in Japan for the past 40 years. Section 4 presents our econometric analysis. This section reports the impact of structural changes in inputs and output on economic growth. Finally, Sect. 5 concludes.",2
13.0,1.0,Evolutionary and Institutional Economics Review,13 April 2016,https://link.springer.com/article/10.1007/s40844-016-0033-6,Long-term transformation of the economy–environment nexus in Japan: a historical analysis of environmental institutions and growth regimes based on the régulation theory,June 2016,Kazuhiro Okuma,,,Male,Unknown,Unknown,Male,"Ecological crisis is now recognized as a factor that may limit economic growth. There also emerged the ideas of “green growth” and “green new deal” (e.g., UNEP 2009; OECD 2011). The issue of environment has become an imperative element in discussing economic growth. Environmental problems arise with economic growth, cause social conflicts between polluters and victims, and thus lead to regulations and other rules to control them, which in turn may affect the growth. This is a process of institutional coordination of the economy, which fits the framework of the régulation theory. Environmental policy often faces opposition from the viewpoint of economic growth. Neo-classical environmental economics has not been able to adequately address it, limiting itself to advocating market instruments such as carbon pricing. Approaches from the régulation theory will contribute to filling this gap by providing historical perspectives on growth with political reality. Moreover, it should be beneficial for the régulation theory to incorporate this issue into its framework to enhance its relevance and completeness. Among economists in the régulation school, though Lipietz provided valuable insights and perspectives on the environmental issue (e.g., Lipietz 1995, 1999), it is only recently that works aiming at more analytical approaches incorporating environmental issues into the core of the theory have emerged (e.g., Becker and Raza 1999; Rousseau and Zuindeau 2007; Zuindeau 2007). They analyzed the theoretical features of régulation theory and environment-related studies (i.e., political ecology, ecological economics, and sustainable development studies) to bridge the gaps between them, and recommended conducting analyses on specific forms of the relationship between the economy and the environment based on the framework of régulation theory.Footnote 1 Becker and Raza (1999) also suggested that the relationship between the economy and the environment be considered as a sixth institutional form. These studies provide a valuable basis for environmental analysis using the régulation theory. Building on these studies, the historical or comparative analyses on real economies need to be explored.Footnote 2 Further theoretical consideration is also required to obtain robust frameworks for such empirical analyses. For this purpose, the author has proposed an analytical framework for the relationship between environmental policies and economic growth by recognizing the institutional coordination between the economy and the environment as the sixth institutional form of the theory (hereafter, the “economy–environment nexus”).Footnote 3 This includes the applications of key concepts of the theory towards coordination in the economy–environment nexus, a formalization of environmental costs as key indices, and a variant of the Kaleckian model incorporating these indices as key parameters (Okuma 2012). Using this framework, the current article conducts a historical analysis on Japan from post-war period to around 2010 to understand the evolution of the relationship between economy and the environment. First, as basic information, we trace the historical development of institutions in the economy–environment nexus. Second, we conduct long-term estimation of environmental costs to obtain quantitative indices. Third, we conduct econometric analysis of a variant of the Kaleckian model by using these indices. Finally, by combining information from these, we analyze the long-term transformation of the economy–environment nexus and its relation to growth regimes, which leads to some policy implications.",7
13.0,1.0,Evolutionary and Institutional Economics Review,23 March 2016,https://link.springer.com/article/10.1007/s40844-016-0031-8,Diversity of institutional architectures underlying the technological system in Asian economies,June 2016,Hironori Tohyama,Yuji Harada,,Male,Male,Unknown,Male,"Since the 1990s, the spread of knowledge embodied in technology has been accelerated by two major structural changes: increasing globalization, and diffusion of information and communication technologies (ICT). Assuming that technologies are freely circulating semi-public goods, developing or emerging economies could use them to their advantage, in such ways as attaining higher productivity without incurring high development costs. Based on these changes, Asian economies—led by a rapidly growing Chinese economy—might be threatening advanced countries’ leadership in the creation and diffusion of technology (Hu and Mathews 2005). However, technological upgrading or knowledge spillovers are not automatic as the national innovation system literature emphasizes (e.g., Dosi 1982; Lundvall 1992). The matching of technological opportunities and national capabilities, such as in the education and financial systems, is a prerequisite for creation and diffusion of a new idea or technology. As a result, a challenging task for emerging economies is to construct a social and an institutional basis that allows them to upgrade their technological performance. This paper focuses on the institutional architecture bolstering innovation or technological upgrading in Asian economies. Accordingly, we investigate the existence of specific institutional arrangements for Asian technological systems, and whether they differ from those used in advanced economies. In addition, we explore the existence of multiple institutional configurations in Asian economies, which might support the technological systems. We organize the paper as follows. First, we show the theoretical framework used in approaching technological systems or innovations in Asian economies. Second, the methodology and data are explained. Third, we present our results using multivariate analysis. We also clarify the characteristics that are specific to the institutional architecture of technological systems in Asian economies, compare them with those of advanced economies, and the varieties underlying the technological systems. Last, we derive some implications from our findings.",5
13.0,2.0,Evolutionary and Institutional Economics Review,28 July 2016,https://link.springer.com/article/10.1007/s40844-016-0046-1,Comparing the effectiveness of collusion devices in first-price procurement: an auction experiment,December 2016,Jeannette Brosig-Koch,Werner Güth,Torsten Weiland,Female,Male,Male,Mix,,
13.0,2.0,Evolutionary and Institutional Economics Review,21 June 2016,https://link.springer.com/article/10.1007/s40844-016-0043-4,Capital theory ‘paradoxes’ and paradoxical results: resolved or continued?,December 2016,Theodore Mariolis,Lefteris Tsoulfidis,,Male,Unknown,Unknown,Male,"Capital theory controversies and the associated with these ‘paradoxes’ culminated in the decades of 1960s and 1970s. These debates started as a critique of the logical foundations of the neoclassical theory of value and distribution, and they showed that the wage–production price–profit rate curves can display shapes that are inconsistent with the theoretical requirements of this theory. We do know that neoclassical prices are indexes of relative scarcities; thus, it is expected that, as the profit rate rises, corresponding to a fall in the real wage rate, the prices of ‘capital-intensive’ (‘labour-intensive’) commodities rise (fall). The movement of relative production prices of ‘actual’ (linear, closed and single-product) economies has been examined in a relatively large number of studies, and, subsequently, the findings were extended both theoretically and empirically (see Shaikh 1998; Tsoulfidis and Mariolis 2007; Mariolis and Tsoulfidis 2009; Mariolis et al. 2013). Thus, it has been stated that, if Sraffa’s (1960, Chaps. 4–5) Standard commodity (SSC) is chosen as the standard of value, or numeraire, then the price-movement is, more often than not, governed by the ‘capital-intensity effect’, i.e. by the difference between the industry’s vertically integrated capital-intensity and the capital-intensity of the Sraffian Standard system (SSS), where the latter equals the reciprocal of the maximum profit rate. This ‘traditional flavour’ condition can be modified by the ‘price effect’, i.e. the revaluation of the industry’s vertically integrated capital, which depends on the entire economic system and, therefore, is not predictable at the level of any single industry (also see Sraffa 1960, pp. 14–15; Pasinetti 1977, pp. 82–84). In effect, empirical evidence associated with quite diverse economies, and spanning different time periods, showed that the capital-intensity effect overshadows the price effect, although there are cases where the latter effect is so strong that it can supersede the former giving rise to extrema and ‘price–labour value reversals’ (i.e. reversals in the direction of deviation between production prices and labour values). In these realistic but relatively rare instances, i.e. not significantly more than 20 % of the cases tested, the price–profit rate curves are non-monotonic and have no more than one extreme point while cases of price–labour value reversal are rarer. It then follows that the idea of representing the actual price–profit rate curves through linear or, a fortiori, quadratic approximations is absolutely justifiable (Bienenfeld 1988; Shaikh 2012; Iliadi et al. 2014). Moreover, from Sekerka et al. (1970) and Krelle (1977) onwards, a typical finding in many relevant studies is that, although the actual economies deviate considerably from the Ricardo–Marx–Samuelson ‘equal value compositions of capital’ case, the wage–profit curves (WPCs) are near-linear, i.e. the correlation coefficients between the distributive variables tend to be above 99 %, and their second derivatives change sign no more than once or, very rarely, twice, irrespective of the numeraire chosen (also see Ochoa 1989; Petrović 1991; Angeloussis 2006; Han and Schefold 2006; Mariolis and Tsoulfidis 2016, Chaps. 3 and 5, and the references therein). All these findings imply that, although the actual economies cannot be analysed on the basis of ‘neoclassical parables’, the role of price-feedback effects is actually of limited empirical significance. The claim that this paper raises is that, by focusing on both the eigenvalue and singular value distributions of the system matrices, we can (i) further study these theoretical issues; and (ii) derive some meaningful theoretical results consistent with the available empirical evidence. More specifically, we start with a spectral representation of the price–wage–profit system of a closed economy involving only single products and ‘basic’ commodities (in the sense of Sraffa 1960, pp. 7–8). Then the approach is applied to the Symmetric Input–Output Table (SIOT) of the UK economy for the year 1990, which represents an ideal testing ground for our theoretical findings. It should be stressed from the outset that the results obtained are typical for a number of countries that have been hitherto tested (Mariolis and Tsoulfidis 2016, Chaps. 3 and 5–6) and are presented here for the first time. We used input–output data of the UK economy mainly for two reasons: first, the UK data have not been used in similar experiments and, second, the available capital flow matrix for the year 1990 enables to carry out our experiments in both circulating and fixed capital stock matrices. Thus, we can directly compare the results derived from both type of matrices and pinpoint their probable differences.Footnote 1 Finally, this investigation shows that the characteristic value distributions of the matrices of vertically integrated technical coefficients make possible the mimicking of the behaviour of the entire price system through the use of a single or just a few hypothetical basic industries. Those industries bear meaningful similarities to what can be described by the Samuelson–Hicks–Spaventa, or ‘corn-tractor’, model (see Spaventa 1970). The remainder of the paper is structured as follows. Section 2 deals with the theoretical issues lurking behind the shapes of the wage–price–profit rate curves. Section 3 exposes the empirical results. Section 4 argues that the results could be connected to the characteristic value distributions of the system matrices. Section 5 estimates n-by-n corn-tractor type approximations that tend to contain the essential properties of the original system. Finally, Sect. 6 concludes.",8
13.0,2.0,Evolutionary and Institutional Economics Review,16 August 2016,https://link.springer.com/article/10.1007/s40844-016-0045-2,Alternative monetary policies and economic stability in a medium-term Keynesian model,December 2016,Hiroki Murakami,,,Male,Unknown,Unknown,Male,"Nowadays, it is common, especially in developed countries, for the monetary authority to conduct monetary policies by setting several targets, and among others, the rate of inflation and aggregate income (GDP) (or the rate of unemployment) are usually adopted as the monetary authority’s target variables. Indeed, as Taylor’s (1993) seminal work revealed, there is empirically a certain relationship between the target variables (such as the rate of inflation and the existing output gap) and the short-term rate of interest (e.g., the federal funds rate in the U.S., the minimum bid rate in EU, and the call rate in Japan) which is controlled by the monetary authority.Footnote 1 Lively arguments have been made on the optimal rule for the monetary authority to achieve its goal. In addition to the question as to how to find the efficient rule concerning monetary policies, however, there is another important issue on monetary policies: which, of the supply of money and the rate of interest, is the better to be controlled for the monetary authority to stabilize the macro economy effectively. This problem concerns choice of policy instruments by the monetary authority. This topic may seem old-fashioned in economics, because it is thought that the interest rate policy is superior to the money supply policy, but the issue still remains unsettled and the supply of money has not lost its role in actual monetary policy implementations. Indeed, the Bank of Japan has focused mainly on the money supply policy rather than on the interest rate policy since 2013 (cf. Bank of Japan 2013). In this respect, it has recently been more and more important to understand properly the differences between the money supply policy and the interest rate policy. On this topic, there have been several articles which compare the effects of the money supply policy and of the interest rate policy with each other. For instance, Poole (1970), Moore (1972), Turnovsky (1975), and Friedman (1976) attempted to provide criteria to measure the superiority or inferiority of the money supply policy and of the interest rate policy in the short-term static Keynesian IS-LM system with stochastic disturbances, and Yoshikawa (1981) extended Tobin’s (1975) short-term (disequilibrium) dynamic Keynesian model by introducing stochastic disturbances that obey Brownian motions to examine the effects of the money supply policy (the monetarist’s k percent rule in his analysis) and of the interest rate policy. They provided useful insights on choice of monetary policy instruments in the Keynesian system,Footnote 2 but their analyses were short-term ones with capital formation through investment activities ignored and did not consider the inflation-targeting policy. Recently, Clarida et al. (1999) and Walsh (2010, chap. 11) discussed this problem in the new Keynesian dynamic stochastic general equilibrium (DSGE) framework and gave us new perspectives for the optimal selection of monetary policy tools in that they allowed for inflation-targeting policies. However, their analyses have some flaws and one of the most serious drawbacks of their analyses is that they paid little attention to the roles of aggregate demand. In fact, the new Keynesian DSGE model, on which their analyses stand, ignores Keynes’ (1936) principle of effective demand.Footnote 3 Although there have been a lot of studies that discuss the problem of efficient choice among the money supply policy and the interest rate policy, it seems that no satisfactory analysis of this problem has so far been provided in the medium-term Keynesian analysis, in which changes in capital formation through investment are also considered.Footnote 4
 The purpose of this paper is to tackle the issue of “choice among monetary policy tools” by discussing the stabilizing effects both under the money supply policy regime and under the interest rate policy regime in the Keynesian framework. For this purpose, we shall first present a medium-term Keynesian macroeconomic model on the basis of the ideas of Keynes (1936), Kaldor (1940), and Tobin (1975, 1993), and then explore the stability analysis of this model under both the money supply policy regime and the interest rate policy regime. To build a medium-term Keynesian model, we shall follow Tobin’s (1975) argument that the economics of Keynes (1936) should be interpreted as the economics of disequilibrium dynamics, not as the one of comparative staticsFootnote 5 and extend his short-term analysis to the medium-term one by combining his framework with Kaldor’s (1940) theory of business cycles. Moreover, we shall also examine, in the Keynes–Kaldor–Tobin framework, the effectiveness of the inflation-targeting policy, which was not investigated in the short-term Keynesian analyses of optimal choice of monetary policy instruments by Poole (1970), Moore (1972), Turnovsky (1975), Friedman (1976), and Yoshikawa (1981). This paper is organized as follows. In Sect. 2, we shall formalize a simple medium-term Keynesian macroeconomic model in which monetary policies are abstracted from,Footnote 6 and analyze the stability of this model by paying attention to the (de-)stabilizing influence of inflation–deflation expectations. In Sect. 3, we shall evaluate the money supply policy in terms of stability in the Keynesian model formalized in Sect. 2. In this section, the rate of changes in the supply of money is adjusted, by the monetary authority, in response both to the existing output gap and to the gap between the actual and target rates of inflation. Unlike in the preceding works mentioned above, the effects of the inflation-targeting policy (cf. Romer 2000) shall be considered. To discuss the rights and wrongs of the inflation-targeting policy, we shall compare the cases in which the monetary authority pursues the “natural rate” of real output alone and in which the monetary authority is eager to attain the target rate of inflation as well as the “natural rate” of output. In Sect. 4, we shall look into the interest rate policy in the Keynesian framework. For this purpose, we shall formalize the monetary authority’s counter-cyclical and inflation-targeting policies by means of the rate of interest à la Taylor’s (1993) rule. In this section, we shall also compare the cases in which the monetary authority conducts the counter-cyclical policy alone and in which it also performs the inflation-targeting policy as well as the counter-cyclical policy. In Sect. 5, we shall have a closer look at the periodic orbits generated in our models by Hopf bifurcations by means of numerical simulations. In Sect. 6, we shall scrutinize the effects of the money supply policy and of the interest rate policy and the effectiveness of the inflation-targeting policy in both the money supply policy and the interest rate policy.",4
13.0,2.0,Evolutionary and Institutional Economics Review,19 September 2016,https://link.springer.com/article/10.1007/s40844-016-0056-z,Accounting for structural changes in demand for foods in the presence of age and cohort effects: the case of fresh fish in Japan,December 2016,Hiroshi Mori,Toshio Inaba,John Dyck,Male,Male,Male,Male,"“Structural changes” in demand for foods are often mentioned. Numerous papers have referred to “structural changes” in the demand for meat in the U.S., and fish and meat in Japan (Chavas 1983; Eales and Wessels 1999; Sawada 2012; etc.). Most of these studies are concerned with the statistical existence of such changes and identifying when they may have taken place. Explanation for the changes is rarely rigorously addressed (Chen and Veeman 1991; Huang and Bouis 2001). In this article, we try to show that separating out certain demographic factors helps identify structural changes, if any. It has long been recognized that food consumption varies by age (Wold 1953; Prais 1953; Price 1970; Buse and Salathe 1978; Pollak and Wales 1980; Tedford et al. 1986; etc.). When the population as a whole age, both total consumption and per capita individual consumption, of selected food products should change in the presence of sizable age effects. Population cohorts—limited to groups born in the same period in this article—can share similar dietary habits or preferences, which continue to distinguish their food consumption through life. As Schrimper noted many years ago, “all generations [do not necessarily] follow the same transformation of eating habits over the lifecycle” (Schrimper 1979, p. 1059). Evidence from Japan and Korea leaves little doubt that conspicuous cohort effects exist in the consumption of fish (Mori and Saegusa, EIER
2010), selected fresh fruits and rice in particular (Mori and Stewart 2011; etc.)—on top of age effects. Since it is known that age and cohort effects show different patterns, i.e., older individuals exhibit declining age effects on consumption, but the older generations exhibit apparently larger and positive cohort effects on consumption than the newer generations, as in the case of rice and selected fresh fruits in Japan (Mori et al. 2006, 2009; Mori and Stewart 2011; etc.). An implication of these different patterns, i.e., shapes and magnitude, is that a linear trend term, T, or a single demographic variable such as the number of children in the household or “a habit persistence effect” or “smooth transition regressions” (Mori and Stewart 2011, 164–166; Pollak and Wales 1981; Chen and Veeman 1991; Holt and Balagtas 2009) will not always do a good job in compensating for possible structural changes in aggregate consumption, unless both age and cohort effects are explicitly accounted for. Incorporating age and cohort effects into traditional demand analyses, either through a single commodity or demand system approach,Footnote 1 can mitigate the problems of structural changes in estimating elasticities of economic variables. This also aids in refining the search for other possible changes in structure. For example, when using the available Japanese data on various food products from 1980 to 2013, we discern non-negligibly large period effects—residuals left unexplained by age and cohort effects and economic variables—implying that there still remain different structural changes unidentified (Mori et al. 2015). To investigate these changes, interdisciplinary research, such as with sociologists and dieticians knowledgeable about the specific industries, may be helpful.",3
13.0,2.0,Evolutionary and Institutional Economics Review,07 November 2016,https://link.springer.com/article/10.1007/s40844-016-0063-0,Special feature on new directions in econophysics,December 2016,Wataru Souma,Yoshi Fujiwara,,Male,Male,Unknown,Male,,
13.0,2.0,Evolutionary and Institutional Economics Review,10 August 2016,https://link.springer.com/article/10.1007/s40844-016-0052-3,Analyses of aggregate fluctuations of firm production network based on the self-organized criticality model,December 2016,Hiroyasu Inoue,,,Male,Unknown,Unknown,Male,"Providing stimulus to firms and prompting the spillover effect is a way for a government to affect its economy, which includes purchasing goods and services, giving grants to firms, and fine-tuning taxes. Governments consider fiscal policy as an important determinant of growth (Easterly and Rebelo 1993; Romp and de Haan 2007). Currently, the analysis of input–output tables is considered a strong tool to predict the spillover effect (Leontief 1936). Doing so enables us to obtain a single predicted value of the spillover effect caused by the stimulus. Therefore, it is normally expected that the result would be close to the prediction, provided that the volume in the simulation is the same as that used as stimulus. This concern, of whether the expectation is correct, is the main topic of this study. If the size of the spillover effect is close to the average prediction, it should be true that the propagation is never amplified or reduced through firm networks. However, Gabaix showed that if the firm size distribution is fat-tailed, the hypothesis breaks down (Gabaix 2011). In addition, Acemoglu et al. noted that microeconomic shocks may lead to aggregate fluctuations in the presence of intersectoral input–output linkages (Acemoglu et al. 2012). These studies suggest that the stimulus and spillover effects in reality are not similar to the prediction. In other words, normal distribution is usually assumed, but this assumption is not correct. To investigate aggregate fluctuations, Bak et al. (1993) proposed a micro model. The study of the micro-model is the foundation for the studies cited in the previous paragraph. Furthermore, Iino and Iyetomi (2009) investigated whether the size of avalanches follows a power law using artificially created scale-free networks. This study reveals how external demand shocks cause the spillover effect. We use a micro-model developed by Bak et al. and employ observed data. We clarify the following points: (1) the diversity of the spillover effect; this is because it appears that spillover effects depend on the industries affected by the shocks. We also address (2) the extent of involvement in the spillover effect; this also appears to reflect industry heterogeneity. The remainder of this paper is organized as follows. In Sect. 2, we introduce the dataset. Section 3 describes the methodologies that we employ in the analyses. Section 4 presents the results. Finally, Sect. 5 concludes.",2
13.0,2.0,Evolutionary and Institutional Economics Review,07 November 2016,https://link.springer.com/article/10.1007/s40844-016-0059-9,Large directed-graph layout and its application to a million-firms economic network,December 2016,Yuji Fujita,Yoshi Fujiwara,Wataru Souma,Male,Male,Male,Male,"Economics can be investigated in terms of network science, as an economic agent tends to interact with a limited portion of the entire set of agents. Some companies maintain a large number of economic relations whereas many of the others have relatively fewer relations. Information regarding products, profits, and financial states is sent and received through those relations, and such activities can be summarized as a macroscale economic phenomenon. Unfortunately, studying such a system of commercial relations tends to be difficult as a result of its scale and complexity. Visualizing relational data often assists this kind of investigation, but creating a proper visualization itself is not easy because of its scale and complexity. This study focuses on one of the largest datasets for a nationwide economy, namely how firms are connected with each other in the giant Web of the huge number of suppliers and customers in Japan. It is a challenge to visualize millions of nodes and links as a directed graph to understand the real economy at a nationwide scale based on such microscopic data of an economic network. Economic relationships such as buyer–seller relation are asymmetric by nature. Various methods are available in the study of graph drawing and visualization (see Tamassia (2013) for a recent review), but directed-graph drawing does not look like a major concern. In this study we developed a new method called direction-aware multidimensional scaling (DMDS), which is combined with a force-directed method to obtain the final result. While force-directed methods are readily capable of handling link directions by adding magnetism to the edges (see Sugiyama and Misue 1995), we had to modify MDS to provide a good initial vertex arrangement by making it sensitive to the directions of the links. Through the visualization (Fig. 1), we discovered interesting features of the economic network. It has a striking feature of clusters or communities; firms cluster into tightly knit groups with high intragroup density and lower intergroup connectivity, reflecting industrial sectors. We would like to identify in our visualization how those clusters depend on the sectors that firms belong to. Another issue we address in this study is to reproduce stable configurations of positions of nodes and links in our visualization while retaining the direction of economic relations (the supplier–customer relationship depicted vertically as much as possible). The issue matters because force-directed methods usually begin with random initial positions and inevitably result in different layouts from one simulation to another. Hence, the benefit maintaining the direction of economic relations in Fig. 1 and other visualizations is quite obvious. Overall network in a single image. Node size represents the number of links, colored for their respective sectors. Links are placed facing upward, with supplier down and customer up. The icon in the lower right corner shows this direction indicating that goods are transferred upward and money downward",3
13.0,2.0,Evolutionary and Institutional Economics Review,09 September 2016,https://link.springer.com/article/10.1007/s40844-016-0055-0,Long-term firm growth properties derived from short-term laws of sales and number of employees in Japan and France,December 2016,Atushi Ishikawa,Shouji Fujimoto,Tsutomu Watanabe,Male,Unknown,Male,Male,"Economic growth is one main subject of macroeconomics. Using the long-term GDP data of each country around the world, economic growth has been discussed, and the Solow growth model and Romer model have been proposed (Solow 1956; Romer 2008). Since the exhaustive financial data of firms in countries all over the world have become available to researchers, we can now analyze the growth of firms whose financial data constitute the GDP of each country. In this study, with these large-scale data (big data), we investigate economic growth. In econophysics, economic data are investigated from a physics perspective (Mantegna and Stanley 1991; Saichev et al. 2009), and various statistical laws and universality in firm activities have been studied (Pareto 1897; Newman 2005; Clauset et al. 2009; Bonabeau and Dagorn 1995; Render 1998; Fujiwara and Aoyama 2010; Aoyama et al. 2000; Yamano 2004; Mantegna and Stanley 1995; Axtell 2001; Podobnik et al. 2010; Fu et al. 2005; Podobnik et al. 2008; Okuyama et al. 1999; Gopikrishnan et al. 1998; Mizuno et al. 2002; Nirei and Souma 2007; Iyetomi et al. 2012; Levy and Solomon 1996; Sornette and Cont 1997; Takayasu et al. 1997; Fujiwara et al. 2003, 2004; Ishikawa 2006, 2009; Gibra 1932; Sutton 1997; Ishikawa 2006, 2007; Tomoyose et al. 2009; Ishikawa et al. 2011; Coad 2010a, b; Bottazzi et al. 2008; Miura et al. 2012; Fujiwara 2004; Ishikawa et al. 2015a, b; Coad 2009; Ishikawa et al. 2016; Mizuno et al. 2014; Luttmer 2011; Petersen et al. 2012; Bureau van Dijk; Statistics Bureau; Matia et al. 2004; Takayasu et al. 1997). The distribution of social quantities at a point in time has been frequently investigated. Power law distribution, which is observed in the large-scale range, is well known (Pareto 1897). If firm’s sales, assets, and number of employees (firm size variables) in calendar year T are signified by \(x_T\), the distribution obeys a power law over size threshold \(x\)
th in a number of years and countries as follows: Here, \(P(x_T)\) is the probability density function (PDF) of \(x_T\), and exponent \(\mu\) is called Pareto’s index. Although firm size \(x_T\) varies annually, power laws have been surprisingly confirmed in the large-scale range. At the same time, Pareto’s index of firm size data in Japan has not fluctuated for a 30-year period (Mizuno et al. 2002). This is also an interesting property. On the other hand, in the middle-scale range, firm size variables \(x_T\) under size threshold \(x_\mathrm{th}\) obey the following log-normal distribution: Here, \(\alpha\) is a parameter related to the standard deviation of the log-normal distribution, and \(x_0\) is a parameter related to the logarithmic mean value. In firm size data of two successive years \((x_T,x_{T+1})\), statistical laws are also observed, such as inverse symmetry, quasi-inverse symmetry, Gibrat’s law, and non-Gibrat’s law. Inverse symmetry means that the system is symmetric under the time reversal exchange of variables \(x_{T+1} \leftrightarrow x_{T}\), and the system is represented using joint PDF \(P_J(x_T,x_{T+1})\) as follows (Fujiwara et al. 2003, 2004): Quasi-inverse symmetry, which denotes that the system is quasi-static, is expressed symmetrically under the exchange of variables \(x_{T+1} \leftrightarrow a~{x_{T}}^\theta\) as follows (Ishikawa 2006, 2009): Here, a and \(\theta\) are parameters. The important point in Eqs. (3) and (4) is the invariance of functional form \(P_J\) under exchange of variables \(x_{T+1} \leftrightarrow x_{T}\) or \(x_{T+1} \leftrightarrow a~{x_{T}}^\theta\). Gibrat’s law (Gibra 1932; Sutton 1997), which is observed in the large-scale range like the power law, is represented as 
\(R \equiv x_{T+1}/x_T\) is the growth rate of the firm size variables, and \(Q(R|x_T)\) is the conditional PDF. Equation (5) denotes that \(Q(R|x_T)\) does not depend on initial value \(x_T\) over size threshold \(x_\mathrm{th}\). We observed non-Gibrat’s law in the middle-scale range like in log-normal distribution and showed that in the case of firms’ positive profits, PDFs are approximated (Ishikawa 2006, 2007; Tomoyose et al. 2009): Here, \(r \equiv \log _{10} R\) is a logarithmic growth rate and \(q(r|x_T)\) is a conditional PDF that is related to \(Q(R|x_T)\) by \(\log _{10} q(r|x_T)=\log _{10} Q(R|x_T)+r+\log _{10}(\ln 10)\). Non-Gibrat’s law shows the dependence of \(q(r|x_T)\) on \(x_T\) under size threshold \(x_\mathrm{th}\). Note that when \(\log _{10}q(r|x_T)\) is described by the linear functions of r, under inverse symmetry, the dependence of \(q(r|x_T)\) on \(x_T\) is uniquely determined by Eqs. (6) and (7), which we call the first non-Gibrat’s law. In the case of firm sales, \(q(r|x_T)\) can be approximated (Tomoyose et al. 2009; Ishikawa et al. 2011): Here, \(c,~C_1,~C_2,~C_3\), and \(\alpha\) are parameters and \(r_c\) is a cut-off parameter. In this case, if \(q(r|x_T)\) is described by the quadratic functions of r, and \(\log _{10}q(r|x_T)\) does not depend on \(x_T\) in the range of \(-r_c<r<0\), under inverse symmetry, the dependence of \(q(r|x_T)\) on \(x_T\) is uniquely determined by Eqs. (8) and (9), which we call the second non-Gibrat’s law. Note that laws at a point in time are derived from short-term laws observed in two successive years. First, the power law (1) in the large-scale range is derived from Gibrat’s law (5) under inverse symmetry (3) Fujiwara et al. (2003, 2004). Second, the log-normal distribution (2) is inferred from the first non-Gibrat’s law (6), (7) or the second non-Gibrat’s law (8), (9) under inverse symmetry (3) (Ishikawa 2006, 2007; Tomoyose et al. 2009; Ishikawa et al. 2011). Third, quasi-statistically varying power law and log-normal distributions are analytically derived from Gibrat’s law and non-Gibrat’s law under quasi-inverse symmetry (4), respectively Ishikawa (2006, 2009). These three analytical derivations are confirmed using firms’ positive profits, sales, and publicly assessed land values in Japan. The study of laws observed in firm size distributions at a point in time has a long history, and the laws at a point in time are recently related to the short-term laws observed in two successive years. Next, we concentrate on long-term properties that are statistically observed. This subject has not been as deeply investigated as studies of the laws at a point in time or short-term laws. Few studies have approximated the firm age distribution by an exponential function (Coad 2010a, b; Bottazzi et al. 2008; Miura et al. 2012): Here, t is a firm’s age, P(t) is the PDF, and \(\lambda\) is a parameter. A similar property was identified in bankrupt firm age distribution (Fujiwara 2004). In a previous study, we showed that exponential firm age distribution is explained by the constant decay rate of firm activities in Japanese firms (Ishikawa et al. 2015a). To the best of our knowledge, this was the first idea that derived firm age distribution in the long term from the decay rate of firms observed in the short term. We found that the decay rate of firm activities in the United States decreased exponentially as firms age. Using this property, the firm age distribution, which deviates from an exponential function, is analytically derived and confirmed in empirical data in the United States (Ishikawa et al. 2015b). At the same time, we examined the firm growth (Coad 2009), for instance) from short-term laws. From the extended Gibrat’s law and Gibrat’s law observed in the short term, we analytically derived that the geometric mean value of firms’ sales rapidly grows under a power law function and gently grows under an exponential function with a small exponent. This result was confirmed in the sales data of Japanese firms (Ishikawa et al. 2016). In this study, GDP data, which are employed in economics, were replaced by aggregating around a million firms. This up-to-date method is possible using big data. In this study, based on these earlier works, we investigate the long-term growth not only of firms’ sales but also of the number of employees in Japan and France to confirm the growth property observed in the firms’ sales data in Japan. The rest of our paper is organized as follows. In Sect. 2, our database is described. By analyzing the exhaustive data of firms’ sales and the number of employees in these two countries, we find the growth properties of the geometric mean values that resemble those of the sales of Japanese firms. In Sect. 3, Gibrat’s law and non-Gibrat’s law are confirmed as short-term laws observed in two successive years that employ the sales data of Japanese firms as one example of firm size variables. In Sect. 4, we introduce a stochastic model based on short-term laws in Sect. 3 and show that the model leads to power law growth and the subsequent exponential growth shown in Sect. 2. The last section concludes this paper.",5
13.0,2.0,Evolutionary and Institutional Economics Review,31 October 2016,https://link.springer.com/article/10.1007/s40844-016-0061-2,Constructing of network from topics and their temporal change in the Nikkei newspaper articles,December 2016,Shinya Kawata,Yoshi Fujiwara,,Male,Male,Unknown,Male,"Every day, new information is transmitted. We need to be aware of the changes in information that is to be updated. Newspapers, despite a change in their transmission medium, have been providing information to people for many decades. In the current information transmission methods, such as social networks, which are represented by Twitter and Facebook, people are able to obtain a wide variety of information. In the case of the newspaper Nikkei, there are roughly 125,000 articles per year; articles are classified into economics, politics, and sports. However, we search for relationships not only in the classified areas but also unclassified areas of the newspaper. It is important to understand changes in society; thus, we extract temporal topics and changes in relationships among articles. This study produces two significant results. First, we develop an analysis method that evaluates long-term newspaper article data. Second, we construct a network using the extracted topics by categorizing articles using latent Dirichlet allocation (LDA) (Blei et al. 2003; Blei 2012). The network calculates the topic similarity coefficient for each month, which is created by connecting topics with high similarity coefficients of capsules of the topic. For analysis that takes into account the temporal change when the topic of this information originates from a social network, such as newspaper articles, blogs, and twitter, techniques such as DTM (Blei and Lafferty 2006) or the topic tracking model have been proposed (Iwata et al. 2009). These approaches (Kim and Oh 2011; Wang and McCallum 2006; Bolelli et al. 2009), which group data sets together, are intended to capture time-series changes. However, our interest is topics that arise repeatedly throughout the year, treating the terms as continuous regarding the topic, whether the topic is going to disappear, the phenomenon that creates the expression, and differentiation and disappearance of newspaper articles, which can be extracted from the topic in different ways (Kelinberg 2003; Takahashi et al. 2011). To extract this information, it is necessary to consider similarities among topics based on relationships in different data sets. By further clarifying the relationships of each topic, it is possible to determine whether strong associations with any topic words show any comparable behavior in the past. In this paper, the analysis method presented in Sect. 2, which describes LDA, uses article data from the Nikkei newspaper from 2012 and 2013. The results and discussion are presented in Sect. 3. The results are summarized and directions for further study are presented in Sect. 4.",4
13.0,2.0,Evolutionary and Institutional Economics Review,10 September 2016,https://link.springer.com/article/10.1007/s40844-016-0054-1,Trading strategy of a stock index based on the frequency of news releases for listed companies,December 2016,Yoshifumi Tahira,Takayuki Mizuno,,Male,Male,Unknown,Male,"Asset prices are moved by an endogenous mechanism that follows past price changes created by trend followers and by an exogenous mechanism of reactions to sudden financial market news (Jiang et al. 2007; Sornette 2006). Investigation of historical tick-by-tick data of prices has shown that endogenous mechanisms follow a multiplicative process. This multiplicative process generates a fat tail of price changes (Newman 2005). Exogenous mechanisms have also been investigated using news archives and historical tick-by-tick price data (Mizuno et al. 2015; Hisano et al. 2013; Alanyali et al. 2013; Mizuno et al. 2012; Petersen et al. 2010; Rangel 2011). Natural language processing for news articles has identified a relationship between price changes and text words in news articles (Bollen et al. 2011; Schumaker and Chen 2009; Thelwall et al. 2010). Furthermore, investigations have addressed the relationship between people’s reactions to news and price changes. In prospect theory in behavioral economics, people are more concerned about avoiding loss than earning profit in stock trading (Tversky and Kahneman 1991). When there is a risk of a stock falling, people desperately gather information on this stock to avoid that risk. Therefore, the number of internet hits for the stock increases before its price falls. Preis et al. showed that the browsing frequency of Wikipedia pages for 30 companies listed in the Dow Jones Industrial Average (DJIA) is related to future DJIA changes (Preis et al. 2013; Moat et al. 2013; Curme et al. 2014). The number of Wikipedia page views increases before the DJIA falls. However, we cannot determine why people look at a Wikipedia page from only the number of views. Since news media concentrates on topics that attract attention, the number of news articles on a certain topic might increase before a stock market index falls, as in the relationship between the browsing frequency of Wikipedia pages and DJIA. In this paper, we show that the trading strategy performance of a stock index based on the frequency of news releases for the stock market is better than that of a trading strategy that randomly buys or sells the stock index. This result suggests that the frequency of news releases is related to future stock market conditions. We can understand why news attracts attention because news articles contain detailed information of events. The remainder of this article is organized as follows: In Sect. 2, we explain the Thomson Reuters’ news data. In Sect. 3, we introduce a stock index trading strategy based on the frequency of news releases for listed companies. In Sect. 4, we apply this trading strategy to the S&P500 index. In Sect. 5, we review typical news topics where the trading strategy performance is good. In Sect. 6, we list the annual performance from 2008 to 2011 and we discuss our conclusions in Sect. 7.",1
13.0,2.0,Evolutionary and Institutional Economics Review,22 September 2016,https://link.springer.com/article/10.1007/s40844-016-0057-y,Power laws in market capitalization during the dot-com and Shanghai bubble periods,December 2016,Takayuki Mizuno,Takaaki Ohnishi,Tsutomu Watanabe,Male,Male,Male,Male,"Since B. Mandelbrot identified the fractal structure of price fluctuations in asset markets in 1963 Mandelbrot (1963), statistical physicists have been investigating the economic mechanism through which a fractal structure emerges. Power laws are an important characteristic in the fractal structure. For example, some studies found that the size distribution of asset price fluctuations follows power law Mantegna and Stanley (2000), Mizuno et al. (2003). In addition, it is shown that firm size distribution (e.g., the distribution of sales across firms) also follows power law Stanley et al. (1995), Axtell (2001), Okuyama et al. (1999), Mizuno et al. (2012), Clementi and Gallegati (2016). The power law exponent associated with firm size distributions is close to one over the last 30 years in many countries Mizuno et al. (2002), Fujimoto et al. (2011). The situation in which the exponent is equal to one is special in that it is the critical point between the oligopolistic phase and the pseudo-equal phase Aoyama (2010). If the power law exponent is less than one, the finite number of top firms occupy a dominant share in the market even if there are infinite number of firms. One of the most important issues along this line of research is power laws associated with asset price fluctuations, and several models describing asset price dynamics were proposed Richmond et al. (2013), Abergel (2016). In particular, asset price bubbles were regarded as an important research topic. For example, the PACK and LPPL models simulated the price fluctuations of a stock during bubble periods Sornette (2004), Takayasu et al. (2010). In economics and finance, asset price bubbles are defined as the deviation of the price of an asset from its fundamental value. However, it is not easy to obtain information about fundamentals. For example, it is often stated by economists that the fundamental stock value of a company equals to the present discounted value of dividends delivered by the company in the years to come. However, it is hard to get a reliable estimate of future dividends; therefore, it is next to impossible to estimate fundamental stock prices. Without accurate information on fundamentals, it is impossible to detect bubbles. This is a serious issue for policy-makers, like governments and central banks, since the emergence and the burst of bubbles often lead to intolerable economic disasters, such as financial crisis. The purpose of this paper is to propose a method to detect stock price bubbles in a timely manner. Our basic idea is closely related to a method we proposed as a way to detect bubbles in the context of real estate prices Ohnishi et al. (2012). In this study on real estate prices, we looked for houses that are similar in various respects, including the location of a house, the size of a house, and the age of a house. We argued that houses with similar attributes can be regarded as having similar fundamental values; therefore, the prices for these houses should be similar if there are no bubbles in the housing market. Based on this idea, we looked at the distribution of house prices for houses with similar attributes, showing that it is close to a log-normal distribution during normal periods, but it has a heavy upper tail during bubble periods. In the present paper, we apply this idea to stock markets to detect stock price bubbles. In this paper, we use a dataset compiled by the Thomson Reuters Corporation that covers daily market capitalization and annual income statements of all the listed firms in NASDAQ and SSE from 1990 to 2015. This period includes the 2000 dot-com and 2007 Shanghai bubbles. We focus on the distribution of market capitalization in NASDAQ and the Shanghai stock exchange (SSE) for these bubbles. Kaizoji et al. showed that the upper tail of stock price distribution in the Tokyo stock exchange grew fat during the dot-com bubble period Kaizoji (2006a, b). However, if we accurately investigate the firm size in stock markets, not only the price but also the outstanding shares must be taken into consideration because share consolidation and splitting often occurs in the market. The rest of the paper is organized as follows. Section 2 examines the power law of market capitalization distribution using an expansion of the Castillo and Puig test Fujimoto et al. (2011), Malevergne et al. (2011), Del Castillo and Puig (1999), Hisano and Mizuno (2010). In Sect. 3, we observe that the power law index fluctuates around one, depending on economic conditions, and tends to become smaller during bubble periods. In Sect. 4, we find that net assets are most reflected in market capitalization for firms listed in NASDAQ during non-bubble periods. The price-to-book ratio (PBR, P/B Ratio) distribution, which is defined as market capitalization divided by net assets, got fat during the bubble periods. These results suggest that speculative money is excessively concentrated on specific stocks during bubble periods. Section 5 concludes the paper.",2
13.0,2.0,Evolutionary and Institutional Economics Review,12 August 2016,https://link.springer.com/article/10.1007/s40844-016-0051-4,Dynamical cross-correlation of multiple time series Ising model,December 2016,Tetsuya Takaishi,,,Male,Unknown,Unknown,Male,"Asset price returns are known to exhibit some universal properties that are now classified as the “stylized facts,” see, for example, Cont (2001). The notable properties included in the stylized facts are the fat-tailed distributions of returns and volatility clustering, which are not explained by the standard Gaussian process. A promising explanation on the asset price dynamics is the mixture of distribution hypothesis by Clark (1973), that is, the price return \(R_t\) is described by a Gaussian process with time-varying volatility, \(R_t=\sigma _t \epsilon _t\), where \(\sigma ^2_t\) and \(\epsilon _t\) are the volatility and standard normal random variable at time t, respectively. Under the mixture of distribution hypothesis, the volatility changes according to the rate of information arrival. Since the rate of information arrival is latent in the real markets, Clark uses volume as a proxy of it. Empirically, the price return dynamics is examined by checking whether \(R_t/\sigma _t\) recovers the standard normal random variable, and studies using the realized volatility claim that the price return dynamics is consistent with the Gaussian process with time-varying volatility (Andersen and Bollerslev 1998; Andersen et al. 2000, 2001a, b, 2007, 2010; Takaishi 2012; Takaishi et al. 2012). To simulate a financial market, Bornholdt proposed an Ising-based model by including Ising spins that have either of the two states, namely “buy” state and “sell” state (Bornholdt 2001). It is shown that the model successfully captures major stylized facts such as fat-tailed distributions and volatility clustering (Bornholdt 2001; Yamano 2002; Kaizoji et al. 2002; Krause and Bornholdt 2013). The model was extended to a Potts-like model where three spin states are considered, and it is confirmed that the Potts-like model also exhibits the stylized facts (Takaishi 2005). Further, the return dynamics of the Ising-based models were checked by testing whether \(R_t/\sigma _t\) recovers the standard normal random variable, and it is verified that the return dynamics is consistent with the Gaussian process with time-varying volatility same as the real financial markets (Takaishi 2013a, 2014). The real financial market is a complex system that includes many stocks correlated with each other. Measuring correlations among stocks is of great importance to investigate the stability of financial markets, and a considerable number of studies are devoted to unveil properties of correlations among stocks (Plerou et al. 1999; Laloux et al. 1999; Plerou et al. 2000, 2002; Utsugi et al. 2004; Kim and Jeong 2005; Wang et al. 2011). While the original Bornholdt model simulates only one stock, the model was extended to simulate multiple stock time series in Takaishi (2015a, b), and simulations including up to three stocks were done. In this study, we make a large-scale simulation that includes 100 stocks and investigate the dynamical properties of cross-correlations among stocks. Further, we apply the principal component analysis and measure the cumulative risk fraction to monitor the states of the financial system. Finally, we calculate the inverse partition function (IPR) and its higher power version and claim that they are also useful to investigate states of the system.",4
13.0,2.0,Evolutionary and Institutional Economics Review,11 August 2016,https://link.springer.com/article/10.1007/s40844-016-0050-5,A model of transaction signs with order splitting and public information,December 2016,Joshin Murai,,,Unknown,Unknown,Unknown,Unknown,,
13.0,2.0,Evolutionary and Institutional Economics Review,09 September 2016,https://link.springer.com/article/10.1007/s40844-016-0053-2,Investment time horizon and multifractality of stock price process,December 2016,Koji Kuroda,,,Male,Unknown,Unknown,Male,"In this article, we construct a multifractal random walk for log-return process \(X_t= \log S_t/ S_0\) from time 0 of a stock price process \(S_t\) using a method of abstract polymer expansion developed in the study of mathematical physics. Let us consider a stochastic process \(X_t\) with stationary increments and \(X_0=0\). If \(X_t\) satisfies for non-linear function \(\zeta _q\) of q and for some positive constant \(K_q\) and \(T>0\), then we call \(X_t\) multifractal process or multifractal random walk. In case of a linear function \(\zeta _q\), it is called monofractal process. Self-similar processes such as Brownian motion and fractional Brownian motion are monofractal processes. Multifractality is obtained from the condition called continuous cascade equation, that is, where \(W_\lambda\) is a positive random variable independent of \(X_t\) satisfying 
\(\log W_\lambda \sim\)
N(0, \(-c_2 \log \lambda )\), and \(\sim\)means an equality of their probability distributions. Then \(X_t\) satisfies the multifractality, Using a stochastic integral with respect to a Brownian motion we construct a multifractal random walk \(X_t\) by where \(w_r(u)\) is a log-volatility process which is a Gaussian process and satisfies where \(D_\lambda\) is independent of \(w_r(t)\) and \(D_\lambda \sim\)
N
\((0,-f_1 T \log \lambda\)) and \(\alpha >0\) is a timescale parameter. Remark that \(e^{w_r(t)}\) plays a role of volatility of log-return process at time t, and the relation (3) implies the continuous cascade equation for \(\{w_r(t)\}\). The log-volatility process \(w_r(t)\) has a singularity at \(r=0\), where positive constants \(f_1\) and \(f_2\) will be given in a sequel. This singularity makes it difficult to define a multifractal random walk, and we cannot exchange the order of limit \(r \downarrow 0\) and the integral in (2). A multifractality of \(X_t\) follows from the continuous cascade Eq. (3) for \(\{w_r(t)\}\). Bacry and Muzy (2003) constructed a multifractal random walk \(X_t\) in this form (2) with respect to a Brownian motion \(B_u\). In their model \(w_r(u)\) is defined by a regionary independent random media (measure). The regionary independence means that there is no interaction among random media. Their result was extended to a case of a fractional Brownian motion (see Abry et al. 2009; Fauth and Tudor 2014). Another way to define a multifractal random walk is a method of subordination (see Mandelbrot 1974, 1997): where \(B_u\) is a self-similar process such as Brownian motion and fractional Brownian motion and M(0, t) is a random measure on [0, T] satisfying a similar condition to the continuous cascade Eq. (1). If \(B_u\) is a Brownian motion, definition (2) coincides with the definition (4). In this article, we first construct a log-volatility process \(w_r(t)\) as a scaling limit from a discrete time process \(W_r^{(n)}(t)\) defined on a discrete time interval \([1,n^\alpha T]\). We consider a random media with inverse power law interaction and apply the abstract polymer expansion by Kotecky and Preiss (1986) to our model to obtain a scale limit, where c(n) is a scale function well matched with the timescale \(n^\alpha t\) and is given in Theorem 1. A notion of investment time horizon is introduced in our model and its probability distribution is given by inverse power law. The method of abstract polymer expansion has been investigated in studies of phase transitions in lattice spin systems (Kotecky and Preiss 1986; Brickmont et al. 1985), and applied to finance models for long memory (Kuroda et al. 2011, 2013). After obtaining a scale limit (5), we find a condition for the continuous cascade Eq. (3), and construct a random measure M(0, t). Finally, we construct a multifractal random walk \(X_t=B_{M(0,t)}\).",
14.0,1.0,Evolutionary and Institutional Economics Review,03 May 2017,https://link.springer.com/article/10.1007/s40844-017-0071-8,Theoretical model of institutional ecosystems and its economic implications,June 2017,Takashi Hashimoto,Makoto Nishibe,,Male,,Unknown,Mix,,
14.0,1.0,Evolutionary and Institutional Economics Review,01 February 2017,https://link.springer.com/article/10.1007/s40844-017-0067-4,Revisiting the existing notion of continuous improvement (Kaizen): literature review and field research of Toyota from a perspective of innovation,June 2017,Shumpei Iwao,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Evolutionary and Institutional Economics Review,29 July 2016,https://link.springer.com/article/10.1007/s40844-016-0047-0,Limited access states and elections: an unexpected economic consequence,June 2017,Richard Grabowski,,,Male,Unknown,Unknown,Male,"Douglas North along with J.J. Wallis and B.R. Weingast published an influential book focused on long-run social and economic development in 2009. It was titled Violence and Social Orders: A Conceptual Framework for Understanding Recorded Human History. It was essentially an analysis of economic history which focuses on stages. Specifically, there were three historical stages: hunter gatherer societies, limited access orders or the natural state, and open access orders. Most of the attention of North and his coauthors was focused on the latter two stages. The organizing principle for the development of their analysis revolves around the problem of violence. Human history has been focused on the development of institutions and organization aimed at limiting and containing violence. The initial type of social structure which evolved to deal with the problem of violence was the natural state or the limited access order. This social order was not only one of the earliest types to evolve, but it remains to this day the predominate social mechanism for constraining violence and providing social order for most of the nations of the world. In a nutshell, in this type of social organization dominant political coalitions of powerful groups are created by developing institutional structures which limit access to economic resources, thus creating economic rents that can be used to reward powerful members of the ruling coalition. An alternative basis for social order is provided by open access societies, which began to evolve in the sixteenth and seventeenth centuries and serve as the foundation for a limited number of states today. In a nutshell, these societies are characterized by economic and political institutions which allow unhindered access to economic and political resources and the formation of economic and political organizations is also relatively unhindered. The resulting political and economic competition limits the creation of economic rents (for any long period of time) and constrains the exercise of power by the state. There is no historical process that automatically results in historical evolution from limited access or natural states into open access societies. There are certain doorstep conditions which North et al. (2009) argue are necessary, but not sufficient, for the development of an unlimited access social structure out of the natural state. However, what is more important for the purposes of this paper is another aspect of the theory. Economic and political reforms carried out in a limited access society often have very different outcomes from the same reforms applied in an unlimited access society. For example, market reforms carried out in the former often have dramatically different effects from these same reforms in the latter. Institutions constructed in the former often have quite different impacts relative to the operation of the same type of institution in the latter. In particular, in this paper the focus will be on the impact of the establishment of institutions forming the basis for political elections in which parties compete for political power within the context of developing countries which are limited access societies. It will be argued here that elections determining the control over political power have had some unexpected consequences for the process of structural change in these developing countries. Structural change plays a critical role in the overall development process. All successful examples of rapid economic development have involved a dramatic shift in the composition of production and employment. As structural change occurs, the share of agriculture in terms of both GDP and employment decline with the rate of decline of the latter being generally slower than in the former (Timmer 2009). The shift of labor and production has typically been into the manufacturing sector with the share of manufacturing in both production and employment rising. Eventually, as economic development continues modern sector services begin to play an increasingly important role with the share of production and employment in manufacturing declining. This process has been incorporated into models of dualistic economic development of which the most important was, arguably, that constructed by Lewis (1954). These models generally involve at least two sectors with significant labor productivity differences between them. For example, a traditional agricultural sector in which, for a variety of reasons, labor productivity is low and a modern manufacturing sector in which labor productivity is much higher. As a result, structural change involves labor moving from a sector where labor productivity is low to a sector where it is high. Thus, there is a comparative static gain resulting from structural change. Recently, it has been argued by Rodrik (2013) that there is also a dynamic gain from structural change. Specifically, unconditional convergence occurs in manufacturing. Thus, manufacturing sectors established in developing countries often find labor productivity in these sectors rapidly converging to that found in developed countries. Thus, there is a dynamic gain which comes from structural change which complements the comparative static gain. However, the pattern of structural change recently seems to have altered over the last several decades. McMillan and Rodrik (2011) have found that while labor does seem to have shifted out of agriculture in many developing countries, it has not moved into manufacturing. Instead there seems to have been a movement of labor into traditional sector services where overall labor productivity is not much higher than in traditional agriculture. The work of Amirapu and Subramanian (2015) goes further and argues that much of the developing world is experiencing premature deindustrialization. If one plotted the relationship between the share of a country’s total employment in industry against the level of economic development (measured by GDP per capita) for developing countries, one would find an inverted U shaping with the share of employment in industry rising, reaching a peak, and then declining as GDP per capita rises. However, as time has passed (1988–2010) the curve has shifted downward implying that at any given level of GDP per capita, the share of manufacturing in total employment has declined. In addition, the curve has also shifted to the left meaning that the level of GDP per capita at which the share of industry peaks is occurring at lower and lower levels of GDP per capita. Developing countries find industry and manufacturing slipping away before they become developed (premature deindustrialization). Some have argued that this is the result of technical change and the globalization process. For example, Baldwin (2011) has argued that dramatic changes in technology have led to an unbundling of the manufacturing process. In the early postwar period the development of a manufacturing sector involved a country in the difficult process of constructing a domestic supply chain resulting in the production of final manufactured goods. However, technical innovation has allowed the production process to be unbundled with different parts of the supply chain now being located in different parts of the world. As a result, the share of manufacturing in any particular economy is likely to be significantly lower. Subramanian and Kessler (2013) argue that one characteristic of the globalization process has involved the dematerialization of economic activity. That is, world economic growth has increasingly involved trade and the production of services. They characterize this as moving from the production of stuff to the production of “fluff” (tangibles to intangibles). Thus, one would expect that manufacturing would decline in importance not just for developed countries, but also for developing countries. While arguments based upon the evolution of technology and the global trading network would seem to be important, a much simpler explanation arises out of the thinking concerning the natural state and the impact of creating some characteristics of democracy, elections and party competition, within the context of the natural state. In limited access societies in which elections are not part of the ruling process, the agricultural sector and the bulk of its participants are relatively powerless. Why does this sector remain powerless when the bulk of the population resides in this sector and it produces a significant share of total GDP? The work of Olson (1971) provides an answer. The interests of the rural sector can be thought of as a collective good. Thus, efforts to organize farmers to achieve these collective interests are likely to suffer from free riding and are thus unlikely to succeed. In addition, contradictions in interests may occur within agriculture which hinders organization for the achievement of political power. Elections involving multiple parties contesting for power do not mitigate the collective good nature of organization in agriculture, but the rural sector now becomes the potential source for a large number of votes. These votes can be manipulated via the use of patron–client style politics in which members of the rural sector are provided rents in return for votes. These rents can be provided by increasing the prices received for output and/or reducing the costs of production, via input subsidies. This implies that the ruling coalition must be adjusted so as to incorporate some of the interests of the agricultural sector. It is argued here that the increase in prices and reduction of costs, via input subsidy, for agricultural products will divert resources away from the manufacturing sector and this will tend to cause a process of deindustrialization. Perhaps more importantly, this also raises the relative cost of labor causing existing domestic manufacturing to become increasingly capital intensive, even though labor may be physically abundant. Moreover, the increase in rents going to the agricultural sector is likely to increase the ability of the rural sector to organize for political purposes since the rewards to such organization will have risen dramatically. Thus, pressures to expand rent creation for the rural sector or parts of the sector are likely to increase through time. Thus, deindustrialization, rising capital intensity in manufacturing, and thus slow employment growth will become characteristics of the development process. The rest of the paper unfolds as follows. Section 2 will discuss the natural or limited access state in a little more detail and present empirical evidence that the creation of some aspects of democracy (voting, elections, political parties, etc.) has led to increases in prices and other support for the agricultural sector. Section 3 will present some theory concerning the economic impact of such policies on the development of manufacturing. An institutional failure common to most developing regions will also be discussed. Finally, Sect. 3 will present two examples of the process: the Philippines and Indonesia.",2
14.0,1.0,Evolutionary and Institutional Economics Review,18 April 2017,https://link.springer.com/article/10.1007/s40844-017-0072-7,Property bubble in Hong Kong: A predicted decade-long slump (2016–2025),June 2017,Peter Richmond,Bertrand M. Roehner,,Male,Male,Unknown,Male,"In addition to being an economic phenomenon, speculative bubbles have important social aspects. They give rise to a speculative frenzy which leads buyers to forget previous episodes that ended in disasters.Footnote 1
 The sheer size of property bubbles ensures they are one of the most important speculative bubbles. Their main characteristic is that during such episodes prices are lifted up by the dynamics of speculation to the point that in the final stages of the bubble, supply and demand or interest rate levels become largely irrelevant.Footnote 2
 It is this aspect which allows the effect to be described and predicted by a fairly simple mechanism as demonstrated in previous papers (Roehner 2001, 2004; Richmond 2007; Richmond and Roehner 2013). In such episodes one can define three prices which provide a schematic description of the price trajectory: the price \(p_1\) at the beginning of the upward phase, the price \(p_2\) at the top of the peak and the price \(p_3\) at the end of the downward phase. In relation with these prices, one can also define the corresponding amplitudes: \(A_1=p_2/p_1\) and \(A_2 =p_2/p_3.\) Finally, the times \(T_1, T_2\) and their ratio \(\theta =T_2/T_1\) define the temporal shape of the price trajectory (Fig. 1). The basic rule which emerges from the study of previous historical episodes is that \(p_3\) is only slightly higher than \(p_1\). In short, the more prices go up, the more they must come down. Moreover, the duration \(T_2\) of the declining phase is only slightly shorter than the duration \(T_1\) of the rising phase. In other words, the most basic approximation (a more accurate picture will be given subsequently) is that speculative price peaks are symmetrical with respect to their two phases.Footnote 3
 Needless to say, such predictions cannot take into account external factors such as government interventions or major disruptions (e.g., the worldwide financial crisis of 2008). In other words, to test if our understanding is correct one needs instances of property bubbles in which there is minimal incidence of exogenous factors. In a general way the two most common exogenous factors are (i) changes in interest rates (ii) government subsidized loans to prop up the market. In HK there were no such changes for two different reasons. The first is that interest rates remained near zero for the largest part of the speculative episode because they followed US rates (see subsequent explanations about the currency board monetary system). The second reason is the existence of an important government sponsored housing sector, a factor which removes the temptation to subsidize the private sector. An exogenous factor that is specific to HK is the interference of investors from the mainland. In recent years Chinese investors have been active in several foreign real estate markets (e.g., in Australia) and also in HK. The rules set by the HK and Beijing governments are crucial in this respect. This is a cause of uncertainty. For our research an important advantage of the Hong Kong case was the fact that the website of the “Rating and Valuation Department” of the Hong Kong government provides very detailed data not only for prices, but also for rents. 
a Definition of the parameters which describe the price peak of a speculative episode. The peak is modeled on the speculative episode which took place in London between 1982 and 1993. b Recurrent speculative episodes in Hong Kong. The key-feature is that yield (i.e., annual rent divided by price) and price are moving in opposite directions; the basic reason for that is that rents have much more inertia than prices. Here yields and prices have a negative correlation of −0.76 (the 0.95 confidence interval is −0.88,−0.54). The prices are annual prices for apartments of less than 40 sq.m in the “New Territories” which constitutes by far the largest part of the Hong Kong Territory. The prices of the last quarter of 2015 and first quarter of 2016 were added to show the beginning of the downturn. This size-class was selected because it has the largest number of transactions and more transactions result in a smoother curve; the curves for the other classes are parallel to this one. It can be noted that between 2003 and 2015 the wages were practically stagnant. The dotted curve on the right gives a rough projection based on previous episodes. A more precise prediction will be given below. The numbers within parenthesis give the amplitudes of the upward and downward phases of the respective peaks; the question marks indicate that the corresponding amplitudes cannot be measured because the data are not available. The yield scale (expressed in percent) is on the right-hand side. Source: Rating and Valuation Department of the Hong Kong Government. “Trading Economics” website for the evolution of wages From its start around 2003 to its climax and burst in the fall of 2015 (Fig. 1b), the rising phase of the Hong Kong residential property bubble offers so to say an ideal text-book case of a speculative bubble. Why? To answer this question we need to outline the mechanism through which property bubbles spring up. The mechanism which leads to a property bubble involves the following stages, In the initial phase of the process low interest rates facilitate purchases by persons who “need a roof over their head” and buy an apartment to live in it. To distinguish them from investors and following a standard terminology (Roehner  2004; Richmond and Roehner  2013), these persons will be called users. As the average price level goes up users are progressively shut out from the market with the result that the proportion of the transactions done by investors increases steadily. Among investors a further distinction is in order between those who seek a yield (we call them y-investors) and those who seek a capital gain (we call them c-investors). For c-investors the price level is irrelevant provided they can resell at a higher price. However, because the progression of the rent is bounded by the income of tenants, the yield (i.e., annual rent divided by the purchase price) is bound to dwindle as prices shoot up. Thus, y-investors are also progressively driven out of the market. A market dominated by c-investors is necessarily very unstable because any substantial downward oriented price fluctuation may trigger a market downturn. The previous process corresponds to the ideal case where there are no government interventions. The government may try to “cool off” the market for instance by increasing interest rates, by making market access more difficult for investors who hold already several properties or by increasing taxes on the profit generated by short-term sales. Moreover, following the burst of the bubble, the government can prop up the market by bailing out property developers or by buying unsold buildings to diminish excess inventories. A good illustration of this kind of anti-cyclic measures is provided by the policy of the South Korean government from 2007 to present (i.e., 2016). In a first phase the measures were aimed at cooling the market whereas in a second phase (still underway) their purpose was to revive it. In recent years Singapore and China provided two other examples of government interventions in property markets. In a general way government intervention is the rule rather than the exception. On the contrary, in Hong Kong there were few interventions. As a consequence of Hong Kong’s currency board monetary system (more detailed explanations are given below), the (real) short-term interest rate necessarily follows the US rate. Thus, the fact that US rates have been close to zero over half a decade provided a fertile ground for the development of the Hong Kong bubble. As the market downturn occurred only 6 months ago, we do not yet know if the government (whether the Hong Kong government or the central government) will come to the help of bankrupt property developers or whether it will prop up the market in some other ways. That is why one should distinguish two parts in the present paper. In the first part we analyze the rising phase and show that almost all the patterns that one expects to observe were indeed displayed. This includes the following effects, The recurrence effect by which we mean that the 2003–2015 price development was basically the same (in length and rate) as in the previous episode of 1985–1997. The decreasing yield effect by which we mean that the rent/price ratio decreased from about 5.6% at the start of the bubble to about 3% at its peak. Equivalently, this change can also be expressed in a way more commonly used in stock markets by saying that the price earnings ratio (price/rent) increased from 18 to about 33. The price multiplier effect by which we mean that for different kinds of apartments the amplitude of the price peak is an increasing function of their initial price. The scope of application of the price multiplier effect is by no means limited to property prices. It can also be detected in the price peaks of many other items for which speculation can take place, e.g., stocks, collectible stamps or antiquarian books (Maslov and Roehner 2003; Roehner 2000, 2001 particularly the chapter entitled “Price multiplier effect”). In the second part of the paper we offer a prediction for the trajectory of the price fall during the decade 2016–2025, but it must be emphasized that this prediction rests on the assumption of minimal exogenous interference. If there are major changes in the organization of the market the prediction will no longer apply. First, we explain our analytical description of speculative peaks. This description does not only apply to property prices, but to all kinds of price bubbles including commodity price bubbles (see Roehner 2001, p. 158). Secondly, we will see that the values of the parameters \(\alpha\) and \(\tau\) (defined in the section about prediction) describing the rising phases (1987–1997) and (2003–2015) are fairly similar which leads naturally to the assumption that they will remain similar for the downward phase. The Hong Kong property market has two components each of which represents about one half of the market. The public housing part is subsidized by the Hong Kong government. It offers rental housing (about 30% of the whole market) and subsidized sales (about 18%). Private housing (about 52%). All the price and rent data used subsequently are for the private housing part. The coexistence of a public and private sector is a feature shared by many cities, for instance, Singapore or Paris. What is the global value of residential real estate in Hong Kong? According to official figures for 2015, there were 3.7 million persons in private sector apartments. The average living space per person was 13 square meters and the average price was about HKD 100,000 per square meter. (Hong Kong Housing Authority 2015). Thus, one gets the following estimate for the private housing stock: As a check one can do an alternative calculation based on the number of flats. The same report of the Housing Authority tells us that in 2015 there were 1.5 million flats in the private sector. Assuming an average apartment size of 40 square metersFootnote 4 one gets: The public housing sector comprised 1.2 million flats. However, as their price is not well defined we will limit our estimate to the private sector. For the sake of simplicity let us keep 5 trillion HKD as our estimate. To give a clearer interpretation it will be helpful to translate it into renminbi and USD. With the exchange rates of May 2016 (1 RMB = 1.19 HKD, 1 USD = 7.76 HKD) one gets: To get a better sense of the last estimate it can be compared to the following figuresFootnote 5: The revenue of the Hong Kong government in fiscal year 2014–2015 was HKD 0.48 trillion (Census and Statistics Department of Hong Kong) equivalent to) 0.08 times the value of the Hong Kong housing stock (at 2015 price level). The revenue of the Chinese government in 2014 was RMB 6.5 trillion, equivalent to 1.5 times the value of the HK housing stock in 2015. The value of US Treasury securities held by China at the end of July 2015 was USD 1.24 trillion equivalent to 1.9 time the 2015 value of the Hong Kong housing stock. Because the items 1 and 2 are annual flows they must be compared with changes in the housing stock; for instance one may assume a fall of 10% per year which would represent HKD 0.5 trillion per year which equals the revenue of the Hong Kong government. In short, a collapse of the HK property market at an annual rate of 10% per year may have a substantial impact not only in Hong Kong, but also in mainland China. Through its sheer magnitude, such a property crisis may also have geopolitical implications. We will come back to this question at the end of the paper. Incidentally, the collapse of the previous property bubble was smaller in magnitude for two reasons: (i) The peak price in 1997 was only about one half of its value in 2015 (in constant HKD). (ii) The amplitude of the peak (with respect to its initial level) was also only about one half its value in the second bubble, which means that annual changes were almost 4 times smaller. In many countries, for instance the United States, the cycle of office property is not synchronized with the cycle of residential property. However, in Hong Kong the apartment cycle is not only synchronized with the office cycle, it is also synchronized with the market for retail commercial property and that for flatted factories.Footnote 6 The prices of office real estate started to fall in the third quarter of 2015 after having reached a price level 5.5 higher than the level of 2003 (in constant HKD). If real estate prices are not propped up in some way, the simultaneous downfall of the residential and office markets may create great difficulties.",1
14.0,1.0,Evolutionary and Institutional Economics Review,22 May 2017,https://link.springer.com/article/10.1007/s40844-017-0075-4,The mutual effects between the fiscal relations of central and local governments and economic growth in post-reform China,June 2017,Zhongren Zhang,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Evolutionary and Institutional Economics Review,19 December 2016,https://link.springer.com/article/10.1007/s40844-016-0064-z,Relationship between population density and population movement in inhabitable lands,June 2017,Shouji Fujimoto,Takayuki Mizuno,Tsutomu Watanabe,Unknown,Male,Male,Male,"Population is one of the most basic indices for estimating human statistics. The relationship between population density and various values has been investigated by several researchers. The correlation between population density and GDP was reported in Sutton et al. (2007) and Pan et al. (2013). The relationship between urban density and transport-related energy consumption was reported in Newman and Kenworthy (1988); Lefèvre (2009). The correlation between population density and crime was investigated in Hanley et al. (2016). A model of urban productivity with the agglomeration effect of population density was estimated in Abel et al. (2012). Models that demonstrate a negative correlation between population density and human fertility has been proposed (Lutz et al. 2007). The concept of density is effective when dealing with an object that has a simple internal structure. By estimating the density of a region, we regard the region as having uniform density. However, it is necessary to be careful when dealing with complex social phenomena. Population density is normally obtained by dividing population by land area. However, the area inhabited by people is not the entire land area. We introduce definition for three types of land area to compare differences in population density. We focus on population movement as it is easy to observe the effects on population density. Each person moves for various reasons, such as proceeding to the next stage of education, finding employment, or marriage. Regardless of reason, it is considered that the majority of people move from low population density places to high population density places. The result of these movements is the phenomenon of population agglomeration. Agglomeration effects have been modeled in several papers (Fujita and Ogawa 1982; Berliant and Konishi 2000; Ciccone 2002). This effect is related to various problems in modern society, such as population explosion in developing countries, aging in developed countries, and immigration issues. It is important to establish a method of analyzing agglomeration effects from empirical data. In this study, we propose a method of analyzing agglomeration effects by observing relationship between population density and population increase or decrease. The data used for the analysis is census data and land use data for Japan. Population growth can be regarded as a function in terms of population density. We are then able to cleanly observe the phenomenon using integrated function because fluctuations in the original function are cancelled during integration. When aggregating a population and an area, it is necessary to determine regional units for aggregation. Regional units can be determined using various methods of spatial division. The division method usually occurs at the administrative level, such as borders of prefectures. However, using administrative divisions is not optimal for reasons such as historical factors. For these reasons, various spatial division methods were proposed in Holmes and Lee (2009), Rozenfeld et al. (2011) and Fujimoto et al. (2015). We analyze agglomeration effects under both administrative spatial division and square block spatial division. The results obtained do not depend on the method of spatial division. A key point in obtaining the results is definition of land area for evaluating population density.",4
14.0,1.0,Evolutionary and Institutional Economics Review,26 April 2017,https://link.springer.com/article/10.1007/s40844-017-0069-2,Special feature: the global energy transformation and human survivability,June 2017,Yuichi Ikeda,Dimiter Ialnazov,,Male,Unknown,Unknown,Male,,
14.0,1.0,Evolutionary and Institutional Economics Review,08 April 2017,https://link.springer.com/article/10.1007/s40844-017-0068-3,Fourth International Symposium on Human Survivability: the global energy transformation: a quest for solutions from the perspective of human survivability,June 2017,Dimiter Ialnazov,Yuichi Ikeda,Yosuke Yamashiki,Unknown,Male,Male,Male,"The theme of the Fourth International Symposium on Human Survivability was the shift from fossil fuels to cleaner energy sources that has been taking place around the world, or the global energy transformation. The symposium addressed a number of largely unanswered questions such as what should be the balance between the speed and tolerable costs of the global energy transformation. To avoid the dangerous impacts of climate change, we need to reduce our reliance on fossil fuels and use low-carbon sources of energy like the wind, the sun, hydropower, geothermal and nuclear energy. But if we do that very fast, isn’t it going to be excessively costly? Some other questions addressed at the symposium were: (1) Instead of “the shift to cleaner or low-carbon energy sources”, should we aim for a transition to 100% renewable energy (mainly wind and solar)? (2) What lessons could be drawn from an international comparison of the energy transformation in UK, Germany, and Denmark? (3) What solutions could be offered to the problem of grid instability that occurs due to the integration of renewables into the electricity system? (4) What solutions could future battery storage technologies offer? Furthermore, the Fourth International Symposium on Human Survivability aimed to provide a novel perspective on the global energy transformation based on a new integrated field of academic research called “human survivability studies” (HSS). To develop HSS, the faculty members and students at GSAIS strive to learn from the history of crises that have endangered the survival of mankind, as well as to analyze present vulnerabilities and future risks. The HSS also adopt a holistic and trans-disciplinary approach that goes beyond the discipline-based style of academic research. The symposium was organized as follows: First, the Dean of GSAIS, Prof. Shuichi Kawai gave an opening speech about human survivability studies. Then, Dr. Felix Christian Matthes delivered his keynote lecture about the challenges of the transition to a sustainable energy system. After that, we held three plenary sessions where we looked at the global energy transformation from the economic, environmental, and innovation perspectives, respectively. Each plenary session consisted of two presentations by invited speakers followed by a discussion. At the end, we had a panel discussion where we not only summarized various insights generated during the opening and the plenary sessions, but also touched on some issues that we could not discuss before due to time constraints.",
14.0,1.0,Evolutionary and Institutional Economics Review,18 January 2017,https://link.springer.com/article/10.1007/s40844-016-0066-x,Energy transition in Germany: a case study on a policy-driven structural change of the energy system,June 2017,Felix Christian Matthes,,,Male,Unknown,Unknown,Male,"Energy systems are characterized by long-lived capital stocks and significant inertia but have always been dynamic systems. On one hand, they have been endogenously driven by technology and the respective availability of natural resources for energy production. Starting from biomass and hydropower, the energy supply of modern societies shifted first to coal, was complemented by mineral oil and gas and partly by nuclear energy, before modern renewable energy sources like wind and solar reached a level of technology advancement that allowed them to play a more significant role. On the other hand and equally important, the energy sector has been perceived as a strategic sector for most industrialized societies for a long time and as such was always subject to political constraints and drivers. This political framing of the energy sector focuses typically specific primary energy supply structures, e.g., with regard to the use of domestic resources to ensure security of supply or protect domestic industries, but is also relevant for large network infrastructure projects which often have geo-strategic elements. Even if the energy systems changed significantly during the course of the twentieth century, most of these changes occurred as relatively soft changeovers (natural gas or nuclear fuel substituting coal in large steam generation power plants) or the phase-in of completely new segments of the energy system (e.g., the mineral oil-driven motorization based on the internal combustion engine). In distinction from these kinds of changes, a new type of energy system change becomes visible after the turn from the twentieth to the twenty-first century. It is essentially policy driven, is mainly based on risk considerations and actively addresses an accelerated structural change of the energy system, which is supported by major innovations that are—at least for some important elements (e.g., modern renewable power generation technologies) and at least for the commercialization part of the innovation chain—induced by the efforts of energy policy. In this paper, this new policy approach, termed in different jurisdictions and with different ambition levels as “energy transition”, “transition énergétique” or “Energiewende”, is defined as a policy-driven structural change of the energy system, with the combination of strong policy drivers and the structural dimension of the change forming the constitutive elements of the concept. Germany is one of the countries that embarked comparatively early on an energy transition policy of this kind, triggered by specific political circumstances in terms of policy awareness and governance structure, but also by its self-conception as an innovation-based economy with a strong focus on system solutions. This paper tries to summarize some of the lessons learned in the one and a half decades of practical experience gathered with energy transition policies in Germany and to indicate the emerging challenges and potential solutions ahead. Energy transition is, especially in the framework of deep decarbonization policies, relevant for the whole energy system, including transportation and the industrial, commercial and residential end-use sectors as well as other greenhouse gas emitting sectors (e.g., industrial processes, waste management, and agriculture). This paper addresses, however, exclusively the electricity sector, which is the largest single source of greenhouse gas emissions (40%) in Germany and—due to the electrification option—of strategic importance for other sectors in the context of ambitious climate policies.",19
14.0,1.0,Evolutionary and Institutional Economics Review,04 June 2016,https://link.springer.com/article/10.1007/s40844-016-0041-6,Economic hazards of a forced energy transition: inferences from the UK’s renewable energy and climate strategy,June 2017,John Constable,Lee Moroney,,Male,,Unknown,Mix,,
14.0,1.0,Evolutionary and Institutional Economics Review,12 August 2016,https://link.springer.com/article/10.1007/s40844-016-0049-y,The analyses on the economic costs for achieving the nationally determined contributions and the expected global emission pathways,June 2017,Keigo Akimoto,Fuminori Sano,Bianka Shoai Tehrani,Male,Unknown,Female,Mix,,
14.0,1.0,Evolutionary and Institutional Economics Review,21 January 2017,https://link.springer.com/article/10.1007/s40844-016-0065-y,The paradigm disruptive new energy storage Shuttle Battery™ technology,June 2017,Ryo Tamaki,Tomohiko Matoba,Hisashi Tsukamoto,,Male,Male,Mix,,
14.0,1.0,Evolutionary and Institutional Economics Review,20 April 2017,https://link.springer.com/article/10.1007/s40844-017-0073-6,Special feature: Computational and simulation paradigms for evolutionary and institutional economics,June 2017,Shu-Heng Chen,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Evolutionary and Institutional Economics Review,11 August 2016,https://link.springer.com/article/10.1007/s40844-016-0048-z,Information aggregation and computational intelligence,June 2017,Shu-Heng Chen,Ragupathy Venkatachalam,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Evolutionary and Institutional Economics Review,06 October 2016,https://link.springer.com/article/10.1007/s40844-016-0058-x,Transitional student admission mechanism from tracking to mixing: an agent-based policy analysis,June 2017,Connie H. Wang,Bin-Tzong Chie,Shu-Heng Chen,,Unknown,Unknown,Mix,,
14.0,1.0,Evolutionary and Institutional Economics Review,31 October 2016,https://link.springer.com/article/10.1007/s40844-016-0062-1,The decomposition and policy meaning of China’s carbon emission intensity,June 2017,Jinhe Jiang,,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Evolutionary and Institutional Economics Review,02 May 2017,https://link.springer.com/article/10.1007/s40844-017-0074-5,Aggregate implications of lumpy investment under heterogeneity and uncertainty: a model of collective behavior,December 2017,Yoshiyuki Arata,Yosuke Kimura,Hiroki Murakami,Male,Male,Male,Male,"A large body of literature on investment has shown that individual investment behavior has two remarkable properties. First, investment activity at the plant level is infrequent and lumpy, that is, periods of low investment activity are followed by bursts of investment activity (e.g., Doms and Dunne 1998). It is shown that in the presence of fixed costs of adjustment capital, the (S, s) type policy, which is characterized by an inactive range between a trigger value and a target value, is optimal (e.g., Caballero and Engel 1999). Namely, when a firm faces a fixed cost of adjusting capital and the loss by nonadjustment is small, it is advantageous to do nothing until the profits of capital adjustment exceed the costs. Thus, firms’ activities are characterized by inactive periods and large investment ones. The second is the relationship between uncertainty and investment. The real option theory (e.g., Dixit and Pindyck 1994) provides a theoretical explanation of investment behavior when decision makers face uncertainty (e.g., uncertainty about future output prices, economic conditions, and rates of return) and their decisions are irreversible. This theory suggests that “there will exist an ‘option’ value to delay an investment decision in order to await the arrival of new information about market conditions” (Carruth et al. 2000, p.120). Accordingly, greater uncertainty raises the “option” value to delay investment and thus firms postpone their investment decisions. Our goal of this paper is to clarify how these microeconomic investment behaviors are related to aggregate investment dynamics. Recently, there is a growing literature discussing the impact of intermittent, lumpy investment on the aggregate investment, but there is no unanimous agreement about the aggregate significance of lumpy investment.Footnote 1 In this paper, the importance of heterogeneity across firms is emphasized. Heterogeneous behavior across firms can be generated by, for example, idiosyncratic fixed costs of capital adjustment or heterogeneous expectations about the economic condition in the future, and the resulting heterogeneity is described by the (cross-sectional) distribution of capital stock across firms, which represents the macroscopic state, i.e., aggregate capital. We analyze the evolution of the distribution underlying macroeconomic variables in order to investigate whether the structure of individual investment activities affects the fluctuation of aggregate investment dynamics. In this sense, our approach is close in spirit to the heterogeneous agents models (see, e.g., Aoki 1996; Hommes 2006; Kirman 2010), where heterogeneity across economic agents plays a crucial role in explaining regularity observed only at the macroscopic level and aggregate behaviors which are different from microeconomic behaviors. In this paper, we consider the effect of macroeconomic uncertainty on individual investment activities as a source of the feedback-loop and the variance of the distribution of the capital stock as the realization of uncertainty of the entire economy. In uncertain circumstances, it is difficult to predict returns and losses associated with economic activities, and the difficulty generates disagreement among business forecasts of decision makers.Footnote 2 As a result, a large disagreement induces a large diversity of individual states across agents. Thus, uncertainty materializes as heterogeneous behaviors or states of microeconomic agents and is measured by the variance of the distribution. In addition, whenever firms decide whether to invest or not, they pay attention not only to firm-specific information but also to others. They may behave more cautiously after observing an increase in dispersion among other firms’ expectations or actions (see, e.g., Bachmann et al. 2013). This process generates a feedback-loop between micro-behaviors and the variance of the macroscopic distribution.Footnote 3 We show that a strong feedback-loop undermines the stability of the macroscopic distribution and gives rise to a remarkable phenomenon, i.e., collective behavior.Footnote 4
 When analyzing the effect of heterogeneity and feedback-loops, one may have difficulty in solving explicitly the optimization problems. In order to overcome the difficulty, we firstly show that the individual investment process can be described by a Feller process, especially a pseudo-compound Poisson process with drift. This stochastic process whose sample path continuously declines and infrequently jumps is adequate for describing infrequent and lumpy activity. Our approach is justified as long as the individual investment process is a Markov process and helps us to avoid the complexity of optimization problems. Thus, we can directly deal with heterogeneity because each realization of the stochastic process reflects heterogeneity in agents’ behaviors. Moreover, by analyzing the evolution of the distribution, we can show that heterogeneity creates the macro-stability in the sense that arbitrary initial distributions converge to a unique stationary distribution. It should be noted that this stabilization effect is completely different from the adjustment of the price mechanism, e.g., the change of an interest rate. Furthermore, the introduction of a jump intensity function dependent on the distribution itself enables us to investigate the effect of feedback-loops between micro-agents and the macroeconomy. Although an closed-form solution is unfeasible because of the nonlinearity, the stability of the capital adjustment process at the aggregate level can be analyzed with the help of Gaussian approximation and numerical simulations. Then, we find that the stabilization effect is defeated if individual investment activity depends strongly on uncertainty. Namely, under the condition that firms are highly sensitive to uncertainty, they collectively “wait and see” in spite of a large deviation from their desired levels of capital, and this collective inaction breaks down the macro-stability. The plan of this paper is as follows: In Sect. 2, we demonstrate that the discontinuous and lumpy adjustment process of microeconomic units possesses the Markov property. In Sect. 3, we first introduce Feller processes and the representation result for them. This representation justifies our strategy of dispensing with an explicit optimization problem. Secondly, the lumpy behavior of microeconomic units is described by a pseudo-compound Poisson process with drift. Finally, we explain the stabilization effect caused by randomness. In Sect. 4, we explain the effect of uncertainty and feedback loops. In Sect. 5, we analyze the stability of the macroscopic distribution by using Gaussian approximation. In Sect. 6, we perform some numerical simulations of our model and demonstrate the stable and the unstable behavior of the aggregate capital adjustment process. Finally, we conclude this paper in Sect. 7.",
14.0,2.0,Evolutionary and Institutional Economics Review,19 May 2017,https://link.springer.com/article/10.1007/s40844-017-0076-3,Controllability analyses of nation-wide firm networks,December 2017,Hiroyasu Inoue,,,Male,Unknown,Unknown,Male,"The economy remains incompletely understood despite the long history of economic research. Governments have to intervene without assurance that doing so will lead the economy to a desirable state. Economic control is considered difficult but necessary. Through intervention, governments stimulate firms and prompt spillovers, for example, by purchasing goods and services, providing firms with grants, or fine-tuning taxes. These are examples of demand-side stimuli. In addition, there is an opposite approach to such policies. If scarcity produces a bottleneck in the production process, governments can help, for example, by importing goods, providing firms with grants, or fine-tuning taxes. These are examples of supply-side stimuli. Such actions taken by governments are referred to as fiscal policy as opposed to financial policy. Governments consider fiscal policy an important determinant of growth (Easterly and Rebelo 1993; Romp and de Haan 2007). When governments carry out demand- or supply-side fiscal policy, it is necessary to estimate their effects, in particular in terms of spillover effects. In general, these estimations are based on input–output tables (Leontief 1936). Input–output tables are important tools for predicting the aggregate influence of a change in some industry on others. It enables us to obtain an estimation of spillover effects caused by stimulus. An input–output table is a matrix of transaction volumes between industries in which all industries are fully connected; that is, every industry is connected to every other industry. However, actual production networks consist of one-to-one connections between firms, not aggregate connections between industries, and it has been claimed that economic systems should be considered complex networks (Acemoglu et al. 2012), as network science has revealed the complex nature of networks (Barabási and Albert 1999, 2002). Utilizing a firm production network, we can observe that some firms are not affected by other firms at all, whereas other firms are susceptible to spillover effects that cannot be observed in input–output tables. Recognizing this diversity is important, because if some firms are not reachable, fiscal policies may affect firms differently. Even if such disparities are inevitable, it is still important to identify the outcomes of policies. Based on the above context, how the smallest subset of vertices to control the network can be determined is an important question. The framework of controllability allows us to determine the vertices (Liu et al. 2011). To utilize this framework, we only need the directed links of the network; we do not need the weight of links.Footnote 1
 We consider the directed links of the firms, i.e., the adjacency matrixes, and a firm-level production network. The network depicts the supplier–client relationships of firms. We apply the framework of controllability to the firm-level production network. “Control” in the framework means that through directly controlled firms, other firms are controlled indirectly. The framework tells us whether an entire network is directly/indirectly controllable by a given set of directly controlled firms. Here, controllable means that an arbitrary state of a network can be led to any desired state within a finite time (Kalman 1963; Luenberger 1979; Slotine and Li 1991). We can consider any type of state as firms’ states, because the framework does not have any assumption as to what it should be, but we can assume sales as states, for example. There are two possible directions to control the production network. If the demand of firms is controlled, the suppliers of the firms are indirectly stimulated. We call this demand-side control. On the other hand, if the supply of firms is controlled, the clients of the firms are indirectly stimulated. We call this supply-side control. Other studies utilize the framework of controllability for economic networks, including studies on the financial relationships between firms (Galbiati et al. 2013; Delpini et al. 2013). Although these studies differ from ours because of the type of network, they show how the framework of controllability is useful for determining the minimum set of drivers needed to control networks. We should note that, other than the weight of links, there is a considerable amount of available information related to networks. For example, we can consider the timing of flows (whether all trades happen simultaneously) or the types of links (such as different products). Moreover, we can decide whether to consider import/export links. Since these attributes are not necessary to calculate controllability, this study does not consider them. This study reveals which firms should be directly/indirectly controlled by applying the framework of controllability (Liu et al. 2011) to the production network of firms using exhaustive trade network data for Japan. The results are analyzed from the viewpoint of the different characteristics in industries. In addition, the results show how clipping networks affect controllability. The remainder of this paper is organized as follows. In Sect. 2, we introduce the data set. Sect. 3 describes the methodologies that we utilize in the analyses. Section 4 presents the results. Finally, Section 5 concludes.",
14.0,2.0,Evolutionary and Institutional Economics Review,20 October 2017,https://link.springer.com/article/10.1007/s40844-017-0084-3,Dependence of the decay rate of firm activities on firm age,December 2017,Atushi Ishikawa,Shouji Fujimoto,Tsutomu Watanabe,Male,Unknown,Male,Male,"Over the last few decades, researchers have studied economies in a different way from traditional economics by applying a method borrowed from physics to economic data. This approach is called econophysics (Mantegna and Stanley 2007; Saichev et al. 2009; Aoyama et al. 2011). In econophysics, large amounts of the financial data of firms (sales, number of employees, assets, and so on) are statistically analyzed to observe a substantial contingent of universal laws (Pareto 1897; Newman 2005; Clauset 2009; Bonabeau and Dagorn 1995; Render 1998; Takayasu et al. 1996; Kaizoji 2004; Yamano 2004; Mantegna and Stanley 1995; Axtell 2001; Podobnik et al. 2010; Fu et al. 2005; Podobnik et al. 2008; Okuyama et al. 1999; Ramsden and Kiss-Haypál 2000; Mizuno et al. 2002; Gaffeo et al. 2003; Zhang et al. 2009; Levy and Solomon 1996; Sornette and Cont 1997; Takayasu et al. 1997). One main aim of econophysics is to systematically realize an economy by relating such laws. Many studies have focused on the properties of the financial data of firms by observations in a single year or two successive years (Gibrat 1932; Sutton 1997; Fujiwara et al. 2003, 2004; Ishikawa 2006, Ishikawa 2006, 2007, 2009; Tomoyose et al. 2009; Ishikawa et al. 2011). Compared to such investigations, long-term studies of the properties of firms are scarce. Some observations concluded that the firm age distributions of several countries obey exponential functions (Fujiwara 2004; Coad and Tamvada 2008; Coad 2010a, b; Bottazzi et al. 2008; Miura et al. 2012). This exponential firm age distribution suggests that the decay rate of firm activities does not depend on firm age by an analogy to nuclear decay (Rutherford and Soddy 1903). In empirical data, the decay rate of Japanese firms was nearly constant after comparing their activities in 2008 and 2013 (Ishikawa et al. 2015). The decay rate of firm activities in the United States depends on firm age (Ishikawa et al. 2015). The decay rate of young firms, which was higher than that of more established firms, became lower and eventually was constant as the firms aged. We approximated the distribution of the decay rate by an exponential function and analytically derived the firm age distribution, which deviates from the exponential function, and confirmed the result in American empirical firm data. In Ref. (Daepp et al. 2015), using the Standard & Poor’s.COMPUSTAT, http://www.compustat.com, the authors showed that the distribution of firm lifespans in the database follows the exponential function. The study of the firm age distribution is important for many economic reasons, such as better understanding the structure of an industry by comparing firm age distribution in different industrial classification. By comparing firm age distribution in different countries, the difference of economic structure among countries will be clarified. The study can also evaluate whether a database is representative. In our previous study, we defined the decay of firm activities as a transition from active in 2008 to inactive in 2014 and regarded unknown or not available (NA) data in 2014 as inactive. In the analysis of Japanese firms, since the unknown and NA data in 2014 was \(0.02\%\) of the total active data in 2008, this assumption did not largely affect the results of Japanese firms. On the other hand, for the United States, the unknown and NA data in 2014 was \(32.2\%\) of the total active data in 2008. The treatment of these data did not change the qualitative property; however, it did affect the quantitative result. In this study, we clarified this problem by investigating the activity data of the firms in the countries where the amount of data is statistically sufficient with just NA data in 2014 and observe the dependence of the decay rate on firm age. The countries in our database are as follows: Germany, Spain, France, the United Kingdom, Italy, Japan, Korea, and the Netherlands. In Japan and Spain, the decay rate of firm activities does not depend on firm age. On the other hand, in the remaining six countries, the decay rate does depend on firm age, as in the United States. The dependence of the decay rate on firm age is weak in Germany and the Netherlands. In Korea, the decay rate is high when firms are young and eventually falls to zero as firms age. The rest of this paper is organized as follows. Section 2 describes our database and explains the status data of firm activities. In Sect. 3, we show the analysis results of the decay rate of firm activities and the firm age distribution and describe their qualitative properties. In Sect. 4, we approximate the distribution of the decay rate observed in Sect. 3 by an exponential function and analytically derive the age distribution and quantitatively compare the parameters that are estimated by the decay rate with those estimated by the firm age distribution. The last section concludes this paper.",5
14.0,2.0,Evolutionary and Institutional Economics Review,18 July 2017,https://link.springer.com/article/10.1007/s40844-017-0079-0,Income distribution management to sustain long-term economic growth: does the equalization of income distribution contribute to long-term economic growth?,December 2017,Shungo Sakaki,,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Evolutionary and Institutional Economics Review,17 July 2017,https://link.springer.com/article/10.1007/s40844-017-0078-1,Quantized price volatility model for transaction data,December 2017,Hiroyuki Moriya,,,Male,Unknown,Unknown,Male,"Financial products are exchanged within an aggregation of buyers and sellers. In the processes of financial trading, many anomalous market and asset price behaviors have been reported (Mandelbroit 1963, 1999); however, no underlying theoretical structure of trading has ever been provided to explain such observations. Financial markets are diversifying dynamical systems with many constraints. One constraint is the minimum size of price movements, which may contribute to price dynamics such as volatility and/or spread clustering (Niederhoffer 1965; Hasbrouck 1988; Goodhart et al. 1991; Harris 1991). The price of each transaction is increased or decreased by a multiple of the minimum price increment, and this is a source of anomalies in a market (Christie and Schultz 1994). A set of quantized price increments, known as a configuration, is a source of aggregate properties of dynamical markets. After a price is determined by a buyer and a seller, the securities traded must be delivered against payment. Thus, the traders’ money deposit, payment capacity or budget plan is another constraint (Kupiec 2008; Clark and McPartland 2012). Some changes in constraints, brought about by a change in a common practice or regulation, ultimately may have an effect on the trading processes and properties related to dynamical systems (Goodhart and O’Hara 1997). If such a shift is really linked to changes in the properties of price dynamics, it might lead to useful model development connected with micro- and macro-states (Aoki 1996). An equilibrium or stationary state has been a major concept in the modeling of price movements (Kolmogorov 1941; Wiener 1949; Box and Jenkins 1970; Merton 1973), asset pricing (Sharpe 1964), and valuation of derivatives (Black and Scholes 1973); however, markets certainly evolve over time (Kalman 1960; Bellman 1961). Price volatilities fluctuate over time and may follow patterns or seasonality such as U-shaped (Kawaller et al. 1994; Piccinato et al. 1998; Andersen and Bollerslev 1997b), opening (Baillie and Bollerslev 1990), Monday morning (Goodhart and Figliuoli 1997), and specific time (Andersen and Bollerslev 1997b) patterns. These are more elusive if volatilities are persistently stable over the long term. The effect then is more like reversion to the mean with short-term seasonality. The behavior is represented in ARCH/GARCH (Engle 1982; Bollerslev 1986) and their growing family of models (Hausman et al. 1992; Muller et al. 1997; Andersen and Bollerslev 1997a, 1998b). Another approach has been extensively developed using a mixture of stochastic and realized volatility (Andersen et al. 2001). This works as a data-fitting method; however, it lacks an underlying theory. The analysis introduced here extensively uses and models huge amounts of tick-by-tick data. Some problems with this approach have been reported: the traditional concept of statistical significance does not work due to the enormous numbers of degrees of freedom and non-normality (Granger 1998); the bid-ask bounce gives rise to negative autocorrelation (Hausman et al. 1992); the observed trading and quoted prices are not equal to the equilibrium (Gottlib and Kalay 1985) or even best prices (Roll 1984) with the bid-ask bounce, discreteness of price, and bid-ask spread; or the tick time is not always equal to the clock time (Glosten and Milgrom 1985). Even if the data have these drawbacks, all observations provide useful information in any event in high-frequency data analysis. Roles of tick sizes, order flows (Ane and Geman 2000), and other micro structural phenomena have been investigated to explain anomalous behavior of volatilities and returns of high-frequency transactions on a shorter and a longer time scale (Smith et al. 2003; Tóthab et al. 2012). Section 2 briefly introduces non-negativity, self-financing (Cox and Leland 2000), and distinguishable trades. It also provides the basic concepts and principles used in the present paper, including the minimum size of price movements (tick), the sum of squared price increments (sspi) and similar concepts of realized volatility (Foster and Nelson 1996; Andersen and Bollerslev 1997a, 1998a), the set of occupation numbers (Feller 1957), configuration multiplicity, and the concept of most probable value. When the number of transactions is very small, we cannot observe macro values and any extreme price movements might be consistent with a possible configuration. Section 3 introduces the strictly constrained market to develop the simplest model, and shows that the probability distribution of the price increments takes the form of an exponential, useful for connecting the microscopic state to macro properties. The multiplicity is the bridge between these two states. Then, we relax the constraints and allow the sspi to fluctuate. A probability distribution of the sspi is changed from exponential to normality as the number of transactions accumulates. The fluctuation of the sspi is independent of the size of price increments and their probabilities. However, in certain circumstances, the fluctuation of the sspi might be dependent on the number of transactions. Section 4 explains the similarity of the sspi and the realized volatility (Foster and Nelson 1996; Andersen and Bollerslev 1997a, 1998a). Finally, Sect. 5 concludes.",1
14.0,2.0,Evolutionary and Institutional Economics Review,15 November 2017,https://link.springer.com/article/10.1007/s40844-017-0088-z,Special feature: preliminaries towards ontological reconstruction of economics—theories and simulations,December 2017,Yuji Aruka,,,Male,Unknown,Unknown,Male,,3
14.0,2.0,Evolutionary and Institutional Economics Review,09 November 2016,https://link.springer.com/article/10.1007/s40844-016-0060-3,Complexity and institutional evolution,December 2017,J. Barkley Rosser Jr.,Marina V. Rosser,,Unknown,Female,Unknown,Female,"The link between institutional economics and evolutionary economics dates to the work of Veblen (1898). It is largely in recognition of this fact that the first organization in the USA dedicated to the study of institutional economics is called the Association for Evolutionary Economics,Footnote 1 with similar names being used in other nations for such study, including in Japan. While it was not recognized at the time and remains little known, Veblen not only called for economics to be an evolutionary science, but introduced certain ideas that have since proven to be important in understanding the nature of complexity in economics, particularly that of cumulative causation, oftenFootnote 2 thought to have been introduced later by either Young (1928) or Myrdal (1957), with the latter making the term widely known among economists. Among the various forms of complexity that are relevant to economics, cumulative causation is most obviously tied to dynamic complexity, which leads to increasing returns, multiple equilibria, and a variety of bifurcations in economic dynamical systems. However, it can be seen to be connected also to computational complexity, the main rival to dynamic complexity in economic analysis. An important issue for the matter of how evolutionary theory relates to institutional economics in its early formulation involves Veblen’s relations with John R. Commons and Joseph Scumpeter. Veblen developed ideas of Darwinian evolutionary economics in the early twentieth century in the USA, while Schumpeter is widely viewed as a strong supporter of an evolutionary approach to economic development, particularly regarding the evolution of technology, even as he criticized institutional economics and the application of biological ideas. Also not widely known, Commons (1924) also supported an evolutionary view, although he had more of a teleological perspective on that than did either Veblen or Schumpeter, both of whom saw no necessary direction to technological evolution and change (Papageorgiou et al. 2013). Dealing with a complexity issue, Schumpeter strongly advocated a discontinuous, or saltationalist view of evolution (Schumpeter 1911; Rosser 1992), which Veblen agreed with regarding technological change. Regarding institutional evolution, Veblen mostly saw it proceeding in a more continuous manner through cumulative causation, thus being somewhat closer to Commons on that matter, even as he argued that it was fundamentally unstable and would experience crises and breakdowns. A central issue for institutional economics is the distinction between institutions and organizations (North 1990). This becomes central for the role of evolution in economics, in particular what is the meme that is the locus of evolutionary natural selection. In older literature, the emphasis was more on organizations, such as with Commons (1934) who saw organizations competing with each other, a theme also picked up by Alchian (1950), even as Commons emphasized the deeper structures of institutions in legal systems. While organizations compete, increasingly evolutionary economists have focused on practices and routines as the more crucial memes, with this an especial theme among neo-Schumpeterian followers such as Nelson and Winter (1982). An important element of evolutionary processes is the emergence of higher-level structures out of lower-level and simpler ones. This is more obvious in terms of organizations, but in institutional evolution the role of memes becomes crucial. This fits with the issue of multilevel evolution, long controversial in evolutionary theory (Henrich 2004). Within human systems, this becomes tied to cooperation, with Ostrom (1990) developing how such cooperation can arise through particular institutions. This process of emergence is linked to deep concepts of complexity, with Simon (1962) a crucial developer of this line of thought. Understanding the complex dynamics of institutional evolution can bring about a possible reconciliation or even synthesis between the old and new institutional economics. Coase (1937) recognized that Commons originated the idea of the importance of transaction costs, the centerpiece of the new institutional economics (Williamson 1985). Mikami (2011) has argued that the effort to minimize transactions cost can lead to complex evolutionary dynamics. This can involve Veblen’s cumulative causation, recognizing how this can become linked to complex evolutionary emergence. This reconciliation between the old and new institutional economics through complex evolutionary dynamics extends to a deeper matter of epistemic issues arising from hierarchical emergence that may show deep links between computational and dynamic complexity (Koppl and Rosser 2002). Memes involve information structure systems that are understood by computational complexity concepts, with this form of complexity exhibiting levels. Competition between such structures in markets can see the emergence of higher-order institutional forms in markets as analyzed by Mirowski (2007). Thus, we see a deep unification that fundamentally traces to Veblen’s development of the idea of cumulative causation in institutional evolution, which can underpin these developments.",9
14.0,2.0,Evolutionary and Institutional Economics Review,22 August 2017,https://link.springer.com/article/10.1007/s40844-017-0080-7,Influential factors responsible for the effect of tax reduction on GDP,December 2017,Shigeaki Ogibayashi,Kosei Takashima,,Male,Unknown,Unknown,Male,"Agent-based modeling (ABM) is widely used in social simulations to explain or understand social phenomena (Terano 2008). One important area of research is the application of ABM to macroeconomic systems, although these systems are very complex and include various kinds of agents and interactions between them. When using ABM, it is considered important that the model be as simple as possible, based on the “KISS Principle”, to understand the most essential mechanisms of the phenomena in question (Terano 2008). However, it is also important to consider all factors required to reproduce the desired phenomena, because the structure of the artificial system should be the same as that of the real system to enable the characteristics to emerge as they do in the real system. Establishing the factors essential to reproducing the desired characteristics of the system can be done using a series of computer experiments in which only one constituent factor of the model is changed at a time, while the other factors are held constant (Farmer and Foley 2009; Croson and Gächter 2010). A number of ABM-based studies have focused on various macroeconomic concepts, such as business cycles, innovation, economic growth, the role of banks, monetary policies, industrial dynamics, and wealth inequality (Ashraf et al. 2011; Russo et al. 2007; Dosi et al. 2010; Bruun 2000). Most of these studies reported new findings, but structure of the model was different in each case. This makes it difficult to identify the crucial assumptions of each model and to what extent the assumptions are important in reproducing the phenomena being studied. Researchers have also developed relatively more practical models that simulate multiple-market economic structures as elaborately as possible (Raberto et al. 2011; Sprigg and Ehlen 2004). However, given the nature of these economic phenomena, these studies have not fully clarified the structural factors of the model that are important for reproducibility. Another potential area for applying ABM in the real world is government policy formulation in areas such as tax reduction and public expenditure. According to Keynes’ multiplier theory (Krugman and Wells 2009), government public expenditure and tax reductions are effective policies for promoting a macro-economy. However, the multipliers of these public policies are relatively small when compared with the values expected by the marginal propensity to consume in the Keynes’ multiplier theory, the reason for which is not well understood. Authors recently clarified this reason, see Ogibayashi and Takashima (2017). Motivated by this lack of understanding, the authors constructed a simple, artificial economic model consisting of consumers, three types of producers, a bank, and a government. The conditions required for the model to reproduce the positive influence of a tax reduction on GDP were then analyzed, with the intention of revealing and explaining the mechanism that makes the public policy multiplier so low. The findings showed that inefficiency in government expenditure, executive compensation, and internal funds for investment are all factors responsible for the positive influence of a tax reduction on GDP. Here, inefficiency in public expenditure is defined as the ratio of firm subsidies to the sum of firm subsidies and market purchases (Ogibayashi and Takashima 2013, 2014). In the present study, additional simulations are conducted to clarify why the above factors are responsible for reproducing the positive influence of a tax reduction on GDP. In addition, we derive a set of equations for the tax reduction multiplier based on our revised version of Morishima’s economic linkage table (Morishima 1984), and compare the influence of the above-mentioned factors with the results calculated using ABM. In general, theoretical approaches in macroeconomics that assume complete equilibrium between demand and supply and that neglect the diversity of agents cannot describe complex systems well. However, these approaches have the advantage of being able to describe the mechanism behind the relationships among the influential factors. In explaining the influence of a tax reduction on GDP, the price in equilibrium would not be an important factor, whereas the flow of funds among agents would. In such cases, confirming whether the results calculated using ABM can be explained by theoretically derived equations could be an effective measure for validating ABM studies.",4
14.0,2.0,Evolutionary and Institutional Economics Review,13 April 2017,https://link.springer.com/article/10.1007/s40844-017-0070-9,Investigation of the rule for investment diversification at the time of a market crash using an artificial market simulation,December 2017,Isao Yagi,Atsushi Nozaki,Takanobu Mizuta,Male,Male,Unknown,Male,"Financial products have grown in complexity and level of risk compounding in recent years. For example, mutual funds have come to choose various assets, some of which may have high risk, and there may be some funds whose performances depend on those of the particular assets that the funds hold. Therefore, investors have come to find it difficult to assess investment risk. Companies managing mutual funds are increasingly expected to perform risk control and thereby prevent assumption of unforeseen risk by investors who hold mutual funds. In Japan, The Securities Mutual Fund Law was revised in 2013, and the rule for investment diversification was established in December 2014 (Sectional Committee 2009; The Investment 2014). The rule for investment diversification is a holding weight limitation for mutual funds; that is, mutual funds are prohibited from holding a greater weight of each stock than some limit.Footnote 1 For example, the rule specifies that mutual funds have to keep each stock to 10% or less of the mutual funds’ net asset value (NAV). The rule for investment diversification is applied not to individual stocks, but to mutual funds. Thus, the mutual fund manager must act to ensure that mutual funds do not hold each stock in more weight than the weight limit set by the regulator. On the other hand, individual investors and institutional investors who do not own mutual funds are not affected by the rule for investment diversification. The purpose of the rule for investment diversification is to restrict the investment risk of investors who hold mutual funds. When the number of mutual fund components is small (i.e., the fraction of each stock in the mutual fund is large), if the value of one of them decreases sharply due to bankruptcy or scandal, investors who hold the mutual fund suffer large losses. However, when the number of mutual fund components is large, even if the price of one of them collapses due to bankruptcy, the damage to investors is limited. As you can see from the previous explanation, the purpose of the rule for investment diversification is not to stabilize the market. Thus, there has been no clear discussion on whether the rule for investment diversification can stabilize the market. There have been many empirical studies on diversified investment. Cremers et al. Cremers and Petajisto (2009) showed that the funds with the highest Active Share Footnote 2
significantly outperform their benchmark indexes, while the non-index funds with the lowest Active Share underperform. On the other hand, there has been no clear discussion on the effects of market price formation when restrictions on diversified investments are imposed. An empirical study cannot isolate the pure contribution of these regulations to price formation because many kinds of traders can effect price formation in actual markets. One way of analyzing how particular transactions influence the market is to use an artificial market. An artificial market is a multi-agent based model of financial markets (Chiarella et al. 2009; Chen et al. 2012; Cristelli 2014; Mizuta et al. 2014). Each of the agents is assigned a specific trading (i.e., buying and selling) rule and then set to trade financial assets as an investor. The market can then be observed to see how the agents behave. At the same time, by modeling and incorporating certain restrictions on the market side (e.g., limitations to ensure market stability and efficiency such as short selling regulations), it is possible to examine how investors behave and also what kinds of effects their behaviors induce in the market. Studies on artificial markets to investigate market regulations have had some success in market analysis in recent years (Yagi et al. 2010; Mizuta et al. 2014); however, the only study on the effects of the rule for investment diversification on the market using an artificial market has been Nozaki et al. (2015). Specifically, Nozaki et al. investigated market volumes, the efficiency of markets, and investors’ performance in the following two stable (i.e., the fundamental prices of all assets are constant) markets. In one market, all investors must follow the rule for investment diversification, and in the other one, all investors do not follow the rule for investment diversification. For these markets, they obtained the following three results: (1) the more strict the rule for investment diversification is, the less the market volume is; (2) the efficiency of a market where all investors (i.e., mutual fund managers in the real world) must follow the rule for investment diversification is not always worse than that of a market where all investors do not follow the rule for investment diversification; and (3) the investors’ performances in a market where all investors must follow the rule for investment diversification are not always worse than those in the market where all investors do not follow the rule for investment diversification. On the other hand, we investigate the difference between (1) the market price transitions of two assets whose fundamental prices are constant in both a market where agents must follow the rule for investment diversification and in a market where agents do not follow the rule for investment diversification and (2) the market price transitions of an asset whose fundamental price falls and those of the other asset whose fundamental price is stable in both the markets where agents must follow the rule for investment diversification and in a market where agents do not follow the rule for investment diversification. Our study supposes the following situation in real financial markets. If one of the companies that issues a stock in the mutual fund becomes bankrupt, its financial results are much worse than the market forecast, or it causes a scandal or has serious accidents, then the fundamental price of only this company falls sharply and the market price also collapses. However, even if the above types of troubles occur for a particular company, the impact on the economy is not great, so the fundamental prices and market prices of other companies’ stocks do not decrease sharply. For such a situation, we assess the influence of the rule for investment diversification on the markets. Previewing our results, we found that when the fundamental price of one asset collapsed and its market price also collapsed, the other asset market prices also fell in a market where all investors must follow the rule for investment diversification.Footnote 3
 The structure of our paper is as follows. First, we explain our proposed artificial market used in this study in Sect. 2. Specifically, the order process and the learning process of our artificial market are explained in Sects. 2.1 and 2.2, respectively. In Sect. 2.3, we model the rule for investment diversification in our artificial market. Next, we perform some simulations using our artificial market and analyze the results in Sect. 3. In Sect. 3.1, we assess the validity of our artificial market. In Sect. 3.2, we observe market price transitions of two assets whose fundamental prices are constant in a market where agents must follow the rule for investment diversification and in a market where agents do not follow the rule for investment diversification. In Sect. 3.3, we also observe the market price transitions of an asset whose fundamental price falls and those of the other asset whose fundamental price is stable in these same two markets. Finally, we discuss our conclusions regarding this study and our future work in Sect. 4.",8
14.0,2.0,Evolutionary and Institutional Economics Review,24 October 2017,https://link.springer.com/article/10.1007/s40844-017-0085-2,Some new perspectives on the inter-country analysis of the world production system,December 2017,Yuji Aruka,,,Male,Unknown,Unknown,Male,"It is well known that an integration of any two production systems \(\{A, B\}\) will bring some more efficient production as a whole than a simple sum of the production amounts over the two independent systems. For simplicity, we suppose that each system can produce two goods \(\{x, y\}\) common to the two systems. We then give a simple example. We have two states of each production system where any system can specialize her production to some particular good of any two goods. Thus, we may have a state with specialization and a state without specialization to some particular good. It is shown that specialization can increase the total production of A and B for both goods x and y. An efficient total production can only be achieved if either A or B specializes in producing the good in which they have a comparative advantage. We assume that a production possibility frontier is linear. In comparison between given two systems, a comparative advantage of any system will be then revealed as for the production of one of two goods. For instance, the system A has a comparative advantage in the production of y. The system B has a comparative advantage in the production of y. If it is the case, the production possibilities frontier of the systems A and B will be then depicted in Fig. 1. Thus, the frontiers of both countries have their maximum production on each good, respectively. We denote the system A’s maximum capacity on good x by \(X_{\max A}\), on good y by \(y_{\max A}\), for instance. The same notations are applied to \(X_{\max B}\), on good y by \(y_{\max B}\) for the system B. Now, we employ a numerical example as shown in Table 1. In our instance shown by Table 1, there are two modes of production: independent mode and integrative mode, i.e., the mode “without specialization” and the mode “with specialization.” In an integrative mode, A can produce 50 units of X, while B can produce 100 units of Y. Here, \(x \in X\) and \(y \in Y\). On the other hand, either A or B can only produce less than her maximum amount which they can specialize their production. We set \(\alpha = \frac{Y_{\max B}}{X_{\max A}}\). Hence, it depends on the ratio \(\alpha\) whether A or B should specialize. In our example, if \(\frac{Y}{X} \ge \alpha\), A must specialize in the production of X, and vice versa. We may then allow an intermediate case that A can produce both goods but B can specialize to produce Y. This illustrates how a comparative advantage in integration for specialization can occur given some independent systems. The result is represented by the activity analysis of Fig. 1. In Fig. 1, the specialization to a particular commodity due to its comparative advantage in the system A is demonstrated. However, this kind of specialization does not refer to and/or specify at all how the outputs will be exchanged among the systems. The transactions among the systems should be complex if the outputs were also used as the intermediate goods for production. This is the reason why the classical international trade theory was not updated until Shiozawa (2007, 2016) has mathematically innovated by the use of sub-tropical geometry. In the following, we will argue an integration through international trades. Taking into account international trades, however, the effect of integration of independent systems must be more complex. The effect could not be confirmed by a simple comparison of comparative advantage in terms of real productivity among the independent systems. We will require a more roundabout reasoning on the idea of comparative advantage. Activity analysis of integration of two independent production systems through specialization (drawn by Brechtefeld 2012) We describe the ex post technology of a production unit as follows: We then observe the production set Y. Given a family \(\{a_i\}_{i \in N}\), the short-run total production set is written in the following manner: In economics, there were few to discuss production in context of zonotope. Zonotope is one of the Minkowski space family. A convex polygon is a zonotope if and only if all its two-dimensional faces have a center of symmetry. By the use of this idea, we can observe all possible combinations for production without resort to any restriction of efficiency. 
Hildenbrand (1981) defines the short-run total production set associated with them as the zonotope. The short-run production possibilities of an industry with N units at a given time is a finite family of vectors \(\{a_i\}_{1\le i \le N}\) of production activities: According to Hildenbrand, this idea differs from the traditional production function. We define the projection of Y on the input space \({\mathfrak{R}}_{+}^{l}\): It then hods the traditional production function: The operator max in the above has excluded the possibilities of “certain institutional barriers to factor mobility in aggregating the individual production sets” (Hildenbrand 1981, 1097). In general, the ex post technology of a production unit is a vector i.e., a production activity a that produces, during the current period, \(a_{l+1}\) units of output by means of \((a_1,\ldots ,a_l)\) units of input. The size of the firm is the length of vector a, i.e., a multi-dimensional extension of the usual measure of firm size (Fig. 2). Activity analysis of integration of two independent production systems through specialization Given \(Y=(a_1,\ldots ,a_l)\), a set of generators for n, the zonotope Y is the convex hull of all vectors of the form a; that is, (Y) is the Minkowski sum of all segments \([0, a_i]\), where \(a \in Y\), i.e., \(\sum _{a_i \in Y} a_i\). Also Z(Y) is the shadow of the r-dimensional cube \([0, 1]^{r}\) via the projection.Footnote 1 It is noted that the application of zonotope to production set was recently achieved by Dosi et al. (2016). They employed the volume of zonotope Y in \({\mathfrak{R}}\): Here, \(|\Delta i_1,\ldots ,i_l|\) is the module of the determinant. This kind of discussion will suggest a new growth/innovation theory of production. This approach may be classified into the complex adaptive system. This matter may be referred by the author elsewhere (Fig. 3). The zonohedron, given the original four activities \(\{a_1,a_2,a_3,a_4\}\): \(\sum _{i=1}^4 [0,a_i] \subset R^3\) Another more sophisticated/innovativeinstance has recently been shown by Shiozawa (2016) when he argued the international production system of 2 country–3 goods case (Shiozawa 2016, 13: Some New Topics in International Trade Theory). Here, he dealt with intermediate goods in the international trade. See Fig. 4. Shiozawa’s diagram (Shiozawa’s polygon) is interpreted as Minkowski sum of any two triangles (any two countries A and B). First, each triangle is composed by the commodities space each country separately. However, the composition of two triangles will generate a zonohedron of many facets. Shiozawa’s diagram In the following, we will see the double observations how the countries and the commodities are mutually interacted. In other words, a Minkowski sum of any 2 systems will reveal some complex relationships newly generated by the international trade between two countries. This formation is easily shown graphically in Fig. 5. Minkowski sum of any two triangles",3
14.0,2.0,Evolutionary and Institutional Economics Review,13 November 2017,https://link.springer.com/article/10.1007/s40844-017-0086-1,Special feature: Professor Masahiko Aoki’s conception of economics and institution,December 2017,Kiichiro Yagi,,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Evolutionary and Institutional Economics Review,27 April 2016,https://link.springer.com/article/10.1007/s40844-016-0037-2,Institutional naturalism: reflections on Masahiko Aoki’s contribution to institutional economics,December 2017,Carsten Herrmann-Pillath,,,Male,Unknown,Unknown,Male,"The concept of ‘institution’ is one of the central, yet also most disputed terms in modern economics, since the rise of mathematical economics in the early 20th century often even defining foundational differences between theoretical paradigms: ‘Institutional economics’ has frequently positioned itself as being an alternative to so-called ‘mainstream economics’. Today, institutions are in focus of what has been labeled as ‘mainstream pluralism’ (Davis 2006), yet their meaning is still open to debate. In this paper, I want to elaborate on an approach to institutions that I have called ‘naturalistic’ (Herrmann-Pillath 2013). The naturalistic approach builds on transdisciplinary foundations of human behavior to explain the emergence and the functions of institutions. This is a view that is becoming prevalent in many areas of economics today, especially with the rise of behavioral and experimental economics. I will draw on this literature. However, I will also show that this view remains deficient in aiming at a reduction of institutions to what is seen as being more fundamental causes of human behavior. Against this reductionism, I posit that institutional naturalism would go back to an intellectual tradition which treats institutions as a ‘second nature’ of human beings, thus as an irreducible category, and which had been inaugurated by Hegel, a philosopher who is almost entirely neglected in modern discourses about economics and philosophy. ‘Naturalism’ means that on the one hand, we take biological or psychological characteristics of human beings into consideration when explaining and analyzing institutions, but at the same time, we posit a social ontology in which institutions are seen as a distinct set of causes that transcend those characteristics. More explicitly, this also implies that we treat institutions as ‘physical’ in the sense of (critical) realism.Footnote 1
 I argue that the theoretical foundations for this approach have been developed by Masahiko Aoki, together with important contributions such as Binmore’s or North’s (especially in his later works, such as 2005). Aoki’s approach is most encompassing because he presents a full-fledged basic model of institutions. I will show how this model can be extended to a naturalistic paradigm. My workhorse is a debate that took place recently in the Journal of Institutional Economics to which Aoki (2015) contributed a commentary (which perhaps is one of his last official publications). In this commentary, he approvingly cited my own work as a useful extension of his model (dubbed ‘Neo-Hegelian’) (Herrmann-Pillath and Boldyrev 2014).Footnote 2 In turn, his model was also cited in the lead article that was the target of the commentary, by Hindriks and Guala (2015) who claim to have developed a new approach to institutions that can reconcile competing interpretations and, above all, the economic and the philosophical approaches, in particular Searle’s, which also plays a prominent role in my thinking. This debate is extremely insightful, as it failed creating a consensus between the authors and Searle who also contributed a long comment (Searle 2015). Actually, the tensions between the different views were even more pronounced as a result, evident from the authors’ response to Searle’s criticism. In this apparently unsurmountable opposition, the question of social ontology plays the crucial role: Searle claims that the economic approach does not recognize the ontological autonomy of institutions, which for him is evident from the fact that institutions provide ‘desire-independent reasons for actions’ (a notion that was seminally deployed by Hegel, see Brandom 2011). Economics seems to reduce institutions to ‘desires’, firmly staying in the tradition of Hume, an essential reference also by other contributors to the debate, Binmore and Sugden. I argue that Aoki’s approach transcends this discussion, after some critical reflection.",2
14.0,2.0,Evolutionary and Institutional Economics Review,15 November 2017,https://link.springer.com/article/10.1007/s40844-017-0087-0,Masahiko Aoki’s conception of institutions,December 2017,Hirokazu Takizawa,,,Male,Unknown,Unknown,Male,"Since its very inception in the eighteenth century, economics has paid considerable attention to the importance of institutions, as best exemplified by the economic policy proposal of Smith (1759/1976; 1776/1991). However, this interest diminished as the neoclassical theory established itself as mainstream in the twentieth century, whereby most economists focused exclusively on the workings of the market institution/mechanism. This approach succeeded because the marginal gain from this type of study was extremely high. The late twentieth century saw this trend being reversed for several different reasons. First, as the economic advantage of capitalism over socialism became obvious in the 1980s, economists’ attention shifted toward the difference in institutional arrangements between major capitalist economies, which seemed to exert great impact on their respective countries’ economic performances. Second, the experience of transitional economies after the collapse of some socialist regimes showed that the market does not stand on its own, but its performance is actually supported by various non-market institutions, such as the institution of property rights, commercial law, and so on. However, the most important factor came from within economics itself. The early 1970s experienced a theoretical revolution in economics, which directed our attention to how institutions are set up to alleviate problems arising from “asymmetric information” (Akerlof 1970). In the 1980s, almost all works in this field came to be rewritten by using game theory. Of course, important works in institutional economics as well as in game theory already existed well before the 1980s. However, the fruitful marriage between the two meant that we became able to talk about the workings of diverse institutions (not only of the market) by rigorous mathematical standards that most economists were satisfied with.Footnote 1 This achievement has been welcomed by traditional economists, since it helped us understand how various non-market institutions support the market mechanism (McMillan 2002), which is the central theme of classical economics. In retrospect, this extension of the economists’ capacity to analyze institutions led to another extension of their interests. Institutional phenomena are ubiquitous in our everyday life, deeply rooted in our social/behavioral dispositions. Combined with the rise of behavioral economics, it was then that some economists began to pay attention to human social behavior constituting institutional phenomena. This shift obviously called for a transdisciplinary approach, because there are many disciplines outside economics concerning human behavior/sociality related to institutions. Therefore, some, if not all, economists came to show their interest in the research results in those areas. It is against this backdrop that several economists began to pose a question about the general nature of institutions from their own perspectives.Footnote 2 The above described nature of the renewed interest in institutions, however, exerted enormous impact on those attempts: under the overwhelming influence of game theory, almost all conceptions of institutions so far proposed in economics borrowed their defining terms from game theory. Thus, one strand of these attempts regards institutions as “the rules of the game”. This approach, henceforth called “institutions-as-rules” view, is best represented by North (1990), which begins with a famous passage: “Institutions are the rules of the game in a society or, more formally, are the humanly devised constraints that shape human interaction” (pp. 3–4). He also identified the importance of “informal rules”, such as codes of conduct, norms of behavior, and conventions. Institutions are thus supposed to channel and incentivize human behavior, while constantly changing the shape of society and economy until today. The other strand regards institutions as the equilibria of the game (we will henceforth call it “institutions-as-equilibria” view). This approach is already found in an early contribution by Schotter (1981), which defines institutions as “regularity in behavior”, but later joined by Greif (1994). The institutions-as-rules approach has its strength in explaining the selection process of institutions. Despite North’s important distinction between formal and informal rules, attention was mainly concentrated on formal rules created by a polity, since it is difficult for policy makers to affect informal rules. The process was supposed to be ultimately driven, rather exogenously, by the institutional entrepreneurs’ motivation to change property rules to their advantage. Whereas North himself emphasizes the importance of enforcement, this approach does not explicitly explain how institutions are stabilized in a society, which may require understanding of how people’s behavioral regularity emerges. On the other hand, the institution-as-equilibria view has the advantage of being able to explain how institutions often have long-standing stability. This is mainly because this view identifies an institution as the Nash equilibrium of a relevant game, which has the so-called “self-enforcing property”, whereby any unilateral deviation does not benefit a player. Thus, enforcement is endogenous in this approach. This view also enables us to understand why there is diversity of institutions in similar domains across economies, by resorting to the existence of multiple equilibria of the relevant games. However, this approach also has its own weakness. While it is very amenable for explaining how institutions are maintained/stabilized, it is hard to explain how institutional changes occur, and we sometimes observe institutional creativity/novelty in the changing process. The understanding of this process is indispensable if economists are to take the explanation of historical changes seriously. It is in this context that Masahiko Aoki started to elaborate his own conception of institutions, which is sometimes simply understood as belonging to the institution-as-equilibria view. However, that would prove to be a misunderstanding if one carefully reads his books/papers. What motivated him to examine this difficult question was his desire to overcome the weakness of the equilibria view. As we will see later, he wished to understand the process of institutional changes/novelty in a way consistent with the equilibrium view of an institution. What really interests us is what he invented to address this conundrum. Some qualifications are in order at this juncture. While the present study focuses on Aoki’s theoretical conception of institutions, it will not deal with his ideas on policy issues. This restriction is actually unfortunate, since he never lost interest in political issues, and his notion of institutions is intertwined with his ideas on policy issues. However, discussing the relationship between his theoretical conception and his view on policy issues would be beyond my ability and thus outside the scope of this study. This may be partly justified, since his conception of institutions was increasingly directed toward understanding the “institution in deep structure” in his later writings. The organization of the paper is as follows: Sect. 2 closely examines Masahiko Aoki’s continual attempts at conceptualizing institutions, with focus on Aoki (2001) and Aoki (2011). Section 3 looks at a debate that took place in the Journal of Institutional Economics. Seemingly the debate revolves around a topic paper submitted by Hindriks and Guala (2015). However, their view, called the “rules-in-equilibrium” view, is essentially Aokian in its spirit. Therefore, the debate incidentally illuminates the difficult problems faced by Aoki’s conception of institutions. Section 4 first discusses those difficulties in some detail, and suggests a possible solution, by referring to Herrmann-Pillath and Boldyrev’s (2014) Hegelian approach to institutions. Finally, Sect. 5 concludes the paper.",3
14.0,2.0,Evolutionary and Institutional Economics Review,27 September 2017,https://link.springer.com/article/10.1007/s40844-017-0083-4,Professor Aoki when he was interested in dynamic processes in the market economy,December 2017,Yoshinori Shiozawa,,,Male,Unknown,Unknown,Male,,1
15.0,1.0,Evolutionary and Institutional Economics Review,19 May 2018,https://link.springer.com/article/10.1007/s40844-018-0093-x,Road to evolutionary and institutional economics in Japan,June 2018,Kiichiro Yagi,,,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Evolutionary and Institutional Economics Review,22 August 2017,https://link.springer.com/article/10.1007/s40844-017-0081-6,Quantal response equilibria in a generalized Volunteer’s Dilemma and step-level public goods games with binary decision,June 2018,Toshiji Kawagoe,Taisuke Matsubae,Hirokazu Takizawa,Male,Male,Male,Male,"The Volunteer’s Dilemma game (VOD) was first formulated by Diekmann (1985) to elucidate “social dilemmas” or “social traps” broader than those covered by the prisoner’s dilemma. A typical social situation is helping behavior of people witnessing an accident or crime, as best exemplified by the murder case of Kitty Genovese examined by Darley and Latané (1968). It is said that her life could have been saved if only one of the bystanders had paid a small amount of cost (e.g., making an emergency call to the police). An interesting issue concerning VOD is the effect of the group size on the tendency to cooperate or contribute, the so-called “bystander effect”. A large amount of evidence has been accumulated by political scientists and psychologists to investigate factors affecting this effect (Latané and Nida 1981). Biologists have also found that similar situations arise in groups of animals. It is known that one member of the group occasionally looks up and checks for a predator for protecting the group as a whole. Archetti and Scheuring (2010) extend VOD to the situation where more than one volunteer is necessary (we call it a generalized VOD) and derive an approximation formula for the equilibrium probability of contribution. Technically speaking, a generalized VOD is a special case of the step-level public goods game with binary decision (Croson and Marks 2000). However, to the best of our knowledge, the formal characterization of the mixed strategy equilibria in this class of games has not yet appeared. Against these backdrops, this paper is first seen as an attempt to derive the necessary and sufficient condition for the existence of the mixed strategy equilibrium of generalized VOD. Furthermore, we will analyze a generalized VOD with a widely accepted behavioral models with bounded rationality, quantal response equilibrium (QRE) and level-k analysis.Footnote 1 Goeree and Holt (2005) report their QRE analysis of this game, but their analysis does not seem to cover all cases. We will provide almost complete characterization of the QRE of this class of games. Finally, we will give some econometric estimation using laboratory data in previous research. Our results show that QRE does a better job than pure and mixed strategy equilibria as well as level-k analysis in explaining the data.",2
15.0,1.0,Evolutionary and Institutional Economics Review,25 May 2017,https://link.springer.com/article/10.1007/s40844-017-0077-2,Crossing the divide: an integrated framework of the commons,June 2018,Yan Zhang,,,Male,Unknown,Unknown,Male,"The concept of the commons refers to anything that are held in common, ranging from tangible resources to intangible ones such as culture and cultural belonging, as well as the institutional commons such as public goods, and it concerns both inter-generational sustainability and intra-generational equity. The concept of the commons stretches over all levels from the local to the global, like village irrigation, like cultural identity, like global combat of climate change. However, in the current literature tends to be restricted to physical resources such as water, forests and the air we breathe. Welfare, security and other public institutions are discussed as separate concepts, while shared but intangible commons, like culture and language, are usually excluded from consideration. All of those things are communally shared by all, or by many people. Commonly shared natural resources, institutional welfare and cultural sentiments reinforce each other by constructing the very meaning of the commons as something shared by all. However, the very definition of sharing immediately indicates that human cooperation is difficulty. Institutions and policies are designed to overcome the governance dilemma of collective action and cooperation. Therefore, we need to widen the concept of the commons, in its original meaning of the communal sharing of assets of collective interest, and use it as a mechanism linking interdisciplinary and multidisciplinary analyses. The concept of the commons defined from its etymological meaning of anything shared by all, encounters three key questions, each responding to a specific social dilemma that constitutes the commons. Firstly, what is the commons? Is it the common grassland for grazing that might degrade because of overusing? Is it the communally shared language of a small tribe that might disappear because of underusing? Or could it be the national defence that protects all people? They are the commons because they all suffer from the commonly sharing dilemma because of scarcity and/or various degrees of excludability and rivalry. The commons means sharing, which then leads us to the second social dilemma of human cooperation. Why do people cooperate? People give up individual freedom/interest for the common good. However, the clear awareness of the common interest remains a challenge even people understand well of its importance. Some externalities (positive/negative) are difficult to observe or predict, that’s when the free riding might occur. So how do we encourage people to cooperate? This brings out the third social dilemma of the commons—the governance dilemma of how do people cooperate. Not only the objective/outcome of cooperation matters, another important issue is the dynamic procedure composed of interactive action arenas and various actors with different capability and accountability. What would be the good governance, the great Leviathan of the state (coercive force under public authority), or the invisible hand of the market (imposing private property rights and price), or Ostrom’s third way of self-governance by the community (internalizing commitment issues)? This paper analyses the commons as a comprehensive concept that encompasses all those questions responding to the three-layered social dilemmas, by reviewing the philosophical origins of the commons and introducing the new framework of the integrated commons. For example, when analysing a commons of river we need to analyse it as an integrated commons that is composed of natural commons of water system, ecological diversity, environmental carrying capacity; institutional commons of welfare system in the river valley; and the cultural commons of identity, ethnicity and religious belief of the communities living in the river valley. Seeing in the commons as a whole helps us to frame out its governance system in more holistic and dynamic way within different boundaries (environmental, social and cultural). Such framing does not only portray the complex reality of the commons with different actors and actions, but also integrate their underlining values, perspectives and relevant experiences. Theories of the commons, mostly developed in the West, study collective action from an individual perspective, and repeatedly emphasize market-alike rational action based upon self-interest. However, people have repeatedly questioned the hypothesis of homo economicus, because rational solutions to enclosure (via privatization or state control) to overcome the ‘Tragedy of the Commons’ were not as effective as expected. Human cooperation takes place within a particular natural or social environment. The immediate microenvironment has obvious impacts on individual behaviour. The wider context of the political, sociocultural and economic environment and natural resource systems is also relevant for collective action. Moreover, individuals can influence the method of setting and altering the rules that apply to them. The individual is a component of the collectivity, so that the success of collective agency directly determines whether the individual is better-off or worse-off. Defining a broad concept of the commons is for the purpose of overcoming dilemmas of sharing, cooperation and collective action. So what exactly could be included in the concept of the commons? Who are the stakeholders? And how can a complete and clear awareness of the commons be secured for each stakeholder? This paper aims to expand the current understanding of the commons and to reinforce its explanatory strength across different theories of the commons.",2
15.0,1.0,Evolutionary and Institutional Economics Review,21 August 2017,https://link.springer.com/article/10.1007/s40844-017-0082-5,"Thorstein Veblen, the evolution of the predatory instinct, and the origins of agriculture",June 2018,Vincent Barnett,,,Male,Unknown,Unknown,Male,"In The Instinct of Workmanship and the State of the Industrial Arts, Veblen provided his most substantial analysis of the nature and role of instincts in generating human behavior.Footnote 1 However, few later institutionalist economists have attempted to reconstruct or build on Veblen’s understanding of instincts, in part because the rise of behaviorism in the USA across the first half of the twentieth century quickly eclipsed the type of instinct psychology that Veblen had used to ground his approach to understanding economic behavior.Footnote 2 However, recent developments in the field of evolutionary psychology have brought the instinct-based agenda back into intellectual focus (Cosmides and Tooby 1994; Barnett 2015),Footnote 3 so a re-examination of Veblen’s work on instincts using more contemporary research is certainly a worthwhile endeavor. In order to contribute to the potentially rewarding task of further developing Veblenian instinct analysis, this article will consider in detail the nature of one particular instinct that Veblen hypothesized as existing—what he called the predatory instinct. It will explore the nature of this particular instinct, how it evolved across geological time, its concomitant role in the long-run evolution of various systems of agricultural and industrial economy, and how it was linked to the evolution of the cognitive architecture of the human mind. It will also trace the origins of Veblen’s conception of predatory behavior in the work of various authors in the fields of psychology and anthropology, which he had studied as part of his pioneering effort to transform economics into a post-Darwinian evolutionary science.Footnote 4
",7
15.0,1.0,Evolutionary and Institutional Economics Review,27 December 2017,https://link.springer.com/article/10.1007/s40844-017-0089-y,The contribution of J.R. Commons to migration analysis,June 2018,Franklin Obeng-Odoom,,,Male,Unknown,Unknown,Male,"The current migration crisis requires distinctive institutional economics answers: what causes migration? What are the political economic effects of migration on the host population and on the host economy more generally? Does migration increase or decrease economic activities in the country of origin? What policies can be pursued? Answers to these questions have typically been shaped by three approaches—(1) neoclassical, (2) new economics of labour migration and (3) historical–structural synthesis—and the interactions among them (Abreu 2012; Piché 2013; Pröbsting 2015; Obeng-Odoom 2017a). The neoclassical methodology is based on an individualist unit of analysis and the assumption of rational economic man. This homo economicus—unlike the real man—makes only rational calculations based on a calculus of costs and benefits which are either actual or perceived. It is this calculus that was canonised as ‘the laws of migration’ by Ravenstein (1889) and later formalised by Lee (1966) as the push–pull framework of analysis. Within this approach, there are specific theoretical positions such as the Harris–Todaro model (Todaro 1969; Todaro et al. 2006) that claims that rural–urban migration happens in response to perceived income differences. There are qualifications to such theories, of course: the role of information availability and transaction costs, for example, but, overall, these models are based on a methodology of neoclassical economics individualism. The new economics of labour migration approach requires some comments too. Broadening the unit of analysis of the neoclassical model to the household, it tends to claim that it is the interaction between the individual and the household that drives the migration process. Therefore, the rational economic man is replaced with the rational economic household. Much like the relationship between neoclassical economics and new institutional economics, this approach to migration is consistent with the core neoclassical economics model. The historical/structuralist approach to migration provides an entirely different ontological and epistemological framework centred on institutions such as class and race. The approach is sensitive to historical forces and draws attention to fundamental structural processes. The most advanced methodological approach within this eclectic approach is the Marxist framework of migration, which emphasises how structural capitalist forces push and pull labour from one area to another and how, in the process, capitalist processes shape migration. It has been applied by Marxist political economists such as Rosewarne (2010, 2012, 2014, 2016) and Castles (2011, 2012, 2015). Although, in practice, there is much mixing of paradigms in policy making (Obeng-Odoom 2017a), the neoclassical approach tends to offer inspiration to conservative politicians, while new economics of labour migration approach inspires neoliberal policies. Humanistic policies are hard to place because they tend to have a broad base of ideas (including Papal humanism), but some of it can be traced to the historical/structural analytical perspective too. What about institutional economics: what can it offer and how distinct or related is this approach to existing approaches? This essay highlights and clarifies a simple institutional economics framework provided by Commons (1862–1945). Following Zouache (2017), I use the notes provided by Commons in his 1907 classic: Races and Immigrants in America as fairly representative of Commons’ approach to migration. The period within which the book was written is significant. Commons was in his final stage of intellectual development—after his initial thought (1882–1894) characterised by strong religious views formed through his associations with members of the Social Gospel Movement and his second stage (1895–1899) characterised by some broadening out of his intellectual sphere of action from the church to legislative bodies as one important source for people to change the working rules of society (Gonce 1996). He was also without a formal university position in the lead up to writing the book, so he had much more time to do ethnographic research, to leave with and to study among immigrants, to investigate how they were organising or not, and to observe at first hand the negotiations between workers and employers —as Commons himself narrates in chapter IV of Myself (Commons 1934/1964), his autobiography. In terms of practical policy experience too, Commons had been tasked by the US Industrial Commission to work on migration in the 1899 (Commons 1934/2009, p. 2) –1900 period (Commons 1934/1964, p. 67). He wrote up his report in 1902 (Commons 1934/1964, p. 77) and, in 1904, returned to a university position again, bringing with him his real world encounters (Gonce 1996, p. 660). Races and Immigrants in America (1907) was written three years after Commons returned to academia. Compared to his other books, the literature on this masterpiece is thin. Given that migration was an important focus for Commons and the name of the book itself, as Commons recalls in Myself (Commons 1934/1964, p. 74), ‘was the title of one of my first courses of lectures at the University’, Zouache’s (2017) recent analysis of the book is a valuable addition to the literature on Commons. Zouache’s (2017) three contributions are first ‘to dissect his conception of the relation between races, institutions, and economics’ (p. 343), second to ‘show how Commons’ conception reveals the intimate but often hidden connection between race, culture, and economics and the ambivalence on the relative effects of heredity and culture than neo-Lamarckism permitted’ (p. 343), and finally to demonstrate that ‘Commons’ volume, with its ambiguity, its racial references, but also its treatment of religion, culture, and nationality, shows that biological racism and culturalism are intimately interrelated’ (p. 344). My focus is rather different because I seek to clarify Commons’ approach to analysing migration more widely. The focus on migration is particularly important because according to Commons himself, it was through his study of migration that he came to develop many of his ideas such as ‘working rules’ and his policy emphasis on trade unions as an important institution to improve the conditions of labour. Indeed, Commons’ entire institutional economics is fundamentally shaped by his study of migration commissioned by the US Industrial Commission in 1900 (Commons 1934/1964, p. 67). As Commons notes in Myself (1934/1964): After about 6 months …studying the immigrants themselves, I took a trip to the headquarters of about half of the national trade unions to discover the effects of immigration on unionism (Commons 1934/1964, p. 71)… While attending… their national ‘joint conference’ of mine workers and their employers, I was struck by the resemblance to the origins of the British Parliament. On one side of the great hall were nearly a thousand delegates from the local unions, an elected representative body. On the other side were about seventy employers appearing directly, without election, as owners of the coal mines. It was evidently an industrial House of Commons and House of Lords, but without a King. I named it ‘constitutional government in industry’… The essential point, as I learned in 1900 at the miners’ joint conference, was the elimination, as far as possible, of a third party, the arbitrator—whether King, legislature, governor or dictator, handing down rules and regulations from above—and the substitution of rules agreed upon collectively, by conciliation. It was to be, as I then learned in 1900, not Democracy in the historic meaning of a majority overruling the minority, but representation of organized voluntary but conflicting economic interests. After thirty years I attempted to work my discovery of 1900 into a system of institutional economics (Commons 1934/1964, pp. 72–73, emphasis added). Therefore, studying Commons’ approach to migration is vitally important. I argue that Commons’ approach is radically different from the mainstream in terms of its unit and scale of analysis as well as its concept of labour. Grounded in trans-actions rather than homo economicus, while being both multi-scalar and historical, this approach does not consider labour as ‘capital’ in which to ‘invest’ to produce more goods or commodity merely to be sold as migrants. In turn, its policy orientation is also entirely different. This institutional framework has much in common with the Marxist alternative, for example, in terms of emphasising the class basis of migration. However, it is sufficiently different from that approach to constitute a distinctive paradigm that can help to better understand and transcend migration. That said, the approach by Commons has important drawbacks. Its sympathies with eugenics or ‘scientific racism’ weakens its grasps of how race moulded migration in the twentieth century in which he was writing. In addition, the overconcentration of Commons' policy proposals on destination settlements and the disconnect between his diagnosis of the causes of migration and some of his policy proposals limit Commons' otherwise useful alternative. However, these problems can be addressed without losing the essence of Commons' approach.",4
15.0,1.0,Evolutionary and Institutional Economics Review,18 January 2018,https://link.springer.com/article/10.1007/s40844-017-0090-5,"Profit sharing, labour share and financial structure",June 2018,Kenshiro Ninomiya,Hiroyuki Takami,,Unknown,Male,Unknown,Male,"How does income distribution between wages and profits affect aggregate demand and economic growth? Many believed that the Japanese economy was strong because Japan adopted the Profit-Sharing rule (or the Japanese management system) in the 1980s. The Profit-Sharing rule refers to a compensation system that allots a fraction of a firm’s profits to its union or workers. Despite the belief in the strength of the Japanese economy, Japan was trapped in a prolonged recession after the collapse of the bubble economy in the 1990s. Japan’s management system and its regulated financial system have come under criticism as factors of the prolonged recession.Footnote 1
 On one hand, many studies have discussed the effects of adopting the Profit-Sharing rule since Weitzman (1984) advocated it as a prescription for stagflation in the 1970s. Several studies show that introducing the Profit-Sharing rule lowers the rate of unemployment and reduces price levels (see Weitzman 1985, 1987; Fung 1989; Sørensen 1992). On the other hand, Steindl (1952) pointed out that consumer demand would stagnate with adoption of the rule, since increasing profit margins imply a rise in profit share. As a result, the emergence of oligopolistic competition with increasing profit margins tends toward a state of chronic stagnation. This view is called a ‘stagnationist regime’.Footnote 2 Further, Bhaduri and Marglin (1990) insisted that a higher profit mark-up would stimulate aggregate demand and raise capacity utilisation. This view is called an ‘exhilarationist regime’.Footnote 3
 This strand of literature is highly interesting, but it does not take financial factors into account. Minsky (1986) introduced the financial instability hypothesis, proposing that the complicated financial structure underlying the capitalist economy generates business fluctuations and crises.Footnote 4 Wolfson (1994) stressed the fall in profit rate as a factor of financial instability. Jarsulic (1990) showed that a fall in the rate of profit share would induce financial fragility in a macroeconomic model. Ninomiya (2007a) introduced the dynamic equation of debt burden into a macrodynamic model and demonstrated that a fall in profit share would make the economy unstable. Fanti and Manfredi (1998) examined the effects of the Profit-Sharing rule on Goodwin’s growth cycle model and showed that the rule would stabilise the economy. However, Ninomiya (2007a) did not consider the Profit-Sharing rule and Fanti and Manfredi (1998) did not consider financial factors.Footnote 5
 We begin by deriving a condition that a firm and its union adopt the Profit-Sharing rule voluntarily. We emphasise that there are two factors in the Profit-Sharing rule in Japan. Next, we construct a macrodynamic model of financial instability and examine the effects of the Profit-Sharing rule. In our model, the sharing parameter affects the rate of capital accumulation. For example, a decrease in the sharing parameter might promote investment because of an increase in firms’ internal reserves.Footnote 6
 The game theoretical model in the first stage presents the endogenous determination of the ratio of the firm that adopts the Profit-Sharing rule.Footnote 7 For example, when firms that adopt the Profit-Sharing rule seriously decline, the labour share may not increase, even if the economy is expanding and labour unions bolster their bargaining power. The macrodynamic model in the second stage introduces the dynamic equation of these two factors. This paper aims to demonstrate that the stability of the dynamic system depends on the financial structure of the economy. In summary, the Profit-Sharing rule in Japan makes the economy unstable when the financial structure is fragile. The results imply that criticisms of the Profit-Sharing rule (or the Japanese management system) in the 1990s might be correct. However, we emphasise the importance of the stable financial structure.",
15.0,1.0,Evolutionary and Institutional Economics Review,19 January 2018,https://link.springer.com/article/10.1007/s40844-018-0091-z,Labour union strategy and non-regular worker unionisation: an institutionally adjusted insider–outsider model for Japan,June 2018,Nicolo Rosetti,,,Male,Unknown,Unknown,Male,"The Japanese labour movement faces significant challenges in representing the interests of the labour force. A high wage differential between regular and non-regular work, alongside falling union density, a falling share of regular workers, and a consistent failure of unions to represent non-regular workers, all suggest a narrow and weak representation of the labour’s interests at macro level. Unions pursue collaborative relations with management, even when this limits their bargaining power and precludes strike action as a negotiation tactic. At the same time, most do not represent non-regular workers, even if they now account for 40% of the workforce. We review the existing literature, which focuses primarily on the rise of non-regular employment and its detrimental effect on labour union density. We believe that long-term changes in employment patterns cannot be assumed to be exogenously determined; the relationship cannot be represented by a one-way causation, as this would entirely negate the role of the labour movement in shaping employment institutions through time. Labour union strategy causes a response from employers and political parties, and the interaction between these chosen strategies and responses is what shapes the labour institutions. We set out to investigate these trends from an institutional perspective by building an insider–outsider model of wage determination adjusted for Japanese labour institutions.Footnote 1 Our approach explains the changes in employment patterns endogenously, by creating a model that links subjective, institutionally bound choice sets of unions with macro-level outcomes. These outcomes shape the following period’s institutional constraints on choice sets and determine the path-dependent long-term direction of institutional change. We use UA ZensenFootnote 2 as a case study, looking at how the major alternative trendsetter in the unionisation of non-regular workers was able to grow its membership base by over 500,000 between 2002 and 2012 and what lessons can be drawn from this strategy. Next, we analyse membership data for major labour organisations to examine past efforts towards inclusion of non-regular workers. Finally, we review the implications of our findings for labour organisations and discuss the scope of further research on the subject.",
15.0,1.0,Evolutionary and Institutional Economics Review,13 April 2018,https://link.springer.com/article/10.1007/s40844-018-0092-y,Effective extension of trading hours,June 2018,Kotaro Miwa,,,Male,Unknown,Unknown,Male,"Periodical market closures are said to have a negative impact on trading activity and price efficiency. First, considerable studies, for example, Kyle (1985), Glosten and Milgrom (1985), Foster and Viswanathan (1990), and Easlay and O’Hara (1992), show that public and private information accumulates overnight while information asymmetry declines over the course of trading periods. These studies suggest that market closures may induce a delay in the incorporation of information into stock prices, which can widen divergence between stock prices and their fundamental values. Second, periodical market closure may reduce price stability, especially at the beginning and end of the trading session on an intraday basis. Wood et al. (1985) and Harris (1986) find that a standard deviation of returns is especially high at the open and close of trading. Finally, the studies reveal that periodical market closures can cause skewed trading activity, that is, trading is concentrated at the open and close of sessions. Jain and Joh (1988) document a U-shaped intraday pattern of trading volume; trading volume is especially high at the open and close of the regular-hours session. It seems that this problem can easily be solved by extending trading hours. In fact, the extension of trading hours for stocks to improve trading opportunities and price efficiency has increasingly been discussed (Osaki 2014). In several markets (e.g., NYSE and NASDAQ), both pre-market and after-hours sessions have already been introduced and the Tokyo Stock Exchange is considering extending trading hours by implementing these extended-hours sessions and/or shortening the midday recess. However, the problem is not so simple. Miwa and Ueda (2017) show that the extension of trading hours could disturb price formation and trading activity if investors’ participation during the extended-hours session is limited. Actually, there are a limited number of market participants during the extended-hours session: there is less than 5% as much trading per time unit in after-hours sessions versus regular-hours sessions in the U.S. stock market (Barclay and Hendershott, 2004). Thus, it is possible that the extension of trading hours has a negative impact on trading activity and price formation; the effect of extending trading hours is not straightforward. To uncover the complex feature of the effect of extending trading hours, it is necessary to analyze what kind of the extension is effective on price efficiency and price stability. In particular, while Miwa and Ueda (2017) consider a quite simplified case where an extended-hours session ranges from the close of the regular-hours session to the opening of the next day’s regular-hours session, many stock exchanges are considering or have implemented a more complicated extension of trading hours; in the U.S. stock markets, extended-hours trading occurs in two sessions: the pre-market session (from 4:00 AM to 9:30 AM) and the after-hours session (from 4:00 PM to 8:00 PM). Thus, I should address the effect of implementing the pre-market session and the after-hours session, and the most beneficial session length. The analysis can definitely give an important indication of the design of trading hours for exchange markets. In this study, I expand the model of Brock and Hommes (1998) and Miwa and Ueda (2017) by incorporating the pre-market session and the after-hours session. I analyze whether implementing the after-hours session and pre-market session can improve price efficiency and trading activity, and investigate the duration of the most beneficial (or the least harmful) session length. I evaluate the effect of the implementation of those sessions on price efficiency and trading activity by the following three factors: the deviation between stock prices and fundamental values (as a proxy for price efficiency), volatility of stock returns especially at the open and close of regular hours session (as a proxy for price instability), and trading volume especially at the open and close (as a proxy for trade concentration). As above mentioned, if the extension mitigates the negative effects induced by periodical market closure, the deviation from fundamental value, return volatility, and trade concentration at the open and close should be smaller than when there is no extended-hours session. The paper is constructed as follows. Section 2 derives the market model with limited pre-market and after-hours sessions trading; Sect. 3 presents the simulation settings; Sect. 4 presents simulation results for whether and when the extension of trading hours is beneficial; Sect. 5 discusses the results; and Sect. 6 concludes.",1
15.0,1.0,Evolutionary and Institutional Economics Review,20 June 2016,https://link.springer.com/article/10.1007/s40844-016-0044-3,The past and future of evolutionary economics: some reflections based on new bibliometric evidence,June 2018,Geoffrey M. Hodgson,Juha-Antti Lamberg,,Male,Unknown,Unknown,Male,"After decades when the word was taboo in the social sciences, since 1980, the word ‘evolution’ and claimed ‘evolutionary approaches’ have proliferated, particularly in areas related to business and innovation research. From economics (Boulding 1981; Nelson and Winter 1982, 2002; Friedman 1991; Hodgson 1993, 1998, 1999; Nelson 1995; Witt 2003), the terms ‘evolution’ and ‘evolutionary’ have spread to other disciplines including organizational, innovation and management research (Aldrich and Ruef 2006; Durand 2006). Some scholars (Aldrich 1999; Geroski 2001) have argued for a meta-theoretical ‘evolutionary perspective,’ to express the conceptual core and unite separate disciplinary approaches. But as yet there is no agreement on this core. Modern ‘evolutionary economics’ of the Nelson–Winter variety has had more impact on research in business schools and departments of innovation studies than in departments of economics. This is confirmed by evidence (including some provided here) that it receives more citations from business and management journals than from core journals of economics. This is neither surprising nor necessarily alarming, as the analytical perspectives of mainstream and evolutionary economics are quite different. But this bibliometric study also confirms the fragmentation of developments in evolutionary economics, and highlights problems for this stream of research.Footnote 1
 Because of changes in the character of mainstream economics and the growth of interdisciplinary academic arenas, such as business schools, many practitioners of Nelson–Winter type ‘evolutionary economics’ emigrated from departments of economics. This migration was most pronounced in the United States and other Anglophone countries, where business schools expanded rapidly. But the development of business schools was not uniform globally, and other countries tell a different story. Notably, in Italy and Germany, for example, evolutionary economics retains a stronger footing in departments of economics. Residence in business schools, departments of innovation studies, or departments of science policy, created both opportunities and problems for the theoretical development of this field. The opportunities and successes are apparent in the rapid impact of ‘evolutionary economics’ in empirical studies of technological change, national innovation systems, and science policy (Dosi et al. 1988). On the other hand, theoretical cohesion and communication are more difficult to develop with researchers located in multiple disciplines or subdisciplines. In such contexts, a key developmental problem for evolutionary economics was that of enhancing its theoretical core through trans-disciplinary conversations. These trans-disciplinary features (being partly located in business schools and other interdisciplinary institutions) give evolutionary economics a unique character. The lack of a consensus over a clearly identified theoretical core, combined with the well-known communication barriers between disciplines, mean that the standard sociology of scientific disciplines (Whitley 1984, 1986) is inadequate to deal with evolutionary economics. As Van Raan (2000) argued, bibliometric analysis can at least have a preliminary diagnostic role in dealing with the problems of interdisciplinarity, by making communicative ‘maps’, identifying key actors, works and research areas, and showing structural changes in the field through time. Our analysis helps to assess the nature and scale of the problems for ‘evolutionary economics’. Its theoretical fragmentation has been noted by other authors, but our analysis is more extensive. We analyze published ‘evolutionary’ research in the fields of business studies, economics, and sociology by combining co-citation analysis (Small 1973; Griffith et al. 1974) with cluster and document centrality analysis.Footnote 2
 No previous bibliometric study in this research area is as large and systematic as ours. Two earlier studies are confined to evolutionary economics (Dolfsma and Leydesdorff 2010; Silva and Teixeira 2009); Dolfsma and Leydesdorff (2010) addressed research linked to the Journal of Evolutionary Economics only; Witt (2008) built on an opinion survey of meanings of the word ‘evolutionary’ adopted by users in the field. Another study confined itself to innovation and technology research in the context of evolutionary economics (Verspagen and Werker 2003). Bhupatiraju et al. (2012) applied network analysis to a citation database confined to the fields of entrepreneurship, innovation studies, and studies in science and technology. While these three fields have links with ‘evolutionary economics,’ they exhibit independent trajectories and are no more than segments of its whole field.Footnote 3
 Only two earlier systematic reviews take a longitudinal bibliometric approach and attempt to show the evolution of the field through time (Dolfsma and Leydesdorff 2010; Silva and Teixeira 2009). Dolfsma and Leydesdorff (2010) consider the years 2000–2005 and 7,534 journals citing or cited by the Journal of Evolutionary Economics. Silva and Teixeira (2009) addressed the 1958–2008 period and use 2,510 journal articles for their survey. In comparison, our study covers 1986–2010. We accessed 8,474 articles, which in turn cited 349,750 further usable works. This is by far the largest bibliometric study of the ‘evolutionary’ field to date. This is also the first systematic bibliometric analysis covering economics, sociology, management, and business. Because of its multidisciplinary scope and time span, it is able to address the development of such key problems as fragmentation and disciplinary division, to an unprecedented depth and degree. In particular, the nodal role and lack of development of immediate offshoots from Nelson and Winter (1982) is less clear in earlier studies. Our analysis maps the research field, including the most influential authors, publications and research areas. It identifies a diversity of ‘evolutionary’ research clusters, of which few cross disciplinary boundaries. A crucial problem highlighted by our study is for this ‘evolutionary’ field to maintain a common research agenda and momentum across these boundaries. Loose and vague terms such as ‘evolution’ and ‘selection’ are insufficient to retain connectedness and interdisciplinary conversation, while enhancing theoretical development across a highly diverse field of study. This essay has three further sections. Section 2 gives an overview of our bibliometric analysis of the ‘evolutionary’ field and draws out implications for ‘evolutionary economics’. Section 3 addresses the problems of identity and strategy for ‘evolutionary economics’. Section 4 offers a further discussion and concludes the argument. The Appendix outlines the bibliometric techniques employed.",22
15.0,1.0,Evolutionary and Institutional Economics Review,06 June 2018,https://link.springer.com/article/10.1007/s40844-018-0094-9,Hodgson’s bibliometric report and the reconstruction plan of economics,June 2018,Yuji Aruka,,,Male,Unknown,Unknown,Male,"The work of Hodgson and Lamberg (2016) is an unprecedented work on bibliometric analysis of evolutionary economics in its range and scope. First, this analysis retrieved 8474 articles. Second, it separated the development of the time profiles in the subsequent 5 year periods: 1986–1990, 1991–1995, 1996–2000, 2001–2005, and 2006–2010. Thus, this study was able to refer to the present trend and the fashions around the different and cross disciplines. In this sense, the observed facts that Hodgson and Lamberg discovered seem to be almost all reasonable from our institutional overview. Furthermore, in connection with the author’s great intelligence, this bibliometric analysis has finally given an insightfully useful perspective to reconstruct economics for us. In spite of wider penetrations/success over diverse fields except for economics over years, as Hodgson and Lamberg (2016) stated, it must be true that almost no one was successful in replacing the core engine with the main stream’s one. His suggestions fortunately coincide with our original plans appeared in Aruka (2017) to revive economics ontologically. This is the reason why I decided to comment on this article. As Hodgson and Lamberg recommended, the raison d’être of evolutionary economics must be the innovation of the core theories of the traditional economics. In other words, the belief of the main stream economics is that everything is at best. On the contrary, our belief is that everything is evolving. We continue to comprehend that “evolutionary economics” as “economics to be evolved”. In this point of view, first of all, the reconstruction of political economy mattered. In this sense, Japan Association for Evolutionary Economics (JAFEE) adopted another purpose against the standard definitions of evolutionary economics at the very outset of the foundation. She was sustaining her effort to construct the core theories, such as the theory of value, as an alternative to the main stream. Based on that, Shiozawa (2007, 2015) used to sub-tropical geometry to provide a general proof of international trade theory of value in the case of the transaction of three commodities including intermediate goods among three countries. This is the great innovation since Ricardian theory of value. Due to Shiozawa’s contributions, the existing theory of comparative cost and its marginal version has lost their justification. Once upon a time at Cambridge UK, Kaldor (1955) purposed constructing “alternative theories” of distribution to replace the neoclassical marginal productivity theory. He and his followers’ attempts have never been successful at rejecting the existing core theory of marginal productivity with their doctrine. The Sraffian theory of value survived longer in being an alternative candidate in value and distribution. However, the classical principles of political economy have been thoroughly replaced with equilibrium economics. Equilibrium economics always depends on the belief that everything is at best, even in serious depressions. It is interesting to note that the Leibzian idea of everything is at best was criticized by Voltaire (2015). We regard evolution as an idea of the antithesis against the everything-at-its-best proposition. The real world of evolution is then irrelevant to the grand design of God. Biological evolution does not contain an original intention to optimize the whole system to coordinate other organs consistently, at least at the outset of mutation. In this context, evolution is rather motivated by creative coincidence in Mainzer (2007) as a consecutive source of innovation. The same mechanism that causes the skin’s blood to condense causes blood condensation in the brain. In general, evolution does not give any normative optimality at all. Thus, the use of the idea of evolution becomes a strong support to the construction of the core theories of production and preference without the idea of optimization and human rationality.Footnote 1
 The main stream economics updated herself to survive. Despite of Hirofumi Uzawa’s (1928–2014) expectation, the rational expectational hypothesis internationally spread in the economics departments. As demonstrated in Aruka (2015), it is noted that the idea of bounded rationality is considered the raison d’être of rationality. That is to say, bounded rationality is not any modification of rational economics at all but a refined form of it. Reality is always skewed and polluted, because the secular world is not the world of God if we are faithful to the tradition of Thomas Aquinas.",1
15.0,1.0,Evolutionary and Institutional Economics Review,13 June 2018,https://link.springer.com/article/10.1007/s40844-018-0096-7,What methodology is suitable to describe diversity found in the course of history of economics as well as evolutionary economics?,June 2018,Makoto Nishibe,,,,Unknown,Unknown,Mix,,
15.0,2.0,Evolutionary and Institutional Economics Review,08 November 2018,https://link.springer.com/article/10.1007/s40844-018-0109-6,Towards an epigenetic understanding of evolutionary economics and evolutionary economic geography,December 2018,Jon Barrutia,Jon Mikel Zabala-Iturriagagoitia,,Male,Male,Unknown,Male,"Since its beginnings, economics, as a scientific discipline, has imported knowledge developed in other sciences (i.e., physics, ecology, mathematics, etc.). In particular, biology has extended to penetrate a considerable part of the scientific production on economics. As illustrated by Griffiths and Gray (2005): 422 today, a number of young but flourishing research programs have embraced the idea that critical aspects of the environment are a product, as well as a cause, of evolution, including organism–environment co-evolution and multiple heredity systems. In the light of the recent developments in evolutionary biology, Knottenbauer (2013: 305) calls for the need to introduce new analogies in economics based on the new concepts that have emerged in such aspects as evolutionary developmental biology, plasticity, epigenetic inheritance, etc. With this paper, we aim to further contribute to this debate by focusing on one of these contemporary findings, epigenetics, assessing how it can help evolutionary economics to gain a better description of economic phenomena. In such endeavor, we follow Hodgson (1988: 46) for whom, despite many problems and dangers, modern biology is a rich source of ideas and approaches from which a revitalized evolutionary view on economics may emerge. Our goal is to bring the concept of epigenetics into the methodological tools of the economics of organizations and territories, and examine it vis-a-vis new biologist and environmentalist approaches used by modern evolutionary economic geographers. We draw a biological analogy to introduce the concept of “Epigenetic Economic Dynamics” (EED) in the field of evolutionary economic geography. The interest of our conceptual contribution lies in understanding how firms and territories adapt to their respective environments in contexts of high velocity. Breslin (2016) deliberates on how interest in organizational and territorial co-evolution has recently increased due to business environments becoming faster, more competitive and turbulent, and increasingly complex and interconnected. Accordingly, even if we acknowledge that the two levels of analysis (i.e., firms and territories) have demarcated differences, in this paper we consider firms and territories as analogous entities, since both face the challenge of continuous adaptation to rapidly changing contexts. Firms and territories are hereby approached as complex adaptive systems, which show different principles such as self-organization, interdependence, co-evolution, complexity and chaos. High-velocity environments are characterized by being inherently unpredictable, having ambiguous structures and blurred boundaries. In them, routines are linked to newly created knowledge and unpredictable results (Eisenhardt and Martin 2000). According to Bourgeois and Eisenhardt (1988: 816), they are defined as those environments in which there is rapid and discontinuous change in demand, competitors, technology and/or regulation, such that information is often inaccurate, unavailable or obsolete. Epigenetics represents the study of heritable changes in the phenotype or gene expression caused by non-genetic mechanisms (Knottenbauer 2013: 297). The concept of epigenetics comes from the latest findings in the field of biology, particularly molecular biology. One of these new findings was the discovery that changes in the environment can exercise a permanent influence on genetic regulation and genetic expression, and that through this (epigenetic) mechanism, heritable changes in the phenotype can be induced (ibid). As we will discuss, and in line with Hay (2002) and Dawson (2014: 296), the theory of evolutionary biology essentially advocates that periods of evolutionary change (i.e., long periods of incremental adaptation) are punctuated by episodes of rapid change. In other words, change can be evolutionary, but it can also be revolutionary (Martin 2012b). These sudden and intense revolutionary episodic changes, in which radical changes are introduced in the environment, also demand radical interventions/adjustments in organizations/territories, so these can “survive” under the new conditions. As we will see, the epigenome constitutes the means by which organisms can have a rapid adaptation to the environment. These changes in gene expression are not recorded in the genome; however, they are inheritable. As a result, epigenetic processes may lead to changes in evolution as compared to what it was originally encoded. From our point of view, theoretical contributions like the one in this paper are relevant and pertinent in a moment in which most markets and sectors, even those regarded as traditional, are being “shaken” by elements originated in high-velocity industries such as software or communication technologies. These, sudden but strong “punctuated discontinuities” (in the language of Dawson 2014: 296) are not only influencing the survival of the firms embedded in such turbulent markets and industries, but also are having rapid implications on the adaptation (or difficulties to adapt thereof) of many territories. These organizational and territorial co-evolution and adaptation are also having consequences which are increasingly being observed in various spheres globally (e.g., economic inefficiencies, blockage of competition, barriers to innovation, tax evasion). The manuscript is organized as follows. Section 2 starts by introducing the concept of epigenetics and outlining the EED approach. Section 3 provides an overview of the concepts and methodological frameworks that have emerged in the field of evolutionary economic geography (EEG) such as developmental systems theory, evolutional developmental biology, and the concept of resilience, analyzing them through an epigenetic lens. Section 4 conceptualizes on the role that the time and the environment play in adaptation, providing a framework for EED within EEG. It first frames the time dimension between linear and punctuated temporality. Then, it divides the analysis of the environment into external (i.e., ecological and selective), operational and internal environments. It also places special emphasis on certain organizational routines related to decision-making and how these can be interpreted from an EED perspective. Section 5 provides a research agenda that would help the establishment of an epigenetics in understanding of EEG. Finally, Sect. 6 concludes.",4
15.0,2.0,Evolutionary and Institutional Economics Review,27 September 2018,https://link.springer.com/article/10.1007/s40844-018-0103-z,Market institutions and the evolution of culture,December 2018,Ginny Seung Choi,Virgil Henry Storr,,Female,Male,Unknown,Mix,,
15.0,2.0,Evolutionary and Institutional Economics Review,11 June 2018,https://link.springer.com/article/10.1007/s40844-018-0095-8,From the optimal planning theory to the theory of the firm and the market: a quest in Masahiko Aoki’s early works,December 2018,Masashi Morioka,,,Male,Unknown,Unknown,Male,"This article is one of a series of papers commemorating a great evolutionary economist Masahiko Aoki. See Volume 14, Issue 2 of this journal. Masahiko Aoki (1938–2015) is widely acknowledged as an originator of the game-theoretic firm theory and the comparative institutional analysis. However, his first book as an economistFootnote 2 The Economic Theory of the Organizations and the Planning (Aoki 1971a, hereafter referred to as Theory) is not a study on firms. It tackles the problem of how the planner of an organization should set the procedure of information exchanges with its members to find the allocation of productive resources maximizing a given objective function of that organization. Such information exchanges are required, because knowledge on the technological possibility of production is dispersedly held by individual production units. In Theory, Aoki does not hesitate to express his normative criticism against the contemporary capitalist system dominated by giant corporations, as well as the Soviet socialist system suppressed by an all-embracing bureaucracy. He hopes that the analytical framework of optimal planning theory provides some clues for search of a more desirable society in terms of both efficiency and in morality. A transition of Aoki’s research subject from the optimal planning theory to the theory of the firm was accomplished in his second book Model Analysis of the Firm and the Market (Aoki 1978, hereafter referred to as Analysis). One of the key terms in this book is quantity adjustment mechanism. In Analysis, this term represents a mechanism in which parts of goods are assigned to users based on their shadow prices in individual production processes. The first three chapters of Analysis restate the essence of the analysis in Theory with the aim of confirming superiority of this mechanism to the price adjustment mechanism under a non-convex economic environment. Its fourth chapter characterizes the firm organization as an institutionalized form of the quantity adjustment mechanism. Such a development of the argument in Analysis was accompanied with turnabouts of Aoki’s viewpoint in several respects. It is evident that the center of his concern shifted from the search of a desirable system to exploration of the existing system. Each planning process is now considered as “a prototype model” of adjustment mechanism working in the capitalist system. As to the theoretical approach, in the preface of Analysis, Aoki expresses “reflection” that “the analytic-technocratic method” of Theory was “not so fruitful”. According to his renewed view, distribution of the collective benefit produced by efficiency of the mode of information exchanges inside an organization induces its members to take part in these exchanges. In this way, the collective purpose of an organization is formed through such “coordination and integration of each member’s individual motivation” (Aoki 1978: ix). More concretely, Aoki presented a model in which the firm’s goal is formed through a collective bargaining between employees and shareholders under mediation by the executive manager. Naturally, the theory of the firm is inseparable from the theory of the market as a place, where firms interact each other. Analysis is also a treatise on the functions of the market. Taking a Keynesian position that the short-term adjustment of the markets lacks the mechanism to attain full employment of durable capital equipment and labor, Aoki further argues that the existence of unemployment and unused capacities is required so that the balance of production and demand is not hampered by unavoidable forecast errors. However, partly because Aoki himself did not further elaborate this point in later works, his market theory has not yet received the attention it deserves. The purpose of this paper is to revisit these Aoki’s theoretical quest from Theory to Analysis. Section 2 states that some historical and personal backgrounds under which Aoki chose the theory of optimal planning as his first research subject. Section 3 addresses characteristics of his theoretical approach in construction of the models of the optimal planning. Finally, Sect. 4 elucidates development of Aoki’s theoretical standpoint in Analysis and reflect on its significance.",1
15.0,2.0,Evolutionary and Institutional Economics Review,18 August 2018,https://link.springer.com/article/10.1007/s40844-018-0098-5,A method of building simulation model for organizational decision-making and inter-organizational control,December 2018,Shungo Sakaki,,,Unknown,Unknown,Unknown,Unknown,,
15.0,2.0,Evolutionary and Institutional Economics Review,31 July 2018,https://link.springer.com/article/10.1007/s40844-018-0099-4,Valuing unregistered urban land in Indonesia,December 2018,Franklin Obeng-Odoom,,,Male,Unknown,Unknown,Male,"One of the most important topics in institutional economics is value. ‘…In fact’, Commons (1924/1925, p. 377) famously wrote, ‘the concept of Value itself, on which economic theory, as well as legal theory, turns, is a synthesis of all these other concepts …, and, as such, is always a concept of the present importance of things and persons and classes of persons’. Yet, the question of value and the form in which property must take in the process of economic development also generate enduring controversies in institutional economics, especially in the field of economics and law (Boyce 2007), for which clarifications are urgently sought by institutionalists. Indeed, in 2006, the European Association for Evolutionary Political Economy (EAEPE) awarded its prestigious Kapp Prize to Otto Steiger for his contribution to clarifying aspects of the controversyFootnote 1 (Steiger 2006). Seven years later, Frank Decker published Ownership Economics in the ‘Routledge Frontiers of Political Economy’ to popularise Steiger’s award-winning work (which Otto Steiger developed jointly with the German institutional economist Gunnar Heinsohn) (Decker 2013/2016). Historically, landed property has been a major focus of institutional economics research. Indeed, under ‘the attributes and characteristics of property’ in Property and Contract in their relations to the Distribution of Wealth, the leading institutional economist, R.T. Ely (1914, p. 132), mentioned ‘value’ ‘first as an attribute of property’. For Ely (1914), estimating value—or valuation—is central to institutional economics precisely because in the question about wealth distribution and inclusive society, valuation provides an avenue for institutional economists to analyse and possibly resolve the tension between unbridled individual freedom and collective action. Jeremy Bentham set up the path for a new science based on rationalism, especially individual rationalism. In this approach, the emphasis is on individual choices made through acts of seeking a balance between pain and pleasure. In these acts, it is always the case that individuals have a natural right to property (Commons 1924/1925, pp. 371–373). Neoclassical economists and new institutional economists have extended this line of thinking into extreme individualism, contending that only formal regulation and, ultimately, hedonic valuation can resolve the problem (see, for example, de Soto 2000). This view legitimises land registration programmes being carried out around the world. Proponents (e.g., Field 2005, 2007) contend that registration provides an important source of information for valuation, which proponents contend, helps occupants to obtain better value for their properties. They imply, then, that systematic valuation is impossible in the absence of registration. It is important to carefully ascertain these claims. Existing studies do not do so. They can be classified into three. The first body of research is on how best to value in advanced capitalist societies (see, for example, Canonne and Macdonald 2003; Schulz 2003; Hordijk et al. 2011; Schnaidt and Sebastian 2012). A second body of research brings out inadequacies of valuation in the Global South (Sheehan 2011, 2012; Boydell 2010; Boydell and Baya 2011; Anderson 2006b, 2015). A final group of studies suggests ways of improving valuation in the Global South (Sirmans et al. 2005; Kauko 2012; Owusu-Ansah 2013; Bromley 2016). However, how valuers actually value unregistered land in their daily practice in the Global South is barely researched, an issue recurrently raised by the Royal Institute for Chartered Surveyors (see, for example, Nzioki et al. 2013). The book by Simons et al. (2008), Indigenous Peoples and Real Estate Valuation, is potentially a respectable exception. However, even this important work focuses on the incompatibility of Western and non-Western conceptions of value; not on valuing unregistered urban land or answering the following questions: (a) How is unregistered land valued? (b) What are the strengths and challenges in the existing approaches? and (c) in what ways can the challenges be overcome? Answering these questions is important for three reasons (e.g., Canonne and Macdonald 2003; Nzioki et al. 2013). First, developing an approach to valuing unregistered urban land is a major step towards responding to the widespread dissatisfaction with compensation valuation by the many urban informal settlers who have been evicted without compensation. Second, answers could substantially reduce the existing levels of dissatisfaction with how unregistered land is valued for compensation. Third, by seeking to fill this gap, the paper could potentially contribute to expanding the literature on valuation, which is currently dominated by the application of hedonic valuation. So, this paper attempts the task. In doing so, it adopts a ‘critical postcolonial institutional economic methodology’ (for other successful applications, see Zouache 2018; Obeng-Odoom 2016, Pollard et al. 2011) and uses field data from Indonesia, the case study area. The data contradict claims made by hedonists in three ways. First, valuers have adapted established valuation methods to value different types of land/tenure, whether registered or unregistered. Second, local institutions such as the courts uphold the innovations of valuers, if they are reasonable; and reject valuation of registered land, if deemed unreasonable; and third, local processes exist to resolve conflicts about valuation. On these bases, the paper argues that land title registration is not necessary to address the valuation challenges in Indonesia. Challenges to valuation could be better addressed by determining the highest and best current use of the property or the opportunity cost of current use in a process that entails systematic investigation, including interviewing a range of willing and able market participants and other stakeholders to arrive at values that are more reasonable to stakeholders and the courts. Valuation—whether for registered or unregistered land—can be more useful as a social practice, rather than as an attempt to estimate the rational calculations of an autonomous individual. The rest of the paper fleshes out these findings in four sections. The next section presents the theoretical debates between hedonic economics and J. R. Commons’ institutional economics. This confrontation leads to three research questions. Section 3 defends the particular institutional economics methodology used to address the questions under investigation. Section 4 analyses and discusses the findings.",8
15.0,2.0,Evolutionary and Institutional Economics Review,11 October 2018,https://link.springer.com/article/10.1007/s40844-018-0105-x,A note on discourse ethics and naturalized social contracts,December 2018,Kiichiro Yagi,,,Unknown,Unknown,Unknown,Unknown,,
15.0,2.0,Evolutionary and Institutional Economics Review,23 November 2018,https://link.springer.com/article/10.1007/s40844-018-0113-x,Interpreting the iterated dilemma games using the presentation like “cellular automatons”,December 2018,Yuji Aruka,,,Male,Unknown,Unknown,Male,"We often need to limit the field for discussion. In this sense, the Complex Adaptive System (CAS) is a good example neatly restricted by John Holland. By limiting to 4 properties and 3 mechanism of a system under inspection, Holland provide a basic frame of reference to analyze complexities that are mildly restrictedFootnote 1. The complex adaptive systemCAS is construed on the whole as a constellation of seven basics of four properties and three mechanisms Holland(1995, Chapter 1) Holland (1995): Aggregation (property) Nonlinearity (property) Flow (property) Diversity (property) Tagging (mechanism) Internal models (mechanism) Building blocks (mechanism) Any system characterized by the CAS mainly exhibits a perpetual novelty. The commodity markets are an example of the CAS, because it continually produces a series of new products, or re-labels existing ones and presents them as new. We do not yet need to analyze complexities in general. As well known, in particular, genetic algorithm is a powerful method suited to this idea. However, it has been noted that such a successful application is viable given another classifier system as well. To use an alternative, we need more certain basis, like classifier system, cellular automaton, and so on in particular cases respectively. We argue in detail later interactive cellular automata(ICA). It is interesting to learn that the term of ontology is used to be employed in Information Science. Wolfram language contains a new operator called “entity”Footnote 2. “The core meaning within computer science is [that of] a model for describing the world that consists of a set of types, properties, and relationship types.”Footnote 3 According to this definition, a traditional game like prisoner’s dilemma is definitely formulated by a simple set of explicit types, properties, and relationship types. Thus, it is important for us to incorporate a core system like a classifier system. The traditional game was much considerably modified by Axelrod and Hamilton (1981), as well as Nowak and Sigmund (1993) by transforming one-shot game into iterated games inset by the random matching mechanism, as seen later. This transformation is clearly in line with the idea of genetic algorithm. Hence, we must analyze the mechanism of evolution of the game will evolve, without imposing something special akin to human rationality on it through rules. When arguing for a CAS, a biological system is a good example. Human body is an evolving creature that has adapted over time to various occurrences. Adaptation dominates our evolutionary process, either locally or globally. It should be noted that individualistic humans behavior cannot be allowed to exist infinitely. Due to their questionable mortality, people who act in this manner are invariably at a risk of harm. Thus, we must notice the next proposition. 
Winning game differs from the maximizing payoffs.
 Before we examine the iterative games, we offer a few observations. As I previously cited elsewhere, coagulation is a good example. Indispensable reaction was never designed as an optimal configuration to maintain human body. The body does not have a universal mechanism for such local reactions as coagulation. Coagulation is indispensable for both lower and higher organisms. However, when coagulation occurs in the latter, such as in the human brain, it can be fatalFootnote 4. This raises the question of whether this kind of immunoediting paradox is a Trojan horse. The coagulation system overlaps with the immune system. Coagulation can physically trap invading microbes within blood clots. Moreover, some products from the coagulation system can contribute to the innate immune system through their ability to increase vascular permeability and act as chemotactic agents for phagocytic cells. The process of coagulation and fragmentation can be applied to social modeling and mathematically analyzed within a coagulation fragmentation model.Footnote 5
 However, a classical inference confines the game to a narrow set of rationality principles. By resorting to classical inference, thus, we can not arrive at a unique solution. If a broader frame is designed to allow players access to memories of their past moves and strategy scenarios, the case of the memory of the last two periods can help devise \(2^{21}\) by \(2^{21}\)(4.4 trillion) combinations of strategies in a two-player game. Therefore, we first examine a new game mechanism by using the example of an iterative, standard, two-player, two-strategy game. We have to introduce some interactive decisions in the most recent two turns in addition to informational structure. von Stackelberg (2011) was the first to recognize the inference by observing the complex interaction among the participants in a duopolistic market. However, the procedure of examining strategic interactions using massive amounts of data is beyond human capacities. Thus, we need to alter our approach from a mathematically pedantic inference to an agent-based one. Econophysics emerged at the end of twentieth century owing to its facility with big data. At the same time, using large amounts of simulated data, agent-based modeling was used for the network analysis of heterogeneous interaction and the examination of sentiments. In the following, we provide a detailed analysis of this iterative game. We represent the total number of situations and nodes in the strategy tree as followsFootnote 6: We quickly summarize traditional game theory for readers unfamiliar with economics and then introduce a program provided by one of Wolfram Demonstration Projects. We consider the 2-by-2 game, i.e., involving two players and two strategies game. In general, the pay-off matrix of the 2-by-2 game is as follows: The payoff is confirmed when the choices of both players are confirmed. Let \(i \not = j, i; j =1,2\). For simplicity, choice i is denoted by Ci, and the payoff of Player i is represented as \(P_i(C_i, C_j)\), which indicates the payoff of Player i when Player 1 employs \(C_i\), Player 2 employs \(C_j\). If it is the case, it then holds that The strategy set \(\{C_i C_i\}\) is called Nash equilibrium. In other words, (1), i.e., Nash equilibrium is the situation where the payoff is reduced if Player 1 changes strategy from i to j. On the other hand, we impose another condition. If (1) and (2) are satisfied, the strategy set is called Evolutionary Stable Strategy ESSFootnote 7. It should be noted that ESS contains the Nash equilibrium but not vice versa. It is easily verified that the solution of PD games must satisfy the above two conditions due to its special property of executing dominant strategy. The classic formulation of the prisoner’s dilemma (PD) was provided by TuckerFootnote 8. This game is called the iterated prisoner’s dilemma (IPD) . We assume two independent players. They execute their strategies without exchanging information concerning their activities. We exclude the possibilities of either coalition and negotiation. The players are not allowed to exchange information once engaged into the game. The PD game is based on a scenario involving prisoners who are allied. However, they are arrested and isolated from each other, and each is pressed to give up the other. The original PD game yields its pay-offs in the following manner: In the PD game, the pay-offs are allocated by Here, the negative sign of the payoffs above means “in prison”. In this numerical example, either Player 1 or Player 2 is sentenced to 2 years in prison if he/she refuses to talk and set free if he/she cooperates with the authorities. It is interesting to note that an asymmetrical arrangement is allocated on the off-diagonal cells in the PD game, and this asymmetric implementation can lead to a paradoxical result, irrespective of any symmetric cell on the diagonal elements. Hence, while Choice 0 implies defection, Choice 1 implies cooperation. It may be assumed in this article that Choice 0 is defection and Choice 1 is cooperation. However, the PD game in real-life seldom provides a unique solution as concluded in traditional economics. The choice is purely based on prioritizing the payoff at the cost of the relationship. The latter problem can be improved by incorporating the idea of iterated game. This idea can be generalized by implementing the random matching mechanism, as motivated by Nowak and Sigmund (1993).Footnote 9
 Irrespective of any payoff allocation, the game is repeatable. That is to say that the game will be repeated for some finite times. Given its iteration, we can observe a historical path of rounds of the game. The history of rounds of games may suggest a comprehensive/long-run scope for participants to ponder a serial move. Thus, the entity of the game will be newly enhanced by moving from a one-shot game to an iterated version. The iterated game then requires the examination of the more complicated interaction between players’ strategies in terms of a serial move of strategies over time. Fortunately, we have now sufficiently powerful simulators to inspect the historical paths of the 2-by-2 games provided by of Iterated Games of Wolfram Demonstration Project.Footnote 10 According to this program, the number of historical paths of the 2-by-2 games are set to a maximum of eight. By using the operator ArrayPlot, while Choice 0 is colored with “white”, Choice 1 with “black”. This graphical presentation reveals that the system under investigation is akin to a cellar automaton. This view essentially provides us an unobstructed view, in contrast to the traditionally restrictive view. By way of example, Player 1 might start by playing strategy 1 (Choice 1) and continue playing it unless the history of interactions were such that Player 2 (the opposing player) had played strategy 2 (Choice 0) on his/her last two turns. Such strategy management is a well-known part of the iterated game, and is called “trigger strategy”. We would not retaliate immediately even if the opponent has defected. At the least, we probe, i.e., check whether the opponent will continue to choose to defect. If the opponent defects in the next round, we retaliate. If the opponent retracts defection in the next round, we cooperate again. The roundabout retaliation is called Tit for Tat(TFT). Thus, the following theorem was proven in the traditional framework in accordance with Rubinstein (1979) and others: (Folk Theorem) Trigger strategy plan in  an iterated PD game can, on the average, give a greater total expected gain over the duration of the game period than any other trigger strategy, if the discount rate nearly is close to one. The idea of this proof is intuitive. The participating players always try to achieve as large an expected payoff as they can on average all over the game rounds. To that extent, players must be careful to avoid falling into cut-throat competition. In contrast, the affected player catch up with the defector by retaliating mildly, i.e., making moderate loss against the defector. If there is no retaliation at all, the affected player must profit less than the defector. Hence, the theorem holds. However, this result could be not always be guaranteed, unless players always control themselves and confine their reactions to their narrow span of the game rounds. In addition, little attention has been paid to the fact that the game field is fixed by narrow assumptions pertaining to rationality. Consequently, the variety of institutional designs may be lost in this context. We will subsequently discuss a similar problem also in production theory. In this article, we are rather interested in accepting any kind of players. Players are allowed to plan any kind of time development of their moves. Winning the game in any kind of environment differs from looking for a special/mathematical environment to make the greatest profit. In the secular world, the interest is commonly winning the game rather than behaving rationally. Thus, we must consider all possible situations generated by the game’s environment. This approach is similar to find the locus of the allele of a particular location in gene evolution. We will also refer to this issue.",1
15.0,2.0,Evolutionary and Institutional Economics Review,20 June 2018,https://link.springer.com/article/10.1007/s40844-018-0097-6,"Antifragility, the Black Swan and psychology",December 2018,Brendan Markey-Towler,,,Male,Unknown,Unknown,Male,"We know, and have known for some time in evolutionary and institutional economics, that greater survival in complex economic systems, the ability to acquire the resources necessary for life and its enjoyment, is endowed by the ability to introduce novelty and adapt to radical uncertainty. The purpose of this paper is to develop a theory of the psychological attributes which endow the ability to succeed thusly in acquiring resources and thus “survive” in complex evolutionary economies organised by radically uncertain institutions with reference to Nassim Nicholas Taleb’s notion of antifragility. An antifragile structure benefits and grows because of radical uncertainty, not in spite of it. We will seek to adapt this notion to the personality of the individual within a complex economic system subject to radical uncertainty. In doing this, we will seek to give evolutionary and institutional economics a deeper foundation in psychological theory, specifically personality and Jungian psychology. We will make use of a new theory of the mind as a network structure within which and upon which the psychological process operates to identify the psychological properties of the antifragile personality. We will relate these properties to the “Big Five” personality traits and also discuss their consonance with aspects of Jungian psychology. We will discuss how the theory of the psychological properties of an antifragile personality extends and unifies a number of perspectives on the predicates of success and survival in complex evolutionary socioeconomies organised by radically uncertain institutions. Specifically, our theory extends and unifies within it aspects of the Schumpeterian, Kirznerian and Lachmannian entrepreneurs, Ulrich Witt’s arguments about the necessity of adaptability and imagination, and Caroline Gerschlager’s exegesis of “agents of change” in evolutionary socioeconomic systems. Our theory allows us to make definite predictions about the psychological factors which support survival in complex socioeconomic systems ordered by radically uncertain institutions and offers practical strategic advice. Here, therefore, we take a somewhat different (though complementary) approach to the problem of responding to evolutionary change [Reschke and Kraus (2009)], by looking inward at the psychological characteristics of the individual rather than at the management attributes of the corporation. We will proceed as follows. First, we will discuss the problem of uncertainty in economics and introduce Taleb’s notion of antifragility as a property of some structure whereby it grows and develops because of radical uncertainty, not in spite of it, and adapt this to the psychology of the individual with respect to the antifragility of their personal knowledge. We will then introduce a new theory of the mind as a network structure within and upon which the psychological process operates and use it to identify the properties of a personality which causes personal knowledge to be antifragile—the antifragile personality. We will here also relate this to the “Big Five” personality traits as well as the Jungian psychology of the hero archetype. Next, we will consider the function of the antifragile personality in complex evolutionary socioeconomic systems ordered by radically uncertain institutions and establish its contribution to success and survival in such systems. We will here discuss how our theory extends and unifies perspectives on evolutionary economics from Schumpeter, Kirzner, Lachmann, Witt and Gerschlager. We then derive definite predictions from our theory about the psychological properties which contribute to success and survival in evolutionary socioeconomic systems ordered by radically uncertain institutions as well as practical strategic advice. We conclude by considering what this theory gains for evolutionary and institutional economics.",11
15.0,2.0,Evolutionary and Institutional Economics Review,20 November 2018,https://link.springer.com/article/10.1007/s40844-018-0117-6,Special feature: evolutionary approaches in theory and policy to the diversity of money,December 2018,Makoto Nishibe,,,,Unknown,Unknown,Mix,,
15.0,2.0,Evolutionary and Institutional Economics Review,21 August 2018,https://link.springer.com/article/10.1007/s40844-018-0101-1,"Tensions in the triangle: monetary plurality between institutional integration, competition and complementarity",December 2018,Jérôme Blanc,,,Male,Unknown,Unknown,Male,"This text aims at accounting for the plurality of money by emphasizing the delicate and evolving balance of the legal monetary systems and the continued existence of currencies that stay outside, though their continuation is subjected to chronic difficulties. In agreement with a range of institutionalist and socio-economic works, a working definition of money takes it as an institutionalised ‘principle’ for debt settlement, thus requiring its minting and its use as a unit of account and a means of payment (Alary et al. 2016). The starting point may be the misleading principle of national monetary exclusivism that was critically coined “one nation, one money” by Cohen (1998). Under such a principle, nations should establish their own currency whose use should cover all the monetary operations held within their territory. This principle, though still widely accepted, has proved to be heavily contradicted by facts (Cohen 1998; Blanc 2000; Gómez 2018). Since the years 1970 and the end of the Bretton Woods international monetary system, money have been experiencing major changes that yet could be seen as paradoxical. Currency substitution processes spread over developing countries and then the Central and Eastern European countries; they practically contested the “one nation, one money” principle. While countries abandoned their own national currencies in favour of the U.S. dollar (like Ecuador in 2000 or El Salvador in 2001) or in favour of a new supra-national currency (the euro), others, like the USSR or Yugoslavia, split into sometimes small pieces of a nation and created or restored national currencies. After a short period of monetary sovereignty, some eventually joined the Eurozone, as the Baltic States did. Another interesting feature is the blooming currencies that are linked neither to banks nor to central governments. They include systems set up by community groups, local authorities or even by private businesses for their own interests. They come in various shapes and sizes, ranging from electronic moneys on smart cards to systems for debt settlement by multilateral clearing arrangements via more or less sophisticated notes or vouchers. The space in which they circulate extends from small communal groups to politically borderless cyberspace. They are created to secure customer loyalty, to revitalise an area’s economy, to give impetus to social policies, to act as incentives to virtuous behaviour and for many other purposes. A growing number of activist associations, corporate strategic-watch managers, developers and futurologists are taking up the monetary terminology that seemed to be confined formerly to bankers and monetary authorities. Banks themselves are increasingly considering the disruptive capacity of these innovations (and especially the blockchain technology) while monetary authorities are trying to understand, control and regulate this wave—barely prohibiting them. A whole array of technological, legal, political and ideological boundaries have thus been and are continuing to be pushed back, calling for in-depth discussion of the usually undiscussed conception of the national monetary exclusivism. However, whereas observers cannot but recognize obvious processes like substitutions between national currencies and foreign ones, the question of the nature and the depth of the wider picture of monetary plurality is still to be answered: are they pathologies, in the case of which they could be cured, and the principle of national monetary exclusivism is safe? Or do they reflect the impossibility to cover all monetary uses by one single currency, an analysis that would lead to consider the plurality of money as a normal situation? This article draws on the second hypothesis. It intends to take the wide and increasing variety of moneys into account, notably with the help of Polanyian concepts. The position of these moneys with regards to the monetary system depends on cases and is anyway subject to evolution. This article proposes an analysis based on the representation of a triangle that displays fragile balances and evolving positioning: institutional integration, complementarity and competition. However, paying attention to the burgeoning of non-bank and non-State moneys since a few decades does not involve addressing the large-scale phenomena (since for the most part they are still not quantitatively widespread) but in identifying schemes whose features raise questions about the social representation of money today, about learned considerations on money and about some drivers for the long-run evolution of ordinary money. History displays indeed cases of marginal innovation that eventually penetrated and transformed the official monetary system—as shown by the history of the banknote. Section 2 presents the set of concepts that are considered necessary for such an analysis: the recognition of modern forms of special-purpose money, three ideal types of money and the forms of economic interdependencies on the basis of which spheres of uses can be conceived. The ideal types of money are public, business and associative moneys, depending on the nature of their issuer: political entities, companies or associations. In Sect. 3, the problem of institutional positioning of the plurality of moneys is represented by a triangle, with institutional integration, complementarity and competition as its three apexes. On this basis, Sect. 4 analyses the tensions that generate evolutions and hybrids in this framework. Section 5 briefly concludes.",
15.0,2.0,Evolutionary and Institutional Economics Review,15 October 2018,https://link.springer.com/article/10.1007/s40844-018-0104-y,"Why do people want currency? Institutions, habit, and bricolage in an Argentine marketplace",December 2018,Georgina M. Gómez,,,Female,Unknown,Unknown,Female,"A group of vendors in a weekend market in a square in Buenos Aires, Argentina introduced a system of own currency to be used 1 day near the Christmas 2012. The vendors produced home-made items, such as preserved foods and pastries, handicrafts, artisanal toiletries, and so on. They were barely managing to make ends meet with their sales, and with Christmas approaching, they were thinking of ways to obtain Christmas gifts for their friends and relatives with the scarce income that they were making. They recalled positive experiences with the complementary currency systems that thrived between 1995 and 2003, the Redes de Trueque (Gomez 2009, 2010), and some starting wondering how they could reinstate these complementary currencies. At the same time, some of the vendors were negatively affected by the demise of the Redes de Trueque, so they were reluctant to join in a similar scheme unless sufficient corrections could be made. The idea was implemented with 22 vendors in December 2012 and collectively allowed them to trade almost 2.5 times the value of the currency in circulation. The participants were satisfied with the result and the currency was burnt at the end of the day in a festive event meant to show how easy paper money can be made and unmade by a social group. The mainstream version (for example, Menger 1892; Samuelson 1967) on why money emerged as a key economic institution focuses on its efficiency advantage. It states that money reduces the transaction costs of bartering and allows values to be transferred across time and space (Hodgson 2004; Ingham 2002). However, the case study on which this research is based discloses that these motives could not has appealed the vendors. As soon as the supporting team started discussions with the interested ones, we found out that they were a group of 30 people that have known each other for years. We observed high levels of personal trust and social cohesion among the many diverse opinions, which we considered as preconditions for a functioning social system and for the implementation of a complementary currency (Krabbe 2015). Such traits reflected in a history of collective action, as the vendors organised the market together, negotiated the space with the municipality, and set a number of rules which they managed to enforce horizontally. The vendors made it clear that they needed means of payment to exchange goods for Christmas and did not wish to create a complementary currency system beyond that. Therefore, there was no need to create a system that allowed them to transfer value across time and space and an accountancy system could have solved the problem of the double coincide of needs typical of barter. Still, the vendors were determined to have a complementary currency with physical notes. The support team also explained to the vendors that they could adopt a non-perishable and easily divisible commodity as general equivalent, which scholars such as Adam Smith (1991 [1776]) identify as the origins of money. Instead, the vendors wanted paper money and the puzzle motivated the research question in this article: why do people want a physical currency? The market is the basis of the current study, which seeks to reflect on the nature of institutional innovation within the social and solidarity economy. The artisans asked for the support of experts to design and implement a complementary currency system that would facilitate trade in their market square. Such support was given by the author, municipal social workers, and assistants who kept records of the experience and which became the source of the present research. The supporting team on the ground consulted me throughout the initiative and shared their records that source the data for this article. The data of the case study were collected with a participatory ethnographic method, so while we kept systematic records of every meeting, we were also actively participating in the discussions and making suggestions. In addition, a short questionnaire was distributed to the participants before and after the experience with the goal of defining the profile of the participants and the impact of the experiment. The names used in this study are pseudonyms. We called the vendors’ currency the “money for a day” initiative, because it was a currency that the vendors used to trade with each other and expired at the end of 1 day. The article will first account for the details of the experiment and will analyse the various alternatives that the supporting team discussed with the vendors. It will then present an institutional analysis of the initiative, as described in Sect. 4. The article will argue that the use of money for trade stems from the habit (Veblen 1899) of using money and designing local money, and the choice reveals little of their reflections on transaction costs and efficiency. Veblen’s definition of habits of thought refers to the engrained practices that make individuals and groups almost incapable of engaging with alternative options. The study contends that the “institutional innovation” at the grassroots level presents a process of bricolage (Cleaver 2002; Levi Strauss 1962) based on elements of an established institutional structure that are adapted and recombined to fit the contingencies and contexts of groups, which leads to new institutions. In the approach of this research paper, the cumulative process of adapting, innovating, and creating institutions is evolutionary, because it sustains additional patterns of social interactions. These are “institutional facts” that depend on existent institutions on which future institutions are configured (Searle 1995, 2005). They become the “common knowledge” of a population that differs from that of the past and of other populations (Lewis 2008 {1996}: 78).",2
15.0,2.0,Evolutionary and Institutional Economics Review,09 November 2018,https://link.springer.com/article/10.1007/s40844-018-0108-7,Community dock: a new policy approach for altering institutions,December 2018,Takayoshi Kusago,Makoto Nishibe,,Male,,Unknown,Mix,,
15.0,2.0,Evolutionary and Institutional Economics Review,23 November 2018,https://link.springer.com/article/10.1007/s40844-018-0118-5,Special feature: Econophysics 2017: synergetic fusion of econophysics and other fields of science— Part I,December 2018,Yuichi Ikeda,,,Male,Unknown,Unknown,Male,,
15.0,2.0,Evolutionary and Institutional Economics Review,15 November 2018,https://link.springer.com/article/10.1007/s40844-018-0116-7,The community structure of business establishments and its properties: evidence from joint patent applications,December 2018,Hiroyasu Inoue,,,Male,Unknown,Unknown,Male,"In the face of rigorous competition, knowledge creation is becoming increasingly crucial for individual firms (Porter 1998). One potent means of creating knowledge is incorporating other firms’ knowledge. Indeed, collaboration increases the likelihood of generating combinations of ideas that result in an innovative product (Simonton 1988). Moreover, collaboration can speed up the delivery of innovations (Weisberg 2006), which may involve the parallel validation of initial conceptions and the serial implementation of final ideas. The literature has empirically examined the development of collaborations, and recent studies have indicated that teams can generate better outcomes on average than solo researchers (Wuchty et al. 2007; Inoue and Liu 2015), while other works reveal the increasing use of team-based over solo research (Merton 1979; Jones 2005). Since collaborations are becoming increasingly important, it is beneficial to know when we can expect to observe collaborations between firms. A previous study revealed how geographical frictions are related to existence of such links (Inoue et al. 2013) in the form of bilateral relationships. However, knowledge can be propagated to indirectly connected firms through firms with direct connections, as widely discussed in the literature (Gautam 2000; Fleming et al. 2007a, b; Forti et al. 2013; Gonzalez-Brambila et al. 2013). Therefore, we should consider a structure more complex than bilateral relationships. In other words, we should consider a network of firms that collaborate with one another and examine densely connected components of networks, i.e., communities, as a potent unit of knowledge propagation. Thus far, we have assumed that firms are the nodes in these networks. However, nodes can have different levels of granularity. In fact, we show later that when we use patent data, there are three different possible nodes: inventors, business establishments, and firms. If we were to use firms as the nodes, it would be difficult to discuss the location of inventions because firms can have multiple establishments. On the other hand, if we were to use inventors as nodes, it would be difficult to investigate firm involvement. Therefore, considering business establishments offers advantages over the other type of nodes. Since establishments are not explicitly reported in patent data, our data are unique. Based on the above motivations, we use Japanese patent data to investigate the significant characteristics of communities detected through a collaborative network. Specifically, we analyze four metrics. The first is the similarity in the total number of patents, which can be considered a quantitative proxy for knowledge-creation experience. Establishments with less experience may want to be connected with experienced establishments, or conversely, similarly experienced establishments may tend to be connected. The second metric is the similarity in the total number of citations of patents, which can be considered a qualitative proxy of knowledge-creation experience. As in the case of the quantitative measure, either homophily or heterophily may be found. The third metric is knowledge similarity. If establishments have totally different knowledge, it will be difficult for them to collaborate. Therefore, establishments with similar knowledge would be in the same community. However, as shown in the literature (Berliant and Fujita 2008), if most of the knowledge held by firms is the same, they will be less likely to create new ideas. Therefore, it is difficult to predict the results for this metric. The fourth metric is geographic proximity. A previous study demonstrates the significance of this measure for bilateral relationships (Inoue et al. 2015). However, no previous study has demonstrated its significance for communities. The paper is organized as follows. First, we describe our data and methods. Second, we discuss results. Finally, we conclude the paper.",3
15.0,2.0,Evolutionary and Institutional Economics Review,01 November 2018,https://link.springer.com/article/10.1007/s40844-018-0111-z,Identification of conduit jurisdictions and community structures in the withholding tax network,December 2018,Tembo Nakamoto,Yuichi Ikeda,,Unknown,Male,Unknown,Male,"Economic globalization has great influences on multinationals’ business activities and international taxation. The activities of multinationals cross more borders and become more complex. Accordingly, the number of tax treaties regulating conflicts of each jurisdiction’sFootnote 1 right of taxation increases from 1,200 in 1975 to 3200 today worldwide. Now each jurisdiction’s tax laws and treaties overlap and function as a single legal system (Ogata 2016) or network. However, it occurs a severe problem. When tax laws and treaties of multiple jurisdictions overlap, the differences among them create opportunities for unexpected reductions in the tax burden because each jurisdiction often legislates their laws and treaties individually and without consideration of the overlapping. The phenomenon, that is commonly called international tax avoidance, is one of the side effects of economic globalization. Meanwhile, in recent years, network science has made remarkable progress by offering accurate network information in various fields, and found networks follow the same principle, although the networks come from different fields (Barabsi 2016). They have revealed interesting results that had hitherto not been found in a broad realm from sociology to physics. Our study focuses on “Treaty Shopping,” which is one of international tax avoidance schemes, and attempts to highlight interesting aspects that have not yet been found far in the realm of international taxation. We make use of the methods and knowledge of network science because international tax avoidance emerges through tax laws and treaties’ working as one network. The previous studies incorporating the method of network science only studied withholding tax on dividends (Polak 2014; Van’t Riet and Lejour 2014; Hong 2017), but the other passive incomes, that are interest and royalties, are also important, and relationships among each jurisdiction have not been analyzed in a context of treaty shopping, even though experts pointed out the existence (Honjo 2012). “Treaty Shopping” is clarified through making these points clear by the methods of network science.",7
15.0,2.0,Evolutionary and Institutional Economics Review,20 November 2018,https://link.springer.com/article/10.1007/s40844-018-0110-0,Trade network reconstruction and simulation with changes in trade policy,December 2018,Yuichi Ikeda,Hiroshi Iyetomi,,Male,Male,Unknown,Male,"In this era of economic globalization, most national economies are linked by international trade and consequently form a complex global economic network. The interdependent nature of the global economy has become stronger with increases in international trade and investment. In Japan, it is expected that many small and medium enterprises will achieve higher economic growth in a free environment created by the establishment of economic partnership agreement (EPA), such as the Trans-Pacific Partnership (TPP). Apart from the economy, it is known that various collective motions exist in natural science. The phenomenon of collective motion is caused by strong interactions between constituent elements (agents) in a system. Interesting collective motions should emerge in a global economy under trade liberalization. This paper is organized as follows. In Sect. 2, preceding studies are reviewed. In Sect. 3, we propose a new model to reconstruct the international trade network and associated cost network by maximizing entropy based on local information about inward and outward trade flows. Subsequently, in Sect. 4, we will show the results of trade network reconstruction using the proposed model. In addition to these reconstructions, we will show the results for the effect of reduced/increased trade tariffs and trade barriers on the change in the community structure of the international trade network. Section 5 concludes the paper.",9
15.0,2.0,Evolutionary and Institutional Economics Review,07 November 2018,https://link.springer.com/article/10.1007/s40844-018-0112-y,Multiplicative random cascades with additional stochastic process in financial markets,December 2018,Jun-ichi Maskawa,Koji Kuroda,Joshin Murai,Unknown,Male,Unknown,Male,"Financial markets consist of different participants who watch the market with different temporal resolution and who react to price changes with different time horizons. For example, whereas intra-day traders watch the market continuously and trade multiple times each day, fund managers might reconsider their portfolios over periods of weeks or months. Competition among participants with different characteristic time horizons creates a heterogeneous structure of volatilities measured using different time resolutions. Past coarse-grained measures of volatility correlate to future fine-scale volatility more than the reverse process. This causal structure from a long-term to short-term scale volatility was first pointed out by Müller et al. (Müller et al. 1997). It is thought to be a stylized fact in financial markets (Arneodo et al. 1998a; Cont 2001; Lynch and Zumbach 2003), leading naturally to the idea of volatility cascade from long-term to short-term horizon by an analogy of an energy cascade in turbulence in fluid dynamics (Ghashghaie et al. 1996). In fully developed turbulent flow, the kinetic energy is injected by external forces to create eddies at the largest space scale. According to the phenomenological picture of hierarchical cascade in turbulence, they are deformed by fluid dynamics, with breakage into smaller eddies. Then the energy transfers to a smaller scale. This process is repeated hierarchically several times to the smallest scale, where the energy is eventually removed by dissipation (Richardson 1922; Kolmogorov 1941; Frisch 1997). Intermittency, a common phenomenon in many complex systems, is also a crucial characteristic both in financial markets and turbulent flows (Kolmogorov 1962; Mandelbrot 1963). Intermittency is characterized by the presence of irregular burst of the volatility of asset prices, the strength of velocity of fluids and other quantities of systems and creation of large kurtosis and fat tails in probability distribution of those quantities. One also frequently observes multifractality in the same systems showing intermittency (Frisch 1997; Schmitt et al. 1999). Financial markets and turbulence are not exceptions. Multiplicative random cascade models that will be introduced into sect. 2 relate the fluctuation at scale \(\lambda l\) (\(\lambda <1\)) to that at scale l by the cascade rule where \(W_{\lambda }\) is a random variable that depends only on the scale ratio \(\lambda\). It is a promising model leading naturally to the intermittency and multifractality of the systems (Mandelbrot 1974; Frisch 1997; Arneodo et al. 1998b). As described in this paper, we investigate the validity of a multiplicative hierarchical random cascade model by application of the model to a time series of stock prices. Because the actual hierarchical structure is directly unobservable, we apply a model on a dyadic hierarchical tree structure to the market. The intermittency and multifractality of the time series are verified with prediction of the model. However, although random multiplicative factors linking successive hierarchical layers are assumed to be i.i.d., the corresponding values calculated backwards from the data show strong negative correlation. That result apparently indicates a need for extension of the model. In each cascading step (1) the variable \(\delta _{\lambda l} X(t)\) is usually determined by only one predecessor \(\delta _l X(t)\) at the previous step. We extend the multiplicative model to incorporate an additional stochastic term multiplied by the standard deviation of the variable at the previous step. Such a model was introduced originally by Jiménez as a mixed multiplicative-stochastic model of turbulence (Jiménez 2000, 2007). Empirical results on our data are well fitted by the curve based on the additional stochastic term. Our model is also shown to cope with both well-known characteristics of financial time series such as intermittency or multifractality and the observed negative correlation among multiplicative factors. We try a minimal extension of random multiplicative models within keeping multifractality, not intending to exclude more complicated extensions. We present a more appropriate Markovian model than original random multiplicative model to give a better description of financial time series along the time scale. The paper is organized as follows. In Sect. 2, we introduce a multiplicative random cascade model on wavelet dyadic trees proposed by Arneodo, Bacry and Muzy (Arneodo et al. 1998b), which is the model investigated herein. We next investigate the model validity by application of the model to a time series constructed from the multivariate time series of the stock prices of the constituents of the FTSE 100 listed on the London Stock Exchange. In Sect. 4, we introduce an extension of the model and verify the improved model validity by Monte Carlo simulations. Finally, we summarize the results and provide some remarks about future work.",4
16.0,1.0,Evolutionary and Institutional Economics Review,17 December 2018,https://link.springer.com/article/10.1007/s40844-018-0114-9,Daniel Ellsberg on J.M. Keynes and F.H. Knight: risk ambiguity and uncertainty,June 2019,Yasuhiro Sakai,,,Male,Unknown,Unknown,Male,"The main purpose of this paper is to focus on Daniel Ellsberg with an intensive discussion on how his life and work are related to his two predecessors, J.M. Keynes and F.H. Knight. It is in 1921 that Keynes and Knight published apparently similar books on the economics of risk and uncertainty [see Keynes (1921) and Knight (1921)]. Forty years later, Ellsberg (1961) published an important article on risk and ambiguity in the Quarterly Journal of Economics. Interestingly enough, in the following year of 1962, he completed his Ph.D. thesis in economics. Although this thesis, entitled Risk, Ambiguity and Decision, was a masterpiece, it remained nevertheless unaddressed for long in the academic world: in fact, only in 2001, it suddenly came out as an academic book and has been available to the general public since then. The long span between 1961 and 2001 may eloquently tell us his own turbulent years as the mastermind of the “Pentagon papers” scandal in the 1970s and after: he was once called “the most dangerous man in America.” It seems, however, that Ellsberg might live twice like the “007” in the popular spy movies. At present, his academic influence on the economics of risk and uncertainty is certainly stronger than ever before. So we believe that it is quite worthwhile to look closely into his life and work in special relation to Keynes and Knight In retrospect, Ellsberg seems to be a man in paradox. Although in 1952, he graduated in economics at Harvard University, a top academic institution in the USA, he dared to leave it immediately after graduation to serve as a US Marine. In 1957, however, he came back to Harvard for his graduate studies, starting to work on decision making under uncertainty, a new challenging field of investigation. During his graduate studies, he left Harvard again to join the RAND Corporation as a strategic analyst. In spite of his occupational status as a “practical man,” he never gave up to pursue his academic career, and at the December Meeting of the Econometric Society in St. Louis in 1960, he presented his remarkable paper in which he successfully demonstrated what we may now call the Ellsberg paradox against the standard expected utility theory. Fortunately, it was published as part of a symposium on “Decision under Uncertainty” in November 1961 issue of the Quarterly Journal of Economics, a first-rate academic journal. Unfortunately, however, the issue per se was not paid due attention for a long time. While one year later, namely in 1962, he completed his thesis with an attractive title Risk, Ambiguity and Decision, it became almost forgotten until its publication as an academic book in 2001, almost 40 years later. Of course, there must be the reason for such overdue return of the master! In fact, he was a man of strong will and justice, being so preoccupied by letting the general public know about the Pentagon papers that he could virtually have no time left to participate in purely academic discussions. In his provocative book (2002), he remarked: So the question was, how could I now help to end this war [the Vietnam War], now that I was willing to go to prison. And within a few weeks the idea came to me of putting out the Pentagon papers, which I thought put me to prison for the rest of my life (Ellsberg 2002). It is true that Ellsberg contributed to decision science by introducing the Ellsberg paradox into the academic profession. The more amazing Ellsberg paradox, however, lies in the fact that an accomplished economist specialized in the aversion of risk and uncertainty dared to make a personal choice to risk everything such as degrading his social status and putting him in prison for a long period. Surely, the intellectual legacy of Ellsberg seems to be an intriguing research in paradox.Footnote 1 The remainder of this paper is organized as follows. Section 2 will deal with uncertainties that are not risks. A focal point of discussion will be the similarity and difference between Keynes and Knight. Kenneth Arrow’s skepticism about Knight on uncertainty will also be paid due attention. Section 3, the main part of this paper, will turn to the concept of ambiguity that was first introduced by Ellsberg. The two-color problem and the three-color problem will systematically be examined with the help of numerical representations. Section 3.3 will tell us many alternative ways to solve the so-called Ellsberg paradox. Presumably, the Keynesian approach by means of interval-valued probabilities will be shown to be very simple and highly effective. Section 4 will make some final remarks.",7
16.0,1.0,Evolutionary and Institutional Economics Review,19 November 2018,https://link.springer.com/article/10.1007/s40844-018-0115-8,An examination of market mechanism with redundancies motivated by Turing’s rule selection,June 2019,Yuji Aruka,Yoshihiro Nakajima,Naoki Mori,Male,Male,Male,Male,"When the number of states is large, it may be challenging for us to formulate an appropriate rule. It then seems difficult to attain the desired purpose and the evolution of the system without finding any computable rule. Therefore, in this paper, we research the application of simulations, but in a smarter manner what currently exists (Fig. 1). To examine the rule selection, we cite the experiments on elementary cellular automata. In particular, by the publication, A New Kind of Science” Wolfram (2002), it turned out that the automaton of Wolfram rule 110 fulfills the criteria of Turing completeness. This is among major problems that interested Wolfram. Now, the rule 110 and the similar rules are being explored (Table 1).Footnote 1 Binary-colored presentation of rule 110. *Black squares: 1; white squares: 0 The cellular automaton has a simple structure, but it is potent in terms of generating complicated behavior similar to those in Class 4. The figures of Classes 1–4 are reproduced in Fig. 2. Cellular automata (CA) can be classified according to the complexity and information produced by the behavior of the CA pattern: Class 1: Fixed; all cells converge to a constant black or white set Class 2: Periodic; repeats the same pattern, like a loop Class 3: Chaotic; pseudo-random Class 4: Complex local structures; exhibits behaviors of both class 2 and class 3; likely to support universal computation (Carvalho 2011). *Cited from Wolfram (2002): http://www.wolframscience.com/nks/p231-four-classes-of-behavior/ Class 1–4. By resorting to the Fully Random, Five-Rule Interactive Cellular Automata (ICA) Mitchell and Beyon (2011), we can easily examine the effects of the heterogenous interactions of rules. We employ a reduced version of the Five-Rule ICA, i.e., the Three-Rule ICA, to examine the effects of the heterogenous interactions of three different rules to easily compare the effects due to heterogeneous interactions of different rules around the rule 110. When only the rule dynamics are considered, we can analyze the effects of rules on the overall dynamics (Fig. 3). Three ICA. *These figures are produced by Mitchell and Beyon (2011)’s simulator. One possible application of FRICAs is a more refined classification system based on, for instance, how damaging the inclusion of a given rule is to the universal behavior of rule 110. It is also possible that systems in nature mimic the process of choosing randomly for each operation from a limited set of functional rules. Using the Five ICA, we can determine the effects when rule 110 is increased gradually to five. The initial distribution of the 5 rules is set as {Rule 23, Rule 183, Rule 18, Rule 238, Rule 12}. The first component of the initial distribution is replaced with Rule 110 to be {Rule 110, Rule 183, Rule 18, Rule 238, Rule 12}. By applying the same procedure to the last result, the second component is also replaced with rule 110 to yield {Rule 110, Rule 110, Rule 18, Rule 238, Rule 12}. By repeating a similar procedure, we finally obtain a set in which all components are rule 110. We can observe the different interactive heterogenous rules, as the number of rule 110 varies (Fig. 4). Five ICA. *These figures are produced by Mitchell and Beyon (2011)’s simulator",4
16.0,1.0,Evolutionary and Institutional Economics Review,21 September 2018,https://link.springer.com/article/10.1007/s40844-018-0102-0,Mechanism by which active funds make market efficient investigated with agent-based model,June 2019,Takanobu Mizuta,Sadayuki Horie,,Unknown,Male,Unknown,Male,"There are two types of portfolio management strategies for funds that invest in stocks and/or bonds, “active” managed funds, where a manager chooses stocks expected to rise in price and invests in them, and “passive” managed funds, where a manager expects a return the same as a market index, e.g., the Dow Jones industrial average and S&P 500, and replicates the components of the index. Recently, the assets of passive funds have increased (Fichtner et al. 2017); however, those of active funds have decreased, because as some empirical studies (French 2008; Bogle 2014) have argued, the average return of active funds is less than that of passive funds, and recent regulations have made fund sellers accountable for fund costs,Footnote 1 especially in the United States (Kearney 2016). Since active funds invest in stocks on the basis of the intrinsic value of companies (fundamental value), there are many arguments made that they discover the fundamental value, and this leads to market prices converging with the fundamental price (making a market more efficient); therefore, they play an important role in allocating capital, which is an important function in capitalism (Wurgler 2010). Furthermore, some claim that the rise in passive funds threatens to fundamentally undermine the entire system of capitalism and market mechanisms that facilitate an increase in the general welfare (Fraser-Jenkins 2016). However, it has not been clear whether actual active funds make a market more efficient or not and how much the assets of active funds are needed to make a market more efficient. In addition, there are many types of active funds. An empirical study (Cremers and Pareek 2016) showed that active funds that hold a very different position from a benchmark and that trade infrequently, “patient” active funds, earn moreFootnote 2. A long time is needed for market prices to converge with the fundamental price. Therefore, active funds must endure bad times and wait for profit patiently. Patient active funds endure such bad times and earn more; in comparison, “impatient” active funds cannot endure bad times, cut losses, and earn less (Cremers and Pareek 2016). This implies that the assets of patient active funds will increase, while those of active funds that trade frequently will decrease. At first glance, infrequent trades seem not to impact and change market prices and thus not lead market prices to converge with the fundamental price. In short, at first glance, what patient active funds trade infrequently seems inconsistent with what makes a market efficient. Therefore, it is important to discuss whether patient funds that trade infrequently make a market more efficient or not, and if so, we should investigate the mechanism of how they do so. In fact, there are opposing arguments. On one side, since patient active funds that perform well measure fundamental value more precisely, these funds make a market more efficient, even though they trade infrequently (Albagli 2015; Cremers and Pareek 2015), and on the other side, funds that trade infrequently do not make a market more efficient (Suominen and Rinne 2011). Either way, the mechanism by which funds that trade infrequently make a market more efficient is poorly understood, although several mechanisms have been suggested. An empirical study (Pastor et al. 2016) showed that the volume of active funds varies over time and that funds earn when the volume is larger, which implies that the mechanism of how funds that trade infrequently makes a market more efficient. Such discussion on the mechanism between the micro–macro feedback of certain types of investors is very difficult only using the results of empirical studies. Empirical studies cannot be conducted to investigate situations that have never occurred in actual financial markets, such as ones in which passive funds are more than present. Furthermore, because so many factors cause price formation in actual markets, an empirical study cannot be conducted to isolate the direct effect of changing the distribution of investor types on price formation. An artificial market, which is a kind of agent-based model, can handle situations that have never occurred, such as ones in which passive funds are more than present, and can isolate the direct effect of changing the distribution of investor types on price formation. These are strong advantages for an artificial market simulation. The effects of the distribution and several changing regulations have been investigated using artificial market simulations (LeBaron 2006; Chen et al. 2012; Mizuta 2016; Todd et al. 2016). Not only academies but also financial regulators and stock exchanges have recently become interested in agent-based models, such as artificial market models for investigating financial markets. Indeed, an article in Science by Battiston et al. (2016) stated that “since the 2008 crisis, there has been increasing interest in using ideas from complexity theory (using network models, agent-based models, and so on) to make sense of economic and financial markets,” and an article in Nature by Farmer and Foley (2009) stated that “such (agent-based) economic models should be able to provide an alternative tool to give insight into how government policies could affect the broad characteristics of economic performance, by quantitatively exploring how the economy is likely to react under different scenarios.” Many studies have investigated the effects of kinds of investors on price formation and the effects of several changing regulations and rules using artificial market simulations, for example, leveraged ETFs (Yagi and Mizuta 2016), high-frequency traders (HFTs) (Gsell 2009; Wang et al. 2013; Xiong et al. 2015; Leal and Napoletano 2016; Hanson and Zaima 2016), arbitrage traders between markets that have different latencies (Wah and Wellman 2013), market impacts (Cui and Brabazon 2012; Oesch 2014), financial market crashes (Yagi et al. 2012; Paddrik et al. 2012; Torii et al. 2015), price variation limits (Yeh and Yang 2010; Mizuta et al. 2013, 2014a), frequent batch auctions (Mizuta and Izumi 2016), dark pools (Mo and Yang 2013; Mizuta et al. 2014b, 2015), investor networks and herding (Wang and Toriumi 2017), the increasing speed of order matching systems on financial exchanges (Mizuta et al. 2016), market efficiency (Pruna et al. 2016), the rules for investment diversification (Yagi et al. 2017), market liquidities on the network of banks (Sakiyama and Yamada 2016), interaction between option markets and underlying markets (Kawakubo et al. 2014a, b), extension of trading hours (Miwa 2018) and (Kita et al. 2016), reviewed the U-Mart project which is one of the Japanese top artificial market research projects in the 2000s. Indeed, the effects of market makers and passive funds were investigated using artificial market simulations (Braun-Munzinger et al. 2016). Mizuta and Horie (2017) built a model of active funds in an artificial market model and investigated the effects of active funds on market price formation. However, they did not treat impatient active funds and did not succeed in explaining the mechanism of how patient and impatient active funds make a market more or less efficient. Therefore, in this study, we modeled agents who reflect the characteristics of patient active funds that trade infrequently and impatient active funds that trade frequently. We investigated the effects of patient and impatient agents on market price formation and whether they make a market more efficient or not using the model. We also show what happens in the case of increasing patient funds and decreasing impatient funds. Finally, we succeeded in figuring out the mechanism of how patient and impatient funds impact market prices and in proving that what patient active funds trade infrequently is not inconsistent with making a market efficient.",2
16.0,1.0,Evolutionary and Institutional Economics Review,09 April 2019,https://link.springer.com/article/10.1007/s40844-019-00124-6,Stability of business cycles and economic openness of monetary union,June 2019,Masato Nakao,,,Male,Unknown,Unknown,Male,"The theory of optimum currency area (OCA) considers what needs to be done to achieve successful currency integration. Many economists (e.g., Mundell (1961), McKinnon (1963), and Kenen (1969)) consider the OCA theory and suggest criteria for an OCA. In particular, McKinnon (1963) argues that the criteria should include “a high degree of openness of the economy.Footnote 1” In other words, countries that are very open to trade and trade heavily with each other form an OCA (Baldwin and Wyplosz (2015)). Domestic prices of countries with a higher degree of openness are likely to be affected by international price changes, because the cost of the countries is strongly affected by import prices. In addition, domestic prices are sensitive to fluctuations in exchange rates. Therefore, a nominal exchange rate adjustment by currency devaluation is not very useful, because it changes domestic prices. Countries with a higher degree of openness have low costs, which makes it impossible to devalue a currency by monetary union. The criteria for an OCA can be classified into two viewpoints. The first is whether it is possible to prevent the occurrence of shock. The second viewpoint is whether an adjustment is possible after a shock occurs. The criterion of openness of the economy is compatible with the viewpoint of prevention of the occurrence of asymmetric shock.Footnote 2 
European Commission (1990) indicates that a demand shock has similar impacts on countries participating in a monetary union. The reason is that a monetary union leads to a trade structure in which products of the same category are traded owing to economies of scale. Therefore, a shock tends to be symmetrical as integration progresses. This view on the criterion of openness of the economy is called the European Commission view. However, there is another view on the relationship between openness of an economy and asymmetric shock, the Krugman view. Krugman (1991) points out that a negative relationship exists between economic integration and symmetry of shock, because economies of scale caused by a high degree of economic openness bring regional concentration of industry, so that industrial-specific shocks tend to become a country-specific shock. There is a large body of literature on economic openness based on the difference of opinion between the Krugman view and the European Commission view (e.g., Rose (2000), Rose and Van Wincoop (2001), and Glick and Rose (2002)), and these studies suggest that Krugman’s concern was not realized after the creation of the Euro. Therefore, it does not become necessarily clear whether the high degree of economic openness guarantee the success of a monetary union in theory. The analysis of asymmetric shock is a central tenet of the OCA theory. Thus, the issue is whether openness of an economy causes an asymmetric shock. However, if the business cycles of countries in a monetary union are stabilized by satisfying the criterion of openness of the economy regardless of whether the shock is asymmetrical when countries depart from equilibrium by the shock, then the monetary union area satisfying the criterion becomes an OCA, even if Krugman’s view is correct and an asymmetric shock tends to occur. 
Gächter et al. (2012) point out the relationship between business cycles and the OCA theory as well as problems with the theory. According to the authors, a criticism of the OCA theory is that its different criteria could not be integrated within a uniform framework. Moreover, some listed criteria are difficult to measure (Robson (1998)) or compare (e.g., Tavlas (1994)). In the end, the discussion led to the development of a few “metacriteria” that implicitly subsume some of the individual conditions. In particular, the synchronization of business cycles has become established as a key OCA metacriterion. Regarding the synchronization of business cycles, De Grauwe and Ji (2016) argue that the best possible way to deal with business cycle movements, whose amplitude is unsynchronized, is by introducing a budgetary union. The authors point out that the underlying assumption of the OCA prescription for structural reform is that asymmetric shocks are permanent and that it does not follow that more flexibility is the answer when the shocks are temporary. Nakao (2017) proves this result theoretically and argues that an increase in capital mobility between countries in a capital markets union is a destabilizing factor, whereas an increase in fiscal transfers between such countries is a stabilizing factor. However, if the business cycles of countries in a monetary union are stabilized by satisfying the criteria for an OCA regardless of whether the business cycles are synchronized when countries depart from equilibrium by the shock, they are synchronized at the equilibrium point finally. Therefore, it is important from the perspective of OCA metacriteria that business cycles are stabilized. In this study, we investigate whether an increase in the degree of economic openness is a stabilizing factor and use a Kaldorian two-country model with a monetary union and imperfect capital mobility. Economic fluctuations involving underemployment have become a serious problem in the Euro area but there is little previous research on this topic using a Kaldorian model as a post-Keynesian. Therefore, we use a post-Keynesian model that is suitable for considering an underemployment economy. A Kaldorian model that is due to Kaldor (1940) is a kind of Keynesian model with variable capital stocks. Although there are several studies on business cycles using a Kaldorian (or Keynesian dynamic) international economic model (e.g., Asada et al. (2001), Asada et al. (2003), Asada (2004), and Ninomiya (2007)), little research has been undertaken to consider the OCA theory in Kaldorian terms. An analysis of the OCA theory using a Keynesian dynamic stability concept enables us to theoretically determine whether a monetary union has a shock adjustment function, and has the advantage of facilitating judgment about whether the Euro area can survive. The rest of this paper is organized as follows. In Sect. 2, we formalize the Kaldorian model, which consists of a five-dimensional system of nonlinear differential equations. In Sect. 3, we analyze the uniqueness of the equilibrium solution of the model formulated in Sect. 2. In Sect. 4, we investigate the conditions for local stability of the equilibrium point. In Sect. 5, we present the results of some numerical simulations that support the theoretical analysis in Sect. 4. Section 6 concludes.",2
16.0,1.0,Evolutionary and Institutional Economics Review,17 December 2018,https://link.springer.com/article/10.1007/s40844-018-0121-x,Banks’ disclosure of information and financial stability regulations,June 2019,Naoto Okahara,,,Male,Unknown,Unknown,Male,"Since the 2007 global financial crisis occurred and the risk-free financial system turned out to be an illusion, many economists have developed the models of financial system to examine the causes of instability. These models showed that the system encouraging free competition tends to reach unequal allocation for various reasons, and therefore, the models emphasize the need for government intervention, that is, macroprudential regulations. There is a growing literature analyzing the financial stabilities after the crisis, and some of them focus on the large share of short-term wholesale debt in financial intermediaries’ liabilities, which causes the “maturity mismatch”. Moreover, it is argued that the large share of short-term debt makes the intermediaries illiquid and then makes the financial system unstable. In the years preceding the crisis, banks played an important role in the financial markets and one could implicitly assume that the banks could absorb losses and their creditors were protected. However, the crisis revealed that banks underestimated the risk to which they were exposed and their equity was not enough to absorb the unexpected losses. After the crisis, therefore, the Basel Committee developed a new regulatory framework on banks, Basel III, to make the financial system stable. Basel III is based on three pillars: “minimal capital requirements and liquidity requirements”, “supervisory review process”, and “enhanced disclosure”. Although enhancing disclosure was one of the pillars in Basel II, we should pay more attention to how information about banks affects their creditors’ decision to withdraw funds. This is because one no longer always regards banks as safe after the crisis. In the arguments of the effectiveness of Basel III, however, the creditors are usually assumed to take into account the macroeconomic circumstance, such as good or bad state solely. Therefore, the analyses of the effect of information disclosed by banks on the stability of financial system have not been sufficiently analyzed yet. To address these issues in more depth, we develop a theoretical model in which a bank uses wholesale funding and discloses information about its capital structure. In the model, the bank issues both short-term and long-term debt and uses the funds to long-term investment, and subsequently its short-term creditors decide whether or not to roll over their loan. Analyses of such a model show that without any regulations, the privately optimal amount of short-term debt for the bank exceeds the socially desirable one (Stein 2012). However, this result depends partly on an assumption that the bank’s creditors are so risk averse, that is, they withdraw funds regardless of the bank’s soundness whenever the economy goes into recession. If the bank discloses information that indicates its soundness truly and its creditors’ decisions to withdraw fund depend on the soundness, then there is a probability that the bank would keep away voluntarily from depending on the short-term debt so heavily. This is because keeping the amount of short-term debt adequate is one of the ways to avoid becoming illiquid in a recession. It implies that there is a probability that the financial system can be stabilized by requiring banks to disclose information and requiring their creditors to use the information. Therefore, the aim of this paper is to analyze whether the regulation of the banks’ disclosure of information is effective for the stability of financial system. In the years preceding the crisis, the short-term debt gained popularity since it was considered to be a relatively cheap source of funding,Footnote 1 as a result of which banks began to rely more on the short-term debt than ever. Before the crisis, in the literature, the maturity mismatch of intermediaries’ balance sheets caused by short-term debt was viewed as playing a disciplining role to address incentive problems of banks (Calomiris and Kahn 1991; Diamond and Rajan 2001); therefore, such a debt of banks was thought to contribute to the stability of financial system. Moreover, it was implicitly assumed that banks could control the rollover riskFootnote 2 of short-term wholesale debt properly. After the crisis, therefore, many theoretical models of banks (or financial intermediaries) using short-term funding are developed. Acharya et al. (2011) shows that a small change in the fundamental value of bank’s collateral assets can cause a catastrophic decrease in the amount that the bank can borrow. 
Morris and Shin (2016) separates liquidity risk and solvency one, and shows that relying on the short-term funding increases the former risk. Moreover, Stein (2012) points out that using short-term funding causes fire sale of assets and that the fire sale exacerbates the credit crunch. These models analyze the banks’ ability to repay short-term debt with reference to estimation and change of the value of their assets. In contrast, there are only a few analyses in which banks’ creditors decision-making processes are considered. Some of them analyze the interaction between banks and their creditors using global game approach (Eisenbach 2017; Chen 2015) and show that short-term debt does not always work as the discipline when both banks and their creditors need to estimate other’s action. Since their purpose is to investigate the effects of banks’ short-term funding in the stochastic process on the fragility, they do not analyze effect of banks’ disclosure of information. Baek (2017) assumes that financial intermediaries can disclose costly information about the value of their assets and that the creditors’ decisions about rolling over depend on the information. Then, he shows that the intermediaries’ incentive to disclose information becomes weak when they can share solvency risk by holding securitized assets. Although he considers the creditors decision-making based on the information about financial intermediaries, he is interested in how much information is disclosed by the intermediaries with high leverage. Therefore, he pays little attention to how the disclosure affects the intermediaries’ capital structure. As already mentioned, the large share of short-term debt in banks’ liabilities is argued as one of the causes of the 2007 global financial crisis. Some analyses point out that imperfect information on the other banks’ actions prevents the banks from estimating correctly fire-sale prices of their assets and that the banks issues an inappropriately large amount of short-term debt based on the wrong estimation of their abilities to repay. Based on this idea, Stein (2012) analyzes banks’ decision making process about the amount of short-term debt. In his model, it is assumed that the rate of discount at the banks’ asset sales depends on the amount of sold assets and that the banks do not know this relationship and treat the rate as fixed. Then, he shows that the banks may issue an inappropriately large amount of short-term debt. It is obvious that the banks’ inabilities to estimate correctly the amount of debt they can repay causes the problems. However, the results of his analyses would depend partly on the assumption that the banks’ creditors withdraw funds whenever the economy goes into recession. As it has been already mentioned, when banks’ creditors react to the banks’ soundnesses, the disclosure of information about the banks’ capital structures may prevent them from depending excessively on short-term debt, and this mechanism would works even though the banks cannot estimate their abilities to repay. Thus, the main purpose of our analyses is to examine the effectiveness of the disclosure of information about banks’ capital structure for preventing them from depending heavily on short-term debt, when they cannot correctly estimate their abilities to repay. In our model, when the bank does not disclose information about its capital structure, the creditors solely consider the success probability of the bank’s long-term investment when they decide whether to roll over their short-term loan. In contrast, when the bank discloses information about its capital structure, the creditors take into account that the more the bank issues short-term debt, the more risky rolling over their loan becomes. This is because the bank is likely to be illiquid and to fail to repay its debt when it is heavily dependent on short-term debt. This means that, when a bank discloses information about its capital structure, it must consider the effect of its capital structure on its creditors, which implies that the amount of short-term debt issued by the bank might differ from the amount it issues without disclosing information. Therefore, at first, we consider the model in which the bank does not disclose information about its capital structure, and subsequently compare the result with that we derive from the model with disclosure of the information. If the bank does not provide any information about its capital structure and the cost of short-term debt is sufficiently cheap, then the amount of short-term debt issued by the bank exceeds the socially desirable amount as Stein (2012) shows. In contrast, if it discloses the information and its creditors use the information in their decision-making processes, the model shows that the amount of short-term debt issued by the bank is equal to the amount socially desired. However, the model also shows that this result cannot be obtained unless the bank can correctly estimate the creditors’ risk-aversion parameter. This is because there is the asymmetry of information between the creditors and the bank in that the former have the information about the latter, while the latter has no information about the former. These results imply that the regulation requiring banks to provide more information about their capital structures could be effective to make the financial system stable. To ensure the effectiveness of the regulation, however, we need another regulation that enables banks to have information about the degree of risk aversion of the creditors. The remainder of this paper is organized as follows. Section 2 presents the model in which a bank does not disclose information about its capital structure. Section 3 presents the model in which a bank discloses information, and Sect. 4 examines the results obtained. Section 5 discusses the implication of regulation suggested by the model. Section 6 presents our conclusion.",
16.0,1.0,Evolutionary and Institutional Economics Review,31 July 2018,https://link.springer.com/article/10.1007/s40844-018-0100-2,"Skills tasks, and class- an integrated class based approach to understanding recent trends in economic inequality in the USA",June 2019,Adam James Berg,,,Male,Unknown,Unknown,Male,"The last two decades have seen a dramatic increase in inequality in virtually all industrialized nations. Accompanying it is a revived interest in the study of the causes and effects of economic inequality, highlighted by the recent increase in high profile studies into income inequality. Literature examining earnings in the USA to date typically point to four causes of inequality growth. First, a large body of literature documents an increase in the skill premium in the USA resulting in more skilled workers experiencing higher wage growth. Second, beginning with the ALM framework (Autor et al. 2003), the skill biased technological change literature (SBTC), or more recently the routine biased technological change (RBTC) (Goos et al. 2014) literature, point to technological change and the computerization of the economy as it favors certain tasks and skills over others creating rapid income growth in some jobs and skill groups and deterioration of earnings in others, as well creating a polarization in employment. Third, more recent literature, and in particular Piketty (2014), point to rapidly rising incomes of managers in the USA’s top firms as well as high capital earnings relative to growth as primary drivers of inequality growth, particularly in the upper end of the income distribution. Finally, some studies point to one-off shocks and institutional effects on the income distribution, such as the falling value of the minimum wage, the erosion of labor unions, and mechanical effects such as the shifting composition of the labor force (Card and DiNardo 2002; Lemiuex 2006) as the primary drivers of inequality growth over the last several decades. There is often debate as to which of these causes is the most salient aspect of inequality growth, and yet studies rarely give serious consideration to the impact of all these influences together. Perhaps more importantly, the literature treats the institutional environment surrounding these sources of inequality as exogenous rather than directly associating inequality growth with collective action that shape these institutions and affect the causal linkages. As such, research that treats all these sources of inequality growth as complimentary and endogenizes the institutions that shape the mechanism itself is imperative for understanding the enduring nature of inequality growth as well the appropriate policy for targeting it. This paper considers three primary explanations regarding the causes of inequality growth, (a) the supply–demand model of the skilled labor market (Katz and Murphy 1992; Acemoglu and Autor 2011), (b) the SBTC/RBTC hypotheses (Autor et al. 2006, 2008; Card and Lemieux 2001; Acemoglu 2002; Acemoglu and Autor 2011; Goos et al. 2014), and more recently (c) the “super-manager” hypothesis (Piketty 2014), and interprets them in an integrated class framework. Doing so makes three primary contributions. First, it provides both a conceptual and analytical framework for incorporating the relevant causal processes involved and for evaluating their respective magnitudes of impact on economic inequality. Second, the conceptual framework provided clarifies the distinctions between of skills and tasks that are sometimes conflated in the literature. Third, by understanding the mechanism in an integrated class context, it provides a foundation for endogenizing the collective forces and evolving institutional environment that shape the mechanism itself. The next section provides the class foundation utilized in this paper. Section three maps the causative flows described in the literature regarding skills, tasks, and managers, and combines this literature with a class framework to create class categories. Data supporting each aspect’s relevance in inequality growth is also presented in this section before concluding in the fourth and final section.",1
16.0,1.0,Evolutionary and Institutional Economics Review,03 December 2018,https://link.springer.com/article/10.1007/s40844-018-0120-y,Gravity modeling in social science: the case of the commuting phenomenon in Greece,June 2019,Dimitrios Tsiotas,George Aspridis,Dagmar Škodová-Parmová,Male,Male,Female,Mix,,
16.0,1.0,Evolutionary and Institutional Economics Review,19 January 2019,https://link.springer.com/article/10.1007/s40844-019-00122-8,Dynamic relationship between budget deficit and current account deficit in the light of Nigerian empirical application,June 2019,Ergin Akalpler,Yohanna Panshak,,Male,Unknown,Unknown,Male,"The recent reappearance of large deficits on both external and internal accounts across the globe is attracting a lot of interest in economic literature. This has re-echoed the long age dispute between Keynesian–Mundell–Fleming on and Ricardian Equivalence perspectives on the nature and direction of causality between the current account deficits and budget deficits. The subsistence of a positive link between these variables has long been disputed in economic literature, since huge trade imbalances were seen with massive federal fiscal imbalances in the US economy, particularly in the early parts of the 1980s and 2000s (Çatik et al. 2015). These deficits are regarded as “twins” based on the observation that implementing higher budget deficits as a fiscal policy stance ultimately leads to a deterioration of the current account balance. It has been observed that the need for fiscal consolidation in most major developed countries and the increasing requirement for greater public expenditure on basic and critical infrastructure in developing countries are among the most important economic issues confronting the global economy. These needs are perceived as the remote causes of persistent fiscal deficits and current account deficits irrespective of whether the country affected is high or low income. This fact is based on the understanding that huge budget deficits have the propensity of crowding out domestic private investment through interest rates increases. In the same way, a huge current account deficit has the propensity of decreasing a county’s competitiveness, shifting of wealth to foreigners, waning of gains from international trade, and probably initiating a currency instability.
 In historical perspective, the first major fiscal deficit financing programme introduced in Nigeria was the contraction of the $1b jumbo loan after the civil war of 1967–1970 for the reconstruction, reconciliation, and rehabilitation of the war-torn country. Additionally, significant pressure from the citizens manifestly added to increased spending by the public authorities. This was essentially caused by the increased understanding within the populace that public spending could lead to development. Another reason for the surging budget deficits has to do with high inflationary trend caused by loose public funds that are indirectly linked with real production. Given the high price levels, labourers often bargain for wage increases adding to the burden of the government. Since the substantial amount of the workforce is under government employment, the government response to demands further compounds the situation. Mbanefoh (1993) contends that the fragile balance of payment position experienced was due partly to the financing of budget deficits through money creation and mounting interest repayments on loans. Other factors accounting for the upsurge in deficits are attributable to many factors, such as decreased government revenue (especially fall in commodity prices, shallow tax base) and increased government spending, especially on the rising social infrastructure and security; large public sector; political instability; the systemic failure of institutional values; and to the ever-increasing expenditure and national defiance.
 The graphical illustration of the current account deficits and fiscal deficit to GDP ratio depicted in Fig. 1 demonstrates that the variables move in the same direction and largely rest in the negative territory which indicates poor performance of the internal and external sectors of the economy. A similar correlation could be seen on the scatter diagram in “Appendix 1”. Current account and fiscal balances from 1980 to 2016. Source: obtained using data from African Development Bank, 2017 The underlying data show that Nigeria’s budget was largely in deficit during the whole period of analysis (1980–2016).Footnote 1 The only periods that budget surplus was recorded are 1990–1990, 1995–1995, 2000, 2004–2006, and 2013–2014 only. These fluctuations have significant implications on the country’s current account balances and government spending ability. Over the last 40 years now that Nigeria has implemented several policies to deal with among other things, these two deficits but the internal and external accounts of the still remain largely in crisis. The country recorded the highest budget deficit of about 15% of GDP and current account deficits about 13% of GDP in 1982 in the wake of economic recession. The unparalleled expansion in the especially high budget deficits since 1980s attracted the interest of the World Bank and the International Monetary Fund (IMF), as it was recognized that the trend in fiscal imbalance was not sustainable. The government expenditure-driven growth is often regarded as one close cause of budget deficit in the country. The highest positive fiscal balance was in 2004, following the debt cancellation/relief for highly indebted countries. From 2005 to 2014, public deficit stood at only − 0.18%. However, this dramatically changed in 2015, where budget deficits increased to 4.45% with further signs of increasing beyond the universally accepted threshold of 3% of GDP. To compound the problem, interest repayment on loans at the moment already takes about 30% of federal government revenue, which is higher than that of Kenya (12.34%), South Africa (10.23%), India (25.65%) and even the entire Sub-Saharan Africa (3.30%), South Asia (11.24%), East Asia (6.29%), and Latin America and Caribbean (10.45%).Footnote 2 This raises concern about the sustainability of the fiscal position of the economy over the medium and long terms, especially with the recently acquired $2.5b Eurobond from international financial markets. Simultaneously, intermittent imbalances have characterized the external account balance. The only periods that the current account experiences positive growth include: 1980, 1989–1990, 1996–1997, 2000–2001, and 2004–2014 only. Huge deficits have been recorded in this account since 2015. This study perceives a close relationship between budget deficit and the external account deficit when a cursory scrutiny is carried out. However, understanding the precise causation path of the variables needs further investigation; hence, the necessity for the study. Another motivation of the study is the conflicting submissions of Ahmad and Aworinde (2015) and Alkswani (2000). While the former upheld the twin deficit hypothesis for Nigeria for the period, 1980 and 2009, the latter found that twin deficit hypothesis may not be relevant for an economy that depends on oil exports as in the present case. The author contends that since government spending is central in an oil-dependent economy for productive purposes and distribution of national wealth and given that export of oil products is the most important contributor of national income and explains the capacity of government expenditure, oil-dependent economy may not rely on taxes (or any form of budget deficit financing) to cover government overheads. Therefore, he found a reverse causality between the two deficits of the oil-based economy of Saudi-Arabia. According to the author, this would arise if the government of a country makes use of its fiscal stance to target the current account balance. Thus, Nigeria being an oil-based economy represents a suitable context to shed more light on the validity of the relationship between these deficits by the application of ARDL-bounds test in the presence of structural breaks and with a complementary method of Granger causality. The present paper seeks to investigate the relationship between the budget deficits and the current account deficits in Nigeria from 1980 to 2016. These deficits have significant implications on the country’s macroeconomic stability and overall growth. Consequently, understanding the relationship between these variables will shed more light on the suitable reforms desirable to be adopted to deal with these dilemmas, and hence, adding to literature on the universal relevance and adaptability of the hypothesis. In line with the research objective, we can overtly state the research questions as: Is there any long-run relationship between current account deficits and budget deficits in Nigeria from 1980 to 2016? If the above holds, what is the exact causality path between the variables? The structure of the research is as follows: following the introduction, Section 2 covers theoretical back ground and brief literature review followed by derivation of the core variables in Sect. 3. Section 4 deals with econometric model specification, and empirical estimation in Sect. 5. Section 6 offers the conclusion.",4
16.0,1.0,Evolutionary and Institutional Economics Review,15 June 2019,https://link.springer.com/article/10.1007/s40844-019-00128-2,Special feature: Econophysics 2017: synergetic fusion of econophysics and other fields of science—Part II,June 2019,Yuichi Ikeda,,,Male,Unknown,Unknown,Male,,
16.0,1.0,Evolutionary and Institutional Economics Review,12 February 2019,https://link.springer.com/article/10.1007/s40844-019-00123-7,Macroscopic features of production network and sequential graph drawing,June 2019,Yuji Fujita,Yoshi Fujiwara,Wataru Souma,Male,Male,Male,Male,"Macroscopic feature of the whole economic system is among the primary topics of economics. It is not only because the macroeconomic status of nation-wide or world-wide economy has significant impact on our everyday life, but also because it is difficult to understand, something more than a simple aggregation of consisting elements. For example, although each individual company (or person) has its own empirical view of economic system based on its life-size economic activity, comprehension of nation-wide or world-wide economy can hardly be obtained just by
 collecting each empirical knowledge. The phrase “fallacy of composition” refers to such difficulty. It is the interaction of each agent that get in our way to understand the whole system. To paraphrase, network is the keystone of economics. If we could examine every interaction of the individual economic agents, it should be very helpful to understand the economic system as a whole. Unfortunately, due to the limited availability of large-scale precise empirical data, there has not been much work on real production network until recently. 
Fujiwara and Aoyama (2010) is one of the first research to perform empirical study of the production network of Japan. Economic interpretation of major network communities are given by industrial sectors. Chakraborty et al. (2018) focuses on the overall structural feature of the production network of Japan based on the same dataset of this study. It performs hierarchical clustering analysis on directed network and examines relations among industrial sectors. Krichene et al. (2018) develops a model about how a firm chooses its business counterpart, and applies it to the same Japanese production network data. Inoue and Todo (2018) proposes a model of firm-level production and applies it to the whole production network of Japan. The model proposed in Inoue and Todo (2018) successfully recreates the recovery from the damage of Great East Japan Earthquake of 2011. Based on this model, it estimates economic damage that may be caused by a foreseen large-scale earthquake in Pacific Japan. Adherent stability and flexible adaptability are two superficially contradictory macroscopic features normally found in the ordinary state of nation-wide economy. We will focus on these two properties in the analysis of the nation-wide production network of Japan. The analysis will be conducted using the data that houses approximately one million firms and five million supplier–customer relations. We found interesting macroscopic features by a series of network visualizations which are generated by the method described in Sect. 2. The network has a characteristic three-part topological structure of the directed network known as “bow-tie” structure. We discovered that this particular structure brings the integrity and flexibility to the Japanese economy. Complex network visualization originally began as a development of graph drawing algorithm and its execution. Its one of the most important application was the automated layout of electrical circuit. Consequently, network visualization has a long tradition of generating graph representation embedded in the planar surface, having as few edge crossing as possible. Since then large scale complex network visualization has been developed with different application and purpose (see Di Battista et al. 1998; Kaufmann and Wagner 2001). Today we have almost no need to count the edge crossing number of a large scale complex network drawing because fewer crossing does not guarantee better visualization. Drawing time-dependent changing graph has been extensively studied, especially after the 2000s (see Morreale et al. (2015) and references therein). Previously proposed methods can be divided into two major categories, i.e. “online” and “offline” according to the way how they process the input. The term “offline” generally refers to the method which requires the whole series of input. In the case of graph drawing, it means that this type of method requires the entire sequence of network. In other words, this type of method starts drawing only after the final status is known. The graph layouts are obtained after the whole optimization over the entire sequence have done. On the other hand, the term “online” usually denotes an incremental procedure. This type of algorithm does not require the entire sequence of information. For example, Frishman and Tal (2008) has proposed method only requires previous layout and new (to be drawn) network information to obtain the next layout. Because our visualization method generates network layout using only the previous layout and current network, it is categorized as “online” method. We consider being “online” is a necessary constitution if it is to be applied to the scale of data of this study. In Sect. 2 we describe our visualization method. In Sect. 3 we first explain about the data then conduct the analysis. In Sect. 4 we study the economic implication of our analysis.",1
16.0,1.0,Evolutionary and Institutional Economics Review,03 December 2018,https://link.springer.com/article/10.1007/s40844-018-0119-4,Statistical law observed in inactive rate of firms,June 2019,Atushi Ishikawa,Shouji Fujimoto,Takayuki Mizuno,Male,Unknown,Male,Male,"In natural science, theories are based on distributions of variables in the system of interest. Furthermore, various statistical laws have been observed, and these are related to each other. Statistical laws are also observed in sets of firm-size variables, such as sales, profit, and number of employees. For example, it is known that the firm-size distribution in a certain year follows a power law in the large-scale range of data values (Pareto 1897; Newman 2005; Clauset et al. 2009) but follows a log-normal distribution in the mid-scale range of data values (for example, see Stanley et al. 1995). For instance, the value of the boundary between the large- and mid-scale data ranges is around \(10^4\) thousand dollars for Japanese firms’ sales. These distributions are closely related to statistical laws observed over two consecutive years. Specifically, under reversal symmetry of the firm-size variables, power-law and log-normal distributions are derived from Gibrat’s law and non-Gibrat’s law (Gibra 1932; Sutton 1997; Fujiwara et al. 2003, 2004; Ishikawa 2006, 2007; Tomoyose et al. 2009). Here, reversal symmetry is a property in which the scatter plot of firm-size variables between a certain year and the next year is symmetrical with respect to time reversal. Gibrat’s law reflects a property in which the growth-rate distribution conditioned by firm-size variables does not depend on the firm-size variables. On the other hand, non-Gibrat’s law describes the dependence of the conditional growth-rate distribution on the firm-size variables. These statistical laws are found in firms that continue to be active. Do statistical laws also exist in firms that stop activity? Here, the inactivity of firm refers not only to bankruptcy but also to inactive states such as “In liquidation”, “Merger”, “Handover”, “Division”, “Liquidation”, and “Dissolution”. In recent years in Japan, the number of bankruptcies has been around 10,000 per year (The Small and Medium Enterprise Agency, http://www.chusho.meti.go.jp/sme_english/). According to the database we use, around 150,000 firms have been suspended annually. Among them, the number of firms with reported main financial variables is around 40,000. This is the same number published by the Small and Medium Enterprise Agency of Japan, http://www.chusho.meti.go.jp/sme_english/. Therefore, the database we use is highly comprehensive. The statistical laws of the inactive rate of firms has been studied in relation to firm-age distributions approximated by an exponential distribution (Fujiwara 2004; Coad and Tamvada 2008; Coad 2010a, b; Bottazzi et al. 2008; Miura et al. 2012). In the previous study, we defined the inactivity of firms as the transition of firms from an active state to an inactive state, and we reported that the inactive rate of firms does not depend on firm age (Ishikawa et al. 2015a). Furthermore, we showed that this property leads to an exponential firm-age distribution (Ishikawa et al. 2015b, 2017a). In this study, to develop this research and to clarify the statistical property observed between the inactive rate of firms and firm-size variables, we find that the inactive rate of firms depends on various firm-size variables: total revenue, net income, total assets, and net assets. We conducted this analysis for firms in multiple countries with high data coverage in the database. The structure of this paper is as follows: Section 2 describes the data used in this paper. In Sect. 3, we estimate the dependence of the inactive rate of firms on values of basic financial variables, including total revenue, net income, total assets, and net assets. Section 4 discusses the relationship between the properties observed in Sect. 3 and Gibrat’s law observed in the large-scale range of financial data values or non-Gibrat’s law in the mid-scale data range. Finally, in Sect. 5, we summarize this study and describe future prospects.",4
16.0,1.0,Evolutionary and Institutional Economics Review,22 November 2018,https://link.springer.com/article/10.1007/s40844-018-0106-9,Exogenous shock and multifractal random walk,June 2019,Koji Kuroda,Jun-ichi Maskawa,,Male,Unknown,Unknown,Male,"Multifractal analysis has been used for the investigations in various fields such as turbulence (Parisi and Frisch 1993; Mandelbrot 1974) and finance (Mandelbrot 1997; Stošić et al. 2015). A stochastic process with stationary increments is called a multifractal random walk if there exists \(T>0\) such that the following formula holds: for a non-linear function \(\zeta _q\), where \({\varDelta }_t X_u=X_{u+t}-X_u\). To construct a multifractal random walk, we first construct a log-volatility process \(\{w_r(t)\}\) from a random media with power law interaction and define a multi-fractal random walk by a stochastic integral with respect to a Brownian motion \(\{B_t\}\) independent of \(\{w_r(t)\}\). Intuitively, the amplitude of price variation is given by \(e^{w_r(t)}\), and price up-down is described by a Brownian motion. Precise definition will be given in a sequel. This type of construction for multifractal random walk was first made by Bacry and Muzy (2003) using “regionally independent” random media, and extended it to the stochastic integral of fractional Brownian motion by Abry et al. (2009). The regionally independence means that no interaction works in the random media. In the previous paper (Kuroda 2016), we constructed a log-volatility process \(\{w_r(t)\}\) from a discrete time process \(\{W_t^{(n,r)}\}\) defined on a random media with power law interaction as a scale limit, where \(\alpha >0\) plays a role of time scale exponent and c(n) is a scale function. We look at the system with time scale \(n^\alpha\), and scale it by a scale function c(n). If c(n) is well chosen we can see a non-degenerate process. The discrete time process \(\{W_t^{(n,r)}\}\) is defined on a random media which describes stock market trades by various traders with different investment time horizons. For the precise definition, see Eq. (12). The probability distribution of the investment time horizon is given by a power law interaction with range of order \(n^\gamma\). We look at trades in time interval \([0,Tn^\alpha ]\), define \(W_{[n^\alpha t]}^{(n,r)}\) and consider a space–time scale limit. Remind that scale function is uniquely determined to obtain a non-degenerate scale limit. For example, Brownian motion is constructed from a random walk \(S_n\) as a scale limit \(B_t= {\lim _{n \rightarrow \infty } \frac{1}{c(n)} B_{[nt]}}\) if \(c(n)=\sqrt{n}\). If \(c(n) >\sqrt{n}\) the scale limit becomes zero process and it explodes if \(c(n)<\sqrt{n}\). Only in the case of \(c(n)=\sqrt{n}\), we can see a Brownian motion. In our model, scale function c(n) is also uniquely determined. (See Assumption 2.) We consider a scale limit in the case of \(\alpha <\gamma\) (Assumption 1). This means that we look at the system with time scale \(n^\alpha\) shorter than the range \(n^\gamma\) of interaction. If we change the time scale, different pictures can be seen, that is, different scale limits are obtained in the case of \(\alpha >\gamma\) and \(\alpha =\gamma\). Scale limits in the case of \(\alpha >\gamma\) and \(\alpha =\gamma\) will be discussed in the forthcoming paper (Kuroda and Maskawa 2018). It will be seen that the variance \(V[w_r(t)]\) has singularity as \(r \downarrow 0\). This causes a difficulty to treat a multifractal random walk. To avoid a singularity, we have to consider a cutoff process \(\{w_r(t)\}\) by \(r>0\). ( In equation (12), the definition of a discrete time process \(W_t^{(n,r)}\), we cut off a part with \(1\le j<rn\). Mathematically, a multifractal random walk is defined in the following form: where T is an arbitrary fixed positive number, \(\{w_r(u)\}\) is a Gaussian process, and \(\{B_u\}\) is a Brownian motion independent of \(\{w_r(u)\}\). Remark that we cannot exchange the order of \({ \lim _{r \downarrow 0}}\) and the integral, because of a singularity of \(w_r(t)\) as \(r \downarrow 0\). It can be proved that the log-volatility process \(\{w_r(t)\}\) is a Gaussian process with the following variance and covariance: where \(\delta >0\) and f(x) is a positive \(C^1\) class function (\(f'(x)\) exists and continuous). To obtain a multifractality of the process \(\{X_t\}\), the continuous cascade equation for \(\{w_r(t)\}\) plays an essential role. We say that the continuous cascade equation for \(\{w_r(t)\}\) holds if where \(\sim\) means the equality in the sense of finite dimensional distribution and \(D_\lambda\) is a Gaussian random variable independent of \(\{w_r(t)\}\) with mean zero and variance \(\sigma _\lambda ^2\) . This implies that the volatility \(e^{w_r(t)}\) with smaller time scale \(\lambda ^\alpha t (0<\lambda <1)\) is given by \(e^{D_\lambda } \cdot e^{w_r(t)}\), that is, fine time scale volatility is obtained from the coarse time scale volatility. As the condition for the continuous cascade equation, we derive that for some constant \(C>0\). Remark that the power law function is obtained as the condition for the continuous cascade equation. When the condition (6) is satisfied, the variance and covariance of \(\{w_r(t)\}\) are given by where \(\lambda _0= \delta T\). Remark that the covariance decays logarithmically as \(|t_2-t_1| \rightarrow T\) and this leads to the multifractality of the process. We discuss this problem in the section 3. To define a discrete time process \(\{W_t^{(n,r)}\}\), we consider the following market trades model by various traders with different investment time horizons. Consider n types of traders with different investment time horizons and describe their trade by a polymer \(\mathbf{p}=(j;t_1,t_2;v)\), where \(j\in {\varLambda }_n=\{1,2,\ldots ,n\}\) is a type of trader, \(t_1,t_2\) are 1st and 2nd transaction times of a pair trades by type j trader, and v is an impact on a log-volatility caused by the pair trades. A probability intensity of \((t_1,t_2)\) is given by a power law \((t_2-t_1)^{-\beta _j}\) with exponent \(\beta _j= \beta (\frac{j}{n}) \in [0,1]\) for a \(C^1\)-class monotone decreasing function \(\beta (x)\). Also, \(E_j[v^2]\) of the impact v on log-volatility is given by \(\mu _j= c_2 \mu (\frac{j}{n}) \; (j=1,2,\ldots ,n-1)\) for a \(C^1\)-class monotone increasing function \(\mu (x)\), where \(c_2>0\) is a constant. The precise definition of polymer will be given  in the next section. Traders with large j are considered to be traders with long-term investment time horizon like a pension fund. On the other hand, traders with small j are considered to be traders with short-term investment time horizon like a “day trader”. In this article, we construct a multifractal random walk for a log-return process of stock price from a random media with power law interaction and apply this model to the problem of exogenous shock. The power law interaction will play an important role to study the volatility decay after shock. To introduce an exogenous shock, we multiply v by \(n^\xi\) for polymers with first transaction time \(t_1 \in [0,n^\kappa ]\). We prove that under some condition, where \(\{w_r(t)\}\) is the same Gaussian process mentioned above and \(\{w_r^{\text {ex}}(t)\}\) is also a Gaussian process deriving from the exogenous shock in \([0,n^\kappa ]\). To obtain a scale limit for \(\{w_r^{ex}(t)\}\), we need a condition (Assumption 3). These two processes \(\{w_r(t)\}\) and \(\{w_r^{ex}(t)\}\) are independent. The multifractality is given by \(\{w_r(t)\}\), and the effect from the exogenous shock is described by \(\{w_r^{ex}(t)\}\). Also, we prove that for some constants \(C_1>0\) and \(C_2\).This implies that the exogenous shock happened in \(t=0\) decays as a power law \(t^{-1}\) with exponent 1. This result is matched with the empirical result (Joulin et al. 2008) by Joulin–Leferve–Grunberg–Bouchard. To prove these results, we use the method of Abstract Polymer Expansion (Kotecky and Preiss 1986) by Kotechky and Preiss.",1
16.0,1.0,Evolutionary and Institutional Economics Review,03 November 2018,https://link.springer.com/article/10.1007/s40844-018-0107-8,Analyzing outliers activity from the time-series transaction pattern of bitcoin blockchain,June 2019,Rubaiyat Islam,Yoshi Fujiwara,Hiwon Yoon,Unknown,Male,Unknown,Male,"Bitcoin is the pioneer of the increasingly popular cryptocurrencies and also the unique example of a large-scale sustainable payment system, in which all the financial transactions are publicly available (Whitepaper, Nakamoto 2008). It is issued by no central authority like government, bank or organization rather by mathematical cryptographic protocols in a distributed network system. In this system, the users pseudo-anonymously exchange bitcoins. Some special users are called miners, who only can mint bitcoins in the system by solving cryptographic puzzles by donating high computing power and in return, they earn bitcoins as their proof of work. So far, economic literature on the bitcoin issue is quite limited. Fergal and Martin (2013) and Ron and Shamir (2013) among the researchers of various background had successfully drawn the attention to the analytical aspects related to the information contained in the blockchain. Due to its still relatively low acceptance in the foreign exchange market and its poor performance as a medium of the store of value, some attention in the academic world has attracted the discussion on whether bitcoin can be considered a currency. But the trust in this currency totally comes not on the belief in central monetary authority rather computer technology and cryptography. This paper is basically focused on three aspects of blockchain-based open source financial data. First of all, we re-structured blockchain’s already extracted database (Kondor et al. 2014) for generating a hawk-eye view to observe bitcoin’s daily transaction patterns to understand its economic growth with the course of time. Secondly, we had done the descriptive analysis on that database of daily transaction number and bitcoin volume to understand some of the most interesting and informative statistical distribution. Finally, we had investigated the rank distribution of some of the distinct transactions and their descriptive statistical facts to extract some network topological features. The blockchain is one of the revolutionary databases that had been evolving since the last decade. It stores any information in a decentralized computing system and once stored, data inside it can never be altered or manipulated. It is transparently accessible to all the users logged in the database and they can view all the information published in the blockchain. Bitcoin cryptocurrency along with the sender and receivers’ addresses in the form of a ledger is the financial monetary information that stored inside a blockchain and has introduced world’s first successfully implemented fully digital cryptocurrency. It solved two real-world problems like “double spending” (Diego and Giovanni 2017) and “duplication problem” and made an alternative way to establish fully functional virtual currency based financial system.",11
16.0,2.0,Evolutionary and Institutional Economics Review,20 September 2019,https://link.springer.com/article/10.1007/s40844-019-00134-4,Special feature: workshop on random system solution of the transformation problem: discussion with Prof. Bertram Schefold,December 2019,Kiichiro Yagi,,,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Evolutionary and Institutional Economics Review,21 October 2019,https://link.springer.com/article/10.1007/s40844-019-00140-6,The transformation of values into prices on the basis of random systems revisited,December 2019,Bertram Schefold,,,Male,Unknown,Unknown,Male,"I began to read Marx carefully a little more than 50 years ago. I have always remained convinced that the transformation of values into prices was of central importance in Marx’ own eyes—the reasons are manifold. Let us call this the problem of relevance. It is more difficult to judge whether the transformation, in particular the use of labour values, can be circumvented as far as the theory of accumulation is concerned, for instance by translating the Marxian propositions about the production of absolute and relative surplus value into the language of the modern mathematical theory of prices of production (I think this can be done) or whether labour values can be dispensed with also with regards to other parts of the Marxian theory such as the theory of the forms of value (this is less clear). One speaks here of the question of the redundancy of the labour theory of value (Steedman 1977; Feess-Dörr 1989). Finally, my opinions have changed most dramatically with regard to the feasibility of the transformation, as I shall explain below (Sects. 5, 6). The feasibility is intimately related to the interpretation of what, if anything, has to be transformed. There is the temptation to explain the transformation problem away. I once tinkered with Joan Robinson’s idea of basing the transformation on a given money wage, but I gave that up even before it later became, in more sophisticated presentations, the so-called ‘New Solution’ (Schefold 1973). I shall here return to what I now regard as the best possible—and, I think, quite satisfactory—solution, based on the traditional interpretation of the transformation problem, which is made solvable by means of random systems. It is an example of what historians of economic thought call an analytical reconstruction. Marx never claimed to have achieved more than an approximate solution, based on averages, and this is what the random approach allows to render more precise. I shall begin with a simple description of the traditional framework of the transformation problem (Sects. 1, 2). I shall defend the traditional interpretation by emphasising the similarity of prices of production, natural prices and the normal prices of neoclassical theory in a history of economic thought perspective. Section 3 is a relatively extensive attempt to demonstrate the importance of a transformation which results in the equality of the rates of profit measured in values and in prices. This section concerns the relevance, but can be omitted by those who only want to see how I prove the feasibility of the random solution (Sects. 5, 6). I have added a new necessary and sufficient condition for the transformation to be possible in so-called one-industry systems (Sect. 6), and I have incorporated money (Sects. 2, 5). Section 4 touches the question of redundance. Section 7 presents an evaluation of the results.Footnote 1",9
16.0,2.0,Evolutionary and Institutional Economics Review,17 October 2019,https://link.springer.com/article/10.1007/s40844-019-00144-2,The transformation problem under positive rank one input matrices: on a new approach by Schefold,December 2019,Masashi Morioka,,,Male,Unknown,Unknown,Male,"In the 1950s–1970s, the problem of the transformation from value to production prices as well as surplus value to profits in Marx’s theoretical system was comprehensively clarified using Leontief’s framework (Okishio 1954; Morishima 1973; Steedman 1977). One of the main results of this clarification was a demonstration of the impossibility of double aggregate equalities. In other words, if a numéraire (a bundle of products whose price is defined as one) is chosen so that the total production prices is equal to the total labor value, the total profits is generally not equal to the total surplus value. According to this result, profit cannot be regarded as redistributed surplus value without certain additional assumptions or some redefinition of core terms including “value” and “surplus value”.
 Recently, however, Schefold (2016) presented a set of original supplementary assumptions designed to make  total profits equal to total surplus value (by presupposing the equality of total production prices and total value—hereafter we shall omit this proviso). It consists of the following four assumptions: The input matrix is positive and of rank one, thus, all of its eigenvalues other than the Frobenius root vanish. Let \( {\mathbf{v}} \) be the vector representing the deviations of the labor coefficient vector from the right-hand Frobenius vector of the input matrix (which Schefold calls “the Marx-vector”). Then, the sum of all the elements of this vector is equal to zero. The covariance between the above deviation vector and the gross output vector is equal to zero. The covariance between the above deviation vector and the surplus output vector is equal to zero. The purpose of this study is to examine these assumptions and their consequences. To consider the purely theoretical aspects of this problem, we restrict our attention only to assumptions (ii)–(iv) and shall not argue whether input matrices can actually be approximated by positive rank one matrices. To put it differently, we are exclusively concerned with relationships that hold under positive rank one input matrices and their significance for the transformation problem. In dynamic models using the Leontief-type input matrix, the existence of its eigenvalues other than the Frobenius root sometimes becomes a source of complication.Footnote 1 If we can assume that all non-Frobenius eigenvalues are zero (or are negligibly small), then analyses of these dynamic models are considerably simplified. Therefore, it is of great interest to explore to what extent this assumption on input matrices simplifies the relationship between profits and surplus value. We will present our discussion in the following order. Section 2 confirms several sufficient conditions for the equality between total profits and total surplus value when the input matrix is of rank one. Section 3 shows that assumptions (ii)–(iv), together with assumption (i), certainly guarantee the condition which is one of those sufficient conditions but so far has been regarded as one without any particular economic meaning. Section 4 indicates that assumptions (ii)–(iv) depend on the measurement units of each product, i.e., they hold only under measurement units satisfying certain  conditions. As we will argue, this dependence casts a serious doubt on the validity of the results based on these assumptions.
 Before presenting our main point, we would like to confirm an important common ground that has been established through the Sraffian critique of the labor theory of value. The following passage shows that Schefold still keeps the classical Sraffian position: “prices are … not derived from values, but derived, without having recourse to values, from the structure of production … represented by A and l, and from the distribution, represented by r.” Thus, even if total profit is equal to total surplus value, “The formal redundancy of surplus value remains” (Schefold 2016, p. 177). In this way, it should be noted that Schefold’s new approach is not accompanied by an intention to restore the analytical supremacy of the concept of labor value that characterizes orthodox Marxian economics.",4
16.0,2.0,Evolutionary and Institutional Economics Review,02 November 2019,https://link.springer.com/article/10.1007/s40844-019-00146-0,A comparison of the stochastic approach to the transformation problem with Marx’s original assumptions,December 2019,Kenji Mori,,,Male,Unknown,Unknown,Male,"The problem which was originally set by Marx and has been called since then “Transformation Problem” consists in how to prove the following double invariance: for any output, the total value equals total price, and total surplus value equals total profit. As Schefold (2014) confirmed, a complete, i.e. general solution to the transformation problem turned out to be impossible after a long debate since Böhm-Bawerk’s criticism. That means that a consistent “solution” could be given only by providing new reasonable assumptions in addition to or instead of the original ones made explicitly or implicitly by Marx himself. The stochastic solution provided by Prof. Schefold showed that by allowing variables to be random, the following three conditions are sufficient for the double invariance stated above (Schefold 2019, 26). The eigenvalues \(\mu_{2} , \ldots ,\mu_{n}\) converge to zero for n going to infinity. The vectors of numeraire and of labour are random and independent (i.e. \(cov({\mathbf{m}},{\mathbf{v}}) = 0\) or \(cov(y,l) = 0\)). The surplus vector and the labour vector are random and independent (i.e. \(cov({\mathbf{s}},{\mathbf{v}}) = 0\)). These three new assumptions are, as we will show, not only added to Marx’s original (implicit or explicit) assumptions, but they also change some essential part of his original argument. In the following, we will take up particularly two examples of major alteration arranged by the stochastic approach.",4
16.0,2.0,Evolutionary and Institutional Economics Review,23 October 2019,https://link.springer.com/article/10.1007/s40844-019-00136-2,Profit and value in a random system: interpretation of professor Schefold’s 2016 article,December 2019,Yoshihiro Yamazaki,,,Male,Unknown,Unknown,Male,"Clearly, the transformation problem of finding a rule to transform labor value to production price has long been one of the most important questions in Marxian economics. However, some do not consider it a real problem because both the labor value and production price can be independently deduced from input coefficient matrices. Professor Bertram Schefold’s  (2016) article provided us another viewpoint to consider the transformation problem. His approach uses random matrices as a mathematical tool. This note tries to interpret the mathematical and methodological implications of the article. The latter half of Schefold’s article argues about Marx’s treatment of differential calculus. This note also compares Marx’s treatment with Hegel’s and Leibniz’s views; further, it will clarify the overall implications of Professor Schefold’s method.",1
16.0,2.0,Evolutionary and Institutional Economics Review,16 November 2019,https://link.springer.com/article/10.1007/s40844-019-00149-x,Special feature: institutional design and experimental economics,December 2019,Kazuhito Ogawa,,,Male,Unknown,Unknown,Male,,1
16.0,2.0,Evolutionary and Institutional Economics Review,03 May 2019,https://link.springer.com/article/10.1007/s40844-019-00126-4,Trusting versus monitoring: an experiment of endogenous institutional choices,December 2019,Andrej Angelovski,Daniela Di Cagno,Werner Güth,Male,Female,Male,Mix,,
16.0,2.0,Evolutionary and Institutional Economics Review,24 July 2019,https://link.springer.com/article/10.1007/s40844-019-00129-1,VCG mechanism for multi-unit auctions and appearance of information: a subject experiment,December 2019,Satoshi Takahashi,Yoichi Izunaga,Naoki Watanabe,Male,Male,Male,Male,"This paper investigates whether, in multi-unit auctions, different types of appearance of information
associated with bidding generate different levels of allocative efficiency and sellers’ revenue when the Vickrey–Clarke–Groves (VCG) mechanism is used in the experiment. We examine two types of appearance of information about bidders’ valuations of the item given to them and their bids asked to submit. One type is unit valuations and the unit bids themselves and the other type is valuations and bids, i.e., unit valuations and the unit bids multiplied by the number of units. There are many practical examples of multi-unit auctions: oil and timber sales, flower markets, spectrum auctions, etc. It is known in theory that the VCG mechanism attains allocative efficiency but suffers from its computational intractability when the number of units of the item to be traded is large. In fact, we have not found any report that the VCG mechanism was used in practice. Many experiments of multi-unit auctions have thus focused on the price discount in discriminatory auctions (e.g., traditional treasury auctions in many countries), the demand reduction in uniform-price auctions (e.g., the treasury auction in the US), and the incentive to enter and collude in ascending clock auctions (e.g., electricity spot markets). As Klemperer (2002) noted, these practical issues might be more important than features theorists tend to be concerned with, although these auction rules do not guarantee the allocative efficiency. Accordingly, there is no paper on the experiment of the exact VCG mechanism in multi-unit auctions listed even in the latest comprehensive survey on auction experiments by Kagel and Levin (2016). To improve the drawback of VCG, however, the approximation algorithms that reduce the computational complexity were proposed in the field of operations research (Dyer 1984; Kothari et al. 2005). Based on the Dyer’s work, Takahashi and Shigeno (2011) developed a greedy-based approximation (GBA) algorithm that is much faster than the VCG mechanism in computation time. Takahashi et al. (2018) reported that in their subject experiment where five units of an identical item were auctioned off to three bidders, VCG attained higher allocative efficiency than GBA, although there was no significant difference in seller’s revenue between GBA and VCG; The average rate of efficiency was 97.37% in VCG and it was 93.65% in GBA. The authors did not expect this result on allocative efficiency, because bidders submitted their “unit bids” confirming their “unit valuations” in the experiment. This type of appearance of information is considered as a key feature for the GBA algorithm to work well; in GBA, a bidder who submits the highest unit bid is given priority for obtaining the units of an identical item in the process of the item allocation, and thus bidders can intuitively infer how GBA allocates the item with their own bids, as compared to VCG. When this inference is difficult, bidders may underbid, which induces efficiency loss to some extent. In fact, Chen and Takeuchi (2010) reported underbidding in VCG, although it was in the experiment of combinatorial auctions. Thus, the authors expected that GBA might be even superior to VCG in allocative efficiency as well as in computation time.Footnote 1 After observing the experimental result shown by Takahashi et al. (2018), we next investigate whether the performance of the VCG mechanism is robust against changes of appearance of information in which bidders submit “total bids” confirming their “total valuation” for each unit. This is the main research question of this paper. In theory, there should be no difference in bidders’ behavior, and thus there should be no difference in both allocative efficiency and sellers’ revenue. This experiment was run in the same environment as that in Takahashi et al. (2018), except for incorporating two types of appearance of information. Our main observation is that there was no significant difference on average in either allocative efficiency or seller’s revenue between those two types of appearance of information. Rather, for each appearance of information, there was a significant difference in subjects’ bidding behavior between different display types of draws of unit valuations. This behavioral difference, however, did not significantly affect the difference in allocative efficiency. The performance of VCG is robust also in this sense. In theory, the VCG mechanism has another nice property besides allocative efficiency; It induces bidders to truthfully bid their own valuations of the item for each unit, which is a dominant strategy for every bidder. In our experiment, however, the rate of approximately truth-telling bids were at most 45% in any sessions we conducted. We observed that subjects underbid when unit valuations were drawn at random and shown to them as they were, although they did not necessarily do so when unit valuations were reordered in monotone non-increasing order and given to them as their unit valuations. Regardless of those differences in bidding behavior, the average rate of efficiency was more than 90% in any sessions. These results imply that in allocative efficiency the performance of VCG may be robust against bidding behavior out of theory. Truly, it is difficult for bidders to intuitively infer how exactly VCG allocates the item with their own bids, which may be one of the reasons why we have not found any report that the VCG mechanism was used in practice. According to all the above results, however, the VCG mechanism may be worth applying to practical trades when the number of units of the item to be traded is limited. The rest of this paper is organized as follows. Section 2 describes the VCG mechanism for multiunit auctions as formally as possible. A numerical example is given in the Appendix. Section 3 describes the experimental design, and Sect. 4 shows the results. Section 5 closes this paper with some remarks for further research.",1
16.0,2.0,Evolutionary and Institutional Economics Review,29 May 2019,https://link.springer.com/article/10.1007/s40844-019-00127-3,Why do workers work? Inequality and collective benefits in organisational production experiments,December 2019,Natsuka Tokumaru,,,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Evolutionary and Institutional Economics Review,20 April 2019,https://link.springer.com/article/10.1007/s40844-019-00125-5,"Economic uncertainty, precautionary motive and the augmented form of money demand function",December 2019,Pei-Tha Gan,,,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Evolutionary and Institutional Economics Review,22 August 2019,https://link.springer.com/article/10.1007/s40844-019-00131-7,"Correction to: Economic uncertainty, precautionary motive and the augmented form of money demand function",December 2019,Pei-Tha Gan,,,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Evolutionary and Institutional Economics Review,05 November 2019,https://link.springer.com/article/10.1007/s40844-019-00148-y,"Special feature: varieties of capitalism, civil society, and welfare/environmental policies",December 2019,Hiroyasu Uemura,Hironori Tohyama,Yuji Harada,Male,Male,Male,Male,,
16.0,2.0,Evolutionary and Institutional Economics Review,22 October 2019,https://link.springer.com/article/10.1007/s40844-019-00141-5,Social preference and civil society in the institutional analysis of capitalisms: an attempt to integrate Samuel Bowles’ The Moral Economy and Robert Boyer’s Régulation Theory,December 2019,Hiroyasu Uemura,,,Male,Unknown,Unknown,Male,"In the field of modern political economy, and social sciences in general, how has the concept of “citizen” and “civil society” developed with the institutional analysis of capitalism? After the 1990s, the importance of the concept of “civil society” as the public sphere was emphasized in the research of social thought and political science (Habermas 1990; Walzer 1997). Correspondingly, political economists have also come to focus on “civil society” to understand contemporary capitalist society. In this context, we discuss the recent studies of Samuel Bowles and Robert Boyer, who have had a great influence on Japan’s political economy. In Japan, Bowles is often introduced as a prominent American radical economist, and Boyer is introduced as a leader of the French régulation school.Footnote 1 Furthermore, Bowles and Boyer have often been compared, because their theoretical stance and understanding of modern capitalism have much in common. Moreover, it should be emphasized here that, as globalization leads democracy into a potential crisis, both Bowles and Boyer have developed their own understandings of “citizen” and “civil society,” based on the recent development of modern economics and contemporary political science in Europe and the United States. Boyer uses the term “société civile” (civil society) (Boyer 2015a, b), but Bowles uses the word “citizen” or “civic preference” without using the word “civil society.” Instead, Bowles uses the word “liberal society” (Bowles 2016). In this paper, we consider the meaning of Bowles’ “civic social preference” and Boyer’s “civil society,” and we also discuss the difference between their theories, and their possible development of complementary relationships. Our analysis is based on a reading of their books: Bowles’ The Moral Economy (Bowles 2016) and Boyer’s Économie politique des capitalismes (Boyer 2015a). Bowles’ recent study has reconstructed the incomplete contract theory and game theory on the basis of endogenous preference in microeconomics (Bowles 2004; Bowles and Gintis 2006). Furthermore, based on the tradition of American democracy as well as the results of research in behavioral sciences and microeconomics, especially on incentives and social preferences, he has developed his theoretical framework of the market and “liberal civil culture” (Bowles 2016). On the other hand, based on the régulation theory, Boyer has analyzed the evolving diversity and interdependence of growth regimes in Europe and Asia (Boyer 2015a; Boyer et al. 2018). Furthermore, while inheriting the tradition of French social democracy, he has developed his understanding of the interaction between polity and economy, proposing an idea of a new social democracy based on “civil society.” Bowles and Boyer collaborated on research in the late 1980s and early 1990s, and developed their macroeconomic theory based on the institutional analysis of employment (Bowles and Boyer 1988, 1990, 1995).Footnote 2 In the decade of the 2000s, they were each independently engaged in their own research, but both started to consider “citizen” and “civil society” in the context of the recent development of institutionalist social sciences and the “civil society theory” (Habermas 1990), as well as the crisis of democracy with the concentration of power to few people all over the world (Crouch 2004). Therefore, it is important to consider what theoretical innovations they have in common, as well as what their differences are to establish a new comprehensive understanding of civil society and democracy.",
16.0,2.0,Evolutionary and Institutional Economics Review,14 October 2019,https://link.springer.com/article/10.1007/s40844-019-00139-z,How does a liberalizing market influence a synergy between redistribution preference and social preferences in Asian socio-economies?,December 2019,Hironori Tohyama,,,Male,Unknown,Unknown,Male,"Asian economies, especially East Asian economies as a whole, have shown a remarkable economic performance since the 1990s, driven by the rapidly expanding Chinese economy. This economic growth brought about increased employment, which in turn allowed people to protect themselves from risks such as unemployment and reducing income. As a result, as Holliday (2000) pointed out, Asian people did not need to demand that their government should increase social welfare expenditure. However, as Kim et al. (2017) pointed out, Asian people may demand greater social welfare or redistributive expenditure given that they are currently faced with problems of growing economic inequality and a state of social welfare development that lags far behind its current state of economic development. It is essential for the current Asian socio-economies to investigate how individual attitudes toward redistribution are shaped while trying to understand this issue. Redistribution motives have been explained by the median voter model created by Meltzer and Richard (1981). This model predicts the prevalence of higher levels of redistribution as the gap between the median and average income widens (Meltzer and Richard, 1981). However, empirical evidence such as in Lindert (2004) indicated that redistribution expenditure decreases when there is greater income inequality. While the theoretical insight is convincing, this model fares poorly in the real world. Dimick et al. (2016, 2018) pointed out that the problem with this model is that it explains redistribution preferences through self-interested motives. Recently, experimental economics has shown that it would be impossible to understand redistribution preferences in terms of self-interested motives alone. Fong et al. (2005), Bowles (2016), and Bowles and Polania-Reyes (2012) argued that motives that are referred to as social preferences such as reciprocity or trust are likely to better explain how people behave (Henrich et al. 2001; Bowles and Gintis 2004; Falk et al. 2018). Dion and Birshfield (2010) pointed out that the median voter model is problematic in that it assumes that redistribution preferences are universally associated with self-interested motives. Looking at research on the Asian welfare regime or redistribution, we can see that such concepts as a community-based network, paternalism, and Confucianism effectively characterize the Asian economic society (Chang 2018; Lim and Burgoon 2018). These Asian values may affect public attitudes toward redistribution, resulting in heterogeneous associations between redistribution preferences and motives behind it. While we find a social foundation specific to Asian society—such as superiority of families to individuals—we can also see how rapidly Asian economies have developed, which is exemplified by the term “a compressed economic development” (Sturgeon and Kawakami 2011). These rapid economic developments imply that the market space in Asian society has expanded. Rapidly expanding market space, possibly accompanied by pecuniary incentives, can influence the relationship between social preferences and redistribution preferences. In other words, market incentives can be harmful to social preferences, resulting in crowding them out (Bowles and Polania-Reyes 2012; Bowles 2016). This paper focuses on social preferences in addressing how individual attitudes toward redistribution are shaped in Asian economies. We do not assume that self-interested motives behind redistribution are universal. Given that redistribution preferences depend on an institutional context within which individuals behave, we investigate whether the preference depends on heterogeneity of a social preference space or a market space in Asian socio-economies (Kim et al. 2017; Fossati 2018). Finally, we study how an expanding market space caused by rapid economic developments can affect the relationship between a public attitude toward redistribution and social preferences to investigate whether social preferences can be crowded out by expanding market space and, accordingly, self-interested motives.",
16.0,2.0,Evolutionary and Institutional Economics Review,11 November 2019,https://link.springer.com/article/10.1007/s40844-019-00142-4,Diversity and transformation of institutional configurations and trust structures,December 2019,Yuji Harada,,,Male,Unknown,Unknown,Male,"Globalization and financialization are continuous changes that have commonly been observed in advanced capitalist countries since the 1970s. These changes are often described as neoliberal changes in which the significance of the interaction of self-interested economic actors in the market is emphasized. These trends are seen as a major shift from the Keynesian regime, which is a mixed form of market- and government-led coordinations and was dominant until the 1960s. While some argue that the penetration of these changes in different countries led to a virtuous circle (financial-led growth) in the United States and the UK during the 1990s, others note that most economic coordination depends on the market mechanism which has amplified instability in the relevant economies, leading to a large-scale financial crisis in the late 2000s. It is possible that such a crisis, to a greater or lesser extent, may affect whether and how the neoliberal changes disseminate in developed countries. Nevertheless, even though there is a strong tendency to converge the coordination patterns of each country, it is also noted that economic performance diverges among different countries, and that the detailed process of institutional change in various domains also differs (Boyer 2015, 2016). Thus, it seems that a uniform trend of change and the varieties of capitalism actually coexist. How can one explain such a coexistence between the uniform trend and the variety of changes? How do institutions evolve in each domain? As frequently discussed, this paper recognizes the process of institutional change as the consequence of the interaction between existing institutions and actors. In other words, the institution defines the actors’ behavior on the one hand, but on the other hand, the accumulation of actions or motives and the consciousness of the actors guarantees the existence and change of institutions. In that way, when a particular policy is implemented, what kind of effect it produces or what kind of institutional change it leads to, is dependent on how the concerned economic actors react to the policy. If the motives and consciousness of the actors are divergent, or if the institutional configuration and social context of the economy affecting them are different, even if the same policy is applied, the resulting institutional change and economic performance may differ. Based on the above perspective, this study empirically investigates the relationship between the transformations of institutional configurations and the structure of actors’ consciousness in developed countries since the 1980s when the neoliberal regime prevailed. This paper is organized as follows: Sect. 2 reveals the diversity of the transformation of institutional configurations following Harada (2016). Section 3 is based on Harada (2018) and discusses the types of trust structures and their determining factors. Extending the analysis in Sect. 3, Sect. 4 examines the diversity of the transformation of trust structures, and investigates the relationship between the transformations of institutional configurations and trust structures. Section 5 concludes by organizing the results obtained from the previous analyses.",
16.0,2.0,Evolutionary and Institutional Economics Review,18 September 2019,https://link.springer.com/article/10.1007/s40844-019-00135-3,Welfare society and welfare state in the Japanese-type discourse on civil society,December 2019,Nanako Fujita,,,Female,Unknown,Unknown,Female,"This essay is an attempt to elucidate the significance of “Japanese-type” civil society theory, both in the context of academic history and the present day, through a consideration of its development from the perspective of “welfare.” Broadly speaking, there are three meanings that have emerged within the concept of “civil society”: political society in the social contract theory of early modern Western Europe, the “system of desires” in Hegel and Marx, and a civil, public sphere independent of both the state and the market in the discourse from Gramsci to the present day. However, the development of a distinctly Japanese theory of civil society has also been observed (Sakamoto 2006). This civil society theory original to Japan emerged from the intersection of Smithian and Marxian studies in economics, and in the pre-war years gave birth to the understanding, articulated mainly by Zenya Takashima (1904–1990), that “there is no civil society in Japan.” After the war this discourse was continued primarily by Yoshihiko Uchida (1913–1989) and Kiyoaki Hirata (1922–1995). Uchida began to use the term “civil society” in published writings in 1949,Footnote 1 and the shift in his conception of it to “civil society governed by the law of value” occurred in his 1953 book Keizaigaku no seitan [The Birth of Political Economy] (Uchida 1953; Uemura 2010). At a symposium commemorating the 100th anniversary of Weber's birth, Uchida presented a model of the subject called “civil society youth,” and pointed out the problematic nature of a civil society that cannot be entirely accomodated within pure capitalism, along with the importance of “one good, one price” in Japan (Uchida 1965). Uchida talks about civil society as a trans-historical concept maintained in both capitalism and socialism, and furthermore as a normative idea that shows us the image of the kind of society that should be pursued. Hirata, on the other hand, developed a theory of the “re-establishment of individual property” as a new interpretation of socialism, beginning with his discovery of the importance of “civil society” in Marx’s thought presented in his 1969 book Shiminshakai to shakaishugi [Civil Society and Socialism]. Here “civil society” is viewed as “a society in which ‘all private individuals’ engage in interactions (traffic) as equal proprietors” (Hirata 1969). In the 1970s, however, Japanese-type civil society theory lost both its influence over public opinion and its academic cohesiveness. In the 1990s, a “new civil society theory” emerged, inspired mainly by the second edition of Jürgen Habermas’ Strukturwandel der Öffentlichkeit [The Structural Transformation of the Public Sphere] (1990). This approach emphasized the role of civil society as a third domain, distinct from both the state and the market (the economy), and was much discussed in Western Europe in the context of “the restructuring of the welfare state.” In today’s Japan almost all discussions of civil society involve this conception. Trends in Japanese-type civil society theory have been intimately connected to diachronic changes in the economy, politics and society of Japan, and the particularities of this discourse overlap with the characteristics of Japanese movements from the perspective of “welfare.” Japanese-type civil society theory was developed in the midst of the rapid recovery and growth of a destitute, but when the strength of Japan as a “great economic power” began to be talked about around the world from the mid-1970s through the 1980s it lost its previous vigor. What is the significance of this, and what consequences have it led to in today’s Japan? I will present an interpretation of these developments.",
16.0,2.0,Evolutionary and Institutional Economics Review,09 September 2019,https://link.springer.com/article/10.1007/s40844-019-00133-5,Potential mechanisms for the social regulation of economies on global and local scales: an institutional analysis of ESG investment and community renewables,December 2019,Kazuhiro Okuma,,,Male,Unknown,Unknown,Male,"After 30 years of stable postwar growth, the Keynesian–Fordist way of economic coordination became nonfunctional and new modes of coordination have since been sought in the context of progressive globalization. While globalization has diverse meanings and national economic coordination shows diverse patterns,Footnote 1 international liberalization in trade and finance has had pervasive effects. Akin to Boyer and Hollingsworth’s (1997) notion of “spatial and institutional nestedness”, institutional forms exist and are interlinked in multilayered spatial scales. In this structure, the state is no longer a decisive entity despite its continued importance. Within this complex structure, a crucial question arises, as to how and at what spatial scale economy and society can be coordinated. For Polanyi (1957), economies must be somehow regulated by society. Boyer and Saillard (1995) highlighted the spatial levels of the social regulation of economies as an important research issue in the régulation approach.Footnote 2 In this regard, while the importance of states has continuously been stressed, global and local régulation have been discussed with the understanding that they are not complete, but partial modes of régulation (e.g., Benko and Lipietz 1995; Gilly and Pecqueur 1995). Region (above the state level) is also an important focus and regional integration in the EU and Asia has been investigated (Boyer et al. 2018). Region can form a cohesive unit of economic activity that may also enable social regulation of the economy, though the EU faces issues rooted in its uniform currency (Ibid.). The spatial level at which economy and society can be coordinated remains an open question.Footnote 3 Regarding economic coordination in non-state spaces, notable movements have been emerging in global and local scales, mainly in terms of the economy–environment relation, namely, global ESG investment and community-based renewable energy development (hereafter, “community renewables”). ESG investment is an approach to incorporate environmental, social, and governance (ESG) factors into investment decisions to better manage risk and generate long-term returns, which is also called sustainable investment or responsible investment. Community renewables, in this article, refer to renewable energy projects that are owned mainly by the local community members such as citizens, municipalities, and SMEs. These movements have been rapidly spreading in developed economies and through globalizing economy, gradually gaining influence also over the developing world. They have become priorities in formal policy agendas. For example, the Japanese 5th Basic Environmental Policy Plan (formulated in April 2018) stipulates “Circulating and Ecological Local Economy” as one of its main targetsFootnote 4 and places environmental finance and ESG investment at the top of its priority strategies (Ministry of the Environment 2018). Environmental degradation respects no borders causing global problems, and spatial gaps in national governance appear to be hindering solutions (Okuma 2017). In this respect, it is natural that advanced or experimental economic-coordination movements on a non-state scale emerge in the economy–environment relation. The two abovementioned movements, while being led by environmental aspects, involve social aspects such as labor issues and they exert influence on the social regulation of the economy as a whole. This is exemplified by the abovementioned 5th Basic Environmental Plan, which stresses the interlinkage of environmental, economic, and social issues, using language such as “integrated improvements on environment, economy, and society.” What is the significance of these movements in the social regulation of economy? Can they effectively function as institutional mechanisms for global or local coordination? Is it possible that they can transform economies into ecologically and socially sustainable forms? Such questions should be asked, when considering the current status and the future of economy–environment coordination (or environmental policy) as well as when exploring the possible forms of the social regulation of economy overall. These movements have been studied from various perspectives, such as business studies, sociology, environmental policy studies, and human geography. With regard to ESG investment, there is a sizable literature on its methodology and profitability (e.g., van Duuren et al. 2016; Auer and Schumacher 2016) as well as its causal factors (e.g., Majoch et al. 2017; Mӧrth 2014). Regarding community renewables, research has been conducted on their implementation and economic effects (e.g., Heinbach et al. 2014; Raupach-Sumiya et al. 2015; Nakamura et al. 2012). For both, however, there have been few studies from institutional economic perspectives, especially from the abovementioned viewpoints of potential institutional mechanisms for the social regulation of economy [see, however, Bartley (2007) and Shamir (2011) on the theme of CSR and Gibbs and Jonas (2000) and Bulkeley (2005) on the theme of local environmental policy]. The current article attempts to make sense of ESG investment and community renewables in the context of institutional coordination in the globalized economy. They are analyzed by drawing upon the régulation approach, with reference to economic geography as well as other related studies. Using the conceptual framework of growth regimes and modes of régulation—and, in particular, by applying the concept of institutional hierarchy with attention to spatial scales—these two movements are contextualized in the evolving institutional coordination of capitalist economies.Footnote 5 This approach, as will be discussed later, generates the understanding that these two movements are not isolated phenomena, but are actions taken by society in response to the “glocalization” of the economy. Their potentials are also explored by analyzing their driving forces with attention to the relevant actors and by referring to evolutionary concepts where necessary. Thus, the present article, by examining emerging movements in the economy–environment relation, aims to contribute to the search for institutional mechanisms for the social regulation of economy under globalization. It also aims to contribute to the literature on ESG investment and community renewables in environmental economics and policy studies by making connections with long-term and structural economic changes. The rest of this paper is organized as follows: To obtain a basic framework for the analysis, Sect. 2 reviews transformations in institutional forms since the postwar Fordism era, focusing on their configuration on spatial scales. On this basis, Sect. 3, after providing an overview of the ESG investment and community renewables movements, analyzes their positions in the structure of institutional forms to consider their significance. Then, Sect. 4 discusses their potential as mechanisms for the social regulation of economy. Section 5 concludes the paper and explores policy implications.",3
16.0,2.0,Evolutionary and Institutional Economics Review,12 October 2019,https://link.springer.com/article/10.1007/s40844-019-00143-3,Samuel Bowles: The moral economy: why good incentives are no substitute for good citizens,December 2019,Toshio Yamada,,,Male,Unknown,Unknown,Male,"Although this book is a theoretical and conceptual work, examples and experiments drawn from various sources are also very interesting. For example, in daycare centers in Haifa in Israel. Daycare centers that were concerned about parents arriving late to collect children decided to impose a fine for late arrivals. However, when this fine system began, the number of late arrivals doubled. The busy daycare centers eventually canceled the fine system, but the number of late arrivals remained high. What does this mean? “Putting a price on time” as if it were a general commodity, parents began to regard the late arrival as “a commodity that can be bought.” That is, the “sense of ethical obligation” of parents who do not cause any issue for the nursery staff is drowned out, planting the idea that if money is paid, it is okay to be late. As a result, even if the fine system was abolished, the number of late arrivals, which had already increased, was not diminished. In summary, there is a pull between the goal of “incentive” (in this case, negative monetary stimulus as a fine) and “moral behavior” as a citizen (also expressed broadly as “social preference”), and it was not possible to achieve the original purpose of preventing late arrivals; in fact, the situation became even worse. This example raises a significant question. Why do you kill people’s social preferences using incentives to realize the aimed effect? Why are material incentives no substitute for moral citizens? That is the ultimate theme of this book. Obviously, each branch of economics has built a theory on the premise of selfish individuals (homo economicus) who act in response to incentives. In this sense, Bowles’s book is a fundamental challenge to such economics, a modern version of the “critics of economics,” more specifically, “critics of social science.”",2
16.0,2.0,Evolutionary and Institutional Economics Review,23 October 2019,https://link.springer.com/article/10.1007/s40844-019-00145-1,"Robert Boyer, Économie politique des capitalismes: Théorie de la régulation et des crises, La Découverte, 2015",December 2019,Yuji Harada,Hiroyasu Uemura,,Male,Male,Unknown,Male,"The régulation theory has been developing through both theoretical and empirical studies on contemporary capitalism—not only in France, but globally—for more than half a century. Robert Boyer, the author of the book Économie politique des capitalismes: Théorie de la régulation et des crises, La Découverte, 2015, has led the régulation school and has developed the systematic framework of the régulation theory based on the research stating that the diverse interactions between polity and economy are the core ingredients of the analysis of the different varieties of capitalism.Footnote 1 As the book was published in French, this article provides a careful translation and interpretation of the French words and phrases in English to help readers understand the academic essence of the book. The contents of the book are shown in English as follows: Political Economy of Capitalisms: Theory of régulation and crises Introduction I Foundations At the base of a capitalist economy: institutional forms From iron laws of capitalism to the succession of modes of régulation Accumulation regimes and historical dynamics  A theory of crises II Developments Logics of action, organizations, and institutions  The new institutional arrangements of contemporary capitalism  Polity and economy: a political economy of the modern world  Diversity and renewal of forms of capitalism Levels of régulation: national, regional, supranational, and global From one mode of régulation to another Conclusion: analyze and understand this new tipping in the history of capitalisms The title of the book contains four key words, “political economy,” “capitalism,” “régulation,” and “crises,” which provide alternative concepts to “economics,” “market,” “equilibrium,” and “economic growth” in Neo-classical economics. The contents of the book explain the foundations and developments of the régulation theory. This review starts by introducing the contents of the book with careful translations of important academic terminologies and provides an understanding of the foundations and development of the régulation theory in Sects. 2 and 3. Next, Sect. 4 proposes possible interpretations of the book’s theoretical messages in the context of the development of the Japanese political economy to promote a more mutually beneficial collaboration of French and Japanese social sciences. Finally, in Sect. 5, the régulation theory’s original understanding of the transformations of capitalisms is summarized.",2
17.0,1.0,Evolutionary and Institutional Economics Review,13 January 2020,https://link.springer.com/article/10.1007/s40844-019-00158-w,Wage and employment determination in a dynamic insider–outsider model,January 2020,Marco Guerrazzi,,,Male,Unknown,Unknown,Male,"The insider–outsider theory of employment and unemployment rests on the assumption that there is a fundamental asymmetry in the wage setting process between incumbent workers (insiders) and unemployed workers who are looking for a job (outsiders). On the one side, relying on labour turnover costs and/or firm-specific skills that may create a productivity premium, insiders are assumed to be endowed with a strong bargaining power in the wage setting process—sometimes strengthened via formal or informal unions—and to exploit this power to maximize their pay and foster their employment opportunities. On the other side, outsiders are assumed to have no market power and when they have the chance to find a job, unless they become insiders, their wage is usually quite close to the reservation level and they have no say over their employment prospects. Undoubtedly, the main backers of the insider–outsider theory are Lindbeck and Snower (1984, 1989, 2002) who developed a long array of works that uncover a wide range of consequences in terms of wages and (un)employment triggered by the interest conflict established between insiders and outsiders in the labour market. Relying on that conflict, the two authors provide a sound rationale for a number of puzzling stylized facts that characterized the macroeconomic experience of Western economies during the 1980s and the 1990s such as the existence and persistence of involuntary unemployment, the inflexibility of insiders’ wages, trade union corporatism and the asymmetry of wage-employment movements observed during recessions and expansions.Footnote 1 In the latest survey article dedicated to the theoretical approach that they contributed to start, Lindbeck and Snower (2002) point out that one of the most complex and still open question of the insider–outsider theory is the way in which employment and wages move through time in response to labour market shocks. Specifically, while there are several insider–outsider contributions that show how to fix the levels of employment and wages at any point in time, dynamic insider–outsider models—despite some fair exceptions—lag somehow behind. Similarly, in another survey, Sanfey (1995) argues that modifying traditional union models to consider the distinction between insiders and outsiders is straightforward, but that task is actually much more complicated in dynamic models. In confirmation of the arguments recalled above, the dynamic insider–outsider literature counts a limited number of contributions. For instance, Solow (1985) outlines a two-period insider–outsider model, but he is mainly concerned about what happens in the first and he does not derive any explicit dynamic law for employment and wages.Footnote 2 Drazen and Gottfries (1990) set forth a dynamic optimizing insider–outsider framework developed over an infinite horizon, but they model the evolution of wages and employment by means of few discrete realizations of the two variables. Huizinga and Schiantarelli (1992) develop a discrete-time model in which a firm and an insider union efficiently bargain over the wage and employment, but the two authors focus only on the path of employment adjustments driven by productivity shocks. Fukuda and Owen (2008) build a series of overlapping-generations (OLG) models with human capital accumulation in which the insider–outsider dichotomy is conveyed in terms of firm-specific versus general skills.Footnote 3 More recently, Galì (2016) sets forth a New Keynesian DSGE model with an insider–outsider labour market by exploring the implications of that environment for the design of optimal monetary policies when unemployment is strongly persistent. To the best of my knowledge, the present contribution is the first attempt to provide a dynamic insider–outsider model developed within an optimal control framework with continuous time and infinite horizon. Specifically, I build a parsimonious differential game in which a corporative union of insider workers whose members cannot be fired is called in to choose the common wage trajectory of incumbents by taking into account the optimal employment path selected by a representative profit-maximizing firm that, in turn, is assumed to decide the number of outsiders—or entrants—to hire in each instant on a spot labour market. In detail, in my game new hirings boost union membership at a fixed rate and newly hired workers are assumed to be costly for recruiting entrepreneurs. Furthermore, the reservation wage of outsiders as well as the redundancy rate of insiders are taken as exogenously given. Within that strategic framework, the dynamic interaction between the decisions of the firm and the ones of the union describes how wages and employment are continuously determined and move through time in a typical insider–outsider economy. In my proposal, the labour input of incumbents follows a stock-adjustment process and its marginal cost—the insider wage—evolves according to the preferences of unionized workers who are in the position to manipulate the recruiting decisions of the entrepreneur without caring about the welfare of unemployed people. The theoretical insider–outsider setting developed in this paper differs from recent dynamic union models with bargaining in many directions. For instance, consistently with the search-and-matching framework with a monopoly union set forth by Krusell and Rudanko (2016), the union of insiders described in my differential game embodies in its optimization problem an intertemporal constraint that conveys the optimal hiring decisions of the representative firm. From a dynamic perspective, this feature is at odds with respect to DSGE models with wage bargaining such as Mattesini and Rossi (2008, 2009) and Zanetti (2007) in which unions act as static Stackelberg leaders that do not consider the effects of their optimal wage trajectory on the dynamic path of hirings decided by employers. Other distinguishing marks of my dynamic insider–outsider model are the definition of union membership as well as the treatment of labour inputs. On the one hand, as opposed to Morin (2017), Krusell and Rudanko (2016), Alvarez and Shimer (2014), Zanetti (2007) and Delacroix (2006), union members do not include unemployed workers. Obviously, at the firm level, this makes a sharp distinction between internal and external workers.Footnote 4 On the other hand, the labour input employed by the firm is split into two distinct components, i.e. the insider and the entrant labour force that are not perfectly substitutable for employers. The former follows a stock-adjustment principle whose inflows are not described by a matching function as in Morin (2017), Krusell and Rudanko (2016) and Trigari (2006), but are given instead by the fixed share of entrants that join the insider union. The latter is a flow variable that represents newly hired workers who suffer a productivity gap with respect to incumbents. Such a particular characterization deserves to be stressed since Alvarez and Shimer (2014), Mattesini and Rossi (2008, 2009) and Zanetti (2006, 2007) treat labour input as a single flow variable under the control of employers. In addition, the insider union described in this paper is assumed to set a smooth wage trajectory without relying on any exogenous minimum wage—or wage norm—as is done instead in Alvarez and Shimer (2014), Mattesini and Rossi (2008, 2009) and Delacroix (2006). The results of my theoretical exploration can be summarized as follows. First, under the assumption that the union of insiders commits itself to a given path of wages and behaves in an egalitarian manner, I demonstrate that the dynamic insider–outsider model economy displays an open-loop Stackelberg equilibrium in which the initial stock of insiders pins down the trajectories of incumbents, entrants and insider wages in a well-determined manner (cf. Dockner et al. 2000). Moreover, resorting to numerical simulations, I show that in the dynamic model under investigation adjustments towards the unique steady-state equilibrium occur through damped asymmetric oscillations that mirrors the parallel decline of union membership and union wage premia observed in the US over the last 20 years (cf. Blanchflower and Bryson 2004). According to the logic underlying the game played by the workers and the firm, this pattern is a direct implication of the optimal behaviour of unionized insiders that extract rents from entrepreneurs subject to firing limitations (cf. Alvarez and Shimer 2014; Krusell and Rudanko 2016). In addition, consistently with models of dynamic bargaining, I show that my intertemporal insider–outsider framework delivers a positive equilibrium relationship between the labour market power of the union and the degree of impatience of the firm (cf. Binmore et al. 1986; Osborne and Rubinstein 1990). The paper is arranged as follows. Section 2 develops the theoretical model. Section 3 explores its numerical properties. Finally, Sect. 4 concludes the paper.",1
17.0,1.0,Evolutionary and Institutional Economics Review,10 January 2020,https://link.springer.com/article/10.1007/s40844-019-00155-z,Gender-specific reference-dependent preferences in the experimental trust game,January 2020,Hiromasa Takahashi,Junyi Shen,Kazuhito Ogawa,Male,Unknown,Male,Male,"In the best-known version of the trust game (Berg et al. 1995), an agent (called a truster) is asked what portion of a given endowment he or she is willing to entrust to a complete stranger (called a trustee) with the expectation that the latter will reciprocate by returning more money than he or she initially received. The accounting is made viable by the fact that any investment made by the sender is multiplied by a factor of more than one (tripled in our experiment) before reaching the recipient. While the solution predicted by game theory is that both truster and trustee have no incentive to make any positive offers, Pareto efficiency would require the sender to trust and the recipient to be trustworthy (i.e., if the sender makes a positive offer and the receiver returns an amount not lower than the offer received, both parties are better off or at least not worse off). Although the typical interpretation of the trust game labels the sender’s behavior as “trust” and the receiver’s behavior as “trustworthiness”, it should be noted that trust or trusting behavior carries the risk of negative consequences if the trust is misplaced and met by untrustworthy behavior. Therefore, the truster’s behavior can also, to some extent, exhibit his or her risk preference. Gender differences in behavior have been studied in many economic experiments. Croson and Gneezy (2009) reviewed the literature and identified robust differences in three main sections—risk preferences, social (other-regarding) preferences, and competitive preferences. They concluded that (i) women are more risk-averse than men; (ii) the social preferences of women are more situationally specific than those of men; women are neither more nor less socially oriented, but their social preferences are more malleable; and (iii) women are averse to competition than are men. Acknowledging these gender differences, in this paper, we design a trust game experiment to investigate gender-specific reference dependent preferences. Previous studies investigating gender differences in the trust game provided mixed results on both truster’s and trustee’s behaviors. For truster’s behavior, while a number of studies found no gender differences in sending behavior (Bohnet 2007; Bohnet et al. 2010; Croson and Buchan 1999; Cox and Deck 2006; Schwieren and Sutter 2008), other studies found that men are more trusting or risk-seeking than women (Buchan et al. 2008; Chaudhuri and Gangadharan 2007; Garbarino and Slonim 2009; Snilders and Keren 2001). Only a very few studies found that women are more trusting or risk-seeking than men (Bellemare and Kröger 2007; Bohnet et al. 2010). In addition, for trustee’s behavior, while some studies found no gender differences in trustworthiness or reciprocity (Bohnet 2007; Cox and Deck 2006; Innocenti and Pazienza 2006), others found that women are more reciprocal than men (Buchan et al. 2008; Chaudhuri and Gangadharan 2007; Croson and Buchan 1999; Schwieren and Sutter 2008; Snilders and Keren 2001). Only one study (Bellemare and Kröger 2007) found that men are more reciprocal than women. The current study aims to reinvestigate gender-specific behaviors in the trust game by taking each participant’s reference point into account. In the economic literature of decision-making under risk, the Expected Utility Theory in which the expected utility of an act is a weighted average of the utilities of each of its possible outcomes had dominated this field before Kahneman and Tversky (1979) developed an alternative model called Prospect Theory. In Kahneman and Tversky (1979), they demonstrated that the value obtained from one’s decisions is assigned to gains and losses rather than to final assets, and people evaluate losses (i.e., the situations that any return is below one’s reference point) more than gains (i.e., the situations that any return is above one’s reference point). Given the importance of linking people’s reference points with their behaviors, we believe that it could help us to understand gender-specific behaviors in the trust game at a much deeper level. Previous studies regarding the reference point mainly focused on how one’s reference point affects effort provision or labor supply (Abeler et al. 2011; Andersen et al. 2014; Fehr et al. 2011; Hilken et al. 2013). Abeler et al. (2011) investigated whether expectations could affect effort provision in a real-effort experiment. They found that effort provision is significantly different between treatments in the way predicted by models of expectation-based reference-dependent preferences: if expectations are high, subjects work longer and earn more money than if expectations are low. Andersen et al. (2014) designed a field experiment to study reference-dependent labor supply. They found that, consistent with neoclassical theory and reference-dependent preferences with endogenous reference points, workers (i.e., vendors in open air markets) supplied more hours when presented with an expected transitory increase in hourly wages. In contrast with the prediction of behavioral models, however, when vendors earn an unexpected windfall in the day, their labor supply did not respond. Our experiment conducted in Kansai University is a one-shot trust game experiment. Our main results are that (i) male subjects are risk-seeking even in both the gain frame and the loss frame; (ii) women are not always more risk-averse than men; and (iii) women display other-regarding preferences only when they are in a gain frame. The remainder of the paper is organized as follows. Section 2 describes the issues related to the experiment. The results of the experiment are presented in Sect. 3. Finally, Sect. 4 discusses the results and suggests several possible directions for future study.",5
17.0,1.0,Evolutionary and Institutional Economics Review,16 October 2019,https://link.springer.com/article/10.1007/s40844-019-00137-1,"Network centrality, social loops, and utility maximization",January 2020,Hideki Fujiyama,,,Male,Unknown,Unknown,Male,"Agents’ incentives to exert effort depend on their positions in a social structure (Ballesteret al. 2006; Bramoullé and Kranton 2007; Uzzi 1996). Different positions in a social structure induce different outcomes within companies (Cross and Cummings 2004; Ibarra and Andrews 1993), among companies (Hochberg et al. 2007; Podolny 1993), and even in loosely connected networks such as Internet groups or the Hollywood film industry (Ahuja et al. 2003; Cattani and Ferriani 2008). Although the empirical studies cited above measure a position within a social structure by network centralities, their exact meanings are not described sufficiently. Centralities in a network bear their own social meanings (Jackson et al. 2017). Degree centrality relates to direct communications with others. Closeness centrality relates to ease in communicating with an entire network. Direct and indirect ties refer to paths of acquiring information or resources. Betweenness centrality relates to the benefits generated by bridging the gap between the two different groups or controlling the information among them. Bonacich centrality (Bonacich 1987; Katz 1953) is an extension of degree centrality that includes the additional effects of passing through indirect ties. Although these are simple interpretations of centralities used in earlier empirical studies, there remains a shortcut from centralities to outcomes. Agents’ benefits depend on both their networks and their behaviors. Therefore, first, we develop a theoretical model, and second, we test the theoretical relations between agents’ behaviors and their network positions. Calvó-Armengol et al. (2009) is a seminal work for this undertaking. They show that Bonacich centrality is derived as a consequence of rational behavior and test the relation between individual effort and the centrality index using data for networks of junior and senior high school students. The feature of behavior patterns in Calvó-Armengol et al. (2009) is that individual effort depend on effort exerted by others in the network. At equilibrium, each agent must predict other agents’ effort accurately and respond correctly. In other words, the Calvó-Armengol et al. (2009) model assumes a strategic and rational situation and adopts game theoretic analysis to approximate real situations. We raise another possibility by introducing bounded rationality. If we endorse the proverb “What goes around, comes around,”Footnote 1 then in some situations agents know the importance of loops in social networks but do not respond strategically to others’ behaviors. The literature of social network analysis discusses the importance of these loops (Ballester et al. 2006; Butts 2006; Snijders et al. 2006) and introduces bounded rationality in decision-making (Snijders 2005; Snijders et al. 2010). It is reasonable to assume that agents take account of the feedback from their efforts through loops in a network. As its effects depend on agents’ positions in the network, so do their efforts. Calvó-Armengol et al. (2009) has one limitation. They estimate a discount factor for Bonacich centrality that sometimes leads to divergence of utility. It is difficult to understand this kind of divergence in stable social situations. Therefore, we seek a measure of centrality that excludes this divergence. The remainder of this article is organized as follows: In Sect. 2, we present a theoretical model and show a relation between individual effort and centrality taking account of social loops. In Sect. 3, data of university students is introduced and we test the theoretical relation empirically in Sect. 4. Finally we make discussions and conclusions in Sect. 5.",2
17.0,1.0,Evolutionary and Institutional Economics Review,15 November 2019,https://link.springer.com/article/10.1007/s40844-019-00150-4,"Post-Keynesian institutionalism: past, present, and future",January 2020,Charles J. Whalen,,,Male,Unknown,Unknown,Male,"Delivering his presidential address to the Association for Evolutionary Economics (AFEE) in the 1960s, Gruchy (1969: 5) reported meeting a graduate student a few years earlier: “This young student said he was happy to shake my hand as he understood that I was the last of the institutionalists.” Gruchy dismissed the cheeky remark as a reflection of the view of many distinguished economists. After all, he noted, Kenneth Boulding had said more than a decade earlier that institutionalism’s days had passed and it deserved to be placed in a museum. A half-century after Gruchy’s address, it was my turn to serve as AFEE’s president. Institutionalism—by which I mean the intellectual tradition known as original institutional economics, which originated in the work of economists such as Thorstein B. Veblen, John R. Commons, and Wesley C. Mitchell, and was central to the founding of AFEE—remains alive; and its work remains as important as ever.Footnote 1 In my experience, post-Keynesian institutionalism (PKI), a branch of the institutionalist tradition that draws heavily on common ground shared by many institutionalists and post-Keynesians, is an important reason for the tradition’s continued vitality. This article contributes as a survey of PKI as follows. The first section examines PKI by reviewing its origins and development. That section ends with a definition of PKI and a summary of core ideas. Then the article highlights existing PKI contributions in the course of outlining directions for future research. A concluding section includes reflections on how post-Keynesian economics has helped energize the institutionalist tradition, an acknowledgment of the article’s limitations, and an expression of hope for international participation in the further shaping of PKI.",9
17.0,1.0,Evolutionary and Institutional Economics Review,21 November 2019,https://link.springer.com/article/10.1007/s40844-019-00152-2,Globalization and the erosion of geo-ethnic checkpoints: evolving signal-boundary systems at the edge of chaos,January 2020,Chris Girard,,,,Unknown,Unknown,Mix,,
17.0,1.0,Evolutionary and Institutional Economics Review,21 January 2020,https://link.springer.com/article/10.1007/s40844-019-00157-x,Economic patterns in a world with artificial intelligence,January 2020,Dirk Nicolas Wagner,,,Male,Unknown,Unknown,Male,"Recent advances in computer hardware and software have given rise to “The Second Machine Age” (Brynjolfsson and McAffee 2016) which is increasingly powered by what is commonly called artificial intelligence (AI). “Artificial General Intelligence” (Goertzel and Pennachin 2007: 1) that is comparable to or supersedes human level intelligence will remain out of reach for quite some time, but the so-called “narrow AI” (ibid: 1) has left the research labs and enjoys rapid and widespread adoption across most industries. Current AI rests on technologies like machine learning, deep neural networks, big data, internet of things and cloud computing. As such, contemporary AI can be perceived as a general-purpose technology (Trajtenberg 2019) and has the potential to dramatically change the economy (Furman and Seamans 2019). Whilst organizations of all sizes and from all sectors have started to pursue their goals with the help of AI, limited experience and knowledge is available concerning the impact AI will have on the economy and on society in general. Since AI very quickly makes an already complex world even more complex, economic research that contributes to an understanding of the impact of AI technologies is urgently required but still scarce (Agrawal et al. 2019). The purpose of this paper is to outline relevant economic patterns in a world with AI with the help of an analytical perspective rooted in institutional economics. It is meant to contribute to a better understanding of institutional evolution in an increasingly complex world (Rosser and Rosser 2017). To achieve this, it supports the idea that the time has come for a more “entrepreneurial economics” (Koppl et al 2015: 22) which assumes a creative world where “the system must adapt to unforeseen and unforeseeable changes that represent new possibilities and new opportunities in the adjacent possible” (ibid: 22). Entrepreneurial economics as it is understood here, includes a process of discovery of how economic patterns change under the influence of technological innovation as well as processes of economic design to shape economic patterns. In such a context, it appears to be important to identify, observe, question and discuss economic patterns in the sense of patterns observable in the real world and in the sense of patterns of economic thought to allow our shared mental models (Denzau and North 1994) to appreciate and adapt to a world with AI. The dual role of economics puts additional weight on this endeavor: the discipline does not only provide theories for the explanation but also concepts for the design of a world with AI (Parkes and Wellman 2015; Wagner 2001). In what follows, evidence for the relevance of these patterns is provided and the perspective is brought to life by conducting an interdisciplinary integrative literature review (Torraco 2016). Initially, the institutional economic perspective and the key patterns and problems perceived through this theoretical lens will be laid out. Subsequently, the phenomena in question will be reviewed one by one in more detail. The original contribution of this paper is that it synthesizes interdisciplinary views on a world with AI to derive resulting economic patterns and to make them accessible to institutional economics by establishing suitable notions and analytical frameworks. As a result, guidance for interdisciplinary research to further explore the economic patterns of a world with AI is given.",16
17.0,1.0,Evolutionary and Institutional Economics Review,10 January 2020,https://link.springer.com/article/10.1007/s40844-019-00156-y,From general equilibrium theory to the economics of uncertainty: a personal perspective,January 2020,Yasuhiro Sakai,,,Male,Unknown,Unknown,Male,"I was born in the pre-World War II period. Since so many people were born after the war, I sometimes felt as if I belonged to rare species. During those long years, the people saw a series of extraordinary things such as atomic bombing in Hiroshima and Nagasaki (1945), the rise of People’s Republic of China (1949), the Cuba crisis between the capitalist bloc and the socialist bloc (1962), the Russian spaceship Sputnik over the earth (1964), the first man on the moon by American space project (1969), and the frequent occurrence of violent student movements (1968–1969). Those events which happened between 1940 and 1970 were more or less the products of the so-called Cold War between the two powerful blocs.Footnote 1 I would like to add that another sort of cold war took place on the academic front as well. In this connection, it is recalled that Seiji Kaya, the chairman of the Japan Science Council, once remarked in a newspaper: On reflection it is really ridiculous that mankind cannot live on this globe peacefully with each other when they possess the knowledge and know-how even of making a round-up trip to the moon. The most important thing from now on seems to be to join our efforts in making the time nearer when we can all visit the moon as friendly tourist, instead of being involved in the clash between communism and capitalism. (Kaya 1957) Kaya was then a famous natural scientist, also serving as the President of Tokyo University. Carefully reading Kaya’s article, Shigeto Tsuru, a noted economist with a Ph.D. degree from Harvard University, did not agree with Kaya and made the following counterargument: The distinction between capitalism and socialism as a social system is not due to emotional antagonism of politicians or to doctrinaire rigidity of academic people. Dr. Kaya’s wish for a harmonious world is everybody’s wish; but he should be aware that there does exist here a scientific problem of differentiating different social systems by an objective criterion and that the difference between them cannot be wished away. (Tsuru 1961) The difference of opinions between Kaya and Tsuru was quite clear-cut and seemed to be almost irreconcilable. Speaking of myself, I was then an eager econ-major student, thus being inclined to side with Tsuru rather than Kaya.Footnote 2 In my student days in the 1960s, there existed two popular yet opposing textbooks in economics. They were nicknamed the “red text” and the “blue text.” In hindsight, this was a strange start for my student life. The red text was well represented by Economic Textbook published by the U.S.S.R. Academy Economics Institute (1958), the most authoritative agency of the Soviet socialist bloc. The issue of “socialism versus capitalism” constituted the central theme of the red text. The coauthors of the text took pride in reaching the following conclusion: We have thus far discussed the whole processes of economic development of a society. As a result, we have reached the most important conclusion that from a historical viewpoint, capitalism is destined to collapse, whereas socialism is marching for its final victory over capitalism. There should be no other way! We are so confident of such historical inevitability. (U.S.S.R. Academy Economics Institute 1954) In contrast to the powerful red text, the blue text, presumably being regarded as a strong rival, seemed to be rather modest and even hesitant. To take an instance, Samuelson (1955, 7th edition 1967) wrote the world-popular textbook Economics, a typical blue text defending the American capitalist bloc. Samuelson modestly remarked: America leads Russia, but will the gap narrow? (Samuelson, 7th edition 1967) Comparing those two texts, I was clearly more impressed by the powerful red text than the moderate blue text. After some hesitation, however, I decided to go to the United States so that I could continue my graduate study without unduly political and psychological interruptions. I just wanted to get out of the Japanese university disturbance in the 1960s, thus daring to jump into the core of the capitalist economy.Footnote 3",
17.0,1.0,Evolutionary and Institutional Economics Review,10 February 2020,https://link.springer.com/article/10.1007/s40844-020-00161-6,Special issue: The 7th International Symposium on Human Survivability “Let’s Work Together Toward Achieving the Sustainable Development Goals”,January 2020,Yuichi Ikeda,,,Male,Unknown,Unknown,Male,,1
17.0,1.0,Evolutionary and Institutional Economics Review,23 November 2019,https://link.springer.com/article/10.1007/s40844-019-00151-3,"Vision, identity, and collective behavior change on pathways to sustainable futures",January 2020,Ilan Chabay,,,Male,Unknown,Unknown,Male,"Humanity faces a host of critical challenges arising from patterns of living unsustainably in nearly all contemporary societies. These challenges that exist on global to local scales have been related to fundamental limitations of resources, failures in governance of commons, population growth, and to the risk of transgressing critical bio-geo-physical conditions in the Planetary Boundaries perspective (Rockström et al. 2009; Steffen et al. 2015a, b). The “Anthropocene Era” (Crutzen 2002) captures the planetary conditions and humanity’s increasingly profound role in shaping them. It is both a marker of human impact in transforming the Earth and crucially a call to recognize the need to address the consequences of a changing relationship between our conception of and response to the interplay between social and ecological systems. The similar patterns of abrupt increase and accelerating rate of change in environmental and social phenomena, shown in the “Great Acceleration” graphs (Steffen et al. 2015a, b), adds urgency to the need to change these patterns. In light of these challenges that humanity faces, a safe and just operating space for humanity can be visualized as a “doughnut” (Raworth 2017) or annulus between minimal societal needs in multiple dimensions and the ceiling characterized by the Planetary Boundaries logics. Increasing recognition of the multiple challenges facing human societies led to the approval by the United Nations in 2015 of the 17 sustainable development goals (SDGs) and 169 targets for 2030 (United Nations General Assembly 2015). Setting and agreeing on these goals represent a remarkable and significant step forward by 193 nations. The SDGs are important, because they represent the aspirations agreed upon by all countries for a sustainable future with equity and justice. However, the exact meaning of sustainability, as well as of equity and justice, and the means, governance, and monitoring of progress toward the goals is not agreed upon or shared globally. The exact interpretation of the SDGs and targets and the means of implementing them are open to interpretation by nations and also to questions of priorities in defining and implementing them at the regional and local levels. These arguments and ambitious goals—whether from a natural science or a social science perspective—have helped drive essential discussions of the current and possible future conditions on our planet and in our society and how to shape responses to these challenges. To go beyond these boundary perspectives and stimulate further valuable discourses and actions, this paper draws attention to visionary, normative, and cultural dimensions in play in societies seeking sustainable futures. In considering visions and goals to address the unprecedented changes occurring, a boundary to be overcome rather than avoided is freeing our imaginations from the constraints of what has been done and paths that have been followed. This also opens the conversation to humanities scholarship and arts by bringing in imagination or imaginary worlds (Bendor et al. 2015) and esthetic, moral, legal, spiritual, and philosophical perspectives. As important as the agreement and efforts toward the SDGs are, there are substantive concerns that must be addressed in acting on them. One is that the SDGs represent a complex array of critical issues, which are fundamentally interdependent through multiple connections, and thus require consideration in holistic frameworks, rather than isolated compartments. A second difficulty is the assumption inherent in the SDGs that “achieving the vision underpinning the SDGs will be possible on the foundations of our current global socioeconomic system and its natural environment” (TWI2050—The World in 2050, 2018, Chapter 2). A third concern, also as expressed in the TWI2050 report (ibid), is that “Globalism threatens the future of governance because it disenfranchises many and empowers a technocratic elite. It also stimulates the emergence of identity issues, which in turn give rise to conflict and polarization.” I will return to issues of identity later in this paper in the context of narratives of vision and identity. In addition, an essential question is how societal changes on the scale needed can be implemented through a combination of top-down direction and bottom-up individual and collective behavior change at multiple spatial scales and governance levels. As noted above in regard to the Anthropocene Era, human actions are substantially responsible for the fact that we are living in the midst of accelerating, unprecedented, and complex global to local changes. Consequently, it is both critical and urgent that we change our collective behaviors with concomitant changes in norms and cultural patterns in societies from current unsustainable behaviors and onto pathways to sustainable futures appropriate in each culture and context.",14
17.0,1.0,Evolutionary and Institutional Economics Review,07 September 2019,https://link.springer.com/article/10.1007/s40844-019-00132-6,Measuring countries’ progress on the Sustainable Development Goals: methodology and challenges,January 2020,Michal Shinwell,Guillaume Cohen,,Male,Male,Unknown,Male,"The 2030 Agenda (also called the Sustainable Development Goals, or SDGs) (United Nations 2015) is a complex agenda for monitoring and assessing countries’ achievements. The 17 goals, 169 targets and 244 (232 unique) indicators of the 2030 Agenda cover a broad range of issues, with different metrics, target values and levels of ambition. In order to assess and compare progress across the Agenda, we present a methodology for measuring the distance from achieving the targets by 2030 using a unified metric, and apply this methodology to OECD countries. This methodology is at the basis of the OECD Measuring Distance to SDG Targets study (OECD 2017, henceforth, the study). Results allow identifying areas requiring prioritisation, but considerable data gaps across goals mean coverage is partial, and important data may be missing from the results. The methodology is based on three steps, and decisions made at each of these steps have a significant impact on the analysis and results, and these are discussed in the paper. As a first step, we select indicators based on the UN Global Indicator Framework for the SDGs (henceforth, the Global List of Indicators) as determined by the Inter-Agency Expert Group on SDGs (United Nations 2017). Second, in order to measure distances, an end-value (target) is set for each indicator. Finally, a normalisation method was selected based on a modified z score procedure. This normalisation method represents the idea of a peer group comparison that captures distance from a fixed end-point in a standardised way for each indicator. The methodological rationale is presented for the choices made at each of these steps in this paper, along with a series of robustness checks. The results are presented for the OECD average, at the goal level, and followed by a discussion of different approaches to measuring progress on SDGs.",5
17.0,1.0,Evolutionary and Institutional Economics Review,21 August 2019,https://link.springer.com/article/10.1007/s40844-019-00130-8,Power grid with 100% renewable energy for small island developing states,January 2020,Yuichi Ikeda,,,Male,Unknown,Unknown,Male,"The population size of Small Island Developing States (SIDS) is rather limited. SIDS have a combined population of approximately 65 million (UN-OHRLLS 2015), which is approximately 1% of the world’s population. In SIDS, nearly 30% of the population lives at elevations of less than 5 m above sea level. Therefore, SIDS are often said to be the most vulnerable areas to climate change due to the currently increasing \({\mathrm{CO}}_2\) emissions. The Intergovernmental Panel on Climate Change (IPCC) has reported that emissions resulting from human activities are substantially increasing atmospheric concentrations of greenhouse gases, resulting on average in additional warming to the Earth’s surface (IPCC 1990). Based on IPCC reports, policy makers in various countries, including advanced countries, emerging economies, and SIDS, have proposed energy policies to introduce as much renewable energy as possible to curtailing \({\mathrm{CO}}_2\) emissions. However, skepticism persists due to the high cost of investment in renewable energy and its integration into power grids. Here, we briefly review the sustainable development goals (SDGs) (Sachs 2015) and discuss the synergy and trade-off between these goals. The SDGs consists of 17 goals and 169 targets. Some targets are common to different goals. Le Blanc expressed the SDGs as a bipartite network between the 17 goals and 169 targets (Le Blanc 2015). Contracting the bipartite network, we obtain a network that only consists of targets. The links of the obtained network are weighted, even though the links of the original bipartite network are binary. Many high degree nodes are located at the center of the network, e.g., SDG 1 “poverty”, SDG 10 “inequality”, SDG 12 “sustainable consumption and production”, and SDG 8 “growth and employment”. The synergy between SDGs is identified as pairs of nodes connected by large weight links, e.g., SDG 1 “poverty” and SDG 10 “inequality”, SDG 5 “gender” and SDG 4 “education”, and SDG 16 “peaceful and inclusive” and SDG 10 “inequality”. Conversely, trade-off between SDGs is identified as pairs of nodes without link, e.g., SDG 7 “energy” and SDG 13 “climate change”. Here, we point out the need for a concrete example studying the trade-off between energy goals and climate change. In this paper, we consider a power grid with high rate of renewable energy in SIDS economy, and discuss nexus of energy, environment, and economic growth. The goal of this paper is to estimate the electricity price for a power grid with high rate of renewable energy for an SIDS economy and to discuss how the estimated electricity price affects the economic growth. This paper is organized as follows. In Sect. 2, a model of grid integration of renewable energy is described. In Sect. 3, the detail of our analysis is explained. In Sect. 4, various results are shown and discussion on nexus of energy, environment, and economic growth for a SIDS economy is given. Section 5 summarizes the paper.",5
17.0,1.0,Evolutionary and Institutional Economics Review,21 November 2019,https://link.springer.com/article/10.1007/s40844-019-00153-1,Coevolution of institutions and residents toward sustainable glocal development: a case study on the Kuni Umi solar power project on Awaji Island,January 2020,Natsuka Tokumaru,,,Unknown,Unknown,Unknown,Unknown,,
17.0,1.0,Evolutionary and Institutional Economics Review,06 November 2019,https://link.springer.com/article/10.1007/s40844-019-00147-z,Location-sector analysis of international profit shifting on a multilayer ownership-tax network,January 2020,Tembo Nakamoto,Odile Rouhban,Yuichi Ikeda,Unknown,Female,Male,Mix,,
17.0,1.0,Evolutionary and Institutional Economics Review,18 December 2019,https://link.springer.com/article/10.1007/s40844-019-00154-0,"The shift of food value through food banks: a case study in Kyoto, Japan",January 2020,Ayaka Nomura,,,Female,Unknown,Unknown,Female,"It is commonly believed that the upstream problem of food waste can be significantly reduced by donating excess or unwanted food to needy people. The act of food charity is meant to not only solve an environmental problem (waste) but also a social problem (food poverty). These goals align well with the 2030 Agenda, specifically Sustainable Development Goal 12.3.1 concerning consumption and production patterns, which aims to halve food waste at retail and consumer levels by 2030 (FAO 2017). However, the assumption that food which is donated will necessarily be diverted from waste and be appreciated by targeted groups relies on an optimistic view of food as a universal nutritional commodity. As the history of food aid has shown at the worldwide and national levels, needy people are not able or willing to utilize donated food for a variety of logistical, cultural, and idiosyncratic reasons (Clapp 2012; Douglas et al. 2015; Friedmann 1982). This history suggests that the act of donating food should not intrinsically be considered altruistic or benevolent unless care and effort are given to ensuring that the food is finally utilized. At the national level, food banks are often recognized as brokers of such transactions, helping to match food donations with needy recipients. However, fundamental misalignment between the supply of donations and the demand (or capacity to utilize) by targeted groups cannot be “solved” by such brokers. Using the case of Japan, this paper argues that the burden of achieving social and environmental goals associated with reducing food waste—namely, alleviating food poverty with food donations—is disproportionately borne by final recipients, who are expected to be thankful for charity even as they are inconvenienced by mismatched food donations. Food banks, as an intermediate space for managing food donations, attempt to mitigate this burden for final recipients but are also obliged to receive mismatched or skewed food deliveries from donors who are less sensitized to the challenges of utilizing these foods. The presence of food banks has recently become a norm in Japan, although the concept of food banks was imported from the United States. The initial impetus for food banks in Japan was not chiefly to reduce food waste but was in reaction to the increasing poverty in the country. According to the Japanese Ministry of Agriculture, Forestry and Fisheries (MAFF), there are 77 food banks currently in operation (MAFF 2016). The biggest food bank, Second Harvest Japan (2HJ), started in 2002 and has continuously expanded its range of activities. In line with the expanding scope of food bank activities, the term “food waste” has received more attention in recent years. As the 2HJ slogan states, “transform mottainai (the Japanese word describes things too good to be wasted in this case) into thank you!” (“History”).Footnote 1 In this slogan, it is apparent that food donations are not inherently beneficial, but must be actively repurposed, after which recipients are obliged to feel thankful for the charity. Here, already exists the presumption that the transformed food will indisputably be recognized as usable by final recipients. Often, however, food banks transfer the food to a further intermediary, or so-called recipient organization, which will sort and transform the food once again before providing it to final recipients, such as low-income families and the elderly. A diagram of the exchange of donated food is visualized in Fig. 1. In general, it is important to recognize that donations exchange hands numerous times before they are potentially consumed; at each link in the chain of custody, effort must be made to transform donations, a process which usually entails disposal of some unsuitable food. In short, before donations can be consumed by final recipients, labor and waste are invariably involved. Food banks and the recipient organizations What is also notable from Fig. 1 is the degree of separation between upstream actors, such as donors and food banks, and final recipients. The gulf between these actors can lead to logistical and social challenges in effectively distributing food donations (Douglas et al. 2015), which raises a fundamental question concerning the realistic reductions in food waste achievable through such networks. Indeed, while a general understanding of some of the final recipients’ needs is common, upstream actors are not necessarily aware of how, or if, final recipients use the donated food. A recent report on food banks by MAFF (2016) that traces the amount of food donations handled by each food bank does not consider how much food the final recipients actually utilized. Given this uncertainty, should food donations be considered benevolent even though it is unknown how much of the food will finally be consumed or wasted? This is important, as donors may receive tax benefits or benefit from reduced waste disposal costs even though a portion of the donation is likely to become food waste at a later stage. Unfortunately, there is considerable uncertainty about the portion of any donation that is eventually wasted, because it is hard to track waste along the entire chain of custody. However, what does become visible through research, and is presented in this paper, are the characteristics of donated food that impact its potential usability. These characteristics include: The quality of food: flavor, freshness, damage The balance of food types: is there a suitable range or is there too much of one food? The skill required to prepare the food: do final recipients have the capacity (skill, time, desire) to cook the donated food? The context of the food reception: is the environment to receive or consume donated food socially and culturally acceptable? The timing of the food receipt: can donated food be integrated into the lifestyles of the final recipients? In summary, the complexity of the food donation delivery network and the characteristics of the donated food impact the potential for achieving food waste reductions. This paper explores the divergence between the simplistic understand of food donation as gifts and the more complicated experiences of burden and relief that arise along the uncertain pathway to becoming utilized by final recipients. By becoming aware of this process, I aim to answer an important policy-oriented research question: how can improved transparency of the experiences of food donation and consumption help society to more efficiently reduce food waste and increase utilization of donations? This paper follows with a background of food donations in Japan, a review of important academic literature on food waste, and a description of the research methodology. The subsequent empirical section of the paper presents ethnographic, questionnaire and food diary data collected through observation of food banks, food bank recipient organizations, and final recipients. The results are categorized by theme, demonstrating the various channels of utilization and waste along the chain of custody, and how food donations experienced by each actor. The final discussion section explores the differences in understanding of food donations as a gift or burden, and concludes about the potential for heightened awareness of the food donation experience to increase food waste reduction efficiency.",3
17.0,1.0,Evolutionary and Institutional Economics Review,23 October 2019,https://link.springer.com/article/10.1007/s40844-019-00138-0,"An interdisciplinary study: disseminating information on dengue prevention and control in the world-famous travel destination, Bali, Indonesia",January 2020,Minako Jen Yoshikawa,Rita Kusriastuti,Christina Liew,Female,Female,Female,Female,"The increasing media attention on the emergence of infectious diseases in the recent decade, has heightened the concern regarding travel destinations. The media began intensively covering outbreaks of Zika virus (ZIKV) infection in Brazil in February 2016, highlighting association of the virus with microcephaly, as well as the challenging efforts in controlling the Aedes mosquito vector. A prominent publication advocating cancellation of the Summer Olympic Games in Rio de Janeiro (Attaran 2016) highlighted the fact that the emergence of infectious diseases in travel destinations could often result in cancellation of travel plans and events of mass-gathering. An endemic status or prolonged transmission of infectious diseases can also impose potential economic loss to travel destinations. Higher risk of malaria with weaker health infrastructure at Lombok, Indonesia, was identified as the component hindering investment and tourist choices of destination (Kinzer 2010). Deteriorating profitability of businesses could result in unemployment, which might give rise to social issues. Both emergence and prolonged transmission of infectious diseases could threaten health and life, of not only travellers but also local people. It is thus essential for travel destinations to prepare for, and respond to, threats of infectious diseases. Reduction in such health risks for the well-being of travellers and locals should be of interest to the travel and tourism industry, in addition to local governments in travel destinations. Dengue virus (DENV) infection is the most rapidly spreading mosquito-borne infectious disease in the world today. The disease is endemic in many popular travel destinations in tropical and subtropical countries (Wilder-Smith and Schwartz 2012), including Hawaii, the USA (Effler et al. 2005; Johnston 2016) and Bali, Indonesia (Yoshikawa and Kusriastuti 2013). The influence of cruise tourism on the incidence of DENV infection in Aruba, in the Caribbean, has also been noted (Oduber et al. 2014). Travel is an accounting factor for the transboundary spread of infectious diseases (Rigau-Pérez et al. 1997), whilst risk of DENV infection to travellers to Southeast Asia is cautioned (Jelinek et al. 1997). The female Aedes (Stegomiya) mosquitoes, particularly Aedes aegypti (Linnaeus) and Aedes albopictus (Skuse), mediate the spread of DENV infection, for which a specific treatment is unavailable. The vaccine launched in 2015 is not widely propagated, in part due to its ineffectiveness against the DENV-2 serotype. Other vaccines are still under development. This leaves only a couple of options, including reducing mosquito vector populations, and promoting behavioural changes for people to prevent themselves from being bitten by mosquitoes. Controlling the Aedes mosquito is very important, since Chikungunya virus (CHIKV) and ZIKV are also mediated by the same vector. Locations reporting local DENV infection might well plan and implement prevention and control strategies against CHIKV and ZIKV infections. For example, Singapore—an urban tropical travel destination—started preparation for the Zika threat as early as September 2013, when an outbreak occurred in the Pacific Islands. Local outbreaks of ZIKV infection, which subsequently surfaced in Singapore in August 2016, were contained in about just 2 months. The well-established national dengue control programme was expanded to respond to the Zika outbreak, and included intensified mosquito control measures and multi-lingual public communication and outreach efforts (Tan and Tang 2016). DENV infection causes a spectrum of illness, ranging from asymptomatic or mild febrile dengue fever (DF), to severe and occasionally fatal dengue haemorrhagic fever (DHF) and dengue shock syndrome (DSS). The clinical symptoms of CHIKV infection include severe and sometimes prolonged incapacitating arthralgia. ZIKV infection tends to cause asymptomatic to mild disease, but association with severe symptoms such as Guillain-Barré Syndrome (Cao-Lormeau et al. 2016) and microcephaly, has been noted (Pan American Health Organization 2015). The social burden of diseases can be heavy if a large portion of the community or a labour force misses work. Key family members may become unable to provide care for children and the elderly (Ng and Hapuarachchi 2010; Yoshikawa and Kusriastuti 2013). Lying east of Java, the main island of Bali is located in the Bali Province of the Republic of Indonesia. The province is about 5780 km2 in area, and is situated south of the Equator in a tropical climate zone. Bali Province comprises the capital city, Denpasar, and eight districts (Fig. 1). These are divided into 57 sub-districts, which are further divided into 716 official administrative units of villages (as of 2015). In addition to the beaches, the fertile soil in Bali, supporting rice cultivation on broad and terraced slopes, boast breath-taking beauty. The peaceful, vibrant and rich culture of Bali is well-known amongst travellers, and the unique blend of Hindu and other religions is just another enticing charm of the resort island. Map of Bali Province, including two particularly tourist-concentrated areas, Capital City of Denpasar and Badung District The revenue generating travel and tourism industry in Bali attracted the attention of the central government, who have notably developed and promoted this ‘island of gods’ since the 1960s. These efforts were followed by a national tourism policy and series of development plans, which won the support of international organisations, such as the United Nations Development Programme (Lewis and Lewis 2009). Today, Bali is ‘the tourism capital of Indonesia’, and tourism is ‘the engine of the economy of Bali’ (Antara and Sri Sumarniasih 2017). The researchers note that tourism in Bali generates sources of foreign exchange and taxes; provides a potential market for goods and services produced by the locals; increases community income; creates job opportunities; and supports the activities of artists. Whilst there are no statistical data reporting about tourism in any one category, data combining ‘trade, hotel and restaurant’ are available. Contribution by the combined sector to Bali’s GDP showed an average of 29.1% between 2010 and 2014. In 2014, the combined sector represented 31.35% of the provincial GDP, followed by agriculture (14.64%), transportation and communication (14.22%) and services (13.23%). The percentage of the combined sector would increase if relevant items from transportation and communication, as well as services, are included. However, the social implication of tourism in Bali is rather controversial. Large tourism development works in the 1990s were seen as costs to the environment and culture in Bali (Warren 2007). Environmental stress was attributed to the uncontrollable construction works of tourist facilities (Kinzer 2010). The list of stress includes: forest destruction, land degradation, air pollution, seawater pollution, river pollution, rubbish and waste issues, coral reef damage, and mangrove degradation (Lewis and Lewis 2009). Indeed, the development in Bali surpassed the preservation of locally specific characteristics attracting tourists (Kotler et al. 2003). Furthermore, the ‘commodification of lands in Bali’ resulting from conversion of agricultural fields to tourism areas, was an issue (MacRae 2003). Many Balinese suffered from the losses of lands that were literally taken away by force for the sake of tourism development (Reuter 2003). Thus, some hotels in Bali are not on good terms with the neighbouring community, even today. To make the situation worse for the locals, about 85% of US$16 billion in tourism assets actually belonged to investors from outside Bali, which has invited repeated criticism of local media (Warren 2009). The luxurious hotels sector attracted waves of investment from Asian conglomerates from Jakarta, and other capital from international hotel chains (Kinzer 2010). Occasionally, the travel and tourism industry in Bali experienced setbacks. Tourist arrivals and revenue dropped in so-called ‘the last paradise on earth’, after the devastating bomb explosion in Legian Street on 12 October 2002. There were over 202 deaths, including: 88 Australians, British, American, Japanese, Brazilian, German, and French nationals (Berger 2013; Henderson 2003; Lewis and Lewis 2009). This incidence was soon followed by the global outbreak of SARS and the Iraq War. As people reduced or postponed travel, tourists immediately abandoned Bali, and the recession in the tourist sector continued in 2003 (Schulte-Nordhold 2007). The tragedy was repeated in Kuta Square and Jimbaran Bay on 1 October 2005, when another terrorist attack shook confidence in Bali’s safety and security. Consequently, 40% of the population experienced a reduction in wages, whilst thousands in the travel and tourism industry were laid off (Schulte-Nordhold 2007), leading to the ‘post-bombing downturn’ (Vickers 2012). However, the industry demonstrated resilient recovery, due in part to the promotion of international tourism marketing and the efforts to improve safety and security in response to terrorism threats (Carlsen and Hughes 2010). Additionally, the new visa policy was implemented (Thirumaran 2009), which provided travellers with the flexibility to obtain a visa upon arrival. The total number of foreign tourist arrivals to Bali in 2015 was 4,001,835. Table 1 shows figures with a breakdown of the top five important markets in Bali, for the period from 2009 to 2013: Australian, Chinese, Japanese, Malaysian and Singaporean nationalities. The domestic tourist arrivals also showed strong and promising growth: 6,394,307 in 2014 and 7,147,100 in 2015, far exceeding the total provincial population, which reached 4,104,900 in 2014. As one of the most popular tropical travel destinations in the world, Bali experiences outbreaks of DENV as well as CHIKV infections (Yoshikawa and Kusriastuti 2013). All 34 provinces of Indonesia report severe forms of DENV infection as DHF, which requires hospitalisation. The national disease trend has generally been on the rise since the 1990s; a record outbreak occurred in Bali in 2010, with 11,697 reported DHF cases (Fig. 2), including 34 fatalities. Bali has been known as one of five provinces reporting the highest incidence of DHF in Indonesia, and the incidence rate in 2013 was 168.5 cases per 100,000 people. DENV infection is indeed endemic in Bali. The reported fatalities in 2013, 2014 and 2015 were 5, 17 and 28, respectively (Ministry of Health, Indonesia 2011–2016). Source of data: Ministry of Health, Indonesia Reported DHF cases in Indonesia and Bali Province from years 1991 to 2015. Details such as monthly reported DHF cases and those classified by district levels, are compiled by district and provincial health offices. The Ministry of Health collects data from each province on a monthly basis, and publishes the total annual reported cases per province. Denpasar and Badung District accounted for the majority of the reported cases in Bali consistently for years (Yoshikawa and Kusriastuti 2013). They are the tourist-concentrated areas in Bali. The famous Sanur is in Denpasar, whereas Jimbaran, Kuta, Legian, Nusa Dua and Seminyak are in Badung. Tourists usually move within the island either by car or motorcycle, in the absence of public transportation. The popular tourist-concentrated areas attract local labour force too. Job-seeking migrants from both within and outside the island have been relocating to the tourist spots in Bali, in the hope of benefiting more directly from the economic growth of tourism. The Balinese workers and their families travel frequently between their original communities and their current residences near tourist spots, to fulfil ritual and community obligations of their home villages. The increasing number of tourists and the local population, as well as the inter- and intra-island movements of the people into popular tourist spots, could bring more people in contact with the vector mosquitoes, and hence the viruses (Yoshikawa and Kusriastuti 2013). It is therefore important to control the disease-mediating mosquito populations. However, there is a possible adverse impact of decentralisation on the public health response to infectious diseases in Indonesia, to which Bali is no exception. There is the Provincial Health Office located in Denpasar, which reports to the Governor. The health office in the capital city, Denpasar, and all district health offices in eight districts, report to each respective mayor. Under circumstances where it is challenging for the fragmented health authority to carry out coordinated and sustainable operations for dengue prevention and control, the respective health offices of Denpasar and Badung have mobilised an NGO workforce, locally known as Jumantik (larvae inspectors). These workers visit residential areas regularly to check for mosquito breeding, and occasionally provide public health education to the local residents (Yoshikawa and Kusriastuti 2013). In contrast, other private areas in Bali, namely resort hotels, and hundreds and thousands of international travellers, remain out-of-reach in Bali, since information dissemination on dengue by the government is given in the Indonesian language. Frequent dengue incidence amongst international travellers could promote DENV transmission, which could increase exposure of the local people to the virus. It is therefore important to reach out to international travellers, who tend to arrive unaware of health threats due to mosquito bites, and with insufficient knowledge of local disease patterns, high-risk areas of current outbreaks of infectious diseases, and personal protective measures to prevent mosquito bites. The insufficient knowledge and action amongst international travellers might partly explain why other countries such as Italy (Rovida et al. 2011; Zavattoni et al. 2016), Japan (Fukusumi et al. 2016; Yoshikawa 2011) and Australia (Ernst et al. 2015; Warrilow et al. 2012) continue to report dengue cases amongst travellers returning from Bali. The questions remain: where can health education for international travellers take place? What can be done to improve mosquito control at tourist spots? More critically, who can assume a role in reaching out to international travellers, and strengthening mosquito control at tourist concentrated areas? There are complicated logistical issues to overcome at the major point of entry for international travellers, the Ngurah Rai International Airport in Denpasar, in providing information on DENV infection. Individual shopkeepers and restaurant workers usually only have brief encounters with international travellers. In contrast, hotel personnel have personal interaction with travellers, daily and frequently. It is therefore worth studying the readiness of hotel personnel in taking the initiative to educate international travellers on dengue prevention and control, in addition to ensuring that anti-mosquito measures are carried out at the hotels. Although previous research efforts have paid considerable attention to the public health intervention of DENV infection, often carried out by governments, many less studies have been conducted on the scope of a more socially-oriented approach to improve dengue prevention and control at travel destinations. Control measures practised at tourist accommodation facilities have been insufficiently documented, including in Bali. Traversing the disciplines of Travel Medicine, Entomology and Business Administration, the ultimate purpose of this study was to advocate the proactive role of hotels in Bali, in providing information on dengue prevention and control to international travellers. To empower the hotel sector, we found it indispensable to first investigate the knowledge on dengue of the hotel sector, as well as identify gaps in mosquito control measures implemented at hotels. Since promoting population health is essential to achieve development (Yoshikawa and Surjan 2016), we aimed to improve public health to benefit both travellers and locals in Bali. Such an initiative might assist Bali’s efforts in advancing ‘Good Health and Well-Being’, one of the Sustainable Development Goals. A partnership was formed to launch our study. It included the Bali Provincial Health Office, Denpasar Health Office, Badung Health Office, Bali Hotels Association (BHA), and the authors: an academic researcher from Kyoto University, Japan; the Former Director of Vector-Borne Disease Control at the Ministry of Health, Indonesia; and an Expert Medical Entomologist from the National Environment Agency (NEA), Singapore. In this paper, we present the results of a questionnaire, as well as findings obtained from site visits to several accommodation facilities. We also present previously unpublished details of reported DHF cases from Bali and two other areas in Indonesia. A suggestion is made for the hotel sector in Bali to contribute to neighbouring communities in addition to international travellers, through effective and coordinated mosquito control operations. We conclude that more information and scientific evidence on dengue prevention and control should be provided to empower the hotel sector.",3
17.0,2.0,Evolutionary and Institutional Economics Review,24 January 2020,https://link.springer.com/article/10.1007/s40844-020-00159-0,Financialization in Japan,July 2020,Shigeyuki Hattori,,,Male,Unknown,Unknown,Male,"Since the 1980s, neoliberal reforms have deregulated the labor and financial markets and cut welfare programs, resulting in the collapse of Keynesian economics, the welfare state, and managerial capitalism. Along with neoliberal reforms, financialization has occurred worldwide. According to Epstein (2005, p. 3), “financialization means the increasing role of financial motives, financial markets, financial actors and financial institutions in the operations of the domestic and international economies.” Some suggest that financialization is about the ascendancy of shareholder value, whereas others use the term in reference to the increased economic power of the rentier class or financial institutions. Still others argue that “financialization” refers to the growth of a market-based financial system. However, as Lapavitsas (2013, p. 799) argues, financialization also “represents a historically specific transformation of capitalist economy.” A peculiarity of financialization today is that finance has been decoupled from production; so, the excessive accumulation of reserves has no relation to productive investment. Indeed, corporate governance motivated by shareholder value is seen to weaken production. Minsky (1989) was one of the first economists to notice financialization, and its transformative effect on capitalism. He referred to this new form of capitalism as “money manager capitalism;” however, he did not consider money manager capitalism as self-sustaining, as it is contingent on speculation. When a market crashes, the Federal Reserve should react by enacting monetary easing (Minsky 1989, pp. 394–395). Minsky also did not consider money manager capitalism as desirable; in fact, he thought that it encouraged long-term wage stagnation in the United States (Minsky and Whalen 1996–1997, p. 156). Japan has also undergone neoliberal reforms. Since the 1990s, while battling a protracted period of economic stagnation, Japan has introduced American (or Anglo-Saxon) types of economic and financial reforms, including Hashimoto’s financial “Big Bang,” Koizumi’s structural reform, and Abe’s “third arrow” (i.e., growth strategy). While it is true that these reforms have changed the Japanese economy, they have failed to give rise to a market-based financial system. This has happened despite the call to move “from savings to investments” and big business accumulating reserves that are not being invested. Because shareholder value is given precedence, so paying out dividends is chosen over reinvestment. The Japanese experience is important, because reforms in Japan based on neoliberalism or the American model have given rise to something quite different from what has been seen in the United States. In Sect. 2, we show that the Japanese economy is not a full-fledged finance-led economy. However, as Sect. 3 shows, Japan has been approaching shareholder capitalism, at least partially. Section 4 takes up the peculiarities of the Japanese path. After all, neoliberal reforms in Japan have failed to vitalize the financial sector, as they did in the United States. In Sect. 5, we consider possible turning points under Abenomics. Despite the yen’s depreciation, the outsourcing by multinational corporations has obstructed the recovery in exports. Stagnation in productivity growth has, ironically, increased employment. Overall, it seems likely that the Japanese export-led model has had its day. Section 6 presents concluding remarks.",1
17.0,2.0,Evolutionary and Institutional Economics Review,19 August 2020,https://link.springer.com/article/10.1007/s40844-020-00185-y,Special issue “New Possibility of Cryptocurrencies and Digital-Community Currencies”,July 2020,Makoto Nishibe,,,,Unknown,Unknown,Mix,,
17.0,2.0,Evolutionary and Institutional Economics Review,06 July 2020,https://link.springer.com/article/10.1007/s40844-020-00177-y,The present and future of digital-community currencies: RAMICS 2019 in Hida–Takayama keynote speech,July 2020,Makoto Nishibe,,,,Unknown,Unknown,Mix,,
17.0,2.0,Evolutionary and Institutional Economics Review,30 June 2020,https://link.springer.com/article/10.1007/s40844-020-00179-w,The community currency game “Online Shopping.com”: the prisoner’s dilemma and consumer behavior in a local economy,July 2020,Masaaki Abe,Hitoshi Utsunomiya,Miyoshi Hirano,Male,Male,Unknown,Male,"The decline of local economies has become more serious in recent years. Increasing regional disparities are caused by globalization. According to the report “Reword Work, not Wealth” by Oxfam, an international NGO, “Last year saw the biggest increase in billionaires in history, one more every 2 days. This huge increase could have ended global extreme poverty seven times over. 82% of all wealth created in the last year went to the top 1%, and nothing went to the bottom 50%” (Oxfam International 2018). According to Nishibe (2018), the characteristics of modern capitalism can be expressed by two concepts: globalization and deindustrialization. In developed countries, economies have suffered from the decline of local economies and the collapse of local communities due to the deindustrialization of the economy and the aging population. Therefore, the main purpose of introducing a community currency (CC) is to revitalize local economies and communities. However, in developing countries where the local communities are strong and economic development and poverty resolution are priorities, microcredits and community currency are dominant. As described above, the purpose of using a community currency varies depending on the local situation. In this paper, we will consider community currencies as applied in developed countries. The reason for the widening regional disparities is the drain of people, goods, and money from local economies. Community currencies are considered to be a possible solution to the problem of regional disparities. However, it is rare for a CC to succeed in local revitalization. The “community currency game” (CCG) aims to elucidate the effects of local revitalization through experimental trading using a CC. In this paper, we present a CCG that aims to improve understanding of the mechanism by which the prisoner’s dilemma affects the behavior of consumers in a local economy. Recently, various mail order businesses have developed thanks to Internet services. Internet trading has competitive advantages in terms of price and the large selection of goods. Therefore, purchases from online stores are rational behaviors for local residents. However, these individual rational behaviors are irrational for the economy in the local area because they cause the outflow of money. We need to change our purchase behaviors to resolve the regional disparities and revitalize local areas. The CCG “Online Shopping.com” consists of three mini games. The first is the “silent trading game”, the second is the “conversational trading game”, and the third is the “conversational trading game with community currency”. The main rules of the games are as follows. There are six types of store in a town (toy store, bookstore, supermarket, etc.), and each participant is assigned to one. The participants throw two dice to determine their trade in goods. When the participants buy goods, they must choose either to shop in their town or online (outside of the town). The prices of goods inside the town are higher than in online stores. In the “silent trading game”, each participant trades three goods with no conversation. That is, each participant throws dice three times and fills in the trading record sheets. This game is very simple and there is no need for communication. Therefore, most participants choose online shopping to save money. We obtained the expected result from the game. In the “conversational trading game”, participants trade face to face using legal tender cards and goods cards. They can see each other’s trades. If one is about to buy from an online store, the rival store in the town can call them. Therefore, the ratio of purchases in the town may increase a little. However, most participants still chose online shopping. In the “conversational trading game with community currency”, each participant can use a CC card in addition to legal tender cards. We found that most participants came to choose the town shop and the gross domestic product (GDP) of the town (total amount purchased in the town) increased. Through the three games, we discovered that the CC had the power to allow the participants to escape from the prisoner’s dilemma.",2
17.0,2.0,Evolutionary and Institutional Economics Review,02 July 2020,https://link.springer.com/article/10.1007/s40844-020-00178-x,The effectiveness of distributed ledger technology to replicate the entropic behavior of nature,July 2020,Federico José Camargo,,,Male,Unknown,Unknown,Male,"Humanity is facing a historic technological moment, as its latest innovations have the capacity to adapt the monetary system to the ecological needs of the twenty-first century. In this sense, adapting the monetary system to the objective functioning of nature could make money an instrument of common good (Francisco 2013). The ecological economy has a leading role for sustainability, as this branch aims to avoid the usual dissociation between economic and ecological approaches reconciling in the same root eco the usefulness advocated by those and the stability analyzed by these (Naredo 2011). It has a broad, ecological, interdisciplinary and holistic vision, mainly formed by economists, biologists and physicists, and emerged in the early 1970s in response to the problem of the environmental crisis as a result of human activities and with the objective of building a broader theoretical framework than the hegemonic neoclassical-environmental economy has (Corrons Giménez 2015). Therefore, ecological economy denotes that the theoretical perception of production and consumption processes and their limits cannot be outside the laws that govern the operation of the biosphere itself (Carpintero Redondo 2005). Indeed, the physical livelihood of the ecological economy lies in the natural law of entropy that affects energy and matter. For this reason, ecological economics is the science of sustainability management, because it integrates the logic of economic rationality with the logic of ecological rationality (Pengue 2008). Economics is not limited to the productive system, as it is also composed of the monetary system. Entropy in the productive system is intrinsic, however the monetary system does not respect this natural objective law. The effective application of an Entropic Monetary Rate aims to ensure that money reflects the behavior of nature, to encourage product-money-environment relationships that are more consistent with human relationships and the environment. For the purposes of this document, the following section briefly explains when the Entrophic Monetary Rate is applied to analyze monetary technologies. Because the proposed rate is a monetary innovation that the author is developing in his doctoral thesis, some preliminary details can be found in Camargo (2019a, b). Regarding the other axis of work, technology; it should be noted that BitcoinFootnote 2 led to the birth of cryptoeconomicsFootnote 3,Footnote 4 but this currency is not relevant so much for its monetary advances,Footnote 5 but for the disruptive technological innovation that supports it. Its technology is a decentralized computer system that processes, records and further protects the storage and communication of sensitive information (Broadbent 2016). Bitcoin is processed using Blockchain Technology, which is a type of Distributed Ledger Technology (DLT) that, through the combination of cryptography and game theory, directly and securely executes transactions between unknown agents without the need for intermediaries to provide trust.Footnote 6 This technology even allows people to agree on the status of their affairs and to be able to register that agreement in a secure and verifiable manner. In essence, Distributed Ledger Technology (DLT) is a database that is shared over a network and allows users – who do not necessarily trust each other – to share responsibility for their administration without a central validation authority (Seretakis 2017). For the first time there is technology capable of unifying various information systems supported by the monetary instrument. The underlying benefits of technology and the unification of systems are still unknown because the exploitation, analysis and research of DLT in various areas are in its infancy. Therefore, the objective of this work is to analyze the efficiency of the DLT to replicate the basis of the functioning of the natural system to the monetary system to move towards sustainability. This paper identifies the efficiency of paper money, digital money and DLT for the execution and application of the Entropic Monetary Rate. It also identifies the efficiencies described for the Silvio Gesell Oxidation Rate, because, intrinsically it is part of the Entropic Monetary Rate. To do this, firstly, the support for the Entropic Monetary Rate is briefly explained to understand its implications and its relationship with the Oxidation Rate. Second, the characteristics of the technologies considered are presented to identify the premises of the analysis. Finally, the efficiencies of technologies to replicate the entropic behavior of nature are compared.",1
17.0,2.0,Evolutionary and Institutional Economics Review,18 July 2020,https://link.springer.com/article/10.1007/s40844-020-00183-0,The evolution of the exchange process: from the decentralized to the distributed digital exchange,July 2020,Yuji Aruka,,,Male,Unknown,Unknown,Male,"The classical image of market exchange has been drastically changed recently. Mirowski (2007) described that a market is repeatedly either spun off or complemented from the underlying market forming a highly complicated layered system in the evolution of market. This may be an essential concept for understanding the modern market system. On the other hand, a series of new exchange mechanisms has appeared. In this article, we focus on these new events to depict the modern market reconstruction. Thus, we first follow the traditional subject of classical exchange with an auctioneer. Before going further into the details, we roughly describe the landscape of exchange systems. Here, we utilize some auxiliary ideas to classify systems dealing with of market exchanges. These system are constructed and construed by Artificial Intelligence (AI). The remarkable operation of AI will be revealed by the speed of coordinations and transactions: fast vs slow. Its other characteristics will be divided by the ways of processing: non-symbolic vs symbolic. Non-symbolic way may reflect the way of fast thinking, while symbolic way may reflect slow thinking, as Kahneman (2011) pointed out. From both of these standpoints, thus, there are several footholds to support the structure of the exchange systems. Among financial technologies, HFT is a striking factor. The operation speed is on the order of microseconds, preventing much working from being performed by human intelligence. In fact, the average processing speed of Bitcoin is about 10 min, leading us to excessively depend on computing ability such as “proof of work”. This characteristics is depicted on the vertical axis, from fast processing to slow. The horizontal axis shows the type of processing, from less symbolic to more symbolic. Symbolic processing implies the so-called programming based one. Through the recent rapid progress of pattern recognitions due to machine and deep learning, nonsymbolic processing will begin a new stage of AI work to replace previous human works. Thus, we summarize the rough profile of AI usage of the modern society as Fig. 1. It should be noted that our society currently relies on AI in the above multilayered ways. We will return to Fig. 1 again at the end of Sect. 4. . The landscape of exchange systems We first take the case of auctions with auctioneers to observe how to exchange either a unit commodity or certain units. Here, we leave aside the realistic broader exchange systems to consider a particular smaller exchange such as “auction”. It should be noted that the theories of auction merely refer to a limited domain of specialities on goods, estates, and fine arts. The results, as it is, will never provide a general understanding of the market which can contribute to the capitalistic production. The reader may be reminded of “internet auction” and so on. In this type of setting, the auctioneer or the organizer is assumed in advance. In general, there are two types of auctions: sellers’ and buyers’ auctions. We then call the exchange system a financial market when any agent either sells or buys. Typically, when we use the term auction, it indicates a complete separation between seller and buyer: the seller only asks and the buyer only bids.Footnote 1 In this sense, the exchange environment of auction is a special case for a market exchange in general. Among classical auctions, it is most common to see seller’s auction whose organizer is the seller. Usually, the object will be of a unit or a few units of a good. The latter type is called a multi-unit auction. Usually, the true value of the good is not objectively established, that is, no one knows the true value of it. Otherwise, we do not need to hold auction to collect information. In fact, auction discussions have been popular in game theory and experimental economics. In many cases of auctions, the sellers selects the buyers. Conversely, there are also auctions such as reverse auctions in which the buyer selects the sellers. Let us first take up the discussion of auctions where sellers choose buyers. Currently, mainstream school are also conscious of big data. Bodoh-Creed et al. (2020) is a paper by the authors at UC Berkeley, University of Chicago, and Harvard University, which was recently accepted in The Review of Economic Studies. Here, an eBay auction is chosen and, in detail, the efficiency of Kindle sales on eBay is simulated. eBay obtains a uniform price equilibrium with a multi-unit auction, but specifically, the framework itself is specially limited, with 2 unit auctions. Their analysis results show that this auction does not always achieve efficiency. Generalized second-price auctions are used by Google and Yahoo advertising programs. Clickthrough rates are important in web advertising. The click rate of an advertisement is the number of clicks divided by the number of views. There are n bidders and they choose k (\(n> k\)) different slots. According to the “slot allocation rule” , slots with higher click probabilities are allocated in descending order of bid prices. The bidder with the highest bid is assigned the slot with the highest click rate, and the next bidder is assigned the slot with the next click rate. However, the highest bidder gets the slot at the price of the runner-up bidder, and the runner-up bidder gets the slot at the price of the third highest bidder. The probability that each slot is clicked is \(\alpha _i> 0\). \(n-k\) slots are dummies \(\alpha _m = 0 \; (k + 1 < m \le n)\). The bidder’s utility is \(u_i = \alpha _i (v_i - p_i)\) when assigned slot i. However, it is easy to see that this rule does not guarantee a Nash equilibrium. Now, it is assumed that there are two slots \(\alpha _1 = 1, \alpha _2 = 0.4\); three bidders with valuations \(v_1 = 6, v_2 = 5, v_3 = 1\); bids \(b_1 =6 , b_2 = 5, b_3 = 1\). From the settings, \(p_1 = b_2 = 5\) and \(p_2 = b_3 = 1\). At this time, That is, Bidder 1 can change the bid price from 5 to 4 to increase utility. Therefore, the bid price of this auction is not a Nash equilibrium. Such auctions are held on the Web. Auctions handled by auction theory are “technically artificial designed” even if they are actually human-operated. These kinds of design create the gap between theory and reality. We will not go into details because this article is not a place to study auction theory, but we present some profiles of the basic auction frameworks. We call the auction fulfilling following conditions Japanese auction:  The initial price is set 0 or the seller’s lowest price. Buyers entering the auction house indicate their willingness to buy at the price offered. Buyers can leave at any time but are not allowed to re-enter. The offered price is continuously raised, for example, for 1 yen each time. When there is only one buyer left, the auction stops and the buyer buys at the offer price. Japanese auction has some advantages over Vickrey auction. As the clock progresses, the mechanism is that the buyer indicates at each point whether he or she wishes to remain or leave. Therefore, privacy is preserved, because there is no need to express anything other than the second highest price. Furthermore, even if there is a person who follows the behavior of the buyer, the behaviour of the buyer is not ultimately restricted. On the other hand, since Vickrey auction is a sealed bid, the bid price of the other party cannot be confirmed, so a bidder must choose the highest price to win. Therefore, a bidder has to make a successful bid for more than the true value of the item. However, with a Japanese auction, the true value of the other party can be confirmed as time passes, so a bidder can decide whether to stay or leave according to the bidder’s true value. In comparison with English auctions, this system has the following advantages: In Japanese auction, prices are continuously bid up like the Walrasian adjustment process in stability discussion, so there is no sudden jump in auction prices as in English auction. Many auctions can be designed in addition to the above in auction theory. The idea of correlation equilibrium may also suggest some sophisticated associations with auction games for further generalizations. However, as shown by the mathematician (Kono 2008a, b), the assumption of first-order independence of strategy sets among agents is indispensable in proving the existence of a correlation equilibrium. In most discussions, we introduce a variable s to represent a signal in addition to the true value v and the bid price b, but it is difficult to judge whether a signal is an accident, noise, or an artifact. Inducible signals cannot rule out the possibility of independence assumptions. We also have some problems with the true value. When the seller discusses it, it may be considered to be the reservation price. When the buyer discusses it, it may be a private, true value by connecting it with the utility function. However, it may be difficult to determine the uniqueness of the decision in the probability space in view of discrete utility function of Saari (1959). These considerations must be taken into account when the term truthfully revealed is employed. Next, we will discuss auctions in which the buyer selects the sellers. This is used in tenders held for government procurement. One of the auction mechanisms adopted by tenders is the reverse auction proposed by Glen Meakem who founded FreeMarkets Inc. in 1995. However, in Japan, this has since been applied to the procurement process between businesses, including cost negotiations between private companies, and has developed as a large-scale mechanism, replacing the aspect from traditional theory with an engineering aspect. First, consider Japanese reverse auction. In this type of auction, the organizer starts by calling the opening price. Here, the participants are the sellers, who decide whether to accept or withdraw the price. When all the participants’ responses to this price are determined, the software that implements the scoring function determines a price level lower than the opening price and announces it to the sellers, and the remaining sellers then decides whether to accept at the new price. This is continued until the sellers no longer bids. Unlike traditional auctions based on buyer selection, the procurement business targets so-called platform goods. In that sense, it is already in a different aspect from the previous auctions. Techniques such as value analysis (VA) and value engineering (VE) are used for scoring. Therefore, the simple classical “discovery principle” as explained in Kreps (1990) (1990, 680–703) and Aruka (1996) (1996, Chapter 1, Exercise 2) is not valid. The latter work discussed the mechanism by which sellers reveal the true cost. In fact, it is not easy for a seller to confess the truth. According to Ayres (2007), bribery cases often occur in public works bidding, and auction organizers and bid companies then collude. In 1992, there was a famous scandal called Magic Number Bidding in the school construction business in New York City is famous. Seven companies have obtained the total winning bids of 23 million due to fraud (Ayres (2007), 44). The details of this bribery were clarified in a judicial transaction, after which a study was conducted to explore the bid price of bribery companies (scammers) by quantitative analysis of these auctions. First, we explain the specifications of the fraud called Magic Number Bidding. The auctioneer organizes a collusion with certain operators, and bribing companies inform the organizer of their reservation price in advance. Since it is a reverse auction, the seller who offers the lowest bid price wins. Since the bid price will be presented by the sellers within a predetermined time, the conspirators will bid the bribing company near the end time if the bribing company’s reserve price is lower than any of the other bids. The organizer should say the bid price of the bribing company is slightly lower than the lowest bid price (true bid price). The false bid price should be higher than the reservation price. Therefore, a study after the elucidation of the case, (Ingraham 2005) found that the difference between the true bid price and the false bid price was extremely small in the bribery auction. It should be noted that if the bribing company’s reserve price was higher than the bid prices of other bidders, the conspirators would not offer a false bid price. In the 1992 scandal, it was important for the conspirators to spoof a fake bid price near the end of the auction. However, (Chandler 2011) proposed a new version of the Magic Number Bidding simulator at the Wolfram Demonstration Project. Magic Number Bidding, of course, is a crime, but the following variations are given. In the classic Magic Number bid, the timing of the false bid was near the end time, but the timing can be chosen to be early. This has the effect of hindering surveillance the timing. Therefore, Chandler (2011) designed the game in which the conspirator is free to choose the timing of the false bid price presentation. On the other hand, there are two ways to determine the false bid price: choosing it to be either near the true minimum price or near the reservation price. The latter option is not risk prone, but rather is risk-averse attitude, and in that case, the method will produce little profit even if a successful bid is made. Theoretically, “Benford’s Law” (Benford 1938) can be used to probe fraud in measuring e-voting, but this does not necessarily eliminate voting fraud. The skills of fraud may be improved. We recall that in 2011, the Democratic Government of Japan at the time adopted the “reverse auction” and was told that only good things would happen with the introduction of this auction. According to Krishna (2002) (2002, 1), the historical origin of auction is stated as follows: In 193 A.D., having killed the Emperor Pertinax, in a bold move the Prætorian Guard proceeded to sell off the entire Roman Empire by means of an auction. ...Auctions have been used since antiquity for the sale of a variety of objects. Herodotus reports that auctions were used in Babylon as early as 500 B. C. To be sure, there are many objects of the auctions, ranging from land to the crude oil reserves. But the value should not be known in advance. No one knew the value of the emperor status. In short, auctions are originally “mechanisms” that economically process things of which the “true value” is not known, so we should not target things whose production costs can be objectively expressed. According to eBay founder Pierre Omidia, the first successful bid was a “broken laser pointer”.Footnote 2 It is difficult for both the exhibitor and the successful bidder to determine the “true value”. If anything, the reservation price is the bid price b. For example, in the reverse auction example, it is possible to classify around the reservation price as indicating risk avoidance and that near the bid price as indicating risk preference. However, the reservation price may be given on the basis of the objective cost. Therefore, the true value is not necessarily connected with the reservation price. Economics tends to believe that even if an economic person is bounded rational and the auction mechanism is pertinently designed, the best choice will be naturally revealed through signals. It has already been mentioned that such beliefs do not always hold. In the reverse auction described in the previous subsection, the situation is completely different in a procurement auction conducted between private companies. Secondary costs are incurred in procurement auctions due to engineering cost estimates such as VA and VE. Revealing or detecting true value is no longer complete by relying on subjective rationality and cost efficiency.",3
17.0,2.0,Evolutionary and Institutional Economics Review,01 July 2020,https://link.springer.com/article/10.1007/s40844-020-00181-2,Stimulate currency circulation in the currency community by creating a customized community,July 2020,Maen Alaraj,Makoto Nishibe,,Unknown,,Unknown,Mix,,
17.0,2.0,Evolutionary and Institutional Economics Review,04 August 2020,https://link.springer.com/article/10.1007/s40844-020-00187-w,Special features on behavioral issues in cryptocurrencies,July 2020,Dehua Shen,,,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Evolutionary and Institutional Economics Review,26 May 2020,https://link.springer.com/article/10.1007/s40844-020-00172-3,"Business conditions, uncertainty shocks and Bitcoin returns",July 2020,Yong Jiang,Gang-Jin Wang,Xiao-guang Yang,,Unknown,Unknown,Mix,,
17.0,2.0,Evolutionary and Institutional Economics Review,04 June 2020,https://link.springer.com/article/10.1007/s40844-020-00176-z,Exploring the short-term momentum effect in the cryptocurrency market,July 2020,Ha Nguyen,Bin Liu,Nirav Y. Parikh,,,Unknown,Mix,,
17.0,2.0,Evolutionary and Institutional Economics Review,29 June 2020,https://link.springer.com/article/10.1007/s40844-020-00182-1,Investor attention and the pricing of cryptocurrency market,July 2020,Wei Zhang,Pengfei Wang,,,Unknown,Unknown,Mix,,
17.0,2.0,Evolutionary and Institutional Economics Review,30 July 2020,https://link.springer.com/article/10.1007/s40844-020-00186-x,Special issue: the 7th international symposium on human survivability “let’s work together toward achieving the sustainable development goals”—part II,July 2020,Yuichi Ikeda,,,Male,Unknown,Unknown,Male,,
17.0,2.0,Evolutionary and Institutional Economics Review,13 February 2020,https://link.springer.com/article/10.1007/s40844-020-00160-7,Integrated spatial and energy planning: a means to reach sustainable development goals,July 2020,Gernot Stoeglehner,,,Male,Unknown,Unknown,Male,"The Sustainable Development Goals (SDGs) as agreed in the 2015 UN Sustainable Development Summit in New York set a global framework for the transition towards sustainable development. The concept is based on 17 goals with 169 targets with a time horizon 2030, as well as a list of 232 indicators to monitor the progress towards achieving the SDGs. The SDG 7 calls for clean and affordable energy. Looking at the specific targets until 2030, (1) universal access to clean, affordable, reliable, sustainable and modern energy services shall be granted to all people, (2) the share of renewable energy shall be substantially increased on a global scale, (3) the global rate of energy efficiency improvements shall be doubled, (4) investments in energy infrastructure and clean energy technology shall be promoted, and (5) energy infrastructure shall be modernized, inter alia, to provide sustainable energy services in developing countries (United Nations n.y.). To fulfill the targets of the Paris Agreement, the use of fossil energy sources should be severely reduced or stopped by 2050 especially in industrialized countries (Intergovernmental Panel on Climate Change 2018). Therefore, the European Union defined a roadmap for the energy transition to reach a reduction in greenhouse gas emissions of around 90% till 2050 (European Commission 2011, 2018). To reach these ambitious climate protection targets, a substantial re-organization not only of the energy systems but also of the human activities leading to energy demand are necessary to reach substantial energy saving. At the moment, most scenarios provided to reach the energy transition are not only based on renewable energy provision, but also on substantial energy savings of households, companies and public entities (see e.g. China National Renewable Energy Centre et al. n.y.; Martinot and McDoom 2000; European Commission 2011, 2018). The possibilities to save energy and to utilize renewable energy sources is dependent on different influencing factors like (a) different policies like agricultural, economic, environmental or housing policies, (b) economic practices of economic sectors or individual companies, (c) individual lifestyles, (d) locally or regionally available renewable resources, (e) availability of technologies, (f) the base values of societies as well as (g) the spatial structures in certain areas (Stoeglehner et al. 2014). To address the interrelations between spatial structures and the possibilities to shape the energy transition, the concept of integrated spatial and energy planning (ISEP) was elaborated in Austria in the last decade. This concept starts to influence policy making on all levels of government—from national to regional and municipal level. The author of this article and his team contributed major research findings to conceptualize ISEP, developed planning methods and planning tools. Furthermore, the author consulted policy makers in shaping spatial planning decisions in support of the energy transition to facilitate bringing scientific knowledge into action in Austria. In this article, these experiences of the last ten years of work are summarized in chapters 2 and 3 based on own published work, so that the ideas of ISEP can be introduced to the community of evolutionary economists, which is the first purpose of this article. Second, the article aims at contextualizing ISEP with the SDGs based on an analysis and interpretation of the interrelations between the guiding principles of ISEP and the SDGs (Chapter 4). The article corresponds to an invited presentation that was held at the 7th international symposium 2018: “Let’s Work Together Toward Achieving Sustainable Development Goals” organized by the Graduate School of Advanced Integrated Studies in Human Survivability (GSAIS) at Kyoto University.",14
17.0,2.0,Evolutionary and Institutional Economics Review,07 March 2020,https://link.springer.com/article/10.1007/s40844-020-00163-4,Cost estimation for alternative aviation plans against potential radiation exposure associated with solar proton events for the airline industry,July 2020,Yosuke A. Yamashiki,Moe Fujita,Kazunari Shibata,Male,Female,Male,Mix,,
17.0,2.0,Evolutionary and Institutional Economics Review,31 March 2020,https://link.springer.com/article/10.1007/s40844-020-00168-z,Correction to: Cost estimation for alternative aviation plans against potential radiation exposure associated with solar proton events for the airline industry,July 2020,Yosuke A. Yamashiki,Moe Fujita,Kazunari Shibata,Male,Female,Male,Mix,,
17.0,2.0,Evolutionary and Institutional Economics Review,08 June 2020,https://link.springer.com/article/10.1007/s40844-020-00175-0,“Work Experience Education” in secondary schools in India: a women’s empowerment perspective,July 2020,Satsuki Shioyama,,,Female,Unknown,Unknown,Female,"Women’s participation in the workplace plays an important role in making society progressive, guiding it towards development. Empowerment allows women to reach their full potential, improve their political and social participation, and believe in their own capabilities (Bhat 2015). Empowerment is a process, as well as the achievement of social change. According to the United Nation Women’s Empowerment Guidelines, women’s empowerment has five components: a sense of self-worth; the right to have and determine choices; the right to access opportunities and resources; the right and power to control their own lives within and outside the home; and the ability to influence the direction of social change to create a more just social and economic order, both nationally and internationally. Mondal (2017) has identified three components of women’s empowerment in the Indian context: to be positively reflected and to make decisions by and for themselves in the family and community; to be able to access decision making, knowledge, and resources, habitats, and creeds; and to understand the value of confidence, human rights, and dignity. In short, these three components of women’s empowerment overlap with the UN women’s empowerment guidelines in highlighting women’s access to resources and job opportunities; their right to make decisions and participate in societal and family activities; and their self-esteem and confidence. Three components can be understood as an interconnected cycle in which resources and job accessibility is the most visible component for us to start to handle by social input or change. As indicated in Fig. 1, a big arrow presents, “access to resources and job opportunity” enhances women’s “decision making and partaking activity”. And as small arrows present third component, “Self-esteem and confidence” impacts women who are trying to access opportunities and decision making, and vice versa, “Self-esteem and confidence” is also the consequence of these two components. Thus, the three components interact with each other to produce a synergetic effect on society both outside and inside the home. These three components are necessary to promote any social changes. Relationship between the three components of Women’s Empowerment This study focuses on India, a country in which patriarchal ideas are still prevalent. In a patriarchal social structure, men play a dominant role and remain dominant in all household affairs. Women are rarely treated well by men. In addition, several factors, based on patriarchal structures, reinforce gender inequalities. These include the family, religion, the media, laws and legal systems, cultural beliefs and practices, education, political systems, and other institutions. Such structures reinforce the idea that women are inferior to men and have a secondary status. (Mathu and Jain 2008). In the traditional Indian caste system, people are divided into four groups, based on their occupations: Brahmins, Kshatriyas, Vaishyas, Shudras, and Dalits. Dalits are the scheduled castes and tribes who live outside mainstream society, due to discrimination and their lower socio-economic status. Although Dalit women play a significant role in their own cultural, religious, social, and economic ways of life, they lag behind when it comes to health, employment, education, social and political positions, and empowerment (Kollapudi et al. 2015). In 2015, the Government of India launched its “Skill India” campaign. This policy aims to promote youth employment rates and human development. As a consequence of globalization and cut-throat competition in the international market, the relationship between vocation and education is changing. Work-centered education is an important way to support industrialization in India, where schools and educational processes play particularly important roles in inculcating and disseminating patriarchal values among future generations. The Indian labor force consists of 24.8% women and 81.6% men (World Economic Forum 2020). This is a huge gender gap, especially given the Indian government’s efforts to promote vocational training. Despite facing severe obstacles, Indian women are aspirational and ready to contribute to the economy (Government of India Ministry of Women & Child Development 2016). These statistics suggest that a certain number of women with the potential to contribute are currently hidden and disempowered. In India, basic education and “Education through manual work and handicrafts” were introduced in 1937. Under this education policy, knowledge was provided to enable individuals to earn a living, an essential requirement for a happy life. It aimed to develop the traits of ideal citizenship in the nation and the society. Since then, India’s education policy and curriculum have attached special importance to work, although they now refer to “Craft, Socially Useful Productive Work, and Work Education” (Sehgal and Singh 2017). The present study uses “Work Experience Education (WEE)” to refer to work-related education. According to the National Curriculum Framework 2005 (NCF 2005), the association between work and education is expected to facilitate disciplinary knowledge, developing values drawn primarily from the constitution. It is expected to lead to social transformation and the formation of multiple skills, which are needed to face the complex challenges posed by a globalized economy. This educational process calls for the application of a critical pedagogy to link the experience of productive and other forms of work with general knowledge. In addition, it aims to reduce socio-economic disparities and realize self-improvement, through practical activities and experiences, which include intellectual, skill-based, and knowledge-based education. These were the noted aims and objectives of this subject: acquiring the knowledge needed to understand the facts and scientific principles involved in various forms of work; developing skills to ensure greater productive efficiency; and developing a good work ethic, incorporating regularity, punctuality, honesty, dedication, and discipline. The importance of women’s empowerment in India has been mentioned in previous studies (Bhat 2015). Although women’s improved access to education is currently achieving this empowerment goal, the quality of that education is poor (Sahoo 2016). Studies of women’s empowerment take into account educational policies and access to schools, but do not observe the behavior and activities of students and teachers in the classroom. The latter factors, which are considered to be important in achieving gender equality, have not been previously observed. Previous studies have explained the history and educational philosophy of Indian education through manual work or WEE teaching methods. Mahatma Gandhi, the father of the Indian independence movement, created a new education system to challenge British colonial governance. It was primarily a policy of basic education, introduced in the 1930s, known as “Education through manual work.” It continues to function under the headings: “Crafts,” “Work Experience,” “Socially Useful Productive Work (SUPW),” and “Work Education” (Hironaka 1986, 1987; Kawai 2013). These studies are based on Gandhi’s work, the education policy, and its reports. For this reason, they do not mention the activities and behaviors of teachers and students in the classroom. The present study addresses this gap in the literature by examining how WEE works in schools. It also asks how work-related education is expected to support women’s empowerment. The present study aims to capture the characteristics of WEE in India from the perspective of women’s empowerment. It also has wider implications, as this approach can be applied to similar studies in other countries in South Asia, where patriarchal ideas strongly prevail. In addition, it examines the expectations that female and male students and teachers have of WEE. Behind the huge gap between men and women lie various types of discrimination, based on gender, category, and other characteristics. To solve the problem of women’s participation in society, multiple perspective on these issues are required. This study examines women’s empowerment from the perspectives of gender, the social structure, and the education system.",4
17.0,2.0,Evolutionary and Institutional Economics Review,20 February 2020,https://link.springer.com/article/10.1007/s40844-020-00162-5,Determining a path to a destination: pairing strategic frameworks with the Sustainable Development Goals to promote research and policy,July 2020,Gerald G. Singh,,,Male,Unknown,Unknown,Male,"Through its young history, the field of sustainability studies has had a persistent definitional problem—what exactly is sustainable development? Though there are many principles in sustainable development that have helped establish a research field, the focus of what specific characteristics of a sustainable world have remained elusive (Pezzoli 1997; Mebratu 1998; Gibson 2001; Robinson 2004). While many initial forays into defining sustainable development were focused on the differences between “weak” and “strong” sustainability (Pearce et al. 1989; Neumayer 2003), and also expanded a notion of sustainability beyond “environmental” or “resource” sustainability to think about social and political systems (Meyer 2000), later researchers highlighted the subjective and normative framing of sustainable development and proposed procedural outlooks where the definition emerges from conversations about the kind of world we want to live in (Robinson and Cole 2015; Maggs and Robinson 2016). However, though the concept of sustainable development has developed in sophistication, an actual common view of a “desired future” has remained elusive, making targeted research and policy interventions difficult (Robinson 2004). The adoption of the Sustainable Development Goals (SDGs, or, the 2030 Agenda on Sustainable Development) marked a pivotal change in how sustainability was talked and written about (UNDP 2015). With the SDGs, no longer is “sustainable development” a nebulous concept but one of the concrete policy goals along defined social–ecological dimensions. While there have been some critiques of the structure of the SDGs and legitimate criticism of what has been left out or put in (Koehler 2016; Kopnina 2016; Reid et al. 2017), the SDGs also represent a major accomplishment in international governance. Agreed on by consensus of all UN member states, the SDGs have achieved a kind of global legitimacy on the definition of sustainability that has been elusive for so long (Sachs 2012; Moomaw et al. 2017). The scientific community can and should leverage this unprecedented policy accomplishment to move beyond questions of defining sustainability to questions of “how do we reach sustainability?”. The SDGs provide a template for a research agenda that has not currently been acted on by the scientific community. With knowledge of where we are and where we want to go, we can determine plausible transition strategies. While some researchers have proposed some methodologies and frameworks to address the SDGs, a comprehensive research plan to transition to the SDGs remains elusive (Leal Filho et al. 2018). Now, as never before, is the question of sustainability science one of transition and not of definition. Luckily, the sustainability science literature has identified ways to strategically plan research and policy agendas in theory, but they have not yet been applied to the SDGs.",10
18.0,1.0,Evolutionary and Institutional Economics Review,01 January 2021,https://link.springer.com/article/10.1007/s40844-020-00192-z,An experimental study on voluntary vs. compulsory provision of public goods under the vote-with-feet mechanism,April 2021,Hui-Chun Peng,,,,Unknown,Unknown,Mix,,
18.0,1.0,Evolutionary and Institutional Economics Review,23 February 2021,https://link.springer.com/article/10.1007/s40844-021-00200-w,Towards a unified aggregation framework for preferences and judgments,April 2021,Luigi Marengo,Simona Settepanella,Yan X. Zhang,Male,Female,Male,Mix,,
18.0,1.0,Evolutionary and Institutional Economics Review,06 March 2020,https://link.springer.com/article/10.1007/s40844-020-00166-1,Indicating human capital including non-economic value,April 2021,Ryuichi Okumura,Hiroshi Deguchi,,Male,Male,Unknown,Male,"Primarily, the concept of human capital has been theoretically treated as belonging to the domain of economics. Therefore, when the investment effect of human capital is judged, for instance, it is generally converted to monetary value and measured accordingly. However, the concept of human capital is gradually expanding: the OECD defines the term as ""the attributes that facilitate the creation of personal, social, and economic well-being."" The increased human capital contributes both to economic activity and to the enrichment of each worker’s professional life. Indeed, work has economic aspects, in the creation of goods, and non-economic attributes, such as satisfaction, which are connected to society in terms of self-realization through labor. It is, however, questionable whether these two aspects are compatible. This study positions human capital as a broad concept that includes the creation of psychological value and aims to clarify the following issues. First, can workers improve both economic and non-economic aspects of human capital simultaneously in the process of doing work? Next, if there are workers who can increase both aspects, what are their attributes? Working hours at Japanese workplaces remain high compared to Western countries even though they have been shortened considerably over the past 30 years. A work style reform law was enacted in Japan by the parliament in June 2018, and consideration for the work-life balance of employed workers will progress further in the future. However, unless Japanese workers change their perspectives on labor, it will be difficult for the work hours to ultimately become comparable to Europe. The concept of labor, based on Christian culture, involves the negative aspect of original sin for Western workers. However, for the Japanese people, labor is the natural behavior of human beings, and the notion of occupation assumes a neutral, even positive, aspect of helping people connect with society without having to face isolation. For the Japanese people, work is an important part of life, and separating work and life, or the idea of balancing the two distinct aspects of living, is unfamiliar to the inherent culture of the region. Nonetheless, the fact that the OECD countries have long working hours remains a major problem that needs rectifying since such long hours of work have been known to even cause deaths. This paper attends to the Japanese world view on trying to amend the prevalence of long workplace hours. The Japanese work view assumes that work and life are united, and that it is important not only to earn a living but also to enrich life and to obtain satisfaction from work. If measures to improve employee satisfaction levels as well as earning capacity can be clarified, it may be possible to accomplish “work-life integration” rather than “work-life balance.” This paper will index the human capital concept around this idea. The specific procedure is as follows. First, human capital is redefined in terms of economic and non-economic values. The study will present indices based on the new concept of human capital. Furthermore, based on such indices, the indices’ values will be calculated, and the actual situation and issues of human capital formation will be analyzed in the context of Japanese workers. The present study is structured in the following manner: first, the authors provide a review of previous studies on measuring the size of human capital. Next, we explain the expansion of the concept of human capital, and show the definition of OECD (2001) as an example. Indices are constructed with reference to this definition. Furthermore, index values are calculated from the survey data of Japanese workers, and the situation of human capital formation of Japanese workers is analyzed. Finally, opinions on the creation of more appropriate indices with regard to human capital are contemplated, and a useful viewpoint is postulated for companies to increase their workers’ human capital.",2
18.0,1.0,Evolutionary and Institutional Economics Review,03 May 2020,https://link.springer.com/article/10.1007/s40844-020-00170-5,Trust in generosity: an experiment of the repeated Yes–No game,April 2021,Werner Güth,Hironori Otsubo,,Male,Male,Unknown,Male,"We report the results of a laboratory experiment in which subjects repeatedly play an identical stage game called the Yes–No game (hereafter, YNG) (Gehrig et al. 2007; Güth and Kirchkamp 2012). This game proceeds as follows. Proposer X suggests how to allocate the positive monetary pie \(\pi\). Denote by x and y the shares for proposer X and responder Y, respectively, such that \(x+y=\pi\), \(x>0\), and \(y>0\). Then without learning the choice of (x, y) by X, Y says either Yes (\(\delta =1\)) or No (\(\delta =0\)). The payoffs are \(\delta x\) for X and \(\delta y\) for Y. The YNG resembles buying so-called experience or credence goods (see Dulleck et al. 2011) whose quality is unknown to buyers prior to purchase; buyers (responders) decide whether to buy from sellers (proposers) without verifying product quality. The unique equilibrium of the YNG predicts that X offers the minimum possible amount and Y accepts. For a materially opportunistic responder, Yes strictly dominates No. If X anticipates this and is also opportunistic, the unique equilibrium can also be justified by once repeated elimination of dominated strategies. 
Gehrig et al. (2007) compared one-shot behavior in YNGs with that in closely related games like the ultimatum game (hereafter, UG) and the dictator game. Compared to UGs which allow to monitor whether the proposal (x, y) of X is fair enough, YNGs are trust games whose efficiency increases by avoiding the cost of monitoring (see Angelovski et al. 2019, who varied these costs and other aspects). Unlike dictator or more generally impunity games (see Di Cagno et al. 2018, for example), sanctioning still has to be exercised blindly in YNGs. The results suggest that YNG offers are smaller than UG offers and larger than dictator gifts and that responders never reject. Güth and Kirchkamp (2012) conducted a newspaper and parallel laboratory experiments with two different pie sizes, €100 and €1000, using the strategy vector method (one decides for both roles).Footnote 1 The average offered share is about 37.6% for the pie size €100 and 36.4% for the pie size €1000. Almost in line with the previous study, they observed very low rejection rates; 6.6% of the subjects reject in case of the pie size €100 whereas only 4.5% reject in case of €1000. Gehrig et al. (2007) provided two explanations of why responders are usually willing to “buy a pig in a poke,” namely in dubio pro reo (responders do not want to punish a possibly rare generous proposer) and in dubio pro meo (responders selfishly accept). A shortcoming of the studies so far is that they only observed behavior of inexperienced subjects. Repetition allows participants to learn the structure of the game and adjust over time to others’ behavior in the light of feedback information. So if responders learned that offers become embarrassingly low via information feedback, they would not lose much when sometimes vetoing to punish proposers a lot. This, in turn, might induce more generous offers. Even in case of random rematching, the shadow of the future might render altruistic sanctioning appealing. The study of Avrahami et al. (2013) experimentally explored 100-period repeated play of normal form UGs, letting responders state acceptance thresholds, also based on random rematching. They observed an immediate convergence to proposers offering half of the pie and responders demanding at least half of the pie via their acceptance thresholds. This behavioral convergence to equal sharing is in line with equilibrium behavior but to an equilibrium based on a weakly dominated responder strategy: unlike for YNGs, all pie distributions, i.e., also equal sharing, are justifiable as UG-equilibrium outcomes. Our main goal is to experimentally study the dynamics of quite long repeated YNG play.Footnote 2 Participants played a 100-period YNG using a random rematching design to attenuate reputation building among them. We manipulated the information about the pie size \(\pi\) between subjects. There were two different pie sizes, \(\pi =8\) (small pie) and \(\pi =24\) (large pie). In the Info treatment, both, proposers and responders, knew the exact size of the pie. In the No Info treatment, proposers knew the exact pie size, whereas responders only knew the probabilities of these two pies. Introducing uncertainty about the pie size does not change theoretical predictions.Footnote 3",
18.0,1.0,Evolutionary and Institutional Economics Review,04 July 2020,https://link.springer.com/article/10.1007/s40844-020-00180-3,Why does production function take the Cobb–Douglas form?,April 2021,Atushi Ishikawa,Shouji Fujimoto,Takayuki Mizuno,Male,Unknown,Male,Male,"In economics, firms are regarded as economic entities that produce goods \(\left( Y \right)\) using capital, labor, and other resources, which are called production factors \(\left( x_1,x_2,\ldots ,x_n \right)\). The production activity of firms is modeled as a function that inputs production factors and outputs the total production: \(Y=F\left( x_1,x_2,\ldots ,x_n\right)\). This is called a production function. As a simplified model, economists have proposed various two-variable production functions in which the production factors are capital \(\left( K\right)\) and labor \(\left( L\right)\). A typical example is the Cobb–Douglas production function: \(Y\left( K,L\right) =A K^{\alpha } L^{\beta }\) (Cobb and Douglass 1928). Here, A is the total factor productivity, which is interpreted as a firm’s production efficiency or technological capability that cannot be measured by labor or capital. \(\alpha\) and \(\beta\) are called the capital share (capital elasticity) and labor share (labor elasticity), and represent production’s capital or labor dependency. The constant elasticity of substitution (CES) type production function, \(Y\left( K,L\right) =A \left\{ \delta K^{- \rho }+(1-\delta ) L^{-\rho } \right\} ^{-\frac{1}{\rho }}\), is an extension of the Cobb–Douglas production function (Solow 1956; Arrow et al. 1961). Here, \(\delta\) and \(\rho\) are the distribution and substitution parameters. The CES production function matches the Cobb–Douglas production function at \(\alpha + \beta = 1\) in the \(\rho \rightarrow 0\) limit as well as the Leontief production function: \(Y = A \min \{K, L\}\) in the limit of \(\rho \rightarrow + \infty\). Furthermore, economists proposed a translog-type production function, \(\log Y\left( K,L\right) =\log A + \alpha \log K+ \beta \log L + \gamma _1 \log ^2 K + \gamma _2 \log ^2 L + \gamma _3 \log K \log L\), as an extended version that explicitly includes the Cobb–Douglas production function (Christensen et al. 1973). In economics, various forms of production functions have been proposed and widely used by researchers in microeconomics and macroeconomics. Total factor productivity has been applied to industries around the world since the late 1950s as an indicator of productivity, and many empirical studies have addressed production functions. However, the conventional research is limited to comparisons of the superiority of the production function based on the regression’s good fit to the shape of several production functions described above. Note that there is only a limited argument concerning why a production function takes a functional form, such as the Cobb–Douglas type. Among these studies, Houthakker focused on the similarity between the Cobb–Douglas production function and the power-law distribution, followed by its variables from the microeconomic foundation (Houthakker 1955). Unfortunately, however, his argument fails to reach the constant returns to scale discussed below. In this context, as in Houthakker’s work, where the Cobb–Douglas production function is a power-law function of variables \(\left( K, L, Y \right)\), we argue that the Cobb–Douglas production function can be interpreted as an inverse symmetric plane and the residual from it in the 3D space of variables \(\left( K, L, Y \right)\) (Mizuno et al. 2012; Ishikawa et al. 2013, 2014). Here, we observed the quasi-inverse symmetry in the joint probability density function (PDF) \(P_{KLY} \left( K,L,Y\right)\) under variable exchange: \(Y \leftrightarrow a K^{\alpha } L^{\beta }\), which is deeply related to the power-law distribution in the large-scale range of three variables \(\left( K,L,Y\right)\) and the log-normal distribution in the mid-scale range (Gibrat 1932; Sutton 1997). Importantly, the observed fact that each variable (K, L, Y) of the production function follows a power-law distribution is uniquely concluded from the form of the Cobb–Douglas production function. In previous studies, however, the validation of the above analytical arguments with empirical data was indirect. We have not confirmed whether our quasi-inverse symmetric plane is consistent with the Cobb–Douglas production function, which is treated in economics. The possibility that capital share \((\alpha )\) and labor share \((\beta )\) are different between the quasi-inverse symmetric plane and the multiple regression plane cannot be denied. In this study, we directly observed a Cobb–Douglas symmetric plane using the index of surface openness, which is used in geography, and successfully identified it. Based on this observation, we measured the capital share (capital elasticity) and labor share (labor elasticity), compared our results with the results of multiple regression analysis used in economics, and confirmed consistent agreement in seven countries: Japan, Germany, France, Spain, Italy, the UK, and The Netherlands. Thus, we showed that the Cobb–Douglas production function can be clearly captured in empirical data as a geometric entity with a quasi-inverse symmetry of variables. Our previous research was limited to indirect observation of the power-law region of the variables, but this current research is characterized by direct observation of the Cobb–Douglas symmetric plane in the whole region other than the power-law one. Based on the above discussion, we theoretically clarified why the Cobb–Douglas production function is better fitted to empirical data in economics. The rest of this paper is structured as follows. Section 2 describes the data that we used in our analysis. Section 3 briefly reviews our previous work. In Sect. 4, we explain the method to measure the Cobb–Douglas symmetric plane using an index called surface openness. In Sect. 5, we identify the Cobb–Douglas symmetric plane by identifying a ridge in Sect. 4’s analytical preparation and measuring the capital share (capital elasticity) and labor share (labor elasticity). In addition, we measure the capital share (capital elasticity) and labor share (labor elasticity) by multiple regression analysis and confirm that both shares are consistent in seven countries: Japan, Germany, France, Spain, Italy, the UK, and The Netherlands. Finally, in Sect. 6, we summarize the main points of this paper and present future perspectives.",7
18.0,1.0,Evolutionary and Institutional Economics Review,28 August 2020,https://link.springer.com/article/10.1007/s40844-020-00189-8,Reconsideration of the IS–LM model and limitations of monetary policy: a Tobin–Minsky model,April 2021,Toshio Watanabe,,,Male,Unknown,Unknown,Male,"Financial markets have become increasingly complicated in modern capitalist economies, with monetary factors having induced many financial crises worldwide in recent times. Of course, the discussion on the interactions between financial markets and the real economy is not new. Fisher (1933) focuses on firm liability structures and analyzes the U.S. economy from the period of the Great Depression to the early 1930s. He proposes the theory of debt deflation, which suggests that recessions and depressions are a result of an increase in the real value of the overall debt level and deflation. Keynes (1936) develops an investment theory of why capitalist economies are particularly susceptible to fluctuations, and emphasizes the effects of financial market instability. Minsky (1975) develops his own ideas about financial crises based on his interpretations of Fisher’s and Keynes’ theories and the writings of Henry Simons. He proposes the financial instability hypothesis, which posits that a financially dominated capitalist economy is inherently unstable. He stresses the importance of government interventions in financial markets and the role of the central bank as a lender of last resort, to avoid severe financial fluctuations. Minsky’s ideas led to the development of various mathematical models. Taylor and O’Connell’s (1985) model is a representative model that considers long-run expectations and household portfolios. The model proves that an economy will experience a financial crisis if a decline in the expected profit rate worsens firms’ financial condition and increases households’ preference for liquidity. Recent studies in this direction include Hein (2007), Lima and Meirelles (2007), and Charles (2008, 2016).Footnote 1 These models apply the Kaleckian investment function with factors such as the firm debt–capital ratio, and demonstrate the economy’s instability. Ryoo (2010, 2013a) discusses “the paradox of debt” in the Minsky model, while Ryoo (2013b) contributes to this literature by considering the active role of a profit-seeking bank.Footnote 2 The standard IS–LMFootnote 3 model treats all kinds of financial assets, excluding money, as bonds. However, in the modern economy, bank loans and equity issues are important means of financing, and their volatilities significantly influence the real economy.Footnote 4 Minsky (1986) stresses the role of corporations’ asset structure and banks’ behavior in influencing the real economy by stating “[o]nce corporations dominate in owning capital assets and stock exchanges exist, the holding period of investors can conform to their changing needs and preferences even though the corporation’s commitment to the ownership of capital assets can be for their expected productive life” (p. 315), and “[t]he higher leverage ratio of banks was part of the process that moved the economy towards financial fragility, because it facilitated an increase in short-term borrowing (and leverage) by bank customers” (p.238). Accordingly, our model is characterized by the following features. First, following Tobin (1969), we choose to abandon the perfect substitutability assumption in relation to financial assets.Footnote 5 We consider the financial instability brought about as a result of the relationship between investments and financing, and household portfolio preferences. Therefore, we investigate bank lending and equity markets directly, and eliminate the money market using Walras’s law. We consider profit-seeking bank behavior following Ryoo (2013b) and discuss credit creation by banks. Unlike in Taylor and O’Connell (1985), the supply of money is endogenous in our model. In addition, we explicitly treat the issue of firm equity and household portfolio selection. The equity demand of the household is expressed using the substitutability effect, which is consistent with standard price theory. The goal is to develop a modified LM curve that reflects both the equity market and banks’ behavior. Second, we analyze the effect of monetary policy.Footnote 6 We assume that the bank lending rate is exogenously given at the beginning of the term and held constant during the period. As a response to the economic situation, the central bank can change the interbank rate. Under these assumptions, the bank lending rate is endogenous in the long term.Footnote 7 This approach is similar to that of Fontana (2009), who consolidates the views of horizontalists and structuralists. Fontana interprets the differences among these viewpoints as being the same as Hicks’ distinction between single-period and continuation analysis.Footnote 8 Horizontalists rely on a single analysis built on the assumption that the state of expectations of all agents is constant. Structuralists depend on the continuation of agents, which may change in light of realized results. The integration of both viewpoints gives a more general theory of endogenous money. In our model, the idea of the former is applied to the static analysis and that of the latter to the dynamic analysis. Many studies, including Taylor and O’Connell (1985), analyze the policy effects when the central bank adopts a non-activist monetary policy of fixing the rate of money supply growth. By contrast, we assume that the central bank adopts a monetary policy suited to real conditions. We consider that the central bank changes the interbank rate by referring to the actual profit rate, and investigate the effectiveness of this policy. Finally, we investigate the stability of the economy as a whole, unlike Hein (2007), who examines the effect of monetary policy only at the steady state.Footnote 9 We also investigate the structural factors that affect economic stability and cause financial instability. From the static model, we conclude that an increase in the debt–capital ratio may decrease the profit rate. This result is different from that of Ryoo (2013a, b), who shows that a rise in the debt–capital ratio leads to an increase in the profit rate and, therefore, does not reflect “the paradox of debt.” In our model, the effect on the equity price depends largely on banks’ lending behavior and households’ portfolio behavior. When the sensitivity of bank lending to the profit rate is high, there will be greater volatility in equity prices in response to the debt–capital ratio. In addition, in the dynamic model, we show that the high degree of substitutability between the household’s equity and money in relation to the debt–capital ratio may lead to an unstable economy. Finally, we investigate whether a monetary policy based on the profit rate can effectively stabilize the economy and show the limitations of monetary policy. The rest of the paper is organized as follows. Section 2 presents an overview of the model. Section 3 discusses the behavior of banks and firms, while Sect. 4 discusses the behavior of households. We explicitly derive the lending function from the bank and the equity supply function from the firm. Section 5 considers the equilibrium of the commodity and equity markets, and analyzes the short-run equilibrium of the economy. Section 6 investigates the economy’s stability by constructing a dynamic system and considering the effect of monetary policy. Finally, Sect. 7 presents the conclusion.",2
18.0,1.0,Evolutionary and Institutional Economics Review,22 July 2020,https://link.springer.com/article/10.1007/s40844-020-00184-z,Unfolding identity of financial institutions in bitcoin blockchain by weekly pattern of network flows,April 2021,Rubaiyat Islam,Yoshi Fujiwara,Hiwon Yoon,Unknown,Male,Unknown,Male,"The increase in economic and finance network analysis has enabled the emergence and evolution of rich theories and methodologies over time (Newman et al. 2006), which have a distinct effect on human decision-making and are pervasive. Thus, they are becoming the link between societies and economies. Methodologically, graph theory has refined into a potent and compelling tool for abstruse complex problems. Although it was difficult to construct financial networks at the start of this decade, the rise of crypto-assets has enabled significant available fortuity, for network-related research and analysis. Earlier, information about transaction details was usually considered sensitive and not available for research (Baumann et al. 2014). The crypto-asset system comprised of a repeatedly expanding list of information reserved in a chain is publicly reachable, and bestows scope to analyze transaction networks in detail. The global financial crisis in 2008 exposed financial inequalities throughout the world economies. In January 2009, a mysterious figure named “Satoshi Nakamoto” introduced a virtual currency system called “Bitcoin”, which functioned over a cryptography framework called “Blockchain” with an incentive scheme called “Proof of work” (Nakamoto 2019). Bitcoin is a digital currency that archives transactions and administers autonomously the generation of new units of currency inside the blockchain frame of reference. No centralized authority dominates the operation and logging in a distributed system with private key users proves ownership of coins. A consensus algorithm and a public history of transactions have strengthened security to prevent duplication and double-spending (Narayanan et al. 2016). Publicly attainable transaction data is the main motivation of analyzing financial networks. Several studies have examined the descriptive statistics, network expansion, network topology, and the dynamics of the bitcoin blockchain network. Let us briefly review some of the prolific research that is the key motivation for our own study. The “User Graph” creation and analysis were based on the famous heuristic rule, based on the observation that every input in a multi-input transaction must be linked to a single user, as it knows all the private addresses of those input public addresses. This was elaborated upon in Reid and Harrigan (2013). They also discussed unusual big flows and temporal analyses. Another group of researchers (Ron and Shamir 2013) performed an elaborate quantitative analysis on the big wallets exchange markets, which have a large number of public addresses and by pinpointing the chain with a high range of threshold incoming bitcoin value. Finally, in a detailed investigation, a group of Hungarian researchers (Kondor et al. 2014, 2014), with extensive analysis of transaction networks, applied linear preferential attachment. In their extended work, they proposed a model that shows how structural changes in the network accompany significant changes in the exchange price of bitcoins. This research group has uploaded blockchain data from 2009–2018 (Hungary research group 2020), which is also the main data source of our previous research and this study too. The above studies establish that the de-anonymization of blockchain is very difficult. However, a possible breach of anonymity represents a significant perturbation in the bitcoin system. The purpose of our research is twofold. We propose a new approach to identifying specific important users inside blockchain. Our research goal is to develop a methodology that can track those users and help us understand their activities inside the system. We also want to focus on the “weekly pattern” that we have discussed in our previous research (Islam et al. 2019) and utilize this behavioral pattern of the user in our de-anonymization methodology. The structure of this paper is as follows. In Sect. 2, we focus on daily blockchain data between 2013 and 2018. We select this time period as some of the network analysis had already been done for the first 4 years after its launch (Lischke and Fabian 2016). In Sect. 3 we defined the important terms and network graphs with mathematical notations and equations. We perform the topological analysis of the daily networks by dividing those into the sub-domain of time period to reduce computational difficulties. In Sect. 4, we explain the weekly pattern in terms of the topological change of properties. An edge flow threshold analysis allows us to create suitable sub-graphs for filtering out small volume flows. In Sect. 5, we define the “big players” and validate their existence in our data by matching some popular rich bitcoin exchanges. These exchanges are active around the world in between our preferred sub-domain period. Finally, we propose a methodology of identifying exchange markets or crypto-financial institutions from the network, based on the criteria of very high frequency, persistent daily trades, and weekly pattern of their total daily flow.",3
18.0,1.0,Evolutionary and Institutional Economics Review,09 March 2020,https://link.springer.com/article/10.1007/s40844-020-00164-3,The institutionalisation of zero transaction cost theory: a case study in Danish district heating regulation,April 2021,Søren Djørup,,,Male,Unknown,Unknown,Male,"This paper is concerned with the institutional basis for the intended technological change of the Danish energy supply. It is the official Danish energy policy to transition from the current fossil fuel-driven economy to an energy system solely supplied by renewable energy sources (Energistyrelsen 2012). The radical character of this change gives rise to complex relations between technical and institutional parameters. It requires studies at a very concrete level to identify and understand the economic character of these dynamics. This analysis was initiated by such presumption and indeed do its conclusions reaffirm the Coasian character of real-world economics; a world of positive transaction costs. Studying the specifics of institutional arrangements is the path to improve local economic policies as well as to understand and develop economic theory. The following addresses a peculiar case where the presumed universalities of mainstream economic textbook theories have been institutionalised in the specifics; a case where economic theory and method has become an integrated part of the institutional arrangement. In this role, the theoretical perceptions are no longer only descriptive and explanatory but directly affecting economic activity. The methods and theories themselves become a part of the institutional structure of production. Although the paper partly is a case study, the case provides lessons at a more general level about the difficulties which neoclassical economics may encounter in practice (Djørup 2016a). The general level of the study can be related to contributions from other contemporary scholars. Hermann-Pillath and Boldyrev argue neoclassical economics is deficient for understanding the subject area of free trade because of its institution free approach (Herrmann-Pillath and Boldyrev 2014). They argue that considering the relevant institutions sustaining the free trade system enables us to say something practical in the area of study. While their focus area of practical reality is the free trade system, this study focuses on the need for ‘practical theory’ in the ongoing technological change from fossil fuels to renewable energy. The analysis can also be related to Richard Lipsey and Kelvin Lancaster’s “The General Theory of Second Best” (Lipsey and Lancaster 1956). They argue that if one optimality condition of the economic model cannot be satisfied in the practical reality, optimality in all other variables may not lead to the second-best solution. Thus, it is possible that the second-best solution can only be achieved by changing other variables away from the values that would otherwise be considered as optimal. The consequence of Lipsey and Lancaster’s theory, as well as the argument by Hermann-Pillath and Boldyrev, is that any analysis of optimality and allocation must be based on the concrete institutional foundation of the problem. This approach is in line with Ronalds Coase’s writings on economic theory and method which will be referred to later in the paper. The economic challenge forming the background of this article is basically an economic change arising from the replacement of stored energy reserves with a fluctuating energy supply. Currently, wind power constitutes above 40% of electricity supply in Denmark (Energinet.dk 2019). Studies concerned with technical system analysis suggest that fluctuating primary energy supply could cover above 50% of the total energy supply (including electricity, heating and transport) in a renewable energy system (Mathiesen and Hansen 2015). The district heating system plays a central role in this techno-economic transition since it is a key infrastructure for integrating an increased supply of wind power (Lund et al. 2012, 2014).Footnote 1 This fluctuating supply of wind energy creates techno-economic challenges on both the supply side and the demand side (Connolly and Mathiesen 2014). The supply-side issues relate to the challenges of having sufficient back-up capacity in the electricity sector when the wind is low (Sorknæs et al. 2015; Lund and Mathiesen 2015). The demand side issues are present in high wind periods and are related to the priorities of integrating the electricity and heating sectors by introducing heat pumps, thermal storages, heat network expansions, etc. (Lund et al. 2014; Maxwell et al. 2015). Significant investments in the district heating system are therefore needed in the coming years in order to fulfil its future role in a so-called smart energy system; meaning a system optimised across energy sectors (Mathiesen et al. 2015a, b; Lund et al. 2014). Through such integration and coordination between electricity, heating and transport services, the energy system as a whole is able to deal with the fluctuating nature of renewable supply more cost-effectively (Lund 2014). Given this strong presence of technical spill-over effects, the investment decisions made today must be consistent with the long term strategic energy policy. In the technological transition, it is, therefore, of great importance to examine the institutional structure which influences decentralised investment decisions (Djørup 2016b; Djørup and Hvelplund 2016; Hvelplund and Djørup 2017). A lot of contributions in the economic literature are occupied by the task of developing convincing theoretical arguments against so-called mainstream or neoclassical economics and appears to be right in doing so. What seems to be less flourishing in the literature is studies of the working of those criticised methodologies in the real world. This paper wishes to supplement the rich theoretical literature on institutional economics with a little inquiry into the neoclassical theory at work in the institutional structure of production. As Coase suggested, the way to improve mainstream economics is not through ‘frontal assault’ but by the adoption of a different approach (Coase 1998). This article analyses the official guidelines for socio-economic calculation in Denmark, which are formulated by the Ministry of Finance. In a quite subtle way, these guidelines are part of the institutional structure that allocates investments in the Danish energy system. As such, the guidelines become a subject matter of institutional economics. Especially two elements in the guidelines has been discussed; the first element being the size of the discount rate, the second element being the so-called ‘tax distortion loss’.Footnote 2 Regarding the first element, a discount rate of 4% is currently settled as mandatory (Retsinformation 2015b; Finansministeriet 2013; Energistyrelsen 2013). The size of the discount rate obviously has a great impact on the economic project evaluation. However, this factor is not addressed here since it is already widely discussed in the literature and its impact can be intuitively understood. The second element known as the ‘tax distortion loss’ is something less intuitively understood and it regulates in a more subtle and unpredictable way. Further it is a clear representation of neoclassical theory. In the following, the regulative importance of the guidelines for socioeconomic regulation will be explained with a specific focus on the ‘tax distortion loss’ principle and its use in the Danish district heating sector. This will be done through a review of its application, its theoretical foundation and its possible implications. It is an illuminating example of how mainstream economic method may have troubled times in the context that matters: the context of real-world implications.",3
18.0,1.0,Evolutionary and Institutional Economics Review,06 April 2020,https://link.springer.com/article/10.1007/s40844-020-00167-0,Industrial growth models by input–output analysis and an institutional approach to the automotive industry in China and Turkey,April 2021,Emre Ünal,,,Male,Unknown,Unknown,Male,"The main aim of this paper is to analyze the evolutionary and institutional structures of China and Turkey in their main industries and particularly in the automotive industry. To do that, a hypothetical extraction method (HEM) with a weighted multiplier approach through input–output analysis is implemented to estimate industrial productivity growth. This method helps analyze the industrial competitiveness of the two countries according to the assumptions of industrial growth models. There are several questions to be answered. First, what are the growth models in the main industries? Second, why do the automotive industries in Turkey and China experience trade deficits, and how can they be reduced by institutional factors? This paper has been fashioned using input–output analysis and an institutional approach in a comparative perspective to explain industrial competitiveness by industrial growth models. In terms of input–output analysis, the uniqueness of this work lies in the fact that the HEM with weighted multiplier approach was introduced to calculate industrial productivity growth in consumption goods and export goods sectors. Productivity structure, growth models and competitiveness have always been used to define the position of countries in international trade. This method works to explain industrial competitiveness by constructing growth models. For the institutional approach, the novelty of this work is that it introduces a method which works in apple-pie order with input–output analysis to understand economic phenomenon and reduce economic problems (see Fig. 1). Input–output analysis gives the technical details about an economy, but this is just one side of approaches to the analysis of an economy. Without considering the institutional approach, research can remain limited and incomplete. An institutional approach works to explain technical details and move beyond the back-door of an economy. If a country or industry is technically competitive, what types of institutions facilitate this? Taking input–output analysis into account with an institutional approach is a new and complementary procedure. It is expected that input–output experts can engage with the institutional approach to explain technical details derived through input–output analysis. It is also expected that the input–output and institutional methods in this paper will open a new gateway to help improve understanding of industrial competitiveness by exchange rate systems, unit labor cost (ULC), and wage-productivity structure. Relation between institutional approach and input–output analysis For the analysis, China and Turkey were compared in terms of industrial growth models. The two countries were examined by institutional factors; wage structure, exchange rate, tax legislation and vertical integration in the automotive industry. There are reasons why the two countries were chosen for the analysis. First, China is the cause of the largest ever trade deficit in Turkey. In 1998, China caused an 807 million USD trade deficit. This was below Japan, the South Korea and Germany. In 2005, China caused the largest trade deficit than any country. The deficit caused by China was 6635 million USD. In 2011, the trade deficit increased to 19,227 million USD this was above all other countries.Footnote 1 China is a country configured by export-led growth (Ünal 2016). Export-led growth works to increase competitiveness via a low ULC and an undervalued exchange rate. Discovering what industries are based on export-led growth, and stimulate trade surpluses, can be instructive in reducing trade deficits in specific industries. If it is possible to reveal the reasons for trade deficits by input–output analysis and institutional approach, this model can be applied to other countries as a case study to enable understanding of their industrial competitiveness. Second, the two countries are similar to one another because they are developing countries and their economies are influenced by large amounts of foreign direct investment (FDI) in their medium- and high-tech industries.Footnote 2 There are also technical issues that bear comparison. Both countries have input–output tables which make the analysis possible. Both are very effective in the production of motor vehicles. They have developing automotive industries engaging in a considerable amount of production, but these industries both experience trade deficits. Hence, it is useful to make a comparative analysis between the two countries. ‘Industry’ means specific production that contributes to the output. ‘Sector’ indicates the groups of industries that contribute to domestic consumption and export. The industries that contribute to domestic consumption are categorized as the domestic consumption goods sector, and those that contribute to export are categorized as the export goods sector. For input–output analysis, the sectors were divided using a commodity base approach (see “Appendix”). Industries which significantly contribute to export, and which are not overly dependent on services and natural resources are included in the analysis. Seven industries were chosen from 35 industries in WIOD PYP tables. These are textile and textile products, leather and footwear, chemical and chemical products, rubber and plastic, machinery, electrical and optical equipment, and transport equipment, which is examined as the descriptive industry for the automotive industry. These industries constitute the largest component of export goods. In 2009, the share of these industries in export goods was 79.2% in China and 67.8% in Turkey. In Sect. 2, methodology and a new approach to the analysis are discussed. In Sect. 3, the relationship between input–output analysis and institutional approach are explained. In Sect. 4, industrial growth models are examined. In Sect, 5, the countries are analyzed by the institutional factors in the automotive industry. In Sect. 6, the paper is concluded. In “Appendix”, the HEM and the method for calculating real exchange rate are explained.",5
18.0,1.0,Evolutionary and Institutional Economics Review,08 May 2020,https://link.springer.com/article/10.1007/s40844-020-00169-y,Structural change and financial instability in the US economy,April 2021,Kenshiro Ninomiya,Masaaki Tokuda,,Unknown,Male,Unknown,Male,"The recent global financial crisis, which was triggered by the 2007 subprime loan crisis in the US, still continues to cast a shadow over the world economy. As a result, the financial instability hypothesis proposed by heterodox economist Minsky (1982, 1986) has received renewed attention (Lahart 2007; Krugman 2012). Although many heterodox economists have examined the financial instability hypothesis,Footnote 1 it had been largely ignored before the crisis. For example, Taylor and O’Connell (1985) presented a simple macrodynamic model and indicated that an increase in the expected profit rate, reflecting the state of confidence, ρ, reduces the interest rate, i. A true Minsky crisis occurs when the derivative of i, iρ, is significantly negative.Footnote 2 Ninomiya and Tokuda (2011) introduced the concept of “instability of confidence” and demonstrated that it increased during the mid-1990s in Japan.Footnote 3 Applying a vector autoregression (VAR) analysis, they demonstrated that the financial structure of the Japanese economy changed during that period.Footnote 4 In a subsequent study, Ninomiya and Tokuda (2012) extended their model to an open economy and examined structural changes in the Korean economy, demonstrating that the Korean economic structure became more robust after the Asian monetary crisis of the late ‘90 s. On the other hand, Kregel examined the Asian monetary crisis (Kregel 2000) and the subprime loan crisis (Kregel 2008) based on the financial instability hypothesis and an earlier study (Kregel 1997), explaining that the concept of a cushion of safety that covers the margin of error between anticipated returns and financing costs for an investment project was central to Minsky’s analysis. The idea of increasing financial fragility is built around the slow erosion of margins of safety during conditions of relative stability. Kregel (2000) concluded that the Asian monetary crisis represented a traditional Minsky process.Footnote 5 However, Kregel (2008) insisted that cushions of safety had been insufficient since the beginning of the subprime loan crisis and therefore, the crisis did not represent a traditional Minsky process. This study theoretically argues that instability of confidence makes a dynamic system unstable, even when the economic structure itself is stable, and empirically demonstrates that a clear structural change did not occur in the US economy in the late 1990s. Although the economic structure in the US was fragile before the 1990s, this fragility vanished after the late 1990s. Moreover, the high instability of confidence after the subprime loan crisis has affected investment since the late 1990s. We believe these results may support Kregel (2008). This study therefore proposes that the confidence level, in addition to a robust economic structure, is crucial to an economy. The structure of this paper is as follows. This study presents a macrodynamic model that examines the economic structure and instability of confidence that affects a dynamic system as described in Sect. 2. In Sect. 3, we examine the economic structure of the US economy using the VAR analysis. Section 4 is the conclusion.",2
18.0,1.0,Evolutionary and Institutional Economics Review,25 May 2020,https://link.springer.com/article/10.1007/s40844-020-00173-2,The twin deficits in the ASEAN countries,April 2021,Cosimo Magazzino,,,Male,Unknown,Unknown,Male,"The aim of this paper is to analyze the trade balance-public budget deficits nexus for the Association of Southeast Asian Nations (ASEAN, hereinafter) countries in the 1980–2012 period, using IMF data. Even though this issue has been well-debated in literature, few studies have analyzed Southeast Asia. Our analyses consider two groups of countries: ASEAN-6, which includes the former member states, and ASEAN-10, which concerns all countries of the Southeast Asia region. Since these subgroups of countries could represent an optimal currency area, sharing a common money, their monetary and budgetary policy might be integrated (Magazzino 2014b). These countries recorded huge trade and budget deficits for most of 1990s. Interestingly, post 1997 crisis, the deficits amounted to about 4 percent. As we are in the midst of recession, the dynamic movement of both deficits would be a concern for policymakers worldwide (Lau et al. 2010). Fiscal positions were either in balance or in surplus, unlike in the early 1980s when budget deficits in Malaysia and Thailand exacerbated the private sector imbalances. Like Singapore, the high degree of openness of the ASEAN economies provided confidence that the current account deficits were also efficient market outcomes (Magazzino 2014a). If the Twin Deficits (TD, hereinafter) hypothesis is correct, the policy prescription would be a reduction in the budget deficit via a tax increase. The fiscal consolidation would directly decrease the budget deficit, and would indirectly reduce the external deficit (Normandin 1999). According to the Keynesian absorption theory (Keynes 1936), an increase in the budget deficit would induce domestic absorption (an expansion of aggregated demand) and hence, an increase in imports, causing an increase or a worsening of the current account deficit (Marinheiro 2008). According to the Mundell (1968) and Fleming (1962) framework, an increase in the budget deficit induces an upward pressure on interest rates that, in turn, will cause capital inflows and an appreciation of the exchange rate, ultimately leading to an increase in the trade balance (Kouassi et al. 2004). On the contrary, a “twin divergence” hypothesis is presented in Kim and Roubini (2008). The Ricardian Equivalence (RE, hereinafter) theorem, in contrast, states that a tax increase would contract budget deficits but would not alter trade or current account deficits. In his articulation of the “equivalence theory” Ricardo (1817) suggests that government budget deficits should not alter capital formation and economic growth or the level of aggregate demand (Barro 1974; Magazzino 2012b). Perfect RE implies that a reduction in government saving, due to increased budget deficit or reduced budget surplus, is fully offset by higher private saving, so the aggregate demand is not affected (Forte and Magazzino 2013). Hence, under the RE hypothesis, budget and current account deficits are causally independent. The Neo-classical view argues that a deterioration in the current account results into diminished economic growth, and subsequently leads to a deterioration in the budget balance. The causal relationship in this instance proceeds from the current account deficit to the budget deficit (Magazzino 2012a). The fourth approach is the bi-directional causality between the BD and the CAD. This causation is called “current account targeting” by Summers (1988). The author argues that the external adjustment can be accomplished through fiscal policy. While budget deficits may cause current account deficits, the existence of significant feedback may cause causality between the two variables to run in both directions (Kim and Kim 2006). On the road to establishing the Economic Community of ASEAN and considering the Chiang Mai Initiative (CMI), it is relevant to examine the interdependency between the economies of this area. Despite the trajectory of the European Community (EC), ASEAN has not discussed the adoption of a shared currency within a monetary union, not even at ASEAN + 3 (Alvarado 2014). Therefore, it is significant for ASEAN countries to address the questions of how trade deficits are related to budget deficits in a perspective of deeper economic integration, possibly up to an optimal currency area. To our knowledge, this is the first study that applies panel data econometric techniques to the ASEAN countries in order to empirically test the TD hypothesis, investigating stationarity, cross-section dependence, cointegration, causality and convergence for these series. However, our results may highlight the (eventual) heterogeneity and divergence of ASEAN member states, shading light on the opportunity for these countries to create a monetary union. The rest of this paper is organized as follows. Section 2 outlines the theoretical background and empirical evidence about this issue. In Sect. 3, we briefly illustrate econometric methodologies and data. Section 4 shows the empirical analyses, and Sect. 5 concludes, giving some policy implications.",5
18.0,1.0,Evolutionary and Institutional Economics Review,09 September 2020,https://link.springer.com/article/10.1007/s40844-020-00188-9,Employment practices in Japan’s automobile industry: the implication for divergence of employment systems under globalization,April 2021,Mari Yamauchi,,,Female,Unknown,Unknown,Female,"Lifetime or long-term employment (hereafter, LTE) and the ‘seniority-plus-merit principleFootnote 1’, or nenko, the cornerstone of traditional Japanese human resource management (Dore 1973; Koike 1987), has come under significant pressure due to slow economic growth and intensified global competition, and many authors have pointed to erosion and a diversification in employment systems as a result (Morishima 1995; Miyajima et al. 2003; Miyamoto 2006; Yamauchi 2013, 2016). Other observers stress that institutions which have long undergirded the traditional employment system—e.g., new graduate-centred hiring or internal promotion—are actually hindering the transition of Japan’s labour market into one with greater liquidity and diversity that can meet the challenges posed by a rapidly changing and globalizing technological and product environment where international competition over talent is intensifying. (Waldenberger 2016; Conrad and Mayer-Ohle 2019). Globalization is thus a key factor in determining the direction of HRM among large Japanese MNCs. In this vein, some scholars (Hirano 2006; Olcott 2009; Ono 2007; Yamauchi 2013, 2016) have studied differences in HRM practice according to ownership—Japanese vs. foreign—finding that foreign MNCs operating in Japan have been a major factor driving divergence in employment policy. Others have conducted sector-specific research into HR practices of major Japanese corporations, concluding that deviation from the traditional employment model has been most pronounced in sectors such as finance (securities companies) and pharmaceuticals (Yamauchi 2013, 2016; Suda 2015, 2017), where global competitors have rapidly expanded operations in Japan, taking market share from Japanese local players as soon as local markets were deregulated. The key factor driving these developments is intensified competition with new foreign players—following deregulation and globalization of product and service markets—which has quickly led to fierce competition over human resources in local labour markets. Thus, growing divergence in HRM practice in Japan appears to reflect differences in the relative competitive strengths or weaknesses of firms; these differences manifest themselves only after product and service markets are deregulated and, as a result, global competitive pressures intensify. In other words, in sectors where strong institutional pressures continue to prevail, and thus global competitive pressures are subdued or non-existent, relative competitive advantages or disadvantages are not an issue, implying firms in such sectors have little motivation to change employment practices. In deregulated sectors, however, relative advantages and disadvantages do manifest themselves and, where Japanese companies exhibit little competitive strength, foreign players are able to quickly expand operations in local markets, forcing native firms to adapt and change. The present study is exploratory in nature. It examines HR practice within Japan’s auto sector and discusses, more broadly, how the employment system has been modified or adjusted in recent decades in those sectors where the traditional Japanese model has contributed to Japanese firms establishing strong positions within global markets (Porter and Takeuchi 2000; Fujimoto 2003). It then addresses the implications of these findings for change and diversification of employment systems in Japan, presenting a hypothetical overview of divergence in light of findings from other sectors. Since the turn of the century, the auto sector has also seen radical innovation (e.g., the electric vehicle, automatic driving, car sharing) mainly driven by start-up firms in the US, and involving a number of international and domestic business tie-ups and alliances. However, researches on recent developments in HRM in this sector have been limited. This paper examines the effects of such innovation on HRM for core white collar employees in the sector. It also looks at how global HR, or policies to align HR practices across global operations, are being introduced in the sector. To the extent Japanese auto sector firms enjoy competitive advantages in global markets, we would expect HR policies to evolve based on principles of the traditional employment model, while introduction of global HR should entail adoption of core Japanese practices by overseas operations. The following section introduces prior studies which have addressed change and diversification in HRM practice in Japan; it then reviews recent developments in HRM in the automobile and other comparable industries, focusing in particular on the impact of globalization. Section 3 covers research methods and provides profiles of the firms which make up this paper’s case study. Empirical findings are reviewed in Sect. 4, with summary findings presented in Sect. 5. Finally, Sect. 6 presents conclusions and offers possible scenarios for future trajectories of divergence among employment systems in Japan.",2
18.0,1.0,Evolutionary and Institutional Economics Review,21 December 2020,https://link.springer.com/article/10.1007/s40844-020-00193-y,Correction to: Employment practices in Japan’s automobile industry: the implication for divergence of employment systems under globalization,April 2021,Mari Yamauchi,,,Female,Unknown,Unknown,Female,"The article Employment practices in Japan’s automobile industry: the implication for divergence of employment systems under globalization, written by Mari Yamauchi, was originally published electronically on the publisher’s internet portal on 09 September 2020 without open access. With the author(s)’ decision to opt for Open Choice the copyright of the article changed on 01 December 2020 to © The Author(s) 2020 and the article is forthwith distributed under a Creative Commons Attribution 4.0 International License (https://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made.",
18.0,1.0,Evolutionary and Institutional Economics Review,14 September 2020,https://link.springer.com/article/10.1007/s40844-020-00190-1,Schumpeter’s business cycle theory and the diversification argument,April 2021,Vipin P. Veetil,,,Unknown,Unknown,Unknown,Unknown,,
18.0,1.0,Evolutionary and Institutional Economics Review,12 January 2021,https://link.springer.com/article/10.1007/s40844-020-00197-8,Correction to: Schumpeter’s business cycle theory and the diversification argument,April 2021,Vipin P. Veetil,,,Unknown,Unknown,Unknown,Unknown,,
18.0,1.0,Evolutionary and Institutional Economics Review,23 October 2020,https://link.springer.com/article/10.1007/s40844-020-00191-0,The missing piece in E. Cassirer’s theory of symbolic forms: the economy,April 2021,Georg D. Blind,Raji Steineck,,Male,Male,Unknown,Male,"Ernst Cassirer, a disciple of the neo-Kantian school, is a heavyweight in philosophical discourse. With well over 50,000 citations, his works have been cited more often than those of famous economists such as Jan Tinbergen, Reinhard Selten, and Vernon Smith (1969, 1994, and 2002 Nobel Laureates). While the economics literature continues to show some traces of its origins in moral philosophy with a small but steady stream of references to philosophers, Cassirer’s work is almost entirely absent from it. Cassirer’s magnum opus, his three-volume Philosophy of Symbolic Forms (1923‒1929; henceforth PSF), regroups what may be regarded as a cultural philosophy of language, mythical thought, and science. While the absence of a corresponding treatise on ‘the economy as a symbolic form’ may explain the lack of references to PSF in the economics literature, it does not imply that his work is irrelevant for the discipline of economics. On the contrary, Klattenhoff’s recent suggestion that we should understand money as a symbol with important functions in Cassirer’s sense (Klattenhoff 2016, 2018) and Miklautz’s analysis of the sphere of commodities as a symbolic form (Miklautz 2005) illustrate the potential contribution of PSF to economic thought. With the growing recognition of economics as a cultural science (Throsby 2001; Dopfer 2011) and as a cultural “artefact” itself (Hartman 1977), a critical appraisal of PSF’s potential bearing on economics seems a very timely endeavour. Notably, the economics discipline has recently witnessed a strong surge in discussions around the insufficient recognition of how economic rationality and behaviour depend on cultural conditions. Adding to such variety ‘in space’ (i.e., between individual actors), a second stream of criticism toward received economic theory refers to its lack of reflection on change as ‘variety in time’. Mueller (2004) offers a concise overview of these two discussions. While an “ultimate reduction of all cultural forms to the one form of logic seems to be implied by the concept of philosophy itself” (Cassirer 1923–1929[1953–1957]: I,84),Footnote 1 Cassirer eschews any reductionist reading of that principle. On the contrary, he sets out to build a theory of culture as a system of mutually irreducible, complementary domains of meaning (starting with language, myth, and science in the three volumes of his magnum opus). As evident from his writings on technology or law, and his last systematic exposition, the Essay on Man, Cassirer came to think of culture as an open, evolving system of such domains. We want to argue in this paper that economy is fully deserving of a place in this system.Footnote 2 Cassirer’s theory builds on the key observation that the human grasp of reality is fundamentally mediated through symbols. This implies a mutual existential dependency of the universal and the particular (Cassirer 1923–1929[1953–1957]: I, 86).Footnote 3 A symbol in the strict sense is a material entity that is produced to “stand in” for something else (like a red traffic light for the order “stop and wait”). It is dependent on the material conditions of production (no traffic lights without electricity and individual motorized transport), and on socially shared rules of articulation and application (colour coding, traffic regulations). While the said rules allow one symbol to stand in for a manifold of objects, or thoughts, or modes of behaviour, they are bound to a specific normative domain. In the words of Luft, they belong to distinct “cultural spaces of meaning” (Luft 2004). And they develop and change over time, not least due to changes in technology and other socio-cultural conditions. The context dependence of symbolic representation implies a twofold differentiation, in normative domains, and in path-dependently evolved configurations (Steineck 2014: 5, Steineck 2020). In other words, while each instantiation of a normative domain/symbolic form represents a fundamental normative orientation, its actual formation is shaped by the particular social and historical context. Accordingly, it has to negotiate general rules of society that define the “opportunity space” of “permissible operations” (Dopfer and Potts 2008: 9) for each domain.Footnote 4 At the same time, each symbolic articulation (scientific paper, work of art, and religious ritual) has to operate with a transmitted repertoire and the available sign systems to be understandable. This implies that innovation is possible, but only within specific boundaries.Footnote 5 Our application of PSF to the economic sphere takes the form of an initial exploration. A full systematic study, which would have to present a wealth of source materials reflecting the self-understanding of economic agents through global history is obviously beyond the scope of this paper.Footnote 6 In selecting our material, we hold to the general idea of gaining epistemological benefit from a ‘reciprocal comparison’ (Pomeranz 2000) across time and space, in this case an under-explored geographical and historical juxtaposition that contrasts England to Japan during the latter country’s later medieval period (1185 to 1600). This choice of case study also highlights how the “model of economic rationality” found in orthodox economics “does a very poor job of predicting human behaviours” (Quinn 2016) outside a narrowly defined subject matter. As our analysis will show both the internal historic formation of the economic sphere, and its entanglement with other symbolic forms—here, most importantly the religious and the economic, are instrumental for understanding variation in configurations of the economy across time and space. This insight has several implications. First, for economic theorizing, it suggests the need to allow for variation in the reasoning of agents. This is significant, because it downgrades the received assumptions of universal economic rationality to the status of a special case, applying at best to economies dominated by modern capitalist relations of production. For the great many other particular cases, economic theorizing will need to rely on specific forms of rationality. Second, it implies that economic methodology needs to open further toward multidisciplinary approaches. And third, it necessitates more complex empirical enquires for its call to add and integrate the study of reason to that of economic outcomes. We structure this article as follows. Section 2 introduces the core elements of Cassirer’s PSF. Section 3 develops a sketch of the economy as a symbolic form. Section 4 compares the medieval economies of Japan and England to illustrate how symbolic representation informs economic outcomes. Section 5 reflects on theoretical implications, methodological consequences, and empirical practice. Section 6 concludes.",
18.0,1.0,Evolutionary and Institutional Economics Review,30 May 2020,https://link.springer.com/article/10.1007/s40844-020-00174-1,The transformation of values into prices on the basis of random systems revisited: reply to my commentators,April 2021,Bertram Schefold,,,Male,Unknown,Unknown,Male,"I should like to thank Professors Kenji Mori, Masashi Morioka and Yoshihiro Yamazaki for their challenging comments and the editors of this journal for the opportunity to reply. It remained undisputed that the Marxian interpretation of profits as redistributed surplus value is a central proposition in Marxian economic theory; the question was, whether a new explanation for the equality of aggregate profits and aggregate surplus value, based on random systems, is an adequate analytical reconstruction of Marx’s ideas, whether it is a special case or whether it has a more general meaning, and how it relates to other interpretations. As a proposed solution to the transformation problem, obeying the condition profits equal surplus value (\(P = M\)), it is clearly “more narrow than the conditions regarded as necessary by Marx or Engels and broader than modern critics, from Bortkiewicz to Sraffa, thought” (Schefold 2019, p. 261). It is not a transformation in the literal sense, since labour values and prices of production are first calculated separately from the input–output data and then related to each other in such a way that the identities are confirmed, which were used by Marx in his attempt to derive the prices from values. My main concern in Schefold (2019) had been to discuss the context of this analytical reconstruction by looking at the principal Marxian propositions in which the condition \(P = M\) plays a key role. Values are redundant for the calculation of prices, yet indispensable for the understanding of the meaning of this identity—if it holds. But the discussion of the commentators focussed more on the formal properties of the solution, and so my reply will have to go back to these. All three criticise the special nature of my assumptions as if I had ever failed point out that special assumptions are needed to make \(P = M\) a result that can be rigorously proved. Did I ever promise more? The claim was not to have found an absolute truth, but to have found a new solution of the transformation problem that sheds light on the Marxian interpretation of capitalist reality. And so I said, for instance:”It has turned out….that Marx’s transformation is correct, after all, if the investigation is restricted (emphasis added—BS) to so-called random systems” (Schefold 2019, p. 267). The solution had been compared to others in Schefold (2016a). It is more general than the assumption that prices of production are proportional to labour values because the organic compositions of capital are equal. This condition is not needed. It is also shown that Sraffa once thought that the Marxian \(P = M\) was correct as a rough statement about averages: the capitalists get a surplus which is essentially the same, whether it is looked at in terms of prices or in terms of values, and that Sraffa thought that such an idea could be made rigorous if the economy worked like a one commodity economy, like a corn-model, where it is clear which share of production goes to which class. But to render this idea rigorous, Sraffa thought that it was necessary to assume that the system was in standard proportions and that the capitalist received a share of the standard product. Here it was shown that this restriction could also be overcome—the system did not have to be in standard proportions and yet \(P = M\) could be derived, but certain random properties had to be postulated. To understand why Marx could see profit as redistributed surplus value, it is necessary to show first why value may be visualized as a substance. The metaphor of ‘substance’ reminds the modern reader of the conservation laws for matter and energy in Newtonian physics over time. We do not find a substance in this strict sense in Marx, since the means of production must be revalued as values and prices change over time as the systems of production change with technical progress, and particularly and most drastically in periods of crises. The theory of the forms of value, which extends to the third volume of Das Kapital, shows only how the process of reproduction may be seen as a circulation of the value substance as long as no such revaluations take place. I have tried to show how this vision may be maintained, if prices of production are introduced, although, for the derivation of the price system taken in isolation, values are redundant. The view that profits are redistributed surplus value imposed itself for Marx, since he thought that prices originated from values. How can profits again appear as a substance related to surplus value, once both are separately derived in a value system on the one hand and in a price system on the other? It is the random character of the system, which allows this metaphorical view of a circulating value substance to reappear after the separate derivation of values and prices from the structure of use values had led to the insight that the same surplus, measured in terms of values and in terms of prices, could not generally be equal. On average, Marx thought, the quantity of the substance of value must be the same, looked at in terms of values and in terms of prices. There is no law of conservation of the substance if the system changes and employment changes. The vision can be maintained, however, if the structure of production persists and has random properties, for then \(P = M\) holds. That was the point to be made, and it does not matter much whether the assumptions needed for ‘randomness’ are very special or a little special. I believe that the future discussion should focus more on the interpretation of Marx and on the question, whether my analytical reconstruction can be regarded as an adequate expression of Marxian ideas, given our modern knowledge about prices of production. Kenji Mori’s comment is divided into two parts, of which each takes up one aspect of this problem. Yamazaki and especially Morioka concentrate on the formal aspects of the underlying model, and this is also useful; how the model works must be clarified. They each have their objections and raise doubts. I think these can be dispelled by means of a reexamination of some central concepts. Whether I am successful or not in convincing my readers, I remain grateful to my commentators for having compelled me to reconsider the issues, which I shall now take up one by one. I have argued (Schefold 2016b) that the introduction of randomness is important not only in relation to Marx but also to neoclassical theory, as it evolved in the last third of the nineteenth century. There is no transformation of values into prices in neoclassical theory, but there is an analogue to the proposition that profits are redistributed surplus value. The great change, which occurred in the transition from classical to neoclassical economics, consisted of an explanation of functional distribution between wages and profits in terms of supply and demand for factors of production. But to treat capital as a factor of production, demanded and supplied according to the level of the rate of interest, aggregate capital had to be given prior to the rate of interest so that effects of changes in the rate of interest on the value of capital had to be assumed away, and this is possible in conditions similar to those encountered when the rate of profit changes in Marx in such a way that the value of the surplus does not change. It thus appears that different economists in the nineteenth century shared the same simplified view about the relationship between distribution, prices and the valuation of capital. The view of the structure of production as random in large aggregates allows to represent this vision in both cases, Marxian and neoclassical. In both cases, the conditions hold only exceptionally, and that is a criticism of the theories concerned, but the analyses of those conditions allow better to understand the inner logic of the theories and to asses their potentials and their limitations in a pluralist approach to economics.",
18.0,1.0,Evolutionary and Institutional Economics Review,07 March 2020,https://link.springer.com/article/10.1007/s40844-020-00165-2,Facebook’s Libra is far from broad acceptance as a world currency,April 2021,Naoyuki Iwashita,,,Male,Unknown,Unknown,Male,,
18.0,1.0,Evolutionary and Institutional Economics Review,12 May 2020,https://link.springer.com/article/10.1007/s40844-020-00171-4,"A review on Yoshinori Shiozawa, Masashi Morioka, and Kazuhisa Taniguchi, Microfoundations of Evolutionary Economics, Springer, 2019",April 2021,Yoshio Inoue,,,Male,Unknown,Unknown,Male,"Imagine you are walking along the street and happen to feel thirsty. You go to a convenience store to buy a bottle of water, and perhaps you can get it with no trouble. There seems to be nothing interesting in this scene that attracts our attention. However, if you are an economic professional, you ought to feel uneasy about this scene because you happened to feel thirsty without any previous awareness or notice. Nevertheless, there were a lot of water bottles whose buyers were not decided as if they had been waiting for you. This means that these commodities had been supplied before your demand appeared. How was that possible? This is the question that Shiozawa et al. (2019) who are sure to play a monumental role in the history of evolutionary economics, devoted their whole study to explore. Why does this scene matter in neoclassical economics? If the price mechanism functions well, demand and supply must be equalized under the equilibrium price. The demand here comes from the ones who have already responded to the equilibrium price (at the equilibrium the supply is equal to that demand). Therefore, this supply does not allow for a surplus amount for cases where the demand that has not yet appeared. How, then, can you get a water bottle? Your demand was not exhibited when the equilibrium supply was decided. In the ordinary market theory, your demand must be treated as excess demand. It will only be acknowledged after the price has risen due to the pressure of excess demand, and after firms increased their supply responding to that rising price, that you can relieve your thirst. Nevertheless, the authors of this book ask the question “have you ever experienced such an inconvenience?”. If the price mechanism functions well, your unexpected demand will not be satisfied. However, that is certainly not the case in our common experience, at least for people living in advanced economies. If the price mechanism does not function well, and the excess supply is left behind for some time, your accidental demand can indeed be satisfied. For instance, if we admit almost all markets have become oligopolies, and the myth of price mechanism is now out of date, it will not be so difficult to make an economic model that can satisfy your unexpected demand. However, the neoclassical view rejects the idea that this is a normal condition of the modern economy. Some other neoclassical economists may say, aside from the pure theory that firms in the real world decide their production based on a kind of expected demand which factors in some unexpected demand. This is a far more realistic explanation than the excess supply story, and is also the course of discussion this research seeks to explore. However, if firms really behave as scholars assume, the market price is going to reach the level corresponding to the demand that includes the additional “unexpected” amounts. Moreover, it is surely unreasonable to suppose that the price rises before any extra demand has appeared. Therefore, whichever of the two directions you choose, it seems difficult to represent our daily experiences. What is a standard theory that cannot explain standard common experiences? This book tries to present a new economic theory which draws on our daily economic experiences, in order to ask and answer the simple but important question: “how can you get what you want, anytime and anywhere?”. The basic structure of the book is as follows. Chapter 1 investigates the presupposition of the standard price theory and offers an alternative supposition of the economic agent. The standard price theory can be considered the outcome of the presupposition of an agent, that is, Homo-economicus. To be free from the standard theory, we need to rebuild the new image of an economic agent by basing it primarily on our daily experiences. The criticism against the concept of homo-economicus leads us to criticize the neoclassical image of the firm that, in principle, does not distinguish between an individual producer and an organizational firm. The critical history of homo-economicus is summarized here, which is in a sense the history of heterodox economics, where we can find an origin of evolutionary economics as well. Chapter 1 fixes the historical and theoretical foundation of this book, introducing a new type of economic agent. Chapter 2 presents an original new price theory. The price in this theory is no longer supposed to be a mere parameter which coordinates demand and supply. The new role of price is reformulated as a necessary condition for sustaining the reproduction system. Furthermore, this new price theory shows that the condition of demand plays no role in the making of prices. If the price does not take the role of coordinating demand and supply, what elements do instead? It is the inventory, of course. Firms try to decide on production which includes consumers’ unexpected demand to a certain degree. That is why you can satisfy your sudden demand without waiting for the price to rise. Nevertheless, if the planned production is unfortunately short of demand, and the demand is not satisfied even after the whole inventory is released, what will happen? In such a case, it only comes to an end when some customers cannot buy what they need, and they have to wait for the next delivery. There is no reason for the price to rise in this case. If firms intend to raise their prices because of the shortage of supply, they will lose their good reputation in the eyes of customers. Price coordination is no longer the choice that firms can take in the real situation. Based on that, this book offers an original new production theory which is true to the experimental presupposition from Chapters 3–6. In a situation where demand exceeds the inventory, customers principally have to wait. However, some customers will try to get the good through a second-hand or an online market, where prices may not be based on production costs. This book seems to admit that the standard price theory can be applied to these situations, especially in Chapter 7, and it seems rather odd because this book has stressed the essential difference between the original price-production theory and the standard theory. However, their new price-production theory is created for commodity markets, that is the flow market, but the second-hand market is basically the stock market. The authors reasonably distinguish the price mechanisms in the flow market and the stock market. In that sense, they seem to follow the genealogy of the dichotomy of price theories which was founded by M. Kalecki. However, while the traditional price dichotomy is based on the difference of market structures, that is, whether the market is competitive or monopolistic, the dichotomy shown in this book is not based on the market structure, but on the different dimensions of the market (in other words, whether the market is the flow market or the stock market). It can be said that the dichotomy in this book is more general than it used to be. The authors argue that the price mechanism in a stock market is their future subject, but Chapter 7 displays the scope of their book is not restricted to the production theory in the narrow sense.",2
18.0,2.0,Evolutionary and Institutional Economics Review,25 March 2021,https://link.springer.com/article/10.1007/s40844-021-00209-1,Psychology of evolutionary economic behaviour,September 2021,Brendan Markey-Towler,,,Male,Unknown,Unknown,Male,"Why should we, as evolutionary economists, care about psychology? The current vogue for behavioural economics notwithstanding, why ought we not follow Paul Samuelson (Wong 1978) in purging economic theory of all psychological content? Why not found our view of economies as complex evolving systems on the sure footing of tautological revealed preference theory? One answer—the answer we will adhere to—is that this is intellectually and scientifically unsatisfying. It leaves the behaviour change that drives evolutionary dynamics in the economy largely unexplained as the outcome of a tautology: preference-maximising behaviour changed because the behaviour that maximises preferences changed. It leaves largely unexplained the process by which new technology—understood in the broadest sense as a new method of combining resources in production or consumption—originates and is adopted by a self-organising process of diffusion manifesting in behaviour change. In standard neoclassical models (Swan 1956; Solow 1956; Romer 1986, 1990; Aghion and Howitt 1992) the reason for behaviour change that leads to the adoption of new technology is almost tautological. Behaviour is determined by the most profitable course of action, and technology is defined as that which changes the most profitable course of action. While this model has a point that is worth maintaining, evolutionary economics ought to have a better model than this. Behaviour change to originate and adopt new technologies is the core dynamic by which evolutionary change in complex economic systems is brought about. Now, canonical models of evolutionary economics (Nelson and Winter 1982; Metcalfe 1998) have a basic consonance with cognitive psychology in any case, with its canonical models firmly grounded in the “behavioural theory of the firm” (Cyert and March 1963; March and Simons 1958; Simon 1947) of production and consumption being determined by technologies manifesting in “routines” or “rules” (Dopfer 2004). But there is increasing recognition that a psychological perspective of greater depth on behaviour change that causes the origination and adoption of new technologies offers value for evolutionary economics. This is especially the case for work led by Almudi et al. (2021), who continue to expand on the importance of “absorptive capacity” within the economy for the adoption of new technologies (Cohen and Levinthal 1990; Zahra and George 2002), as well as the value of understanding “utopia competition” in the shaping of attitudes across complex economic systems (Almudi et al. 2017a; b; Novak 2019). It is also the case of monumental work led by Witt (1999, 2001) and Herrmann-Pillath (2013) as well as (to an extent) Saad (2007) who seek to ground evolutionary economics in (essentially) evolutionary psychology and infuse naturalistic biological perspectives into the theory of human behaviour in complex economic systems. We take a slightly different approach to these writers in proposing that a still deeper integration between psychology and evolutionary economics has value to offer by offering us a new perspective on the dynamics that underlie the behaviour that drives evolutionary change. Evolutionary psychology and a naturalistic biological perspective underlie ours, but we also draw on a broader range of psychology and neuroscience to understand the behaviour change by which self-organising processes origination and adoption of new technologies by diffusion occurs. Our approach also incorporates elements of the subdiscipline known as behavioural economics but is not based primarily on this literature. Behavioural economics, strictly defined and excluding work more in cognitive psychology such as that of Gerd Gigerenzer, originated in a series of experiments designed to demonstrate inconsistencies between observed behaviour and the predictions of canonical rational agent models (Thaler 2015, 1992; Lewis 2016). Generally speaking (with the exception of Kahneman’s dual-process psychology discussed below), this field has tended to approach the integration of psychology and economics as an exercise in introducing “distortions” into rational agent models to account for this empirically identified deviation from rationality or that. The problem with this is that the resulting models begin to resemble complex and intractable Ptolemaic astronomy, with its epicycles modifying the perfectly circular orbits of celestial bodies around the earth to account for this observed deviation from predicted location or that. Behavioural economics thus becomes somewhat intellectually unsatisfying as an integration of psychology and economics due to its piecemeal, largely critical (contra constructive) approach, and scientifically unsatisfying insofar as it rapidly becomes analytically intractable. Instead of drawing on psychology to modify rational agent models, in this work we draw psychology directly into the foundation of our model of behaviour. In this work, we draw on cognitive, personality, affective, social and behaviourist psychology and neuroscience to inform this perspective, and experiment with Jungian analytical psychology, before drawing these together within an integrated model of human behaviour as the outcome of psychological driving and restraining forces. We then indicate the importance of overcoming restraining forces relative to driving forces in behaviour change. By way of example of what value this model can create for us in evolutionary economics, we show how it elaborates a new depth of understanding of the dynamics underlying self-organisation by meso-rule diffusion (Dopfer et al. 2004; Dopfer and Potts 2007)—a popular model of the evolutionary process in complex economic systems. We conclude by indicating avenues for further research in formalisation, adapting psychometric techniques to evolutionary econometrics, and development of appreciative theory. We do not intend to provide an end-point for research into the psychology of evolutionary economic behaviour, but rather a contribution to an invigorated cross-disciplinary research program.",1
18.0,2.0,Evolutionary and Institutional Economics Review,25 July 2021,https://link.springer.com/article/10.1007/s40844-021-00218-0,The stabilizing effect of fiscal policies on the dynamics of effective demand and income distribution in Japan,September 2021,Ryunosuke Sonoda,,,Male,Unknown,Unknown,Male,"In this study, we investigate the possibility that Japanese fiscal policies have a stabilizing influence on the dynamics of effective demand and income distribution. For this purpose, we build a Kaleckian model that incorporates fiscal policies. Since Bhaduri and Marglin (1990), Kaleckian models have focused on how a change in income distribution influences effective demand and have shown that various demand regimes can exist. However, basic Kaleckian models assume that income distribution is exogenously given and do not consider how fluctuations in effective demand influence income distribution. To capture the relationship between effective demand and income distribution, we must focus on not only demand regimes but also distributive regimes that show how wage share changes over the business cycle. The characteristics of distributive regimes are determined by the institutions of the labour market in societies. By deriving both demand and distributive regimes, we can specify the dynamics of effective demand and income distribution and investigate their stability. Recently, many Kaleckian models incorporating both demand and distributive regimes have been developed. However, few empirical studies have been conducted in this field. Barbosa-Filho and Taylor (2006) and Tavani et al. (2011) are valuable studies that estimate the demand and distributive regimes in the US. They show that the US economy exhibits a profit-led demand regime and a pro-cyclical wage share regime. These estimated results indicate that the dynamic system of effective demand and income distribution is stable in the US. Similarly, Proano et al. (2007) estimate demand regimes and distributive regimes in the US and the Eurozone and confirm that both areas exhibit a profit-led demand regime and a pro-cyclical wage share regime. Few studies have attempted similar estimations in Japan. Sonoda (2017) is one of the few studies that estimate both demand and distributive regimes using data on the Japanese economy. Sonoda’s (2017) estimated results show that the Japanese economy exhibits a profit-led demand regime and a counter-cyclical wage share regime. As noted below, these results suggest that the Japanese economy exhibited instability and that some factors stabilized the dynamics. It is extremely important, both theoretically and empirically, to examine which policies are effective in stabilizing the system in a society where a combination of demand and distributional regimes may destabilize the system. Several previous studies have examined the effect of monetary policy on the stability of the Kaleckian model. For example, Proano et al. (2011) demonstrate a model that focuses on stabilization through monetary policy. Rezai (2015) analyzes the effect of monetary policy on the dynamics of effective demand and income distribution in the US. Few empirical studies of the Kaleckian model examine the stabilizing effects of fiscal policy; however, examining whether counter-cyclical fiscal policies aimed at effective demand management have actually contributed to macroeconomic stabilization is significant for Kaleckian theory, which emphasizes effective demand and income distribution. Therefore, in this study, we add the government sector to Sonoda’s (2017) model and investigate the effect of fiscal policies on the stability of the dynamics of effective demand and income distribution in Japan. We obtained two main findings. First, even if the combination of demand and distributive regimes has the potential to destabilize the system, the system may stabilize if counter-cyclical fiscal policy has a sufficient effect on effective demand and if the government does not take an excessive budgetary austerity stance. Second, in Japan, the dynamics of effective demand and income distribution may have been unstable; and fiscal policy has not contributed to systemic stability. Third, the main reason Japan’s fiscal policy did not have a stabilizing effect was not the stance of the government’s fiscal policy, but rather the insufficient effect of the policy on effective demand. These results suggest the existence of different factors that made the Japanese economy stable. This paper is organized as follows. Section 2 specifies the demand regime, distributive regime, and fiscal policy regime, and constructs a dynamic model of the rate of capacity utilization, wage share, and government expenditure. Section 3 investigates the stability conditions of the dynamics. Section 4 estimates the parameters of dynamics in Japan from 1977 to 2007 and analyses whether fiscal policies stabilized the dynamics. Finally, Sect. 5 concludes the paper.",
18.0,2.0,Evolutionary and Institutional Economics Review,02 August 2021,https://link.springer.com/article/10.1007/s40844-021-00219-z,Economic populism and institutional changes in wage–labor relations,September 2021,Emre Ünal,,,Male,Unknown,Unknown,Male,"According to Commons (1934), the creation of a stable economy requires an authority that can bring individuals to a common point in order to reduce conflicts. This authority, created by government or legislators, plays the role of a decision-making mechanism (Gruncy 1973). What emerges from Commons’ view is that social unrest cannot be resolved without an institutional approach. If there is no political will, conflicts of interest occur between individuals in an economy. If there are two individuals in the economy, the government can use its authority to create a balance that avoids power conflicts arising from each individual acting in his own interest. To reduce conflict in working life, Commons (1934: 342–45) highlighted the government’s ability to introduce collective bargaining to labor organizations, luring workers away from ideas of cooperative production and socialism. Governments can play a legislative role in the relationship between industries and workers. This development was also a criticism of both liberal economists and communists, because the role of institutions could not been foreeseen by them. Veblen (2004) questioned constitutional government, because for him, such a government is effectively a business government, and is not an institution that can be relied on to produce successful regulations and economic policies (Tilman 2007). Veblen claimed that the inclination of government is to support business ends instead of those of the general public (Atkinson 2007). Furthermore, constitutional government is a business government, and political parties reflect little more than different circles of business interest (Plotkin 2012). Government is a political tool in institutional economics. For Commons, government can work to create a balanced relationship between individuals. However, for Veblen, government merely works to protect the interests of business enterprises (Spithoven 2018). Nevertheless, via institutional changes in wage–labor relations, governments can be a tool of significant support for the working class by putting less emphasis on the interests of industries. The main issues addressed in this work are as follows: first, when economic populism emerged and wage–labor relations were institutionalized, it caused a path dependency in the Turkish economy. Second, what was the connection between economic populism and institutional changes that resulted in chronic inflation in the economy? There can be many definitions of economic populism. In this work, economic populism is held to mean when the government starts to use wages and the exchange rate to protect the purchasing power of the working class with little concern about the risks of inflation, instability in the exchange rate and other macroeconomic imbalances.Footnote 1 Economic populism emerges when there is a potential for conflict between individuals and the government acts in the interests of the working class. It begins instituting wage growth that is far above the rational rate and legitimate wage increases, effectively neglecting industrial interests. This means that wage growth is not limited in accordance with, for example, productivity growth. This can stimulate inflation and cause large depreciations in the exchange rate. In the Turkish economy, the government does not behave like a branch of industry, but as a proponent of the working class by promoting economic populism to attract the support of middle-class society. If collective bargaining is influenced by the hand of government, then economic populism can emerge. This non-rational regulatory power can serve to benefit the government and shore up its political position, especially if there is strong competition between political parties. However, this economic populism does not satisfy the interest of business and that of the working class, as it stimulates high inflation and causes depreciation in the exchange rate. Ultimately, this can cause dissatisfaction for the working class and lead them to demand yet higher wages to cover their purchasing power against inflation, from which develops a wage–inflation spiral. Globally, there is a rising trend in populism centering around insecurity, equality, immigration, and other political factors (Inglehart and Norris 2016). For instance, Brexit is a politically populist act in support of the United Kingdom’s leaving the European Union. The recent policies of the US against China can serve as another example, intended to increase its competitive position in global trade by using protectionism in order to attract the support of American middle class society. The yellow vests movement in France has resulted in economic populism. The French government promised to increase the real minimum wage, reduce taxes, and improve the economic situation of the working class (Ünal 2020). There has been considerable research into populism in Latin American economies, especially in the case of macroeconomic populism (Pereira and Dall’Acqua 1991). Recently, globalization became caught up with populism. Rodrik (2018) analyzed populism in Europe and Latin America and its reaction to globalization. Aytaç and Öniş (2014) compared political populism in Turkey and Argentina. However, research has concentrated far more on political than economic populism. For Turkey, the literature of economic populism is quite limited. I will explain how economic populism emerged in the Turkish economy via institutional changes that gave way to a path dependency that caused the economy to suffer for decades. Moreover, I will explain how institutional changes created an element of political risk, whereby a political party falls from government if it fails to satisfy the working class. Political risk increases when a government seeks to implement institutional changes which weaken the position of the working class. This work clearly shows which institutional changes in wage–labor relations, starting in the mid-1960s, gave rise to an economic populism that destabilized and damaged the Turkish economy. In addition, the work describes how these institutional changes grew into an obstacle in the evolutionary path of the economy. The case of the Turkish economy can shed light upon the condition of other developing countries facing deep macroeconomic problems caused by economic populism. The paper is organized as follows: In Sect. 2, the definition of economic populism, its forms and its connection with wage–labor relations are discussed. In Sect. 3, how economic populism developed historically is explained. In Sect. 4, the fading of economic populism in the 2000s is demonstrated by focusing on a number of institutional changes. Sections 5 and 6 consist of a political discussion and conclusion. Appendix shows the method of calculating productivity growth and real exchange rates using input–output analysis.",
18.0,2.0,Evolutionary and Institutional Economics Review,21 August 2021,https://link.springer.com/article/10.1007/s40844-021-00221-5,The pausing view of unemployment,September 2021,Vipin P. Veetil,,,Unknown,Unknown,Unknown,Unknown,,
18.0,2.0,Evolutionary and Institutional Economics Review,28 February 2021,https://link.springer.com/article/10.1007/s40844-021-00203-7,"Franklin Obeng-Odoom, Property, institutions, and social stratification in Africa, Cambridge University Press, Cambridge, 2020",September 2021,Toyomu Masaki,,,Unknown,Unknown,Unknown,Unknown,,
18.0,2.0,Evolutionary and Institutional Economics Review,14 June 2021,https://link.springer.com/article/10.1007/s40844-021-00215-3,Correction to: Analyzing outliers activity from the time-series transaction pattern of bitcoin blockchain,September 2021,Rubaiyat Islam,Yoshi Fujiwara,Hiwon Yoon,Unknown,Male,Unknown,Male,"The article “Analyzing outliers activity from the time-series transaction pattern of bitcoin blockchain”, written by Rubaiyat Islam, Yoshi Fujiwara, Shinya Kawata and Hiwon Yoon, was originally published Online First without Open Access. After publication in volume 16, issue 1, pages 239–257 the author decided to opt for Open Choice and to make the article an Open Access publication. Therefore, the copyright of the article has been changed to © The Author(s) 2021 and the article is forthwith distributed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License, which permits any non-commercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder to view a copy of this licence, visit http://creativecommons.org/licenses/by-nc/4.0/. The original article has been corrected.",
18.0,2.0,Evolutionary and Institutional Economics Review,12 June 2021,https://link.springer.com/article/10.1007/s40844-021-00216-2,Correction to: Unfolding identity of financial institutions in bitcoin blockchain by weekly pattern of network flows,September 2021,Rubaiyat Islam,Yoshi Fujiwara,Hiwon Yoon,Unknown,Male,Unknown,Male,"The article “Unfolding identity of financial institutions in bitcoin blockchain by weekly pattern of network flows”, written by Rubaiyat Islam, Yoshi Fujiwara, Shinya Kawata and Hiwon Yoon, was originally published Online First without Open Access. After publication in volume 18, issue 1, page 131–157 the author decided to opt for Open Choice and to make the article an Open Access publication. Therefore, the copyright of the article has been changed to © The Author(s) 2021 and the article is forthwith distributed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License, which permits any non-commercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc/4.0/. The original article has been corrected.",
18.0,2.0,Evolutionary and Institutional Economics Review,29 March 2021,https://link.springer.com/article/10.1007/s40844-021-00211-7,Correction to: Capital-labor conflict in the Harrodian model,September 2021,Takashi Ohno,,,Male,Unknown,Unknown,Male,"The article Capital-labor conflict in the Harrodian model, written by Takashi Ohno, was originally published online on 9 March 2021 with Open Access under a Creative Commons Attribution 4.0 International License (https://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. With the author’s/authors' decision to cancel Open Access the copyright of the article changed on 17 March 2021 to © Japan Association for Evolutionary Economics 2021 with all rights reserved. The original article has been updated.",
18.0,2.0,Evolutionary and Institutional Economics Review,28 August 2021,https://link.springer.com/article/10.1007/s40844-021-00223-3,Special feature: promenade in the history of economic thought,September 2021,Kiichiro Yagi,,,Unknown,Unknown,Unknown,Unknown,,
18.0,2.0,Evolutionary and Institutional Economics Review,12 January 2021,https://link.springer.com/article/10.1007/s40844-020-00196-9,"Marx’s theory of capital in the history of economics: Marx’s concept of capital, classical school, Austrian School, and growth theory",September 2021,Kiichiro Yagi,,,Unknown,Unknown,Unknown,Unknown,,
18.0,2.0,Evolutionary and Institutional Economics Review,25 March 2021,https://link.springer.com/article/10.1007/s40844-021-00210-8,The discontinuity between value and price form: tracking the subtraction of the qualitative,September 2021,Uroš Kranjc,,,Male,Unknown,Unknown,Male,"Recall the old parable from Adam Smith’s Wealth of Nations about a butcher who seldom sees his beef or his mutton directly exchanged for either bread or beer. Instead of bartering, he brings the products of his labour to the market and exchanges them for money, but only just so he can immediately use it as a means for acquiring other products of labour from his fellow workers. His commodity is being exchanged on the market for money, as if it were a completely natural and historically universal task. On the other hand, his actions seem to emerge from some unconscious habit, creating a kind of routine, changing forms from a money commodity to other commodities, back and forth. But what did the butcher have to do beforehand? He had to invest his own physical body, time, and energy in a determinate work activity to produce a finished product, one that could be exchanged on the market. Smith’s use of such rudimentary parables uncovers the simple grounding and justification for his labour theory of value, i.e. production’s concord with human activity, the metamorphoses of merchandise into different commodities, and the site of their circulation and exchange—the market. Where once we encountered shepherds, blacksmiths, and porters, today we see new professions, such as influencers, fitness and lifestyle coaches, personal data managers, and so on. However, their actions remain much the same; they all bring to the market either their crafted products or, as is more usual today, their know-how, while the unfortunate ones can still offer only their own work-capacity—being free to possess and make use of their own labour-power—for exchange among equal but diversified merchandise. It does not really matter who the individual subjects, whether early craftsmen or today’s service or knowledge workers, are. This simple story depicts the very persistence of the dynamic in a historical mode of production, i.e. the capitalist mode of production, nowadays conveniently and metonymically referred to as the market economy. It encompasses all the required economic categories and principles necessary to understand the basic working of a classically-conceived market-evolved community. Smith also makes it obvious from the outset that for him, labour represents the prime category, on which all other categories rest. The labour category not only entails the true essence of man, or conversely, his abilities to reap the wealth of nature, for Smith it also possesses the predicate of labour’s continuous divisibility. This attribute can be conceived as the general contribution to the evolution of eighteenth-century capitalism: the division of labour as a core principle and rationale for new economic organization of productive forces in a new becoming of civil society. One has to ask whether the labour category was introduced and discursively centred by Smith only to derive its much more powerful correlate, that of its division, resulting in the everlasting specialization of work. The division of work processes into more elementary activities can be seen through a historical analysis, where it was soon obfuscated by a different and complimentary process, namely automation and standardization. It, for example, induces individuals to acquire new skills, necessary for them to achieve higher productivity. As was true in Smith’s lifetime, so it is today: this process enhances the comparative advantages of whole nations. At the heart of it all lies the fundamental (philosophical) principle of self-interest; individuals and their atomized dexterities provide for better quality final products and through exchange result in rising utility for everyone. A further point. Before individual labour creates utility by consuming its results, it more significantly represents the sole source of value. Now, this category of value is of quite a particular character, since it embodies a scission (before drawing on antagonistic opposites in dialectics)—an opposition between use value of the product (utility) at its consumption stage and exchange value relating together other commodities (power of purchasing) at a preceding stage of exchange on the market. If labour time is the concrete sole source of value, why do then the commodities exchanged on the market need a price tag? Why not just use invested work hours to evaluate their exchange values? Smith was fully aware of the fact that people have different talents and levels of skill and knowledge, i.e. specialization, that makes this kind of scaling and comparisons both qualitatively and quantitatively difficult and undesirable. Eventually, price-mediated exchange comes to the fore, bringing alongside it the money commodity, a pure metal, a simple coin, a paper banknote, cryptocurrency, etc.—the means of measure in exchange and payment. Smith’s proposal has come full circle: production (labour and its division)—circulation (specialization of skills and tasks)—distribution and exchange (exchange values, prices and money on the market)—consumption (utilities or/and re-entry into production). The dialectical interplay between nature appropriation, along with labour capacity and its division, brings us successively to price-mediated money market exchange. A word of caution is pertinent for that particular time of classics: the principle of the division of labour and its magnitude do not yet give the final answer on how people come to intersubjectively value their products of labour. In this article, we will try to trace, illuminate and articulate the “scientific break” from the objective to subjective value that occurred when economic science progressed from its classical period, proceeding by its critique (of political economy) and finishing up with the marginalist revolution. This body of theory and discourse remains prevalent today in economic historiography. The first part of the article will examine historical cornerstones and steps that constitute the ‘Accepted view’ of this transition, pertaining to the times before Alfred Marshall finally put all the fragmented pieces into his Principles of Economics. A special section will look at the old mercantilist and subjectivist line of argument that led to the complete abolition of the labour theory of value. We touch upon the rise of Walrasian equilibrium and expound on the undecidability of a general equilibrium point in terms of its computability, examining the consequences of this fact for the category of value. A further sub-section will move the debate to a modern-day setting with Friedrich A. von Hayek and the individual’s planning at heart of today’s broader mainstream orientation. The second part will seek to extricate the return to value form from the present price form by tracking qualitative changes (from objective to subjective and back) relating to Karl Marx’s notion of relations of production and Alfred Amonn’s social relations among individuals. What connects the first and the second part is a crucial qualitative loss/subtraction in conceptuality observed in these ‘scientific’ historical shifts. Our thesis and elaboration in the third part will be that this endless displacement is a consequence of inadequate concept-building, one that can be theoretically outlined with the misapprehension and loss of the concept of real abstraction in economic science—a problem already highlighted by Marx and emphasized by Theodor W. Adorno and “The New Reading of Marx”.",
18.0,2.0,Evolutionary and Institutional Economics Review,22 April 2021,https://link.springer.com/article/10.1007/s40844-021-00213-5,Was Adam Smith an evolutionary economist?: Smith’s study of botany and Charles Darwin’s theory of morality,September 2021,Tetsuo Taka,,,Male,Unknown,Unknown,Male,"The standard and prevailing understanding of Smith’s theory of sympathy can be summarized as follows. The spectator’s sympathies to the imaginary change of situations must be examined in its proprieties of conduct, and repetitions of their approbations and disapprobations gradually lead to the formation of social norms or morality as the general rule of conduct. This seems essentially true, although too broad and abstract to distinguish the features of Smith’s theory of sympathy into relief. However, there seem to be two decisive clues, hitherto overlooked, to reconstruct the core argumentsin The Theory of Moral Sentiments (hereafter TMS). First, why did Smith add a long subtitle “or, An Essay towards an Analysis of the Principles by which Men naturally judge concerning the Conduct and Character, first of their Neighbours, and afterwards of themselves” to the main title of The Theory of Moral Sentiments in the 4th edition? Second, why did he capitalize the word “nature” in nearly fifty instances in the 4th edition,Footnote 1 and what did it imply? Although Smith made no reference to these changes, they appear to be important emendations that we cannot dismiss altogether. To answer these questions, it is pertinent to note that Smith was in London and made strenuous efforts to complete the manuscript of An Inquiry into the nature and causes of the Wealth of Nations (Smith 1976b, hereafter WN) in 1774. He wrote to Andreas Holt in October 1780: “I continued to live in Kirkaldy for six years in great tranquillity, and almost in complete retirement. During this time amused myself principally with writing my Enquiry concerning the Wealth of Nations, in studying Botany (in which however I made no great progress) as well as some other sciences to which I had never given much attention before” (Smith 1977, 252. Hereafter Corr.). Smith’s basis for capitalization of the word “nature” was due to his newly acquired knowledge, possibly from C. Linne’s Systema naturae 12th edition (1768–70), that animals possess innate instincts.Footnote 2 This new biological knowledge of instinct acquired in the periods, as we see later, was fully utilized in the changing of “nature” to “Nature.” However, we have no textual evidence that his biological study directly helped in the formation of “the nutritional theory of value” in WN. Although Lectures of Jurisprudence (Smith 1978) does not include any theory pertaining to land rent, chapter 11 of WN titled “Of the Rent of Land,” where Smith emphasizes his nutritional theory of value, accounts for nearly 45% of Book I. This indirectly demonstrates the extent of Smith’s efforts toward studying agriculture, agricultural technologies, and the development of new strains of farm products after his return from France.Footnote 3 D. Hume’s response to WN indicates his failure to grasp the uniqueness of Smith’s theory of rent: “Euge! Belle! Dear Mr Smith: I am much pleas’d with your performance,” Hume further continues, “I cannot think, that the rent of farms makes any part of the price of the product, but that the price is determined altogether by the quantity and the demand” (Corr. 150, 1 Apr. 1776, from Hume to Smith). More interestingly, however, the proponent of the theory of evolution, C. Darwin, mentioned and highly praised Smith’s theory of sympathy in Part I of TMS in The Descent of Man ([1871] 1874). The close similarities in their discourses on the formation of morality is surprising, eliciting questions regarding the parallels between the two. The idea that Smith might be an evolutional economist without knowledge of Darwinian evolutionary science,Footnote 4 therefore, seems significant and stimulating for both Smith scholars and evolutionary economists, so the argument consists of the following: first, examinations of adding the subtitle and capitalization of the word “nature” in the 4th edition of TMS; second, a brief examination of the nutritional theory of value in WN; third, comparison between Smith’s theory of morality and Darwin’s theory; and finally, concluding remarks.",
18.0,2.0,Evolutionary and Institutional Economics Review,12 January 2021,https://link.springer.com/article/10.1007/s40844-020-00194-x,"Thorstein Veblen on economic man: toward a new method of describing human nature, society, and history",September 2021,Noriko Ishida,,,Female,Unknown,Unknown,Female,"Since its emergence, the field of economics has addressed the mechanisms of the production, exchange, and distribution of wealth. The question arises as to what the prime mover of these mechanisms is. Thorstein Veblen (1857–1929) would, without hesitation, have replied that it is not the invisible hand of God or the providence of nature, but human existence. Veblen, who was a keen critic of orthodox economics, coined the term “neo-classical.” Concerning the subject of human action, a pivotal change in the history of economic thought occurred with the abstraction of the human model, and it accelerated such a movement during the last quarter of the nineteenth century. Since the neo-classical human model was precisely uniform, economists developed a highly mathematical description of the human being. As a result, economics assumed a position equivalent to natural science. However, some schools, such as the German Historical School, warned against the acceptance of the abstract human model and called for the redefinition of the scientific method of economics; some scholars of the United States were in agreement. Veblen, a post-Darwinian scientist, was one of them. He was convinced of the need to replace the excessively abstract neo-classical model of human nature with a more matter-of-fact formulation of human action, and therefore adopted not only a brand-new method of interpreting non-teleological historical processes but also the original way of acknowledging the active, or teleological, action of human beings.Footnote 1 Needless to say, his attempt included the Kantian methodological approach to teleology. Veblen examined the widely accepted methodology of economics and suggested its “rehabilitation” (EW Jorgensen and HI Jorgensen 1999, 194; Veblen 1919, 56) in the post-Methodenstreit age of the late nineteenth century. How do we describe a human being in economics? The previous debate on this issue focused on whether Veblen’s critique of “economic man” would mean the assertion that economics must accept the more realistic human model. Most scholars responded in the affirmative to this interpretation. Jensen (1987) argued that in addition to Veblen, a long list of institutional economists subscribes to the approach of the pursuit of reality when describing human nature as one of the common theoretical characteristics. However, Veblen’s assertion implies more than that. The principal difference between Veblen and others is a unique idea of instincts at the core of the theory of human nature. According to Mayberry (1969), Veblen’s hidden normative point of view is embedded in his idea of human instincts. A normative model of economic society would be described by the intermediary of the concept of instincts; for example, a sense of value to “make directly for the material welfare of the community” (Veblen 1914, 25) would be implanted in the instinct of workmanship, while a sense of parental bent would have “a large part in the sentimental concern entertained by nearly all persons for the life and comfort of the community at large, and particularly for the community’s future welfare” (Veblen 1914, 27). However, great care should be taken in interpreting Veblen as the value-neutral scientist typified by Samuels (1990). Such a position implies that Veblen’s value judgment should not be included in his historical perspective and future forecast, just as there is no implication of progress or excellence in the Darwinian concept of evolution.Footnote 2 An issue unexpectedly received scant attention in the previous studies on Veblen. For instance, Jensen (1987) regarded the institutionalists only as methodological opponents of orthodox economics as it treated the difference from neo-classicals as very crucial. Meanwhile, the argument of Samuels (1990) focused entirely on the scientific methodology as it stressed his image of a rigorous scientist. Moreover, Mayberry (1969) was certainly a socio-philosophical consideration as it tried to extract the “ideal of human behavior,” or a kind of “moral framework” from the world Veblen drew (322). Although all of these interpretations appear persuasive, there is no obvious answer concerning which of them is correct. However, in a strict sense, these studies are not from the standpoint of economics or economic methodology.Footnote 3 Therefore, identifying the type of “economic phenomena” on which Veblen’s revised explanatory model for economic action could successfully shed light would still be an open question. In this paper, the examination of this problem does not intend to contradict the existing interpretations. It merely attempts to explain the implications of Veblen’s critique of neo-classical economics with the inclusion of a comprehensive review of his theoretical results. In other words, this study examines his methodology of economic scienceFootnote 4 together with his analysis of the economic society of the day in works such as The Theory of the Leisure Class (Veblen 1899), The Theory of Business Enterprise (Veblen 1904), The Instinct of Workmanship and the State of the Industrial Arts (Veblen 1914), and The Engineers and the Price System (Veblen 1921). Previously, both his methodology and his analysis of economic phenomena were often discussed separately. Therefore, perhaps surprisingly, a conscious attempt at understanding his contribution to both fields together is still to be made. This study aims to examine Veblen’s ambitious methodology from three specific perspectives—the theory of human nature, the view of history, and the concept of causal cognition. The remainder of the paper is organized as follows. Section 2, which deals with Veblen’s theory of human nature, examines the meaning of his analysis of economic action using a unique concept of instincts. In his methodology, the human model, which is given a sociality by instinct, dynamically renovates the interpretation of society to form a worldview that can relativize the historical process. Section 3 discusses his logic, and the study shows that the Veblenian model of economic action is in sharp contrast to orthodox economics. Finally, Section 4 presents that Veblen’s methodology of economic science, based on his theory of human nature and view of history, was original in that he recognized the causal relationship between economic phenomena focusing on the facts belonging to the qualitative category. The purpose of this article is, from the perspective of economics or economic methodology, to identify the economic phenomena that Veblen’s revised model of human nature successfully explains. As a conclusion to this study, the reason that the concept of “economic man” should be modified is, according to Veblen, because we need to examine economic phenomena from the viewpoint of the qualitative category to overcome the limitations of the traditional neo-classical model. In doing so, we can grasp the problem of value concerning people’s actual utility or happiness, deeply examine the various aspects of differences in efficiency, and broaden the possibility of interpreting capital, assets, cost, technology, and knowledge in society.",
18.0,2.0,Evolutionary and Institutional Economics Review,04 June 2021,https://link.springer.com/article/10.1007/s40844-021-00214-4,The influence of Tidology and Astronomy in Alfred Marshall’s economics: a reassessment of his economic method for the analysis of periodic and secular variations,September 2021,Naoki Matsuyama,,,Male,Unknown,Unknown,Male,"Economic analysis aims to explain economic phenomena as a causal relationship based on observation and practice. For this purpose, Alfred Marshall (1842–1924) focused on measurable human motivation and laws regulating human behaviour. His partial equilibrium theory is considered an effective method for representing a standard or normal value of commodities. This quantitative and mechanical approach has enabled us to express the relationship between the commodity market and the capital market as a sequential economic equilibrium. However, this theory is effective only if an assumption is posited, that is Ceteris Paribus, all other things being equal. Under these conditions, economic theory can express ordinary economic transactions in the market mathematically and graphically. In some ways, the visualisation of economic transactions based on mathematical analysis of rational economic behaviour, could be considered a scientific revolution in the development of economic theory. For Marshall himself, however, the scope of this mechanical approach was limited. In his masterpiece Principles of Economics (henceforth, Principles), Marshall confirmed that ‘[t]he statical theory of equilibrium is only an introduction to economic studies’ (Marshall [1920] 1961, 461). In particular, he was cautious about the framework of supply and demand analysis in the commodity market when applied directly to the study of the labour market, especially in the theory of wages. Marshall thought that ‘the differences between two cases, though not fundamental from the point of view of theory, are yet clearly marked, and in practice often very important’ (Marshall [1920] 1961, 336). Therefore, the analysis of the labour market required a different approach to the analysis of the commodity market. In Book VI of Principles, Marshall thus studied the secular (ultra-long-term) variations of capital and labour as ‘the distribution of the national income’ to investigate ‘the influence of progress on value’ by means of another framework, considering economic process as qualitative or organic change. As early as 1898, Marshall described economic change as ‘“Progress” or “evolution,” industrial and social, [which] is not mere increase and decrease. It is organic growth, chastened and confined and occasionally reversed by the decay of innumerable factors, each of which influences and is influenced by those around it’ (Marshall 1898, 42–43). For this reason, the analysis of secular movements has been referred to as an organic growth theory. It is known that Marshall discussed how economic processes pursued economic stability through differentiation and integration of industrial organization.Footnote 1 He thus assumed that economic change must be continuous and gradual. In this respect, the dictum ‘Natura non facit saltum’ (nature does not make leaps) was used for the motto of Principles. Also, in his second book Industry and Trade (henceforth, I&T), he indicated the importance of the analytical view of economic development as ‘Natura abhorret saltum’ (nature does not like to leap). In his third book Money Credit, and Commerce, Marshall confessed that ‘although old age presses on me, I am not without hopes that some of the notions, which I have formed as to the possibilities of social advance, may yet be published’ (Marshall 1923, v–vi). Until his old age, Marshall continued to conceive a scheme involving the biological analogy to understanding economic movements as phenomenon of continuous and organic changes.Footnote 2 However, it is still unclear whether Marshall’s organic growth theory is his vision or his social philosophy. At the end of the ninteenth century, Thorstein Veblen criticized Marshall’s economics, including organic growth theory. According to Veblen, ‘[f]or the purpose of economic science the process of cumulative change that is to be accounted for is the sequence of change in the methods of doing things’ (Veblen 1898, 387), but ‘[t]he modern scientist is unwilling to depart from the test of causual relation or quantitative sequence’ (Veblen 1898, 377). Veblen’s economics therefore rejected theoretical perspectives based on the stability of equilibrium point. Furthermore, he believed economics should be an evolutionary science considering the process of cumulative change, not a formulation of natural laws. In this respect, he recognized Marshall’s economics as having a ‘quasi-evolutionary tone of the neo-classical political economy’ (Veblen 1900, 267–268). Deploying Veblen’s critique of Marshall’s vision of economic evolution, it is necessary to investigate whether Marshall’s economics is quasi-evolutionary or not. In this regard, this paper argues Marshall demonstrated not only a biological approach to the organic growth of economy, but also another approach influenced by a science of tendencies, based essentially on tidology applied to the theory of wages in the secular (ultra-long-term) analysis. As an introduction to organic growth theory, we consider the normative and theoretical argument, ‘how should wages be determined?’ in Marshall’s economics. To achieve a gradual promotion of social progress, Marshall’s theory of economic progress implies the realisation of fair wages for the working class through conciliation in wage bargaining. This present article therefore looks into Marshall’s economic methodology influenced by the science of tendencies in the making, which can be identified as an intermediate approach between partial equilibrium theory and organic growth theory.",
19.0,1.0,Evolutionary and Institutional Economics Review,14 September 2021,https://link.springer.com/article/10.1007/s40844-021-00224-2,Economic development and the death of the free market,April 2022,Blair Fix,,,,Unknown,Unknown,Mix,,
19.0,1.0,Evolutionary and Institutional Economics Review,22 March 2022,https://link.springer.com/article/10.1007/s40844-022-00233-9,"Demand multipliers and technical performance of a Southern Eurozone economy in hard times: Kalman–Leontief–Sraffa evidence from the Greek economy, 2010–2014",April 2022,Theodore Mariolis,Despoina Kesperi,,Male,Female,Unknown,Mix,,
19.0,1.0,Evolutionary and Institutional Economics Review,01 February 2022,https://link.springer.com/article/10.1007/s40844-021-00226-0,The credibility of monetary policy and the fiscal response to the pandemic in the Eurozone,April 2022,Pompeo Della Posta,Mario Morroni,,Male,Male,Unknown,Male,"The COVID-19 pandemic is not a repetition of past shocks on supply or on demand, since it has entailed transitory freezing of the economy. The economic and social crisis triggered by the spread of the virus is characterised by negative externalities, asymmetric information and uncertainty. This makes private contracting ineffective: the market action fails, and public intervention is necessary by means of both fiscal and monetary non-conventional new policies. In the short run, fiscal policy must sustain health expenses, families’ income and firms’ fixed costs (to avoid bankruptcies and hold workers). In addition, to create the basis for a future recovery, it is necessary to implement an exceptional plan of public investment whose fiscal multiplier should be much greater than one. Investment should be concentrated on the green transition, digital technology (as also suggested, in the European Union, by the European Commission) and on labour-intensive activities, such as education and research. Additional public debt, then, will have to be created. To guarantee its stability, expansionary monetary policies have been implemented, also in a non-conventional way, especially when the zero-lower bound on interest rates had already been reached. In the European Economic and Monetary Union (EMU), among other measures, a pandemic emergency purchase programme (PEPP) has been adopted, through which the European Central Bank has been buying temporarily existing public debt on the secondary market. As long as there is no effect on prices, such measures should find no objections. Should the European Central Bank go even further? Should the public debt of euro area countries be permanently ‘monetised’ by the central bank on the primary markets, to avoid its further growth and unsustainability? Or, even, should a measure of debt relief on the debt created to respond to the pandemic be implemented? Or, alternatively, should perpetual bonds be issued? Assuming that a consensus could be reached to modify the rules of the Maastricht Treaty that define the operational boundaries of the European Central Bank, would such measures hamper its anti-inflationary credibility and induce moral hazard on the potential beneficiaries? These are the questions that we are addressing in this paper and to which, in our view, a correct interpretation of economic theory allows to give the appropriate answers. This article is organized as follows: Section 2 reports briefly the most important issues raised by the spreading of the pandemic and the responses that they would require. Section 3 analyses the implications of the issuance of perpetual bonds. Section 4 discusses credibility theory and the conclusions reached in a deterministic context as opposed to a more realistic stochastic one. Section 5 shows that the conclusions reached by the optimum currency area theory are fully consistent with credibility theory, when correctly interpreted in its stochastic version. Section 6 applies the theoretical conclusions derived from credibility theory to the European Central Bank, to conclude that an even more active role would not imply a loss of credibility. Section 7 contains some concluding remarks. The appendix proposes a simple credibility model, showing that even if the European Central Bank intervention were to produce a moderate inflation, this might well be optimal—provided that anti-inflationary preferences are not too high—since it would allow to reduce the unemployment rate that would result otherwise from a non-countered shock.",1
19.0,1.0,Evolutionary and Institutional Economics Review,07 February 2021,https://link.springer.com/article/10.1007/s40844-021-00198-1,An experimental study of VCG mechanism for multi-unit auctions: competing with machine bidders,April 2022,Satoshi Takahashi,Yoichi Izunaga,Naoki Watanabe,Male,Male,Male,Male,"In theory, the Vickrey–Clarke–Groves (VCG) mechanism attains allocative efficiency by inducing bidders to report their true valuation for each unit of the item being auctioned off. However, the VCG mechanism suffers from its computational intractability when the number of units of the item to be traded is large. This drawback is one of the reasons why we have not found any report that the VCG mechanism was used in practice, despite there being many practical examples of multi-unit auctions, such as oil and timber sales, flower markets, and spectrum auctions. Thus, approximation algorithms that reduce the computational complexity were proposed by Dyer (1984) and Kothari et al. (2005). Based on the Dyer’s work, Takahashi and Shigeno (2011) developed a greedy-based approximation (GBA) algorithm that is much faster in computation time than the VCG mechanism. However, the VCG mechanism has another drawback: it is difficult for bidders to intuitively infer how it allocates the item according to their own bids. In the case of GBA, however, a bidder who submits the highest unit bid is given priority for obtaining the units of the item, and thus bidders can more easily infer how it allocates the item according to their own bids more easily. To confirm the practical performance of the GBA algorithm, Takahashi et al. (2018) conducted a subject experiment where five units of an identical item were auctioned off to three bidders. They reported that there was no significant difference in seller’s revenue between the VCG mechanism and the GBA algorithm, but the VCG mechanism attained higher allocative efficiency than the GBA algorithm; on average, the efficiency rate was 97.37% in VCG, whereas it was 93.65% in GBA. These efficiency rates in VCG were remarkably higher than expected, even though it is difficult for subjects to intuitively understand how the VCG mechanism works. There are, unfortunately, few papers on the experiments of the performance of the VCG mechanism for multi-unit auctions, and thus, we cannot compare our results with those of other experiments. The efficiency rates we observed for the VCG mechanism were, however, not higher than those in experiment for other types of auctions in experiments reported in a comprehensive survey (Kagel and Levin 2016). This leads us to ask what factors generate higher allocative efficiency in the VCG mechanism for multi-unit auctions. Seeking an answer to this question, Takahashi et al. (2019) next investigated whether the performance of the VCG mechanism is robust against changes of appearance of the information by which bidders submit “total bids” confirming their “total valuation” for each unit. Their main result was that there was no significant difference on average in either allocative efficiency or seller’s revenue between the two types of appearance of information examined. Rather, for each type of appearance of information, there was a significant difference in subjects’ bidding behavior between the different display types of draws of unit valuations. The average rates of efficiency were, again, more than 90% in all sessions. The rates of 95% approximately truth-telling bids were, however, 31.8–43.1% in those sessions, which implies that many efficient allocations were not generated even by approximately truth-telling bids. Do we not need to induce bidders to report their true preferences to attain the allocative efficiency? When subjects compete with human bidders, it may be difficult for them to realize that truth-telling is their dominant strategy, because they do not necessarily report their true preferences. We will examine this point introducing truth-telling machine bidders. Main hypothesis For subjects who compete with truth-telling machine bidders in multi-unit auctions, the VCG mechanism induces subjects to choose truth-telling bids more frequently, and efficient allocations are observed more frequently, as compared to the situation where they compete with human bidders. We test the main hypothesis for two types of appearance of information about bidders’ valuations of the item given to them and the bids they are asked to submit: unit valuations and the unit bids themselves (Appearance 1) and unit valuations and the unit bids multiplied by the number of units (Appearance 2). Takahashi et al. (2019) reported that the VCG mechanism generated no significant difference in the allocative efficiency between Appearances 1 and 2. This leads us to ask which appearance is the allocative efficiency attained more frequently. We had the following observations: (1) In terms of seller’s revenue, there was no significant difference on average; however, in terms of allocative efficiency, there was a significant difference on average between Appearance 1 and Appearance 2. (2) Approximately truth-telling bids were more frequently chosen by subjects and approximately efficient allocations were more frequently observed in Appearance 1 than in Appearance 2. (3) When opponents are truth-telling machine bidders in Appearance 1, subjects chose approximately truth-telling bids more frequently, and approximately efficient allocations were realized more frequently, as compared to the case where the opponents were human bidders. There were no such differences for Appearance 2. Thus, the Main Hypothesis was confirmed in Appearance 1. This is our main result. The main result suggests the possibility that in Appearance 1, subjects learn their dominant strategy not by practicing with other subjects but by practicing with machine bidders in experiments for multi-unit auctions, although the item allocation and payment determination under the VCG mechanism is never intuitively understandable to the subjects. The rest of this paper is organized as follows. Section 2 formally describes the VCG mechanism for multi-unit auctions. A numerical example of the mechanism is given in Appendix 2. Section 3 describes the experimental design, and Sect. 4 shows the results. Section 5 closes this paper with some remarks.",1
19.0,1.0,Evolutionary and Institutional Economics Review,14 August 2021,https://link.springer.com/article/10.1007/s40844-021-00220-6,NK model-based analysis of technological trajectories: a study on the technological field of computer graphic processing systems,April 2022,Ichiro Watanabe,Soichiro Takagi,,Male,Male,Unknown,Male,"This paper quantitatively analyzes how the interdependence of components and the complexity of technology relates to the formation of technological trajectories. Although digital technology is rapidly evolving and becoming increasingly incorporated into daily life, the understanding of the technological evolution process is still incomplete. To improve this understanding, this paper aims to contribute to the elucidation of the mechanism of the technological evolution process in a quantitative way. This paper uses the idea of technological trajectory and a method called a main path analysis. Dosi (1982) described the technological evolution process using two notions, “technological paradigm” and “technological trajectory”. In a technological field, development paths are dependent on the technological paradigm. The technological paradigm is selected in the initial stage of the technological field’s development. Technological development must proceed according to the selected technological paradigm. The direction of technological development is limited by the technological paradigm. Dosi (1982) called these paradigm-dependent directions of development “technological trajectories”. Verspagen (2007) developed this research by introducing a quantitative method. With this method, Verspagen (2007) demonstrates that the technological trajectories of a technological field can be described as the main paths of patent citation networks. The main paths are a technological field’s major flows of knowledge that are represented as citation networks. Technological development is a selective process. While there are many possible directions of technological development, only a small fraction of these directions are realized as technological trajectories. On the other hand, the formation of the main paths is also selective. While there are many possible main paths, only a small fraction of these are realized. Therefore, technological trajectories can be represented by main paths. In previous research, the authors examined how technological trajectories evolve within the technological field of computer graphic processing systems using main path analysis (Watanabe and Takagi 2021). However, the determinants of this evolution are still unclear. This paper aims to elucidate some of the determinants of evolution. The hypotheses are derived from a conceptual model called the NK Model. The NK model, which was originally developed in the field of evolutionary biology (Kauffman and Weinberger 1989), describes the respective roles of the interdependence of components and of complexity in complex adaptive systems. In this paper, the interdependence of components is called simply “interdependence”. The complexity of a complex adaptive system is defined by the interaction between the number of system elements and their interdependence (Fleming and Sorenson 2001; Ganco 2013, 2017). Like animals, technologies are also complex adaptive systems. They are both constituted by components that are dependent on each other. Hence, many studies have applied the NK model to technological evolution analysis. Using the NK model, the inverted U-shaped relationship between interdependence and the usefulness of inventors’ efforts can be understood. In the same way, using the NK model, the inverted U-shaped relationship between technological complexity and the usefulness of inventors’ efforts can also be understood. The inverted U-shaped relationship, which is presented in Fig. 1, means that an intermediate level of interdependence or complexity is optimal for the success of inventors’ efforts. This relationship suggests that technologies with an intermediate level of complexity and technologies with an intermediate level of interdependence tend to be more successful than other technologies. According to this suggestion, two hypotheses can be derived: First, patents on the main paths of a technological field are concentrated at the intermediate level of interdependence. Second, patents on the main paths of a technological field are concentrated at the intermediate level of technological complexity. This paper tests these hypotheses empirically using patents within the technological field of computer graphic processing systems. First, the interdependence and the technological complexity of each patent in this technological field is calculated using the methodology of Ganco (2013). Second, this paper compares the distribution of interdependence of patents on the main paths of the technological field and the distribution of interdependence of all patents in the technological field. This paper also compares the distribution of technological complexity of patents on the main paths of the technological field and the distribution of technological complexity of all patents in the technological field. Additionally, in this paper, the change in interdependence values of patents that are locked-in within technological trajectories is analysed. This additional analysis provides deeper insight into the relationship between the idea of technological trajectory and the NK model. Inverted U-shaped relationship The technological field of computer graphic processing systems is chosen as the target of this research. The importance of this technological field is growing together with the evolution of artificial intelligence (AI). Image recognition is an important technological field of AI. Additionally, many manufacturers use computer-aided design (CAD) software to design products. GPUs are necessary to use CAD software on PCs. The technological evolution of the technological field of computer graphic processing systems has great significance for the productivity of the manufacturing industry. Thus, examining the technological evolution of computer graphic processing systems is particularly significant at the present time, which is why this technological field was chosen as the target of this research. This paper consists of eight sections. After this introduction, a literature review follows. In the literature review, previous studies that are related to the idea of technological trajectories, main path analysis, and the NK model are reviewed. In Sect. 3, a brief history of the technological field of graphic processing systems is presented. In Sect. 4, hypotheses for the main analysis are presented. In Sect. 5, the data and the methodology of the main analysis are introduced. In Sect. 6, the results of the main analysis are presented. In Sect. 7, additional analysis on the change in interdependence values of patents that are locked-in is presented. Discussion about the results follows in Sect. 8.",
19.0,1.0,Evolutionary and Institutional Economics Review,24 September 2021,https://link.springer.com/article/10.1007/s40844-021-00225-1,The institution-growth nexus in Sub-Saharan Africa: new evidence from heterogeneous panel causality approach,April 2022,Mohammed Seid Hussen,Murat Çokgezen,,Male,Male,Unknown,Male,"Over the past 3 decades, there has been a proliferation of empirical studies that examined the nexus between institutions and economic growth. With few exceptions, most of these studies have documented a significant positive relationship between institutions and growth (Acemoglu et al. 2001; Lee and Kim 2009; Rodrik et al. 2004). It has been argued that institutions, which are defined as the ‘rules of the game’, matter most for the development of countries as they influence the incentive structure of the economy. In countries with good-quality institutions—where property rights are protected, rule of law is respected, and regulatory quality is enhanced—the return of investment is secured, the reward of innovation is acknowledged, and resources are efficiently allocated. Therefore, investment raises, innovation triumphs, efficient allocation of resources abounds; consequently, the country prospers. Therefore, according to institutional economists such as Easterly (2001, p. 252), the path to success must begin by setting up “quality institutions.” However, this concept has been challenged by other studies because good-quality institutions may have been the result of successful growth experiences. Thus, it is unclear whether sustained growth is the cause or the consequence of improved institutional quality. In this regard, Chang (2011) argued that because it creates new agents of change who demand a new institutional framework, economic growth causes improvement in institutional quality. In addition, poor countries cannot afford to install high-quality institutions because setting up and building high-quality institutions requires resources. The author, therefore, claims that not only does bidirectional causality exist but also that the causality from economic growth to institutions may even be stronger than the one running from institutions to economic growth. In fact, empirical studies have acknowledged the possibility of two-way causality and employed various statistical solutions to address the problem. One of the statistical solutions is to use an Instrumental Variable (IV) estimation method so that the institutional variable is replaced by variables that are correlated with it but not with the error term. Another way of addressing the problem of reverse causality is to use the Generalized Methods of Moments (GMM) estimation method if the dataset in use is a panel dataset. Despite increasing awareness of the existence of reversal causality and employing advanced econometrics techniques to resolve the problem, little attention has been given to the empirical examination of the nature and direction of causality. The empirical findings of the existing few studies are rather controversial. While one strand of literature has found bi-directional Granger causality (Chong and Calderon 2000; Lee and Kim 2009), others have found evidence of one directional causality. Studies such as (Asghar et al. 2015; Justesen 2008)  documented only one-way causality from institutions to growth, supporting the institutions-led-growth hypothesis. On the contrary, Glaeser et al. (2004) found evidence of growth-led-institutional quality hypothesis, i.e. growth Granger causes institutional quality, but not vice versa. The aforementioned studies have employed a VAR based panel causality approach developed by Holtz-Eakin et al. (1988), which assumes slope homogeneity of individual countries. This assumption, however, is misleading because it may be possible that for some countries institutions might Granger cause growth, and that for others they might not. Therefore, the heterogeneous characteristics of the sample were not taken into consideration by the aforementioned studies. A study by Law et al. (2013) addresses this problem by employing a Hurlin and Venet (2001) heterogeneous panel causality test. Recently, however, Dumitrescu and Hurlin (2012) proposed a new version of non-causality testing in heterogeneous panel data, which provides a procedure for analyzing the character of the causal procedures (heterogeneous or homogeneous). The present study, thus, aims to examine the causal relationship between institutions and growth using this new version of heterogeneous panel causality test for Sub-Saharan Africa (SSA) countries. We argue that analyzing the direction of causality using a sample of SSA countries offers better insight into the nexus between institutions and growth for two reasons. First, there has been economic growth resurgence and improvement in institutional quality in the region since the mid-1990s. Thus, it is worth investigating if the recent economic growth of the region has been driven by improvement in the institutional arrangement as the institutions-led-growth hypothesis suggested, or vice versa as the proponent of the growth-led-institutions hypothesis has argued. Moreover, the experience of individual countries both in economic growth performance and improvement in institutional quality has not been even during this period. This enduring presence of both successful and failed countries within the region necessitates an in-depth analysis of whether the alleged causality between economic growth and institutions is homogenous or heterogeneous. Second, the most recent World Bank country opinion survey in developing countries, including SSA, indicates that governance and institutional quality were regarded as the most important development priority, surpassing education and job creation.Footnote 1 But a compelling argument, that while high-quality institutions may foster growth, wealth makes it possible for countries to own high-quality institutions, is out of the current discourse. Thus, the present study brings these two compelling arguments together and empirically evaluates the panel dataset from the region. To this end, we used a sample of 30 SSA countries for the period 1990–2015 and a Dumitrescu and Hurlin (2012) panel causality test. This causality test addresses the problem of heterogeneity in the panel and thus informs the nature and direction of causality between institutions and growth for the sub-set of countries within SSA. The results of the study show that the direction of causality differs depending on the income level of countries. While institutional-led-growth hypothesis has been confirmed in low-income countries, causality in lower-middle-income countries runs in the reverse direction. The evidence from this study also suggests a bidirectional causality in upper-middle-income countries. These results propose that policymakers in low-income countries should focus on institutional reform. Good quality institutions will promote economic growth that will ultimately turn into a cyclical process between economic growth and institutional development in later stages. The present study thus contributes to the existing institutions-growth literature by presenting evidence on the nature and direction of causality between institutions and growth, arguably the first of its kind that uses heterogeneous panel causality testing of institutions and economic growth in the context of SSA countries. To the best of our knowledge, no attempt so far has been made to investigate the causal relationship between institutional and economic growth using the above-mentioned approach in the context of SSA. The study is organized into five sections, including this introductory section. Section 2 discusses related literature while Sect. 3 presents data source and methodology. Section 4 discusses the empirical results. Finally, Sect. 5 concludes and provides policy recommendations.",1
19.0,1.0,Evolutionary and Institutional Economics Review,06 April 2022,https://link.springer.com/article/10.1007/s40844-022-00241-9,"Special feature: economic dynamics—growth, capital, labor, technology, and money",April 2022,Hiroaki Sasaki,,,Male,Unknown,Unknown,Male,,2
19.0,1.0,Evolutionary and Institutional Economics Review,03 April 2021,https://link.springer.com/article/10.1007/s40844-021-00204-6,Stone–Geary type preferences and the long-run labor supply,April 2022,Tamotsu Nakamura,,,Male,Unknown,Unknown,Male,"A striking feature of the labor market is that working hours in most advanced economies have been steadily declining over time. This is especially true for the Group of Seven, or G-7, countries (see Fig. 1)Footnote 1 Boppart and Krusell (2020) propose a choice and technology theory to explain this behavior of the main macroeconomic variables, and thus present an interesting line of enquiry about the long-run labor supply from the perspective of balanced growth. However, a close look at Fig. 1 also reveals some differences across the countries: working hours in Italy and Japan increased modestly in the 1950s, while a hump-shaped trend is seen for South Korea. However, throughout the entire period, we observe a rising trend in China. This is plausible because a crucial determinant for labor supply is not the time period, but the wage and/or income. Hence, working hours can increase even during the sustained growth. In this study, we develop a simple model to explain these observations for the long-run labor supply. Working hours in long run The long-run labor supply at the aggregate level depends not only on the wage income, but also on income in general. Figure 2 shows the long-run relationships between per-capita income and working hours for the same nine countries as in Fig. 1. We term this relationship the long-run labor supply curve. Although the curves are, in general, downward-sloping, we notice some important differences. We divide the nine economies into three groups, and each group has three economies in turn. Long-run labor supply curves: working hours versus per-capita GDP Panel (A) in Fig. 3 shows the long-run labor supply for the Anglo-Saxon countries, namely, Canada, the UK, and the US. Panel (B) shows the long-run labor supply for the Continental European countries, namely, Germany, France, and Italy. The curves in panel (A) are steeper than those in panel (B), especially at high income levels. Indeed, the curves in Panel (A) become vertical as income increases. Thus, the Continental European economies decreased their labor supply more rapidly with income than the Anglo-Saxon economies. Since the difference is not qualitative but quantitative in nature, it may be explained by the differences in parameter values in a common utility function. Three different types of long-run labor supply In contrast, we observe qualitative differences upon comparing panels (a) and (b) with panel (c), which shows the long-run labor supply curves for the three East Asian economies. The labor supply curves of Japan and South Korea are backward-bending, while China’s is mostly upward-sloping, and eventually becomes almost vertical. This trend suggests that the labor supply is upward-sloping at low levels of income, but downward-sloping at high levels of income. Although Japan is an advanced economy now, its per-capita income after the Second World War was low compared with other advanced economies at the time. Similarly, while the current South Korean economy is rich in terms of per-capita income, it was a developing economy until the 1980s. Although China is the second largest economy in terms of total gross domestic product, its per-capita income has been rapidly increasing. Panel (c) in Fig. 3 shows that the long-run labor curves bend backward between USD 5000 and 10,000. The working hours in China may currently be at their longest. Boppart and Krusell’s (2020) discussions on long-run labor supply mainly focus on the steady-state phenomena, which is plausible if we observe mature economies such as those of the G-7. However, examining the long-run aspects of labor supply for developing economies may require an alternative approach. Accordingly, we focus on the role of non-homotheticity in individual preferences in order to analyze the labor supply, and show that income level can play an important role thereof. One of the most tractable ways to introduce non-homotheticity is to assume that individuals have Stone–Geary preferences. Although standard Stone–Geary preferences (Geary 1949–50; Stone 1954) are useful in analyzing economies at the early stage of development, they are not necessarily suitable for investigating economies at and/or close to the mature stage of development. Therefore, we introduce the modified version into a simple growth model. As an economy develops, the share of its manufacturing and service sectors, especially the service sector’s share, increases. To explain the important structural changes thereof, Kongsamut et al. (2001) assume a non-homothetic utility function that has three arguments, namely, agricultural goods (\(x_{{\text{a}}}\)), manufacturing goods (\(x_{{\text{m}}}\)), and service goods (\(x_{{\text{c}}}\)). To be specific, their instantaneous utility (\(u\)) function is expressed as \(u = (x_{{\text{a}}} - \overline{x}_{{\text{a}}} )x_{{\text{m}}} (x_{{\text{s}}} + \overline{x}_{{\text{s}}} )\), where \(\overline{x}_{{\text{a}}}\) and \(\overline{x}_{{\text{s}}}\) are positive constants. Since our interest is not in the sectoral shifts, but in the long-run labor supply, we use a one-sector model. In this model, the sector has a characteristic of the manufacturing and service sector, especially of the service sector, in preference. Since one-sector models are widely used in macroeconomics, this simplification may be valid. More importantly, we assume that economic growth is mainly led by the service sector. In Table 1 [Table 5 on page 21 in Tcha (2015)],Footnote 2 the share of the primary sector in the 1950s in South Korea is about 40%, and decreases to 3% in the 2000s. The share of the tertiary sector (service industry) is about 40% in the 1950s, and reaches about 60% in the 2000s. Comparing with Table 2 [Table 3.2 on page 29 in Statistics Bureau (2020)], we can state that the South Korean economy has lagged behind the Japanese economy for about 30 years in terms of the structural change. More importantly, the South Korean economic growth after the 1950s has been mainly led by the tertiary sector. We observe a similar trend for the Chinese economy, as shown in Fig. 4 [Figures 1–8 on page in Yang (2020)].Footnote 3 Sectoral shares of value-added, 1954–2018, in China The rest of the paper is organized as follows: Sect. 2 sets up the individuals’ maximization problem to derive the optimal consumption, bequest, and labor supply, and characterizes the static general equilibrium. Section 3 deals with the transition of economy and investigates the steady state of the model in order to derive the relationship between income and labor supply. Section 4 concludes the paper.",1
19.0,1.0,Evolutionary and Institutional Economics Review,25 January 2021,https://link.springer.com/article/10.1007/s40844-020-00195-w,Inequality and institutional quality in a growth model,April 2022,Takuma Kunieda,Masashi Takahashi,,Male,Male,Unknown,Male,"The purpose of this study is to investigate how institutional quality affects inequality across individuals in the economy. According to the United Nations (2013), inequality in developing countries increased by 11% between 1990 and 2010. This report implies that in many developing countries, inequality across individuals remains persistent. In this paper, we attribute persistent inequality in developing countries to institutional quality. Figure 1 presents a scatter plot displaying the relationship between institutional quality and inequality.Footnote 1 In the figure, we observe their presumably negative or possibly inverted U-shaped relationship, although it does not provide information on the causality between them. Chong and Calderón (2000) and Chong and Gradstein (2007) provide empirical evidence that is consistent with this observation. Chong and Calderón show that as institutional quality improves, inequality widens in poor countries but shrinks in rich countries, and Chong and Gradstein show that inequality monotonically declines as institutional quality improves. Motivated by the observation in Fig. 1 and empirical evidence provided by Chong and Calderón (2000) and Chong and Gradstein (2007), we theoretically explore the relationship between institutional quality and inequality. Institutional quality vs. Gini coefficient. Notes. This scatter plot is based on 123 countries. The data on the Gini coefficient are collected from the dataset developed by Solt (2009, 2019). The index of “Legal System and Property Rights” produced by Gwartney et al. (2018) is used as a proxy of institutional quality It is widely known that institutional quality significantly impacts economic performance (e.g., North 1990; Knack and Keefer 1995; Hall and Jones 1999; Acemoglu et al. 2001, 2002; Rodrik et al. 2004). In an economy where institutional quality is so low that property rights are not secured and personal possessions are easily stolen by others (possibly including a corrupt government), individuals lack incentives to work diligently. In such an economy, people are frequently robbed of their earnings, and many of them do not hesitate to become criminals. If many people acquire their earnings by committing crimes rather than by working diligently, the production activities in the economy will be inefficiently implemented. Furthermore, in circumstances where no one including government officials rescues crime victims, the latter have no choice but to become poor; thus, inequality across individuals widens. We model these scenarios. In our model, each agent lives for one period and bears one offspring, as in the model of Aghion and Bolton (1997). Then, each family line continues forever with two adjacent generations being linked by bequest. In the first subperiod of a given period, an agent receives an amount of wealth from the agent’s parent as a bequest. There are two important characteristics to be explained in our model. One is the whereabouts of the bequest, and the other is the agent’s occupational choice. With regard to the bequest, there is no technology that can be used to store wealth until the second subperiod, and thus, the agent must convert it into capital, which is used by a representative firm to produce general goods. However, wealth may be stolen by criminals in the first subperiod. If wealth is stolen, the agent cannot earn interest income in the second subperiod. With regard to the occupational choice, the agent chooses to be a worker or a criminal in the first subperiod. The presence of criminals is an essential factor that produces agents’ heterogeneity in holding wealth, which is affected by institutional quality (measured by the probability of criminals being arrested). If the agent becomes a worker, he or she will earn wage income in the second subperiod. If the agent becomes a criminal, he or she joins a criminal gang and is directed to steal wealth from others by the boss of the criminal gang. The gang converts the collected wealth into capital and earns interest income in the second subperiod. The gang evenly distributes the interest income to all members. However, when committing a crime, any member may be arrested. Even a member that was arrested can nonetheless receive the same amount of rewards from the gang as do the other members.Footnote 2 In equilibrium, the wage income is equal to a criminal’s earnings because of the no-arbitrage condition between being a worker and being a criminal.Footnote 3 Such a condition determines the number of workers, which is less than the population in the economy unless institutional quality is sufficiently high. The number of workers being smaller than the total population is a source of production inefficiency. In this case, the aggregate consumption is smaller and capital accumulation is limited relative to the case of employment of the entire population. We derive not only the law of motion of the aggregate capital but also the law of motion of individual wealth. Furthermore, we obtain the distribution of individual wealth and the Gini coefficient that measures wealth inequality in the stationary state. The Gini coefficient that we derive enables us to investigate the relationship between institutional quality and wealth inequality. Our findings are as follows. If the capital share is small, inequality monotonically declines as institutional quality improves. If the capital share is relatively large, the effect of institutional quality on wealth inequality has an inverted U shape; namely, in the early stage of development of institutional quality, inequality widens as institutional quality improves. However, once institutions have sufficiently matured, inequality shrinks with further improvement of institutional quality. In other words, an institutional Kuznets curve emerges. Furthermore, we identify government policies that reduce wealth inequality and achieve the first-best outcome even though there is no strong protection of property rights. In this paper, the role of institutional quality is important in determining the relationship between growth and inequality. Institutional quality influences both growth and income distribution through each agent’s occupational choice between being a worker and being a criminal. As law enforceability becomes strong, the number of criminals decreases and that of workers increases because agents react to the decreased returns from criminal activities. Consequently, the total output increases and capital accumulates as institutional quality becomes better. However, when per capita wealth is high under good institutions, a larger amount of wealth is lost once a crime takes place. As such, an improvement in institutional quality decreases the possibility of being stolen but increases the amount of wealth stolen. In the case of a relatively higher capital share, the latter effect dominates the former in the early stage of economic development, and thus, inequality increases as institutional quality improves. In contrast, since the converse is true in the late stage, inequality shrinks once institutions have sufficiently matured. Then, an institutional Kuznets curve emerges in the case of a relatively higher capital share. In the case of a lower capital share, an institutional Kuznets curve cannot emerge because the former effect always dominates the latter. In this case, inequality monotonically decreases as institutional quality improves. The mechanism through which institutional quality influences both growth and income distribution differs from that of Chong and Gradstein (2007, 2019) and Gradstein (2007) in which rent seeking plays a crucial role but any criminal activities are not taken into consideration. Whereas only a negative relationship between institutional quality and inequality emerges in the models of Chong and Gradstein (2007) and Gradstein (2007), Chong and Gradstein (2019) obtain multiple equilibria: one occurs with high institutional quality and low inequality, and the other occurs with low institutional quality and high inequality. The remainder of this paper is organized as follows. In the following section, we discuss the related literature. In Sect. 3, we develop a growth model of the occupational choice between being a worker and being a criminal. In Sect. 4, we obtain the dynamics of the aggregate capital in general equilibrium. In Sect. 5, we derive the distribution of individual wealth analytically in the stationary state. In Sect. 6, we compute the Gini coefficient that measures wealth inequality and investigate how institutional quality affects inequality. In Sect. 7, government policies that reduce wealth inequality and achieve the first-best outcome are introduced. Section 8 concludes this paper.",3
19.0,1.0,Evolutionary and Institutional Economics Review,02 March 2021,https://link.springer.com/article/10.1007/s40844-021-00206-4,Growth and income distribution in an economy with dynasties and overlapping generations,April 2022,Hiroaki Sasaki,,,Male,Unknown,Unknown,Male,"The purpose of this study is to theoretically investigate the existence, stability, property of the steady-state equilibrium, and dynamics of the income distribution of the economy in which workers and capitalists coexist. Unlike previous studies that assume constant propensities to save both classes, we assume that both workers and capitalists solve dynamic optimization problems. This leads to revealing the fundamental nature of the capitalist economy. A significant body of literature investigates the property of long-run equilibrium in a growing economy in which workers and capitalists coexist. Pasinetti (1962) is a pioneering study and presents the “Pasinetti theorem” such that the long-run profit rate is given by the natural growth rate divided by capitalists’ propensity to save as long as the capitalists’ propensity to save exceeds the workers’ propensity to save.Footnote 1 This theorem shows that the long-run profit rate is independent of the workers’ propensity to save. On the contrary, Samuelson and Modigliani (1966) point out that the derivation of the Pasinetti theorem hinges on the implicit assumption that the capitalists’ propensity to save is considerably larger than the workers’ propensity to save. In addition, they show that unless the implicit assumption is satisfied, the long-run profit rate is given by the natural growth rate times the output elasticity of capital of the production function divided by the workers’ propensity to save. This is called the “dual theorem.” After these two studies, many studies have been conducted that modify the specifications and assumptions of models and examine whether the Pasinetti theorem and dual theorem are theoretically valid. However, almost all previous studies assume that both the workers’ propensity to save and capitalists’ propensity to save are constant overtime: both classes are agents that do not make future consumption plans. In contrast, this study assumes that workers and capitalists are rational agents that make future consumption plans given the lifetime budget constraints. Specifically, workers solve a two-period overlapping generations (OLG) problem while capitalists solve an infinite-horizon Ramsey problem.Footnote 2 Such an attempt was also made by Michl and Foley (2004).Footnote 3 They build a growth model in which workers solve a two-period OLG model, and capitalists solve an infinite-horizon dynamic optimization model. They use a fixed-coefficient Leontief production function and the real wage rate that is exogenously given according to the Classical economics assumption: the real wage rate is not determined to clear the labor market, but is institutionally determined. However, in reality, substitution between labor and capital is observed in the long run. Moreover, many developed countries experience near-full employment in the long run, and hence, it is reasonable that the real wage rate is adjusted to equate labor demand and labor supply. For this reason, this study uses a neoclassical production function with labor-capital substitution and assumes that the real wage rate is endogenously determined by the full employment condition. Using the model, we investigate (1) whether the long-run equilibrium exists, (2) how many equilibria exist if they exist, (3) whether the long-run equilibrium is stable, (4) which equilibrium the economy converges to, and (5) how the dynamics of the income distribution between workers and capitalists are determined. The contributions of this study are as follows. Unlike existing models that assume exogenously given propensities to save, the model of this study considers each class’s utility maximization. Accordingly, by using meaningful parameters that specify the shape of the utility function, and not by arbitrary parameters-saving rate, we can obtain the conditions under which the Pasinetti steady state and dual steady state occur. For related studies, we refer to the following.Footnote 4 Kaldor (1956) presents an economic growth model in which the saving rates of workers and capitalists are different. However, as Pasinetti (1962) points out, Kaldor’s model is a model in which the propensity to save from wage and the propensity to save from profit are different, and not a model in which the saving rates of workers and capitalists are different.Footnote 5 Pasinetti (1962) argues that if workers save, workers obtain interest income by holding capital stock through savings. Accordingly, the total capital stock of the whole economy is composed of workers’ own capital stock and capitalists’ own capital stock. In addition, he reveals that at the long-run equilibrium where workers and capitalists coexist, the profit rate is given by the natural growth rate divided by capitalists’ saving rate, which is called the Pasinetti theorem. However, Samuelson and Modigliani (1966) reveal that the derivation of the Pasinetti theorem critically hinges on the assumption that the capitalists’ propensity to save is much higher than the workers’ propensity to save. Then, they show that unless the assumption is satisfied, the dual steady state is obtained. With regard to the debate between Pasinetti (1962) and Samuelson and Modigliani (1966), Furuno (1970) examines the speed of convergence toward the Pasinetti steady state and the dual steady state by building a neoclassical growth model with the Cobb–Douglas production function. Faria and Teixeira (1999) introduced a government sector into a two-class neo-classical growth model with the Cobb–Douglas production function.Footnote 6 They show that for the steady-state equilibrium to be stable, the output elasticity of capital in the production function must be close to unity. In this case, the ratio of capitalists’ own capital stock to the total capital stock approaches unity, which is called the “anti-dual” steady state.Footnote 7 Zamparelli (2017) also investigated the anti-dual steady state. He focuses on the case where the production function takes the constant-elasticity-substitution form, the elasticity of substitution exceeds unity, and endogenous growth occurs. In this case, the economy converges to the anti-dual steady state, capitalists’ own capital stock share approaches unity, and the profit share approaches unity. The closest to our study is Commendatore and Palmisani (2009). They extend Michl and Foley’s (2004) model and present a model in which the real wage rate is endogenously determined by the full employment condition.Footnote 8 In contrast to our study, they use a constant elasticity of substitution (CES) production function and investigate a case in which the elasticity of substitution between labor and capital is less than unity. In this case, chaotic dynamics can occur depending on the conditions. On the other hand, the present study uses a Cobb–Douglas production function. The interests of analysis are different from the present study. They focus on the analysis of chaotic dynamics, while we focus on the property of the steady-state equilibrium and the dynamics of income distribution. The main results are as follows. Depending on the size of the parameters, two kinds of equilibria (the Pasinetti steady state and the dual steady state) are obtained, and each equilibrium is stable. In our model, the condition depends on six parameters: discount factor of capitalists, discount factor of workers, output elasticity with respect to capital, population growth rate, technological progress rate, and capital depreciation rate. Moreover, with the introduction of the technological progress rate and capital depreciation rate, the dual steady state is likely to be obtained compared to the case without the introduction. Nevertheless, based on calibrations, it is likely to obtain the Pasinetti steady state. In numerical simulations, we assume a scenario in which the technological progress rate decreases, and hence, the economic growth rate decreases. Then, we investigate the relationship between a decrease in the economic growth rate and income distribution. Depending on whether income distribution is measured by net income share or gross income share, we obtain different results. We show that a decrease in the economic growth rate increases the wealth share of capitalists while decreasing that of workers, and that it increases (decreases) the income share of capitalists while it decreases (increases) that of workers if the income share is measured in terms of gross (net) income share. Therefore, a decrease in the economic growth rate results in income distribution in favor of capitalists in terms of gross income share while making the income distribution in favor of workers in terms of net income share. The remainder of this paper is organized as follows. Section 2 presents our model. Section 3 obtains the long-run equilibria, that is, the Pasinetti steady state and dual steady state, and investigates the stability of each long-run equilibrium. Section 4 investigates the equilibrium to which the economy converges by using parameter values obtained from economic data. Section 5 analyzes the income distribution between workers and capitalists. Section 6 concludes.",
19.0,1.0,Evolutionary and Institutional Economics Review,10 March 2021,https://link.springer.com/article/10.1007/s40844-021-00202-8,A two-class economy from the multi-sectoral perspective: the controversy between Pasinetti and Meade–Hahn–Samuelson–Modigliani revisited,April 2022,Kazuhiro Kurose,,,Male,Unknown,Unknown,Male,"The distribution of income and capital (or wealth) is one of the major issues in current economic analysis. Recent research on distribution has been stimulated especially by Piketty (2014). Following him, many economists are paying attention to studies on not only factor income distribution but also the distribution of income and capital (wealth) among heterogeneous agents. The assumptions underlying the behaviour of the agents are crucial for analysis of the distribution of income and capital (wealth). Caggetti and De Nardi (2008) investigate the various types of general equilibrium models with multiple agents. The first type is the general equilibrium model with infinitely lived agents (ILA). The second type is overlapping generations (OLG) models, which include some elements of life-cycle structures and intergenerational links. The third type mixes features of both the ILA and OLG models. Caggetti and De Nardi (2008) argue that the second type can explain the US distribution much better than the first; and the third type simplifies some aspects of either model and thus, makes the analysis more tractable. When focusing on a specific phenomenon, the simple model is very useful to understand the underlying essential features and mechanism at work. Such a simple model for the analysis of distribution of income and capital (wealth) would be a two-class model. For the two-class model, Pasinetti (1962) is an important point of reference to consider growth and distribution. This work led to intensive debates with Meade (1963, 1966), Meade and Hahn (1965), and Samuelson and Modigliani (1966a, b), as part of the Cambridge capital controversy, in the 1960s and 1970s.Footnote 1 It is well-known that Pasinetti’s two-class economy has three kinds of equilibria: ‘Pasinetti equilibrium’ (PE), in which both the capitalist and worker survive in the long run; ‘dual equilibrium’ (DE), in which the capitalist disappears in the long run; and ‘anti-dual equilibrium’ (ADE), in which the worker disappears in the long run. PE was discovered by Pasinetti, DE by Meade (and then elaborated by Samuelson and Modigliani 1966a, b), and ADE by Darity (1981). Although the representative agent model has become the standard model in macroeconomics, interest in the Pasinettian two-class model has been revived recently. For example, Taylor (2014) criticises Piketty’s (2014) analysis for relying on the neo-classical production function. Taylor (2014) demonstrates that euthanasia, persistence, and triumph of the rentier are all possible scenarios of the Pasinettian aggregate two-class model, whereas Piketty (2014) predicts only one scenario, triumph of the rentier. Meanwhile, Taylor’s (2014) model lacks a microfoundation of agents. Mattauch et al. (2017) examine how public investment financed by capital tax affects the distribution of wealth when a change in substitutability between capital and labour is allowed. The basic setting of the model is based on Pasinetti (1962). In other words, the worker earns his income from wage and profits and saves for a life-cycle purpose, which implies that his behaviour follows the OLG model.Footnote 2 In contrast, the capitalist earns her income solely from profit and is assumed to be an ILA. As a result, it is demonstrated that for any elasticity of substitution greater than a threshold, there exists a capital tax rate at which the economic system switches from the PE to the DE. In addition, for any elasticity of substitution below the threshold, there exists a capital tax rate at which the economic system switches from the PE to the ADE.Footnote 3 Zamparelli (2016) analyses the Pasinettian two-class economy in the neo-classical framework. He assumes a production function with constant elasticity of substitution (CES) between capital and labour and no microfoundation of the capitalist’s and worker’s behaviour. The capitalist earns her income solely from profit and the worker from wage and profit. As a result, if the capitalist’s saving rate is higher than the worker’s and the elasticity of substitution is high enough to ensure endogenous growth, ADE exists. In addition, he shows that capital tax can favourably improve the distribution to the worker in the steady state. Stiglitz (2016) points out new stylised facts on growth and accumulation, and asserts that the standard neo-classical models cannot explain the recent movement of the ratio of wealth to income, even taking technical change into consideration.Footnote 4 Then, Stiglitz (2016) assumes the Pasinettian two-class economy in which the capitalist is the ILA and the worker’s behaviour follows the OLG model. The feature of his two-class model is to introduce land as a factor of production in the model, as taking land rent and exploitation rent into account better explains the recent movement of the ratio of wealth to income. Sasaki (2018) investigates the existence and stability of the steady states obtained by the Pasinettian two-class economy in which the capitalist is an ILA, the worker’s behaviour follows the OLG model, and there is a Cobb–Douglas production function. It is shown that although PE and DE exist depending on the combinations of parameters, PE is stable under reasonable values of the parameters. Furthermore, Taylor et al. (2019) consider a Pasinettian two-class economy in which growth is demand driven and no microfoundation of agents is formulated. Taylor et al. (2019) examine the existence and stability of the PE and DE obtained as the steady states in the demand-driven growth model. In a context of the richest 1% of US households receiving about 7% of wages, Taylor et al. (2019) interestingly consider a case in which capitalists, who have a higher rate of saving than that of workers, receive some wage income besides profit. Then, it is shown that the DE obtained in this case is unstable. To consider social security, Michl and Foley (2004) and Michl (2007, 2009) construct a Pasinettian two-class growth model in which the capitalist is an ILA and the worker’s behaviour follows the OLG model. They show the existence of PE and DE in the model, and analyse various effects of social security. For example, Michl and Foley (2004) show that an unfunded social security system relying on payroll taxes reduces workers’ lifetime welfare and saving, since the change in their saving affects the level of the share of capital owned by the workers but not the rate of economic growth in PE. The change in worker’s savings has no effect on the path of capital owned by capitalists in the model. Therefore, the decrease in the level of workers’ share of capital reduces the overall level of capital, output, and employment without affecting the rate of economic growth. This is called the level effect. The effect is mitigated by the presence of a reserve fund. Their model identifies the social security reserve fund as a potential vehicle for generating capital accumulation and effecting progressive redistribution of wealth. The above-mentioned models with a microfoundation of agents assume the neo-classical ‘well-behaved’ production function with capital as the primary factor of production or are de-facto one-commodity (or one-sector) models. As clarified in the Cambridge capital controversy, the assumption of the neo-classical well-behaved production function with capital as the primary factor of production excludes some phenomena which may arise if capital is regarded as a bundle of reproducible and heterogeneous commodities.Footnote 5 Furthermore, the controversy reveals that the results obtained by the one-commodity (one-sector) model do not necessarily hold in a model with multiple commodities and techniques (e.g. Harcourt 1972). As highlighted by Piketty (2014), physical capital in modern capitalist economies typically consists of reproducible and heterogeneous commodities. It would be significant to analyse growth and distribution in the Pasinettian two-class economy under the assumption of multiple commodities and capital as a bundle of reproducible and heterogeneous commodities. In this study, we examine the Pasinettian two-class economy by a sort of activity analysis (i.e. Leontief–Sraffa model). Since, as already mentioned, the kinds of assumptions made about the micro-behaviour of agents are crucial for our purpose, we consider two combinations of microfoundation of capitalist’s and worker’s behaviour: first, both the capitalist and the worker are ILAs; and second, the capitalist is an ILA and the worker’s behaviour follows the OLG model. The scenarios cover all combinations used in the abovementioned models. However, our examination is confined to the steady states obtained in PE and DE. The first characteristic of our study is that we analyse the switch of the type of equilibria (from PE to DE, or vice versa) simultaneously with paradoxical phenomena in capital theory (reswitching of technique and reverse capital deepening). This analysis is certainly impossible in models which assume the neo-classical well-behaved production function with capital as the primary factor of production. This is because, as mentioned, the possibility of such paradoxical phenomena arising is excluded from the models. Although Pasinetti (1962), Meade (1963, 1966), Meade and Hahn (1965), and Samuelson and Modigliani (1966a, b) use aggregate macroeconomic models, Morishima (1969) constructs a multi-sectoral general equilibrium model to analyse the properties of the steady-state path of PE and DE. Subsequently, Hosoda (1989) extends Morishima’s model to analyse the switch of the type of equilibria simultaneously with the capital theory paradoxes. Since the capitalist’s and the worker’s saving rates are assumed to be exogenously given in both Morishima’s (1969) and Hosoda’s (1989) models, we further develop them to consider the abovementioned combinations of the microfoundation of the capitalist’s and the worker’s behaviour. Consideration of the combinations of the microfoundation enriches the analyses of growth and distribution using the multi-sectoral two-class model with capital as a bundle of reproducible and heterogeneous commodities. It is represented by the case in which the capitalist is the ILA and the worker’s behaviour follows the OLG model. As shown in Subsect. 3.2, from this case, we can derive the saving function consistent with the steady growth in the Pasinettian two-class economic model, which did not receive attention from Pasinetti (1962), Samuelson and Modigliani (1966a, b), Morishima (1969), and Hosoda (1989). Using the numerical example in Sect. 4, moreover, we indicate the difference in the change of income distribution at the switch point between the case in which both the capitalist and the worker are ILA and the case in which the capitalist is the ILA and the worker’s behaviour follows the OLG model. In analysing the distribution of income/capital (wealth), the movements of the ratio of output to capital and the profit (or the wage) share are important variables to be determined in a model. Piketty (2015: p. 52) argues that: the right model to think about rising capital–income ratios and capital shares in recent decades is a multi-sector model of capital accumulation, with substantial movements in relative prices, and with important variation in bargaining power. His claim is based on the assertion, which Stiglitz (2016) also makes, that recent movements in the ratio of capital to output and profit share cannot be sufficiently explained by standard neo-classical models in which a well-behaved production function with capital as the primary factor is assumed. His claim is quite adequate. Although we do not consider the variation of bargaining power in this study, our multi-sectoral model based on Morishima (1969) and Hosoda (1989) shown in Sect. 3 could be the first step of the model-building to satisfy Piketty’s claim. This is because complicated phenomena, which cannot be observed when the well-behaved production function is assumed, can be observed in the movements of the ratio of net output to capital and the income share when capital is assumed to be composed of a bundle of reproducible and heterogeneous commodities. This is the second characteristic of our model. The rest of this paper is organised as follows. Section 2 briefly reviews the concept of the long-run competitive equilibrium in Hosoda (1989), on which our model is based. Hosoda’s (1989) model has one degree of freedom, as in Sraffa (1960), and thus, the rate of profit is treated as exogenously given throughout his study. We follow his treatment of the rate of profit in our model. Therefore, the rate of economic growth in the steady states is obtained as the function of the rate of profit, unlike in Pasinetti (1962, 1974) and Samuelson and Modigliani (1966a, b), whose rate of profit is the endogenous variable and rate of economic growth is the exogenous variable (natural rate of economic growth). This is a device to analyse the switch of the type of equilibria, together with the paradoxes in capital theory. Section 3 presents our multi-sectoral two-class model with a microfoundation of capitalists and workers. We analyse the relationship between the rates of economic growth and profit obtained in each combination of the microfoundation of capitalists and workers. We show the existence of PE and DE, and analyse the properties of the steady states. Section 4 presents the numerical example to examine the working of the model proposed in Sect. 3; we analyse the switch of equilibria and movements of the ratio of capital to net output and the capital intensity in relation to the change in the rate of profit. Section 5 presents concluding remarks.",3
19.0,1.0,Evolutionary and Institutional Economics Review,29 April 2021,https://link.springer.com/article/10.1007/s40844-021-00212-6,Measuring the shift in the short-run production frontier,April 2022,Hideyuki Mizobuchi,,,Male,Unknown,Unknown,Male,"Productivity index is often defined as the ratio of an index of outputs to an index of inputs. Economists think of productivity as measuring the current state of the technology used in producing the goods and services of a firm, which is a technical constraint on the firm’s profit maximizing behaviour. The production frontier, consists of inputs and the maximum output attainable from such inputs, characterizes technology. Hence, the productivity index is interpreted as the shift in the production frontier, reflecting technological change.Footnote 1Footnote 2 There are multiple index number formulae for a productivity index. The idea that the productivity index should capture the shift in the production frontier helps to decide between different index number formulae. This approach to the choice of index numbers is called the economic approach. There are two types of productivity index: total factor productivity (TFP) index and partial factor productivity (PFP) index. The former index relates a bundle of total inputs to output, while the latter index relates a part of total inputs to output. Caves, Christensen and Diewert (1982) use the economic approach to justifying the choice of index number formula for the TFP index. They define the Malmquist productivity index, which measures the shift in the production frontier. Since it is a theoretical productivity index that is defined by the distance functions, one cannot compute it without knowing its functional form and its parameters. They show that the Malmquist productivity index coincides with the Törnqvist productivity index under the general assumption on the distance function. The Törnqvist productivity index is computable from the observed prices and quantities of outputs and inputs. Hence, they provide a good justification for the use of the Törnqvist productivity index. Diewert and Morrison (1986) also adopt the economic approach but use the profit function to define the theoretical TFP index. They show this index coincides with another index number formula of prices and quantities of outputs and inputs. The present paper deals with the PFP index. Our focus is on the labour productivity index in particular.Footnote 3 Following Caves, Christensen and Diewert (1982), and Diewert and Morrison (1986), we apply the economic approach to the index number problem of labour productivity growth. We start from the idea that labour productivity should represent the technical constraint that a firm faces when it decides the optimum level of labour input. To put it differently, labour productivity measures the current state of the production technology that transforms labour inputs into output, holding fixed capital inputs. Hence, the production technology associated with the use of labour can be characterized by the short-run production frontier, which consists of labour inputs and the maximum output attainable from such labour inputs, holding fixed capital inputs. We propose theoretical labour productivity indexes that measure the shift in the short-run production frontier, using the distance function as well as the profit function. Two indexes are purely theoretical indexes. Under the assumption on the particular functional forms to represent the underlying technology, we derive the index number formulae, which coincide with distinct theoretical indexes. Average labour productivity, output per unit of labour input, is the most popular labour productivity measure. The shift in the short-run production frontier is not the only source of the growth of average labour productivity. The decrease in labour inputs could also raise average productivity, exploiting scale economies. This is the reason why average labour productivity steers us to the wrong conclusion about underlying technology constraint for the firm profit maximizing behaviour in some cases. Such scale effect associated with changes in labour input is properly removed from new labour productivity indexes. Triplett and Bosworth (2004, 2008) and Bosworth and Triplett (2007) discussed the phenomenon that the service sector has a much lower growth of labour productivity than other industries and it drag down the growth of the aggregate labour productivity from the early 1970s until the middle 1990s. They call it Baumol’s disease, since it was firstly pointed out by Baumol (1967). However, their analysis is based on average labour productivity. We compare labour productivity in the service sector and other sectors applying new labour productivity indexes, which the present paper introduces, to the U.S. industry data. Nin et al. (2003) also defined the PFP index by the shift in the production frontier. However, their productivity measure is the firm’s productivity of producing a particular type of output amongst a comprehensive set of outputs using all the inputs. They also propose a procedure to calculate their PFP index. However, our study is based on data envelopment analysis (DEA) rather than index number technique. Thus, our result is independent of their result. Section 2 graphically illustrates how to measure labour productivity growth. Section 3 proposes two labour productivity indexes. We also show how they can be calculated from observable prices and quantities. Section 4 explains the good aggregation property of these two indexes. Section 5 applies these two indexes to the analysis of labour productivity in U.S. industries. We compare these two measures with average labour productivity growth. Section 6 concludes.",1
19.0,1.0,Evolutionary and Institutional Economics Review,09 March 2021,https://link.springer.com/article/10.1007/s40844-021-00199-0,Capital-labor conflict in the Harrodian model,April 2022,Takashi Ohno,,,Male,Unknown,Unknown,Male,"In this study, we consider the effect of capital-labor conflict in the Harrodian model through reserve army effects, a Bhaduri–Marglin-type investment function, and technological change.Footnote 1,Footnote 2 By modifying the benchmark framework, we revisit the debate on the stability condition of the Harrodian model. Using this model, we attempt to capture a complete business cycle including not only economic expansion or contraction in the real economy but also turn in economic cycle and steady state. A basic Harrodian model interprets economic expansion as divergent from the equilibrium because the model is unstable. Therefore, the basic Harrodian model cannot endogenously explain the corner from the economic expansion phase to the economic contraction phase and a stable economy (secular stagnation). We attempt to examine the stable Harrodian model condition that can not only consider expansion and inversion but also endogenously explain the corner from the economic expansion phase to the economic contraction phase and a stable economy (secular stagnation). As is well known, the Harrodian model assumes that capacity utilization affects the variance of investment and not the investment level. Hence, the benchmark Harrodian model is unstable; it cannot discuss the effects of various economic policies, like distribution, fiscal, and monetary policies. On the contrary, the neo-Kaleckian model tends to dominate post-Keynesian macroeconomics although post-Keynesian macroeconomics includes all three models: neo-Kaleckian, Harrodian, and Sraffian. As Lavoie (2014) stresses, the neo-Kaleckian model brings about some fruitful results, including a wage-led growth regime and stagnationism, compared with neoclassical economics (Rowthorn 1982; Dutt 1984; Taylor 1985). These results are contrary to neoclassical economics given that the neo-Kaleckian model has definite and strong political economy implications, providing a theoretical foundation for shifting the income distribution in favor of workers to obtain a higher growth rate.Footnote 3 Lately, the Harrodian model has been receiving much attention in the literature for two major reasons. First, Skott (2010) presents a debate between the Harrodian and Kaleckian models on their theoretical presumptions and empirical results.Footnote 4 Second, several studies have proposed a stable Harrodian model to discuss the effects of various economic policies. For example, some studies consider labor market effects (the reserve army of labor) in the Harrodian model and show that the labor market has a stabilizing effect (Skott 2010; Yoshida 1999; Flaschel and Skott 2006; and Sportelli 2000). Lavoie (2016) and Skott (2017, 2019) consider the effect of autonomous demand for a stable Harrodian model.Footnote 5 Conversely, Skott and Ryoo (2008) and Ryoo (2010) discuss the effect of financial markets on the stability condition. Hence, the Harrodian model can be expected to continue as a subject for further research. However, there are weaknesses in the Harrodian model from the viewpoint of capital-labor conflict. Most Harrodian models consider that the investment function is affected only by capacity utilization. Thus, the variation in accumulation rate will be the same when capacity utilization is constant even though the capital share that represents the degree of capital-labor conflict is different. This means that the Harrodian model dismisses situations in which there is a difference in the capital share. However, it is reasonable to assume that a variation in accumulation rate will be affected not only by capacity utilization but also by the capital share that represents the level of capital-labor conflict. For developing a Harrodian model, we consider both the reserve army effect and a Bhaduri and Marglin (1990) investment function, as proposed in the neo-Kaleckian model. Since this perspective has been overlooked in previous studies, there is room to consider the effect of a Bhaduri and Marglin-type investment function in the Harrodian model. From this model, we derive the following results. First, we find that both the reserve army effect and a Bhaduri–Marglin-type investment function are needed for a stable Harrodian model. Second, the degree of technological change must lie within a specific range—neither too low nor too high—for the model to be stable. The remainder of this paper is organized as follows. Section 2 presents the benchmark Harrodian model. In Sect. 3, we modify the investment function and discuss reserve army effects in the medium run. Section 4 discusses the effect of technological change in the long run. Section 5 concludes.",1
19.0,1.0,Evolutionary and Institutional Economics Review,27 March 2021,https://link.springer.com/article/10.1007/s40844-021-00208-2,Alternative monetary policy rules and expectational consistency,April 2022,Carlos Eduardo Iwai Drumond,Cleiton Silva de Jesus,Hiroyuki Yoshida,Male,Unknown,Male,Male,"The inflation target (IT) regime has become the monetary policy norm for a number of central banks around the world since 1990. There are currently about 35 countries that have adopted this monetary regime (Schmidt-Hebbel and Carrasco 2016), and there have been many empirical studies that evaluate the macroeconomic performance of countries following this policy framework (Ball and Sheridan 2003; Goncalves and Salles 2008; de Mendonça and de Guimarães e Souza 2012). Even with the advances in the conventional macroeconomic literature of the last two decades in terms of the construction of theoretical models that explicitly consider the IT regime [for example, Clarida et al. (1999)], the post-Keynesian literature only began to consider the IT framework in formal models recently, in particular through the seminal works of Setterfield (2006) and of Lima and Setterfield (2008). In fact, in its original form, the IT regime derives from a set of hypotheses that are apparently incompatible with the post-Keynesian tradition, especially when it comes to the emphasis given to low inflation to the detriment of concerns related to the level of output. According to Setterfield (2006), this apparent mismatch between the regime and typical Keynesian concerns could be overcome provided that: (i) the output is considered properly as a primordial part of the goals of monetary policy; (ii) the distributive conflict component of inflation is not neglected; and (iii) the role of aggregate demand is considered to determine the real output. Based on Setterfield (2006) and Lima and Setterfield (2008), others began to incorporate the possibility of an IT regime under an alternative framework, with different modeling strategies. A common feature in this literature, both for closed and open economies, lies in the assumption of linearity in the monetary policy rule, as can be seen in the dynamical models developed by Porcile et al. (2011), Santos (2011), Drumond and Porcile (2012), Bertella et al. (2015), and Drumond and Jesus (2016). When a central bank uses a linear monetary policy rule, it is implicitly assumed that its coefficients are constant. If this is the case, the monetary authority’s concern with inflation, for example, is always the same, regardless of whether the economy is expanding or contracting. Reflecting on this issue, (Blinder 1997, p. 06) recognized that academic macroeconomists tend to use “quadratic loss functions for reasons of mathematical convenience” (which culminates in a linear monetary policy rule) “without full consideration as to their substantive implications.” In addition, empirical studies with data from different countries have suggested that cases in which central banks have asymmetric preferences are not infrequent (Martin and Milas 2004; Dolado et al. 2005; Surico 2007; Cukierman and Muscatelli 20080. Considering the above, it is reasonable to assume that the parameters of a monetary policy rule vary over time, so that, for example, they will depend on the phase of the economic cycle that the economy is in. Another substantive issue arises when the IT regime is designed under an alternative framework, one with a concomitant focus on inflation and on output. In this case, the monetary policy will be fully consistent only in the presence of certain policies that will complement the interest rate rule, as is the case of the income policy in the model developed by Lima and Setterfield (2008). It turns out that, as this article will try to demonstrate further ahead, in the absence of the natural output hypothesis, as well as in the absence of policies complementary to the monetary policy, a typical interest rule with a dual mandate cannot guarantee the convergence of inflation to its target. Taking the above observations into account, the main objective of this article is to build a macrodynamic model that allows the interest rule to be non-linear. We have assumed that the closer (distant) the economy is to its full level of capacity utilization, the more (less) sensitive the monetary authority will be to deviations of inflation from the ITs. This dependence of the inflation parameter (in the interest rate rule) on the level of economic activity is a clear distinction between the model developed here and those available in the post-Keynesian literature. In terms of macroeconomic policy, the main contribution of the present paper is to present a monetary policy rule that allows the monetary authority to put as much weight on inflation as on the output without losing sight of the role of the expected anchor IT when conditioned by institutional constraints, especially regarding incomes policy. No less important is the fact that the model should be built without imposing an ad hoc convergence of inflation to its target, and without imposing some limiting form of expectational behavior, such as the hypothesis of rational expectations. This paper is organized into five sections in addition to this introduction and the conclusions. In the second section, we present a brief literature review; in the third section, we develop a post-Keynesian standard model as the starting point for our analysis; in the fourth section, we present the dual-mandate expectational consistency issue; in the fifth section, we present a non-linear interest rate rule in the context of a closed economy; and, in the sixth section we analyze the case of an open economy.",1
19.0,1.0,Evolutionary and Institutional Economics Review,15 March 2021,https://link.springer.com/article/10.1007/s40844-021-00205-5,Keynesian and classical theories: static and dynamic perspectives,April 2022,Hiroki Murakami,,,Male,Unknown,Unknown,Male,"It is no exaggeration to say that the development of macroeconomics has been stimulated by controversies between the Keynesian and classical theories. In general, the Keynesian theory allows for the existence of involuntary unemployment, while the classical theory does not. There have been a lot of arguments on the similarities and dissimilarities between them. The main topic of these arguments is the fundamental cause of the difference in conclusion between them. The purpose of this paper is to reconsider the fundamental difference between the Keynesian and classical theories from both static and dynamic perspectives. This paper is organized as follows. Section 2 summarizes the static properties of the Keynesian and classical theories from a static viewpoint. It confirms Modigliani’s (1944) claim that the fundamental difference between them in statics is due to the rigidity of nominal wages. Section 3 presents a different view on the Keynesian and classical theories from a dynamic (and long-term) perspective. In this section, two dynamic systems with Keynesian and classical features are examined to consider the difference in view on long-run stability between these theories. It is demonstrated that the same long-run equilibrium has totally different characteristics in stability between the Keynesian and classical systems and that the fundamental difference between the theories lies in the difference in stability properties of long-run equilibrium. Section 4 concludes this paper.",1
19.0,1.0,Evolutionary and Institutional Economics Review,05 March 2021,https://link.springer.com/article/10.1007/s40844-021-00201-9,"Numerical analysis of the disequilibrium monetary growth model: secular stagnation, slow convergence, and cyclical fluctuations",April 2022,Shogo Ogawa,Hiroaki Sasaki,,Male,Male,Unknown,Male,"The financial crisis of 2008 and resulting Great Depression have had huge impacts on macroeconomics as well as the real economy (Summers 2014). In particular, persistent stagnation, that is, the downward divergence of GDP growth from its trend, and persistent unemployment are often referred to as “secular stagnation” by Summers (Summers 2014, 2015). Hence, the study of secular stagnation has become the main subject of macroeconomics. New Keynesian economics, the mainstream notion of macroeconomics, regards unemployment under secular stagnation as Keynesian unemployment (KU). Numerous studies have examined the mechanism of secular unemployment. The basic framework of New Keynesian economics is the dynamic stochastic general equilibrium (DSGE) model in which deviation from the optimal growth path arises from a stochastic shock. New Keynesian economics is thought to succeed in justifying economic policies under economic shocks by adding price rigidity into the market mechanism.Footnote 1 After the occurrence of stagnation, New Keynesian economics pays attention to the zero lower bound (ZLB) of nominal interest rates and attempts to explain secular stagnation by the decrease in the natural (real) interest rate that achieves full employment and the ZLB of nominal interest rates that prevents the adjustment of bond markets.Footnote 2 However, as Palley (2019) points out, the lower bound of the interest rate is essentially equivalent to price rigidity; hence, the analysis of the ZLB is Pigouvian rather than Keynesian. Therefore, it is inappropriate to call the analysis of the ZLB “Keynesian.” In addition, he shows the possibility of persistent unemployment due to demand shortage, even without the ZLB. For this reason, we need an alternative DSGE model to analyze depressions and reveal the mechanism of persistent depressions that cannot be explained by price rigidity. Stiglitz (2018) criticizes the framework of the DSGE model and presents an alternative quantity-constrained model (see the appendix of his paper). In his model, supply and demand are adjusted through quantity adjustments under price rigidity; hence, the quantity of supply that the supply side expresses is not necessarily realized. Therefore, economic agents do not know realized employment and realized output in advance. For this reason, his model can directly represent involuntary unemployment such that labor demand is less than labor supply. The quantity adjustment model of Stiglitz (2018), a kind of disequilibrium model, is based on the interpretation of Keynes (1936) by Clower (1965) and Leijonhufvud (1967). As a general disequilibrium model that presents the relationships among markets under disequilibrium, refer to Barro and Grossman (1971), Bénassy (1975), and Malinvaud (1977). In these models, the planned demand and planned supply of economic agents are not necessarily realized, and realized demand and realized supply are determined by the short-side rule. For example, when the planned goods supplied by firms under prevailing prices are higher than the goods demand expressed by households, the actual quantity is equal to the goods demand of households. In this case, goods supply is under quantity rationing; hence, firms need to change labor demand according to the quantity constraint. Then, in the labor market, labor demand can be less than the labor supplied by households. In this way, involuntary unemployment occurs, called KU in terms of disequilibrium economics. Since the determining factor of labor demand is goods demand rather than wages, disequilibrium economics can represent unemployment by a different mechanism than that of New Keynesian economics. On the other hand, when wages are too high, labor demand depends on wages, and this situation is called classical unemployment (CU). Although disequilibrium analysis was fascinating, it was insufficiently studied and evaluated; hence, it was replaced by equilibrium analysis, including the use of the DSGE model (Backhouse and Boianovsky 2012). However, disequilibrium analysis has recently been cited by New Keynesian economists that have investigated economic stagnation.Footnote 3 Nevertheless, these models cannot reproduce KU in the disequilibrium analysis because the labor demand shortage represented by firms arises from nominal rigidity or the ZLB as opposed to the shortage of goods demand. In addition, the consumption behavior of households is independent of the demand–supply gap of employment; hence, those models cannot express the leakage effects between markets arising from disequilibrium. Accordingly, they analyze CU rather than KU, which may distort the analysis of disequilibrium economics.Footnote 4 On the other hand, Ogawa (2020) presents a monetary growth model based on a disequilibrium framework and shows that persistent KU (i.e., secular stagnation) may occurs along the transitional dynamics toward the steady state. According to traditional disequilibrium analysis, his model represents the dual-decision hypothesis such that demand for and the supply of a market is determined by the quantity constraints of other markets, showing that depending on whether the goods market is demand-led or supply-led, changes in the price variables and capital–labor ratio accelerate or decelerate, which he calls the “dual-decision effect”. From this, wages become rigid under the KU regime and capital accumulation becomes unstable. As a result, his study shows that goods demand is stagnant at relatively low levels and that KU continues without resorting to the ZLB or wage rigidity.Footnote 5 He shows the above results using numerical simulations. However, he checks only one initial value that produces a persistent KU path and uses relatively high parameters of expectation adjustment that correspond to adaptive expectation. Therefore, his analysis does not sufficiently show the robustness of KU paths that correspond to secular stagnation. Based on the above discussion, this study extends Ogawa (2020) numerical simulations to investigate in detail how the dynamic paths are affected by the initial conditions and parameters of expectation adjustment. It reveals that (1) persistent KU does not depend on the rationality of expectation adjustment and initial regimes, that is, where the economy is initially located [the KU regime, CU regime, repressed inflation (RI) regime, and equilibrium] and (2) the model shows remarkable behaviors when expectation adjustment is either extremely adaptive or extremely rational. The remainder of this paper is organized as follows. Section 2 presents our model, which is largely based on Ogawa (2020). Section 3 presents the numerical simulations conducted to investigate how the deviation of the initial values from the steady state, regime in which an economy is located, and parameters of expectation adjustment affect the dynamic paths. In particular, we examine whether an economy converges to a steady state and shows monotonic behavior or cyclical fluctuations. Section 4 discusses the remarkable dynamic paths obtained by the numerical simulations. Section 5 summarizes the analysis and presents future research issues on secular stagnation and expectation adjustment.",1
19.0,1.0,Evolutionary and Institutional Economics Review,29 March 2021,https://link.springer.com/article/10.1007/s40844-021-00207-3,Purchase of government bonds by a supranational central bank: its impact on business cycles,April 2022,Masato Nakao,Toichiro Asada,,Male,Unknown,Unknown,Male,"The euro area took various measures to deal with the euro crisis, but the decisive policy that ended this crisis was the Outright Monetary Transactions (OMT) policy. The OMT was introduced by the European Central Bank (ECB) in 2012 and allows the ECB to make unlimited purchases of government bonds issued by crisis-hit countries. The introduction of the OMT has also helped to clarify the ECB’s stance on the euro crisis. De Grauwe (2018) notes that the OMT addresses concerns that were destabilizing the system, and values the OMT as a lender of last resort. Even as the euro area started to emerge from this crisis, it was pointed out that the region was not an optimum currency area (OCA). This was a set phrase that had been gaining ground even before the crisis began. The theory of optimum currency area is a summary of the pre-requisites for successful currency integration. Its foundation was developed by Mundell (1961), McKinnon (1963), and Kenen (1969). Since the 1960s, various economists have discussed the conditions for the establishment of an optimal currency area [see Mongelli (2002), De Grauwe (2018), and Baldwin and Wyplosz (2019)]. However, critics have alleged a problem in the OCA theory itself, in that the different criteria cannot be integrated within a uniform framework. According to Gächter et al. (2012), the discussion of this problem led to the development of a few “metacriteria” that implicitly subsume some of the individual conditions; the synchronization of business cycles has been established as a key metacriterion [(e.g., De Haan et al. (2008) and De Grauwe and Ji (2016, 2017)]. The euro has had a different effect on the synchronization of business cycles before and after its introduction. As the time of the introduction of the euro approached, the business cycle converged [e.g., Altavilla (2004), Camacho et al. (2006), and Darvas and Szapáry (2008)]. This is because it was necessary to converge economic fundamentals, such as the inflation rate and the interest rate, as a precondition for the introduction of the euro. However, since the establishment of the euro, the synchronicity of business cycles among countries within the euro area disappeared. It was under these circumstances that the euro crisis broke out and the possibility of a euro collapse was briefly discussed [see Krugman (2012)]. The euro area is not an OCA, and the need for a fiscal union, which is one of the conditions for an optimal currency area, was pointed out in particular. However, the euro area survived this crisis without meeting the conditions presented by the OCA theory. The OCA theory has not discussed supranational monetary policy, such as the OMT that contributed to the exit from this crisis. One of the reasons for this is that countries lose their independent monetary policy in a single currency area considered to be a type of fixed exchange rate system, as described in the impossible trinity. Then, it is assumed that they will not be able to make monetary policies that are appropriate to their respective situations. The loss of an independent monetary policy, as pointed out in the impossible trinity, occurs in a single currency area; however, it differs from a fixed exchange rate system such as the gold standard. The difference is the presence of a supranational central bank. Regarding the supranational system of controlling the value of gold to maintain the gold standard, Keynes (1930) states: “The ideal arrangement would surely be to set up a supranational bank to which the central banks of the world would stand in much the same relation as their own member banks stand to them.” [(Keynes (1930, p. 399)] In the context of the euro area, this supranational bank is the ECB, as the current central bank. Keynes (1930) also notes important implications for the ECB’s monetary policy in response to the euro crisis. “The supernational bank should also have a discretionary power to conduct open-market operations, by the purchase or sale on its own initiative either of long-term or short-term securities, with the assent, in the case of a purchase though not necessarily in the case of a sale, of the adherent central bank in whose national money the securities in question are payable.” [Keynes (1930, pp. 400–401)] This idea is an important point that is also relevant to the policies of OMT and quantitative easing (QE). However, this suggestion about supranational central banks has been underutilized in OCA theory, even though it is related to how the ECB would eventually deal with the euro crisis. Although the euro crisis ended, the euro area is still experiencing prolonged economic stagnation. In the context of the post-euro crisis, the ECB launched the Public Sector Purchase Programme in March 2015, which was a so-called quantitative easing initiative. Moreover, even as some countries struggle to emerge from the economic stagnation that followed the euro crisis, the COVID-19 epidemic has caused further economic stagnation since February 2020. Under these circumstances, the role of the ECB’s supranational monetary policy has become increasingly important in the euro area, where fiscal policy is constrained. This study contributes to the construction of a uniform framework for the OCA theory by analyzing supranational monetary policy from the perspective of the synchronization of business cycles. Moreover, this study allows us to consider the conditions necessary for the euro area to survive. Therefore, this study examines the impact of government bond purchases by the Eurosystem on the business cycles of countries. The Eurosystem is the monetary authority of the euro area and is headed by the ECB, which is a supranational central bank. In doing so, we carry out a stability analysis of the business cycle with a Kaldorian two-country model with imperfect capital movement. We are interested in how the demand-led economies under currency integration with Keynesian underemployment fluctuate through time and whether the system is dynamically stable or not. Kaldorian business cycle model developed by Kaldor (1940) is a suitable Keynesian model that can analyze these issues. This paper extends the Kaldorian business cycle model of a closed economy to a two-country model under currency integration that is suitable for our purpose. If the purchase of government bonds of a particular country by a supranational central bank stabilizes the business cycles of that country as well as those of other euro area countries and converges them to an equilibrium point, then the synchronicity of business cycles is likely to increase. The results of this study show the purchase of government bonds of a country that is facing a crisis can stabilize the business cycle of the country. The euro area satisfies the metacriteria of an OCA through the purchase of government bonds by the supranational central banking system. These results will help to build institutions for the successful implementation of ECO, which is a common currency that is scheduled to be introduced in the Economic Community of West African States (ECOWAS) in 2020. The rest of this paper is organized as follows. In Sect. 2, we formalize the two-country Kaldorian model with currency integration, which consists of an eight-dimensional system of nonlinear differential equations. In Sect. 3, we investigate the conditions for local stability of the equilibrium point mathematically. In Sect. 4, we present the results of some numerical simulations that support the theoretical analysis in Sect. 3. Finally, Sect. 5 concludes this article.",1
19.0,1.0,Evolutionary and Institutional Economics Review,09 November 2021,https://link.springer.com/article/10.1007/s40844-021-00229-x,Special feature on “The European Union Economy after the COVID-19 Pandemic Outbreak”: the fear of economy and the economy of fear,April 2022,Theodore Mariolis,,,Male,Unknown,Unknown,Male,,
19.0,1.0,Evolutionary and Institutional Economics Review,31 October 2021,https://link.springer.com/article/10.1007/s40844-021-00227-z,"National states, transnational institutions, and hegemony in the EU",April 2022,Costas Lapavitsas,Sergi Cutillas,,Male,Male,Unknown,Male,"The pandemic crisis brought multiple changes to monetary and fiscal policy in the European Union, encouraging greater discretion by member states and the Union as a whole. Some of these changes have been in the offing for several years but were held back partly due to the resistance of Germany, the hegemonic country of the EU. Germany partially relented as the pandemic shock threatened the stability of the euro and even of the EU itself. Discretion replaced rules in economic policy making, as is clearly seen in the operations of the European Central Bank (ECB) but also in fiscal policy. The pandemic shock also shone a fresh light on the hierarchical nature of the EU, including its division into core and periphery. Member states of the EU attempted initially to confront the crisis by drawing on their national strength and resources. Their actions inevitably meant that growing fiscal deficits pushed up public debt, thus making it difficult for heavily indebted states to access international financial markets. Borrowing rates for peripheral countries escalated rapidly. The immediate cause of the escalation was the declaration by Christine Lagarde, the President of the ECB, on 12 March 2020, that: “[w]e are not here to close spreads. This is not the function or the mission of the ECB. There are other tools for that, and there are other actors to actually deal with those issues” (ECB 2020). On the wake of this statement, concern about the ability of Italy to finance its public debt soared, posing a direct threat to the euro. By 24 March the ECB had changed course adopting the “Pandemic Emergency Purchase Programme” (PEPP), which soon acquired major dimensions driving interest rates close to zero. The very loose monetary policy of the ECB, going well beyond its original statutes, confirmed the changes originally spurred by the Eurozone crisis of 2010–12. Fiscal policy also had to adapt to the threat of the pandemic, lifting the rigid framework of austerity that had marked the 2010s. Peripheral EU countries were given scope to boost domestic demand, through automatic stabilisers and active fiscal intervention. Nonetheless, concern about rising public debt forced peripheral countries to strive for a joint EU fiscal response, supported by France. The underlying threat to the euro, and even to the Union itself, led Germany, the hegemonic country, to accede to a joint EU fiscal response, the “Next Generation EU”. Although the plan is not particularly large and had not disbursed a single euro well into 2021, its symbolism for the evolution of the EU is not in doubt. Currently, the EU operates without a clear framework of rules of economic policy making. This article offers an early analysis of these developments in the context of the long-standing theoretical debates on the balance between national states and transnational mechanisms in the EU. It stresses the importance of German hegemony and outlines the characteristic features of core and periphery in the EU. The institutional mechanisms that sustain the core-periphery division within the EU are primarily associated with the European Economic and Monetary Union (EMU) and chiefly the ECB. The euro had profound implications for the tiering of the EU, as is shown by analysing the balance sheet of the ECB and the operations of TARGET2. Thus, Sect. 2 of the article briefly summarises some relevant academic arguments regarding national states and transnational institutions of the EU; Sect. 3 considers Germany’s hegemonic position and its dependence on the transnational mechanisms of the EU; Sect. 4 discusses the institutional transformation of the EU following the outbreak of the Eurozone crisis; Sect. 5 analyses the operations of the ECB focusing on TARGET2, the peculiar clearing mechanism reflecting imbalances between core and periphery within the Eurozone; Sect. 6 briefly examines the lifting of austerity as well as the joint fiscal response of Next Generation EU, and concludes.",2
19.0,1.0,Evolutionary and Institutional Economics Review,26 July 2021,https://link.springer.com/article/10.1007/s40844-021-00217-1,Sustainable economic development in the European Union and COVID-19,April 2022,Chara Vavoura,Ioannis Vavouras,,Female,Male,Unknown,Mix,,
19.0,1.0,Evolutionary and Institutional Economics Review,21 October 2021,https://link.springer.com/article/10.1007/s40844-021-00228-y,"Economic crisis, COVID-19 pandemic, and the Greek model of capitalism",April 2022,Georgios Maris,Floros Flouros,,Male,Male,Unknown,Male,"The last Greek economic crisis affected all aspects of the social life in the country, worsening the economic indicators and the relationship between the citizens and the state. Several studies have been published during last year’s highlighting various aspects of the Greek economic crisis (see among others, Maris et al. 2021; Pagoulatos 2020; Lapavitsas 2019; Featherstone 2011). All these approaches provided specific answers regarding the main causes of the eruption of the Greek economic crisis as well as the proposals to deal with. In this article, we will try to shed light on the economic performance of Greece using the theoretical approach of the varieties of capitalism. More specifically, we are going to study the main characteristics of the Greek model of capitalism focusing on the role of the state which affects all aspects of both political and economic environment in the country. This paper complements other efforts that have been developed during last decade trying to emphasize on the significant effect of institutions to the Greek failure to support sustainable growth rates (Maris 2021; Maris et al. 2021; Sklias and Maris 2013). The approach of varieties of capitalism will be used as a tool to understand not only the reasons of the eruption of the Greek economic crisis, but also the long-run institutional weaknesses and the main characteristics of the Greek model of capitalism. Thus, the main purpose of this paper is twofold: first, to study the evolution of the country's economic performance through the theoretical approach of varieties of capitalism; second, highlighting the performance of institutions to understand how the main characteristics of the Greek model of capitalism have been changed during last years. The main questions of the article are: How the Greek model of capitalism affected not only the institutional but also the economic performance in Greece? Does the COVID-19 pandemic influence the model of capitalism in Greece? By applying the principles of the comparative political economy, we will be able to draw conclusions regarding the institutional performance in Greece and the characteristics of the Greek model of capitalism. As we will argue, the Greek model of capitalism was shaped by the power of the ideas of the country's political leaders since 1980s. These ideas influenced the characteristics of the Greek model of capitalism creating a rather exceptional institutional framework unable to support long-run economic growth in the country. This exceptionalism is associated with an institutional asymmetry in relation with other member states and characterizes the Greek model of capitalism not only as the main cause for the economic crisis, but also as a significant obstacle to any recovery efforts, especially during COVID-19 pandemic. In the first section of this article, we will study the theoretical framework of the varieties of capitalism, while in the second section, we will highlight the main characteristics of the Greek model of capitalism during the period of 1980–2009. In the third and fourth sections of this article, we will evaluate the economic crisis and the current COVID-19 pandemic and then we will reach into the main conclusions.",5
19.0,1.0,Evolutionary and Institutional Economics Review,15 November 2021,https://link.springer.com/article/10.1007/s40844-021-00230-4,The impact of COVID-19 on global stock markets: early linear and non-linear evidence for Italy,April 2022,Theodoros Daglis,Ioannis G. Melissaropoulos,Panayotis G. Michaelides,Male,Male,Unknown,Male,"The pandemic of the so-called coronavirus disease (COVID-19) has gained a global character as it has spread from one country to another, having its origin in Wuhan-Hubei, China (Liu et al. 2020). The fact that the total confirmed COVID-19 cases amount to several millions globally, underlines the importance of this pandemic (WHO 2020). In fact, the case of COVID-19 is unique, differing in many ways from previous disease spread, for instance the severe acute respiratory syndrome (SARS) spread in 2003 (Wilder-Smith et al. 2020), or the H1N1 virus of 2009. However, thus far, the financial consequences of such a pandemic remain less widely discussed. Studies on the macroeconomic effects of other epidemics, for instance the one caused by the SARS virus in 2003, have shown significant effects on the total economy, through large reductions in consumption of various goods and services, increases in business operating costs, and re-evaluation of country risks reflected in increased risk premia (McKibbin and Fernando 2020). In addition, due to the global trade and financial integration, these shocks were also transmitted to other economies as well, whereas the degree of contagion between the different economies varied according to the strength of financial and trade relationships among them. In general, in the case of previous epidemics, global costs were significant (Lee and McKibbin 2004). To cast a glimpse on the economic consequences of such a situation, in terms of monetary units, previous studies showed that the economic effect of influenza on the US economy is estimated to be around $73.1–$166.5 billion (Meltzer et al. 1999). In fact, many economies in their attempt to put a halt to the spread of COVID-19, imposed the closing of many businesses, markets, etc. Other measures that were enforced included the suspension of all entry of immigrants and non-immigrants that had travelled to high-risk zones (The White House 2020). Furthermore, many countries suspended several public transport services across the border (Sohrabia et al. 2020). Even the Olympic games, an event of high importance (Gallego et al. 2020), were postponed. Of course, the financial system, which is a crucial part of the economic system, also faces the adverse consequences of a pandemic. In fact, the financial markets are the first that capitalized the risk induced by the pandemic. A prominent study by Donadelli et al. (2017), showed that the declaration of pandemics, induced by the SARS-COV-2 and the H1N1 virus, had a positive impact on the stock market index of pharmaceuticals that were likely to develop a vaccine and a negative effect on associated stock prices of other firms in different sectors. Regarding the COVID-19 pandemic, several studies show that stock market indices of various economies experienced a negative longstanding effect. See among others, Ashraf (2020), Albuquerque et al. (2020), Cheng (2020), Gormsen and Koijen (2020), Ramelli and Wagner (2020). Nonetheless, most of these studies, examine the impact of the pandemic just by comparing stock exchange data during the COVID-19 period and the pre-pandemic period. Therefore, a mechanism on how the exogenous effect of the pandemic impacts the stock market is still vague in the literature. The facts on the early pandemic stage show that the stock markets of various economies started to experience sudden drops in their overall price index before the 11th of March 2020, when the recent situation was declared to be a pandemic. These market drops are associated with negative news regarding the evolution of COVID-19 and the sudden increase in COVID-19 incidents globally (Akhtaruzzaman et al. 2020; Baket et al. 2020). In other words, the stock markets capitalized the increased exogenous risk from the pandemic, due to the fact that investors anticipated that the spread of the virus will force policy actors to take lockdown measures to secure public health (Haroon and Rizvi 2020; Hoshikawa and Yoshimi 2021). As a result, the lockdown measures will decrease consumption and production diminishing at the same time the margin profits of firms and thus the overall expected return of investors’ portfolio. This mechanism describes compactly how the evolution of the pandemic directly affected the stock market index of various economies. In this paper, for the first time in the literature, using relevant econometric and forecasting techniques, to test the direct effect of the pandemic on the stock market index, we model the impact of COVID-19 confirmed cases on the stock market index. To do so, we will focus on a European stock market that was severely hit in the early stages of the pandemic, namely the Italian stock market. Based on official data that come from the Central Bank of Italy, the Italian stock market faced the eighth most extreme drop in its history in February 2020, when the pandemic in Italy was expanding rapidly (Just and Echaust 2020). Despite the efforts made by policy makers to suspend short-selling actions in the Italian stock market, on the 12th of March 2020, the Italian Stock market faced a tremendous drop of almost 17% (Statista Research Department, 23-11-2020). The aforementioned facts highlight our choice for selecting the Italian stock market as our primary field of research. Next, to study the characteristics of the Italian stock market and their potential dependence from the pandemic, we make use of an autoregressive fractionally integrated model with a generalized autoregressive conditional heteroskedasticity error (ARFIMA-GARCH), we model the volatility of the stock market indices of the aforementioned economy. Our modelling choice regarding the use of an ARFIMA GARCH model lies in previous attempts made in the literature of volatility modeling of stock markets. See, among others, Bekaert and Harvey (1997), Scheicher (2001) Syriopoulos (2007), Abdalla 2012, Daglis et al. (2020). Based on our findings, we witness a negative impact of COVID-19 on the Italian stock index, and an increase in its volatility, whereas both effects are statistically significant. The paper is structured as follows: Sect. 2 sets out the methodology; Sect. 3 presents the results; finally, Sect. 4 concludes the paper.",1
19.0,1.0,Evolutionary and Institutional Economics Review,29 August 2021,https://link.springer.com/article/10.1007/s40844-021-00222-4,The COVID-19 multiplier effects of tourism on the German and Spanish economies,April 2022,Nikolaos Rodousakis,George Soklis,,Male,Male,Unknown,Male,"The coronavirus pandemic (COVID-19) has not only had a significant impact on public health, but it has also severely affected one of the linchpins of the European Union (EU) economy—the tourism sector. According to the latest data from Eurostat, 10% of the EU non-financial business economy belonged to the tourism industries, employing 12.3 million people; 75% of those industries operated in accommodation (14%) or food and beverage serving (61%) activities, while 55% of them were located in Italy, France, Spain and Germany.Footnote 1 As many countries have introduced curfews and travel restrictions to contain the spread of the coronavirus, international travels have come to an almost complete standstill. Moreover, the long-lasting lockdowns imposed in many countries also disturbed the supply chain of the tourism industry, since many tourism related activities were put on hold. The latest data from the World Tourism Organization (UNWTO) show that the international arrivals in Europe dropping by 70%: in Western Europe by 65%; in Southern Europe by 71%; in Northern Europe by 75%; and in Central Europe by 70%. More than a half of the loss of worldwide international arrivals (−1.07 billion persons) belongs to Europe (− 0.52 billion persons).Footnote 2 Thus, while the countries were seeing a novice drop in tourist arrivals, all the sectors that depend on tourism activities are facing similar setbacks, ranging from closures in the food services to decreasing guest numbers in the accommodation (see e.g., Škare 2020).Footnote 3 During the pandemic, a significant number of studies explore the decline of tourism activities and the corresponding impact on the economic system (see, e.g., Farzanegan et al. 2020; Lee and Chen 2020; Mariolis et al. 2020; Qiu et al. 2020; Tsionas 2020; Yang et al. 2020). The purpose of this paper is to provide estimations for the COVID-19 tourism multiplier effects on output, employment and trade balance of the German and Spanish economies. Since tourism has become an important part of economic activities during the last decades, it is reasonable to expect that the COVID-19 shock on the tourism sector will significantly affect the economic system due to (a) the loss of jobs in tourism, which will reduce incomes of the people involved in the tourism industry and, therefore, will diminish aggregate consumption; and (b) the fall in demand for intermediate inputs by the sectors that supply inputs to the tourism industry. Thus, it is important to study the intersectoral effects of the decline in tourism activities. For this purpose, we follow the approach of Mariolis et al. (2020), i.e., we use an analytic framework inspired by the concept of the Sraffian multiplier (Kurz 1985; Metcalfe and Steedman 1981; Mariolis 2008a) and data from the Symmetric Input Output Tables (SIOTs) for the year 2015 (latest data at the time of this research) provided via the Organisation for Economic Co-operation and Development (OECD) website, https://stats.oecd.org. As it has been demonstrated, the multiplier for real-world economies does not constitute a scalar, but a matrix quantity reflecting, in a complex way, the underlying inter-industry socio-technical linkages. It could, furthermore, be shown that the Sraffian multiplier includes as special versions or limit cases the usual Keynesian multiplier, the multipliers of the traditional input–output analysis, and their Marxian versions.Footnote 4 Since Germany (Spain) is characterized by a relatively low (high) dependency on the tourism sector, this empirical analysis will reveal whether the differences in the downturn within the EU economy can be attributed to the degree of the dependency of the economies on tourism activities.Footnote 5 The remainder of the article is structured as follows. The second section presents the method. The third section presents the empirical estimations. The fourth section concludes.",7
19.0,1.0,Evolutionary and Institutional Economics Review,13 March 2022,https://link.springer.com/article/10.1007/s40844-022-00235-7,"Robert Boyer, Les capitalismes à l’épreuve de la pandémie, La Découverte, 2020",April 2022,Kazuhiro Okuma,Yuji Harada,,Male,Male,Unknown,Male,"The COVID-19 pandemic that started at the end of 2019 did not show signs of convergence. How has the economic crisis provoked by the pandemic affected economic societies? In the longer term, how will the pandemic change various forms of capitalism? In this book (published in French), a luminary of régulation theory (which originated in France) examines the recent economic crisis. Whereas many discussions have been proposed with regard to the current crisis, Robert Boyer builds his own analysis in relation to assertions on the following topics, which he has elaborated over the past two decades: a new mode of development based on the evolution of information and communication technology (ICT) (Boyer 2004); financial capitalism (Boyer 2011); European integration (Boyer 2012b); and international regime, including the interaction of different national-regional regimes (Boyer 2012a). Boyer (2015) proposes a comprehensive theoretical framework for régulation theory composed of these arguments.Footnote 1 He explores the present crisis of the pandemic from the viewpoint of the political economy by freely making use of these claims. This book has insightful implications for diverse issues in the political economy. Therefore, this review aims to carefully introduce the book’s main rationale in English to help readers understand its message and to assess the potential of its arguments by focusing on actual issues, especially in Japan.",1
19.0,2.0,Evolutionary and Institutional Economics Review,13 March 2022,https://link.springer.com/article/10.1007/s40844-022-00234-8,Combining preferences and heuristics in analysing consumer behaviour,September 2022,Pere Mir-Artigues,,,Male,Unknown,Unknown,Male,"It is common knowledge that within economic psychology,Footnote 1 the rapid-and-frugal heuristics research programme, also called ecological rationality or environment-consistent rationality, has focussed on the role of simple inferences in people's everyday decisions (Gigerenzer and Gaissmaier 2011: 454). In general, this research programme presents the following theoretical and methodological singularities (based on Martignon and Hoffrage 2002; Gigerenzer et al. 2008: 233–234; Berg 2014a; Tomer 2017: 30 and Schilirò 2017): The mind is a powerful toolbox to adapt to changes in context. The subject of analysis is the correspondence between decision-making procedures and the basic characteristics of the environment in which they are applied (Payne et al. 1988, Gigerenzer 2001; Gigerenzer 2008; Skořepa 2011: 46; Todd and Gigerenzer 2012: 8–12). Therefore, there is no definable benchmark for assessing the effectiveness of decision-making processes (Gigerenzer 2008). The normative quality is given by the finding that a procedure has yielded sufficiently good results in a certain kind of situations. Heuristics are straightforward but not simple inferences, ideal when there are many options, reality is full of uncertainty and accumulating more and more information can be counterproductive: the famous less-is-more paradox (See Medvec et al. 1995 and Gigerenzer 2014: 97). They are cognitive shortcuts like the rule of thumb initially used by carpenters to take measurements by eye, which was not an obstacle to doing a good job. The use of heuristic procedures does not necessarily have harmless consequences, as they seldom prompt decisions that harm individuals’ physical integrity, lower their level of well-being, cause severe economic losses, etc. (Berg 2003; Arkes et al. 2016). Heuristics are neither fickle nor frivolous, so they do not reflect any allegedly flawed design of the human mind. On the contrary, they leave no doubt about its vast creativity and adaptability within the long and contingent evolution of the species (Gigerenzer 2014: 14; Gigerenzer 2018). People have multiple heuristics at their disposal. They choose the one that seems the most appropriate given the circumstances based on the information stored in their memory (which, in turn, reflects their experiences and cumulative knowledge) and the differences in their personalities (for example, not everyone is intuitive in the same way, nor does everyone have the same degree of anxiety). It is not surprising, therefore, that different people choose different heuristics. Heuristics support robust predictions about behaviour (Roberts and Pashler 2000). Analysts therefore design lab and field experiments with great care, as well as using mathematical simulation techniques (Berg and Gigerenzer 2010: 16–17). The aim is to improve decision-making in areas such as health, business, courts, etc. (Gigerenzer and Brighton 2009: 129; Gigerenzer and Edwards 2009). From this conceptual basis, this paper aims to discuss the role of preferences and several heuristics in the final consumption of goods and services. To begin with, Sect. 2 explains the concept of heuristics and lists the most relevant ones in consumption, and the term ‘preferences’ is modified. Section 3 presents the model itself, in which numerous factors are combined in a multi-stage process which ends up in a single final consumption decision. In this section, we discuss the peculiarities of the different heuristics in consumer decisions in some detail. The text closes with brief conclusions and prospects for future work.",
19.0,2.0,Evolutionary and Institutional Economics Review,29 April 2022,https://link.springer.com/article/10.1007/s40844-022-00242-8,The extent of the firm,September 2022,M. J. Histen,,,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Evolutionary and Institutional Economics Review,29 January 2022,https://link.springer.com/article/10.1007/s40844-022-00231-x,Hiding or revealing: their indirect evolution in the Acquiring-a-Company game,September 2022,Werner Güth,Stefan Napel,,Male,Male,Unknown,Male,"Asymmetric information of traders on markets can imply no-trade results (see Akerlof 1970; Bazerman and Samuelson 1983; Samuelson and Bazerman 1985). Policy-makers may try to limit private information (see, for our setting, the Takeover Bid Directive 2004/25/EC, the Transparency Directive 2004/109/EC, and the Unfair Commercial Practices Directive 2005/29/EC of the European Union) or encourage its disclosure, for instance by rewarding “whistle blowing” or by public records about previous behavior. Regarding the latter, market participants may themselves decide whether to hide or to reveal what they privately know in each and every case, i.e., by hiding or revealing acts given the specific situation. Instead we will focus on situations where they rely on a general rule and are committed to categorical hiding or revealing (see Harsanyi 1979, who distinguishes between act and rule utilitarianism). We focus on rule-guidedness but not, like in rule utilitarianism, by engaging in a consequentialist decision between categorically hiding or revealing. Instead, like in evolutionary biology and game theory, categorical inclinations are viewed as inherited rules of behavior whose population shares evolve according to their relative reproductive success. As usual in evolutionary analysis of markets, average or expected profit is assumed to measure the fitness of the different categorical types. Our indirect evolutionary approach (see Berninghaus et al. 2012, as well as Alger and Weibull 2013 for reviews) allows some market behavior, namely the price choices, to be rationally decided, based on how traders perceive and evaluate their market environment. As often in indirect evolution we distinguish between decision-driving utility, which traders maximize given their market perception, and evolution-driving fitness. Market traders need not be aware of the latter; they may be completely ignorant of evolutionary selection and its determinants.Footnote 1 The Acquiring-a-Company game (Bazerman and Samuelson 1983; Samuelson and Bazerman 1985) features a privately informed seller and an uninformed potential buyer of a company. It was investigated by Samuelson and Bazerman by running – to the best of our knowledge – the first stochastic ultimatum experiments which, later on and rather independently of ultimatum experiments (see Güth and Kocher 2014), were repeated by other authors with more or less modifications. Di Cagno et al. (2016) modified the original Acquiring-a-Company game by letting the privately informed seller send a numerical value message to the prospective buyer, who then proposes a price for the seller’s company. Rather than studying the respective (game-theoretically innocent) cheap-talk communication, we use Di Cagno et al.’s setup to investigate the evolution of categorical hiding or revealing. When sellers categorically reveal,Footnote 2 welfare enhancing trade is guaranteed. But revealing sellers may be exploited, i.e., make zero profit, due to the ultimatum power of the buyer. On the other hand when sellers hide, one obtains a no-trade result for a generic parameter region. Trade takes place only outside this region with both traders, the seller and the buyer, gaining from trade in expectation (how much depends on the specific parameter level). Our analysis confirms the intuition that revealing can hardly ever survive without buyers rewarding sellers who are revealing. In line with the robust evidence of ultimatum experiments it will be assumed that sellers do not accept exploitation what, in turn, induces buyers to reward categorically revealing sellers. Will such rewarding increase the population share of revealing individuals and how would this depend on institutional and behavioral aspects? Regarding behavioral aspects we consider loss aversion of buyers who, when confronting a hiding seller, experience gains and losses with positive probabilities. Institutional aspects, on which we focus, are the average potential gains from trade, competition between potential buyers and the technology through which buyers may learn about the categorical type of their seller. Section 2 describes the modified Acquiring-a-Company (AaC) game, which allows for hiding and revealing value messages and which, in case of hiding, does not question the pooling solution of the original AaC game. A first evolutionary analysis justifies that revealing types may not survive and confirms this pooling solution (Sect. 3). The possibility that decision utility and fitness in the sense of expected profits differ is highlighted by allowing buyers to be loss averse. Section 4 lets the buyer invest in costly detection of the seller’s type. Section 5 assumes, in line with the abundant evidence of ultimatum experiments, that buyers reward revealing. We focus on the implications of imperfect type signals in Sect. 6. Section 7 concludes.",
19.0,2.0,Evolutionary and Institutional Economics Review,16 April 2022,https://link.springer.com/article/10.1007/s40844-022-00240-w,Impulse balancing versus equilibrium learning an experimental study of competitive portfolio selection,September 2022,Judith Avrahami,Werner Güth,Matteo Ploner,Female,Male,Male,Mix,,
19.0,2.0,Evolutionary and Institutional Economics Review,12 April 2022,https://link.springer.com/article/10.1007/s40844-022-00239-3,Organizational knowledge actions and the evolution of knowledge environment: a micro-foundations perspective,September 2022,Michalis E. Papazoglou,,,Male,Unknown,Unknown,Male,"Organizations function in certain, dynamic environments by which they are determined and on which they have the ability to exert an influence. The issue of how an organization’s actions and decisions affect and shape its environment is always a topic of great interest for management and organizational researchers. Although there exist theoretical perspectives that focus on the ability of organizations to shape their environment (e.g., institutional entrepreneurship, strategic choice), there is a gap in our understanding of how this shaping is realized. What are the mechanisms that transform organizational actions into environmental characteristics; how the organizations’ environment is built step-by-step by organizational actions? Taking an evolutionary, micro-foundations perspective, Nelson and Winter (1982), in one of the most influential works in social sciences, stressed the importance of this topic by positing that “the core concern of evolutionary theory is with the dynamic process by which firm behavior patterns and market outcomes are jointly determined over time” (1982, p. 18) and hoping that “perhaps in the future it will become possible to build and comprehend models of industry evolution that are based on detailed and realistic models of individual firm behavior” (1982, p. 36). Inspired by this view, this article endeavors to conceptually connect organizational-level actions with their resulting consequences on the evolution of a specific aspect of the organizational environment, namely the knowledge environment. I use the social mechanisms approach to model the interactions between an organization and its environment (Hedstrom and Swedberg 1998) and I employ the VSR processes of evolutionary change to model the pattern of the knowledge environment’s shaping by organizational actions (Campbell 1965; Zollo and Winter 2002). In particular, I propose that in their effort to adapt and respond to the perceived environmental conditions, organizations make certain actions using existing knowledge from their environment. The consequences of these organizational knowledge actions concern not only the organizations that generated these actions, but also their knowledge environment. According to my proposal, the effect of organizational knowledge actions on knowledge environment follows the VSR pattern. More specifically, organizational knowledge actions, as potentially including novel knowledge components, add variations to the environment and, as using existing knowledge components, they alter their importance. These processes drive me to view knowledge environment as a mosaic of knowledge components whose pieces are the novel knowledge components incorporated in organizational knowledge actions and their sizes are determined by the extent of their influence on subsequent knowledge actions. By viewing knowledge environment as a mosaic created by organizational knowledge actions, this paper advances theorizing on the organizations’ ability to shape their knowledge environment. From my perspective, each organizational knowledge action inevitably affects the knowledge environment and it does so in two distinct ways; either by adding new knowledge components or by altering the importance of the existing knowledge components that are reproduced by each knowledge action. This perspective could stimulate a stream of research that will focus on the quantification of the impact of each organization on the evolution of its knowledge environment. The exemplar presented in Sect. 3, which analyzes how a subset of knowledge environment—the patented knowledge—is shaped by organizational knowledge actions, offers an initial step in this direction.",
19.0,2.0,Evolutionary and Institutional Economics Review,29 June 2022,https://link.springer.com/article/10.1007/s40844-022-00244-6,The full employment interest rate implicit in classical economic theory,September 2022,Nicolas D. Cole,,,Male,Unknown,Unknown,Male,"This paper provides a Theorem which proves 3 interrelated facts. Firstly, that in a free-market economy there is a unique full employment interest rate of 4½% at which Labor is not exploited by Capital. Secondly, that the failure of orthodox theory to detect this optimum is due to its disregard of the rate of normal profit necessary to induce savers to invest rather than just earn interest. Thirdly, that the attacks by both Marx (1867) and Keynes (1936) on the free-market economy were misdirected at Classical economics rather than on the manipulation of interest rates, to protect Capital at the expense of Labor, which was commenced in the mid nineteenth century.",1
19.0,2.0,Evolutionary and Institutional Economics Review,18 March 2022,https://link.springer.com/article/10.1007/s40844-022-00236-6,Innovation patterns in firms and intra-industry heterogeneity empirical evidence from Turkey,September 2022,Alp Eren Yurtseven,Mehmet Teoman Pamukçu,,Male,Male,Unknown,Male,"Similar technological capabilities, financial incentives, and constraints may shape common paths for firms. As characterized in the concept of technological regimes by Nelson and Winter (1982), these regularities may direct the firms to organize innovative activities in resembling ways. According to Winter (1984), technological regimes define a knowledge domain’s key features concerning the imitability of technology, the number of knowledge bases about a specific production method, amount of resources committed to a typical project. The sectoral innovation system approach depends on how firms nested in a sector behave in correlated ways since they share information and technology sources and perceive similar innovation incentives (Malerba and Orsenigo 1996; Malerba 2005). Early studies on technological regimes (Nelson and Winter 1982; Pavitt 1984) posit that firms’ innovative behavior is primarily determined by the technological regime in which they operate. Moreover, these studies suggest that such technological regimes are industry-specific due to idiosyncratic technological opportunities and knowledge conditions. A firm’s strategy involves defining goals, setting up a course of action, and mobilizing resources to execute these actions. In this sense, the strategy of a firm can be seen as a pattern of activity. The firm’s dynamic capabilities shape this activity pattern. On the other hand, the firm’s course of action is also affected by the opportunities and degree of competition and the knowledge base and institutional structure of the sector in which it operates. Therefore, a firm’s strategy is also path dependent, and the dominant technological regime in the sector characterizes it. On the other hand, the strategic management literature (Wernerfelt 1984; Barney 1986; Dierickx and Cool 1989; Teece and Pisano 1994; Teece et al. 1997) explicitly argues that firms within an industry may purposefully differentiate themselves from their competitors to cope with competitive pressure. Accordingly, a firm’s resource position, knowledge stock, the capability to generate new knowledge, learning abilities, and competence in forming and maintaining collaborative ties with networks are decisive on its strategy. Similarly, evolutionary economic theory suggests that firms differ in terms of their innovative behavior. Boundedly rational firms develop new skills and advance mainly through local search (Leiponen and Drejer 2007). On the other hand, firms operating in the same environment may opt for different strategies if their landscape entails enough complexity (Levinthal 1997). Several empirical studies indicate that firms’ differential performance in profitability is attributable to firm-specific characteristics rather than sectoral affiliation (Rumelt 1991; Powell 1996). Dosi (2005) reports persisting heterogeneity across firms notwithstanding the competitive process. Grilliches and Mairesse (1995) also address the need for a richer theoretical framework to better explain firms’ heterogeneity within the same business line. Recent empirical studies indicate that different innovation patterns may co-exist within sectors. Moreover, heterogeneity stemming from the co-existence of distinct modes of innovation may vary across industries (Leiponen and Drejer 2007; Frenz and Lambert 2010; Srholec and Verspagen 2012). Although these studies provide compelling evidence regarding the diversity of innovation patterns within industries, the dynamics of such heterogeneity require further attention. Previously, Woerter (2009) constructed a diversity metric based on the average Euclidean distance of firms within a sector based on their R&D spending intensity, share of export revenue over total sales, firm size (proxied by the number of employees), and share of staff with higher education. We contribute to the existing literature by exploring the features that affect the diversity of innovation modes at the sector level. We use a novel approach to quantify the intra-industry heterogeneity of innovation patterns and analyze the underlying sectoral factors. Evolutionary economics suggests that the rate of progress displayed within a particular industry depends on the degree of the economic variety. Nelson (1990) emphasizes the importance of such heterogeneity for economic growth and puts diversity at the core of the evolutionary advance of industries. Nelson argues that the main advantage of capitalism over central planning is offering a broad spectrum of paths in which existing techniques can be improved. Economic growth is often conceptualized as a process in which all underlying technologies increase the efficiency of production methods. However, economic development cannot be solely attributed to quantitative change. Qualitative change alters the composition of the sector, whereas structural change results in creating new sectors. Saviotti and Mani (1995) suggest that technological diversity is a prerequisite for sustainable economic growth. These studies show a relationship between industrial dynamics and the diversity of firm-level innovation characteristics within that sector. Contemporary evolutionary economics is often termed “neo-Schumpeterian” to embrace Schumpeter’s dynamic view of economic processes characterized by discontinuities and constant change. Nelson and Winter (1982; 39) label their evolutionary approach as neo-Schumpeterian. According to Witt (2008), Nelson and Winter’s methodology accommodates a dualistic evolutionary view. Biological and socio-economic evolutions constitute two distinct processes of reality, i.e., generalizations derived from one domain cannot be directly applied to another. We adopt this neo-Schumpeterian approach and operationalize the adjusted Simpson index often used in measuring the biodiversity of ecosystems to quantify intra-industry heterogeneity of innovation patterns. In the next section, we develop hypotheses based on a literature review addressing the diversity of innovation patterns. The third section outlines our methodological approach. We present our empirical results in the fourth section. In the last section, we draw conclusions and highlight policy recommendations and managerial implications.",
19.0,2.0,Evolutionary and Institutional Economics Review,14 February 2022,https://link.springer.com/article/10.1007/s40844-022-00232-w,Role of agricultural productivity growth in economic development: the neglected impact on institutional quality in Africa,September 2022,Richard Grabowski,Sharmistha Self,,Male,Female,Unknown,Mix,,
19.0,2.0,Evolutionary and Institutional Economics Review,11 April 2022,https://link.springer.com/article/10.1007/s40844-022-00237-5,The transformation problem: a mathematical approach of its solution within Marx’s original framework,September 2022,Zhongren Zhang,,,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Evolutionary and Institutional Economics Review,25 May 2022,https://link.springer.com/article/10.1007/s40844-022-00243-7,Correction to: The transformation problem: a mathematical approach of its solution within Marx’s original framework,September 2022,Zhongren Zhang,,,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Evolutionary and Institutional Economics Review,13 April 2022,https://link.springer.com/article/10.1007/s40844-022-00238-4,"Daniel W. Bromley, Possessive individualism: a crisis of capitalism, Oxford University Press, 2019",September 2022,Franklin Obeng-Odoom,,,Male,Unknown,Unknown,Male,"The changing existential crises of society, economy, and environment demand reflection and action. Global responses to Thomas Piketty’s (2014, 2019) two-volume treasure trove of data demonstrate that new diagnoses are also needed. Institutional and evolutionary economists have risen to the task. Many of their resulting analyses have been reviewed in this journal. Microfoundations of Evolutionary Economics (Shiozawa et al. 2019) reviewed by Yoshio Inoue (2021), Robert Boyer’s (2015) Économie politique des capitalismes. Théorie de la régulation et des crises, reviewed by Harada and Uemura (2019), and Property, Institutions, and Social Stratification in Africa (Obeng-Odoom 2020) reviewed by Toyomu Masaki (2021) are examples. Possessive Individualism: A Crisis of Capitalism, written by Daniel W. Bromley should also be reviewed. Three reasons justify the point. First, unlike Geoffrey M. Hodgson’s new book (2019) Is There A Future For Heterodox Economics? Institutions, Ideology and A Scientific Community, a house-cleaning exercise, seeking to criticise institutionalists and other heterodox economists, and suggesting that there is probably no future for the field (see Dow 2021, for a review), Bromley (2019) sees much more power and future in institutional and evolutionary economics, if less sanguine about capitalism. Second, Bromley is such a major voice in institutional economics. Not only is he one of the top-ten most cited institutional economists in the world today,Footnote 1 Bromley is also a winner of the Veblen-Commons award, the highest distinction for an institutional and evolutionary economist. Currently Professor Emeritus at University of Wisconsin-Madison in the United States, his work over the years has helped to define the course and character of institutional economics, among others through teaching Institutional Economics, the course created by J.R. Commons himself and which Bromley taught and transformed for 21 years (Obeng-Odoom and Bromley 2020). Most fundamentally, Possessive Individualism should be reviewed because it engages Crawford Brough Macpherson’s (1962) original book The Political Theory of Possessive Individualism: Hobbes to Locke, but it transcends this important work (for helpful reviews, see Andrew 2012; Lindsay 2012) in ways that are vital for institutional and evolutionary economics. Consider the scale of application. Not only is Bromley’s expanded to regional analysis, it is also applied to the whole world. Macpherson thought of his work in political terms, but Bromley’s Possessive Individualism is a political-economic work. Ultimately, both in respect of form and substance, we need to consider Bromley’s Possessive Individualism as sui generis. In what follows, I explain W. Bromley’s new theory (Possessive Individualism), show how its explanation is structured in the book (Theory Building), and offer a crie de cour (Critique). I conclude the essay by emphasising how the theory could be further developed (Towards Extending Evolutionary and Institutional Economics).",
20.0,1.0,Evolutionary and Institutional Economics Review,12 May 2023,https://link.springer.com/article/10.1007/s40844-023-00251-1,Production economy and industry studies,April 2023,Takahiro Fujimoto,,,Male,Unknown,Unknown,Male,,
20.0,1.0,Evolutionary and Institutional Economics Review,18 January 2023,https://link.springer.com/article/10.1007/s40844-022-00247-3,"How Japanese firms address the issues of environment, society, and governance: a corporate governance perspective",April 2023,Mitsuharu Miyamoto,Hiroatsu Nohara,,Male,Unknown,Unknown,Male,"Interest in environment, society, and governance (ESG) and ESG-related investment issues has increased worldwide. The 2006 principles for responsible investment (PRI) of the United Nations initiated this trend, which the 2008 global financial crisis and the subsequent rise of distrust in (and hostility to) shareholder capitalism amplified. Thus, Mayer (2013) warned that the corporate system would be unsustainable if the pursuit of short-term shareholder interests neglects the responsibility of commitment to stakeholder interests. Recently, the US Business Roundtable, comprising CEOs of major US companies, declares a shared commitment to “all” stakeholders, as if in response to Mayer’s warning. Give these circumstances, Japanese firms may be considered front runners in ESG and ESG-related investment engagement, because they have been characterized by stakeholder-oriented corporate governance. Until recently, however, Japanese firms have rarely addressed ESG issues. Although they paid attention to corporate social responsibility (CSR) in the early 2000s in response to the global trend (Tanimoto 2006), actual activities were sluggish. Japanese firms have often discussed a social contribution as a CSR issue. However, it did not necessarily lead to actual behaviors that address environmental and social issues. As a result, interest in CSR has ended in a temporary boom. A reason for weak CSR and ESG in Japan may be due to the nature of Japanese corporate governance. As conceptualized by Aoki (2010), Japanese firms shaped corporate structure based on the shared cognition between managers and employees, as compared to the shared cognition between managers and shareholders in US firms. Thus, the essential issue of Japanese firms was to secure stable employment for “employees as a stakeholder,” which has long been a matter of CSR for Japanese firms before the term CSR prevailed. However, this situation strengthened insider advantage. Thus, outsiders, including shareholders, tended to be oblivious to operations, as they seemed not to be directly involved in the firm’s activities. Accordingly, outside issues related to environment and society were rarely recognized as CSR issues. Here, it should be noticed that ESG explicitly integrates governance issue to environmental and societal issues of CSR. Governance issue intends to enhance the transparency of management for outsiders by increasing independent board members and diversity. This is exactly the issue of Japanese corporate governance with insider advantage. While Japanese firms have been engaged in corporate governance reform over the past 2 decades, it remains insufficient regarding the transparency of management and outsider orientation. Therefore, if the reform of Japanese firms shifts from an insider to outsider focus, following the governance issue of ESG, involvement in outside issues on the environment and society may increase. However, if the reform advances in the direction of the US style shareholder-dominated corporate governance, it may conflict with the existing stakeholder orientation that provided stable employment as CSR. This situation, thus, creates a challenge for Japanese firms on how to engage and cope with both inside and outside stakeholders. Hence, ESG is a fundamental issue for Japanese firms. Given these perspectives, this study employs corporate-level data to examine how Japanese firms promote ESG engagement from a corporate governance perspective. While previous studies often utilized CSR or ESG indicators via other databases that are already processed, our study is based on raw data from 1522 listed firms during the 2009–2016 period from Bloomberg. The remainder of this study is structured as follows: Sect. 2 presents an overview of the varieties of ESG investments and presents a framework for the following analysis: Sect. 3 explains the panel data used in this study; Sect. 4 examines the determinants of ESG involvement by Japanese firms from a corporate governance perspective; Sect. 5 analyzes how ESG activities treat environmental and societal issues, and Sect. 6 presents the discussion and conclusion of the study.",
20.0,1.0,Evolutionary and Institutional Economics Review,28 March 2023,https://link.springer.com/article/10.1007/s40844-023-00248-w,Diversification and evolution of post-modern money as “ideational money”: from MMT to PMMT,April 2023,Makoto Nishibe,,,,Unknown,Unknown,Mix,,
20.0,1.0,Evolutionary and Institutional Economics Review,23 December 2022,https://link.springer.com/article/10.1007/s40844-022-00246-4,Unemployment persistence with an evolutionary perspective: job creation or destruction (or both)?,April 2023,De-Chih Liu,,,Unknown,Unknown,Unknown,Unknown,,
20.0,1.0,Evolutionary and Institutional Economics Review,07 April 2023,https://link.springer.com/article/10.1007/s40844-023-00254-y,Endogenous constraints on full productive capacity in a free-market economy,April 2023,Nicolas D. Cole,,,Male,Unknown,Unknown,Male,"This short paper clarifies some concepts and extends the implications of the author’s paper concerning the deduction of the full employment interest rate (Cole 2022). The original paper used an inventory-based model and incorporated the rate of normal profit, whilst assuming saving equals investment where firms are only making normal profit under perfect competition. The rationale of the inventory-based model may require a more precise interpretation to reveal the logic behind it Introducing the rate of normal profit is a means to an end, that end being the deduction of the full employment interest rate at which full productive capacity is realised in a free-market economy. The rate of normal profit is the inducement for savers to invest, earning interest and at least normal profit, instead of only interest. Whereas there is no dispute that the concept of the rate of normal profit exists, classical theory (Smith 1776), Ricardo (1817), neoclassical economics (Marshall 1890), (Wicksell 1898), and all subsequent orthodox economic theory, ignored it, because in a saving-investment cross, the inclusion of the rate of normal makes no difference to the result. However, once liquidity preference is introduced, there is a very different outcome.",
20.0,1.0,Evolutionary and Institutional Economics Review,20 March 2023,https://link.springer.com/article/10.1007/s40844-023-00250-2,On the role of social rules in economic development: historical perspectives,April 2023,Jean-Paul Chavas,,,Male,Unknown,Unknown,Male,"The process of economic development is complex and has varied over time and across space (e.g., Polanyi 1944; Weber 1954; North 1990; Mankiw et al. 1992; Pomeranz 2000). Much attention has focused on the role of innovations, markets and property rights in improving productivity and income. Yet, markets rely on socioeconomic and political institutions that affect the economic performance of societies (e.g., Elster 1989; North 1990; Tilly 1990; Fafchamps 1992; Bicchieri 2006). In this context, efficient institutions can be defined as the ones that support economic and policy decisions that maximize aggregate income. North (1990) argues that efficiency gains help motivate the historical evolution of economic and political institutions. These arguments also apply to the analysis of social rules. We define social rules broadly to include norms, laws, regulations, and customs that affect individual interactions in a society. Social rules can be formal (e.g., laws and government regulations) as well as informal (e.g., cultural norms guiding the behavior of members of a family or social network). Our focus on social rules is motivated by their role in guiding interactions among individuals. These interactions are common in societies, some positive (e.g., access to information, improved productivity from teamwork), others negative (e.g., conflicts). In general, the design and implementation of social rules influence how individuals interact, which in turn affect individual behavior, economic performance, and the process of economic development (e.g., Commons 1934; Rutherford 2001). Social rules shaping interpersonal relationships occur at all levels of society, going from the micro level (e.g., formal or informal contracts among individuals within a household or a firm), to the meso level (e.g., dispute resolution schemes) and to the macro level (e.g., government policies) (Dopfner et al. 2004). North (1990) argued that individual interactions can be managed efficiently through the design and implementation of social rules (including contracts and policies). We agree with North (1990) that there is a lot to learn about the historical evolution of social rules. But we argue that North’s focus on efficiency gains is too narrow for three reasons: (1) social rules exhibit a lot of inertia, making it more difficult to document how they change; and (2) social rules vary a lot across space, making it hard to identify which rule is efficient; and (3) most social rules are also affected by distribution issues, stressing the need to go beyond efficiency. Our paper evaluates the historical evolution of social rules. Our historical inquiry is motivated by the fact that many social rules change slowly, and some have long-term effects that cannot be easily identified over short time periods. While our paper examines institutional changes over the centuries, we also strive to provide depth by focusing on four important aspects of society: (1) improved dispute resolution mechanisms; (2) the elimination of debt bondage; (3) reductions in the severity of punishments; and 4) the development of limited liability rules and their linkages with the rise of the modern corporation. In the process, our historical analysis will give new insights in the relationships between social rules and economic development. These explorations lead us to go beyond previous literature in several ways. First, our analysis makes it clear that we need to go beyond efficiency and include fairness considerations in the analysis of social rules. This argument is strongest in our investigation of dispute resolution mechanisms. But we argue that it applies to all social rules (e.g., in the evaluation of punishment, slavery and limited liability rules). This is motivated by the fact that fairness and empathy are common characteristics of human behavior (Batson 2011; Segal 2018). While such arguments have stimulated the inclusion of fairness in the analysis of social rules, the role of fairness remains imperfectly understood for two reasons: (1) the tradeoffs between efficiency and fairness depend on the nature of social structure (Chavas 2020); and (2) the role of fairness has varied over time and across space (as exemplified by the use of punishment and violence; see Foucault (1995) and Scheidel (2017)). Our discussion stresses that institutional innovations are motivated by both efficiency and fairness considerations. Second, our historical analysis examines the linkages between social rules and human capital. We argue that economic development occurs jointly with the elaboration of reciprocity relationships which are conditional on the level of human capital.Footnote 1 In this context, human capital helps improve individual abilities to assess the situation of others, thus making it easier to settle disputes and to increase efficiency and productivity. Under low level of human capital, reciprocity relationships are typically limited in scope (e.g., mostly within the family). But reciprocity relations expand significantly under high level of human capital, in which case social rules become more extensive (e.g., as seen in modern states).Footnote 2 In assessing the progress of civilization, we argue that an important part of the process has been the development of limited liability rules that protect human capital. This gives additional support for the role of human capital as major contributor to economic growth (e.g., Mankiw et al. 1992; Goldin 2016). Our analysis explores the evolving linkages between limited liability rules, financial institutions and the role of the modern corporation in economic development. Third, our historical analysis illustrates that, reflecting both efficiency and fairness considerations, social rules have evolved with the structure of societies around the world. The choice of social rules typically occurs in complex socio-political environments. Institutional changes are often slow and exhibit “path dependence” (Pierson 2000). In addition, choosing social rules is often difficult. For example, seeing societies establishing dispute resolution institutions (such as the Courts) is a testimony about the frequency of failures in private bargaining. In this context, the study of social rules can best be presented in the context of evolutionary dynamics where individuals interact to set up new rules that evolve over time (North 1990; Ostrom 2000; Bicchieri 2006; Young 2015; Chavas and Wang 2021; Nunn 2021). We do not think that a single model can explain the complexities of institutional change. But we see evolutionary dynamics as providing useful insights supporting our historical analysis of social rules. Finally, the investigation presented below is qualitative. It examines the nature of some key institutional innovations and their longer term effects. Our approach will allow us to avoid the fallacy of “the trees hiding the forest” as many changes in the structure of societies have been slow, making it difficult to identify their long-term effects. Our historical approach documents how some key factors have influenced the evolution of social rules and their linkages with the process of economic development. The paper is organized as follows. Section 2 evaluates how dispute resolutions have changed over time and space. Section 3 discusses the evolving role of punishments through the centuries. Section 4 presents an historical overview of an extreme form of inequality: slavery. Section 5 evaluates the evolution of liability rules, with linkages with bankruptcy and the rise of the modern corporation. Finally, Sect. 6 concludes.",
20.0,1.0,Evolutionary and Institutional Economics Review,15 April 2023,https://link.springer.com/article/10.1007/s40844-023-00253-z,Historical institutionalism and policy coordination: origins of the European semester,April 2023,Wen Pan,Madeleine O. Hosli,Michaël Lantmeeters,,Female,Male,Mix,,
20.0,1.0,Evolutionary and Institutional Economics Review,23 March 2023,https://link.springer.com/article/10.1007/s40844-023-00249-9,"Robert Boyer, Une discipline sans réflexivité peut-elle être une science? Epistémologie de l’économie",April 2023,Toshio Yamada,,,Male,Unknown,Unknown,Male,"The subject of this book, entitled Can a Discipline without Reflexivity be a Science? Epistemology of Economics, is to critically examine the transformation and degeneration of mainstream macroeconomics from the end of the twentieth century to the twenty-first century, both in terms of economics itself and the state of the economists’ profession, and calls on economists to engage in dialogue with each other. Although macroeconomics has made great strides since its foundation by Keynes, the author analyzes its recent degradation from the viewpoint of both the historical transformation of economic structure and the various factors at work in the discipline of economics. The book's distinctive feature is that it develops the inherent criticism of mainstream macroeconomics not only in terms of its theoretical content but also in terms of the nature of the professional group of economists, using the term “reflexivity"" as a keyword. Through this unique analysis, the book ultimately leads us to the question of what economics should elucidate and what it should contribute to. The book consists of the following chapters: Introduction Ch.1 The time for clarification of concepts and methods. Ch.2 The diversity of interactions between mechanisms ensuring the structural stability of an economy Ch.3 The illusion of science in the process of completion and the splintering into a multitude of competing paradigms Ch.4 How the search for the microeconomic foundations of macroeconomics has contributed to its crisis Ch.5 Formal coherence is no guarantee of empirical relevance Ch.6 Structuring and professionalization of the economists’ field shape their research strategy Ch.7 Can the contemporary reflexive turn save the discipline of economics? Ch.8 Theories are daughters of history: Covid-19 as an analyzer of the weaknesses of the economic discipline Conclusion",
20.0,1.0,Evolutionary and Institutional Economics Review,28 March 2023,https://link.springer.com/article/10.1007/s40844-023-00252-0,"Book review on Hiroyasu Uemura, Japanese Institutionalist Post-Keynesians Revisited: inheritance from Marx, Keynes and Institutionalism Springer, 2023",April 2023,Hiroshi Nishi,,,Male,Unknown,Unknown,Male,"Eiichi Sugimoto, Shigeto Tsuru, Shigenobu Kishimoto, Yoshihiro Takasuka, Mitsuharu Itoh, Yoshikazu Miyazaki, Hirofumi Uzawa, Tsuneo Ishikawa, Robert Boyer, and Samuel Bowles. If we are political economists or Japanese political economists, we may have ever heard their names at least once. But do well also know their intellectual contributions in detail? Or how can we develop their economic ideas? We can find an answer to these questions in Professor Uemura’s book “Japanese Institutionalist Post-Keynesians Revisited.” Professor Hiroyasu Uemura considers the economists under the name of Japanese institutionalist post-Keynesians (JIPKs) in a broad sense. They were inspired by K. Marx, J. M. Keynes, J. Robinson, N. Kaldor, and Institutionalism and analysed the institutional structures of post-war capitalism. The JIPKs also had a critical attitude towards Neoclassical economics or Walrasian paradigm commonly but in different ways. Simultaneously, from a social thought perspective, they are advocates of the value of social democracy or civil society. The author has long learned from them and has been a leading scholar in this field. Thus, the book is also a journey of the author to explore the historical origin and roots of his institutional economics. Two key words matter to understand the essence penetrating the whole book. One is ""institutional economics in a broad sense"" and the other is ""creative rivalry"" among modern economics. These consist of threads to the anchor of the book, and the author learned ""institutional economics in a broad sense"" from Tsuru Shigeto and ""creative rivalry"" from Eiichi Sugimoto. It is useful to start by confirming what these two keywords are. Institutional economics in a broad sense (p.3) comes from Tsuru Shigeto, which is as follows: "" A. the emphasis on the open-system character of production and consumption, thus a broader view of the scope of economics; B. an interest in the evolutionary course along which the industrial economies are moving, with emphasis on the dynamic process of technological change and circular cumulative causation; C. awareness of a growing need for guidance that can be supplied only through some form of overall social management, or planning; D. recognition that economics must become a normative science, positively formulating social goals and objectives (Tsuru 1993, p.101)."" In light of this perspective, the author provides comparative economics over seven chapters, trying to describe how JIPKs developed academic contributions and criticised and overcame the Walrasian paradigm. It is through this exercise that the author emphasises the importance of Sugimoto’s concept on ""creative rivalry."" This concept implies the significance of academic communication on a common basis and learning constructive insights from different schools to advance academic progress (p.14). Meanwhile, the book is motivated by the current stagnation of economic theory dominated by the Walrasian paradigm. Why do we have to challenge the current situation in economics? It is because to make a significant improvement in the current state of economics dominated by the Walrasian paradigm which weakened the institutional concepts of Keynesian economics (p.114). The author assesses that this paradigm is problematic in a stylised way (pp.15–6). First, it considers the market as an extremely simplified space with rational individuals and perfect price adjustments and complete contracts. Second, it thus exclusively focuses on the economy, separating politics and society from there. Consequently, third, the institutional research resulting from Keynesian economics has been wiped out. Moreover, in reality, strong pressure from globalisation, environmental issues, the COVID-19 pandemic, and increasing social inequality have put civil society and democracy in crisis, which also leads the author to reconsider the intellectual legacy of JIPKs. Then, how can we achieve that goal? It is by reconstructing institutional and evolutionary economics through the intellectual heritage of JIPKs in the twentieth century and by empirical analyses based on the reality of the current world (p.127). For these purposes, inspired by Miyazaki, the author sheds light on “the total image of the economy” which each economist had in the original research in the context of the historical development of capitalism of the twentieth century (p.2). He briefly mentions their policy proposals during these periods. On these bases, he emphaises the importance of constructing the theoretical foundations of ""a new political economy of institutions and evolution (p.5)."" Now is the time to learn from the JIPKs and construct alternative economic theory. I would say that this is the slogan of the book. By doing so, the book aims to provide a way to overcome the current third crisis of economic theory. I try to summarise the contents of the book as minimally as possible (Sect. 2) so that in this review I can argue more about the contributions (Sect. 3), and some critiques, questions, and remaining issues (Sect. 4).",
20.0,1.0,Evolutionary and Institutional Economics Review,27 November 2022,https://link.springer.com/article/10.1007/s40844-022-00245-5,"Christian Bessy and Michel Margairaz eds. Les biens communs en perspectives. Propriété, travail, valeur (XVIIIe-XXIe siècle), Editions de la Sorbonne, 2021",April 2023,Kota Kitagawa,,,Male,Unknown,Unknown,Male,"Les biens communs en perspectives is a collective work based on seminars and sessions hosted by the laboratory Institutions et Dynamiques Historiques de l’Economie et de la Société (IDHES) at the École Normale Supérieure, Paris-Saclay, a bastion of a French form of institutionalism called the economics of convention (EC). Each contributor, who is related to the laboratory in some way, presents a detailed description of an empirical study based on profound historical and sociological investigations. Every case is thick or rich, because it is not stylized on the basis of the prevailing framework of the “commons.” The prevailing framework tends to see the commons as having already been pooled and as residuals based on the logics of gifting, sharing, and “baseline communism” (Graeber 2011), categorized neither as state (a world based on the logic of public goods and sovereign power constituted and enforced as laws), market (a world based on the logic of exchange, exclusiveness, privateness, and equality), nor business firms (a world based on the logics of profit maximization and the pursuit of scale economy, productivity, hierarchy, and appropriation). This book presents a unique perspective that views the commons as being formed and reformed by collective actions. This perspective has two main advantages. First, it can include not only natural resources but also tangible and intangible artifacts (and their hybrids); it can involve not only the commons that have been accumulated in the “past” but also relationships being recreated by involved parties in the “present” and their collective forecast in the “future.” Consequently, this perspective can reframe the commons as dynamic collective assets. Second, it can avoid trinomial conflicts between “the market,” “the commons,” and “the state” because the collective actions forming the commons involve legislative and legal actions by citizens, associations, the legislature, the government, and judges; there are both nonprofit-based and profit-based actions by community members and firms. Therefore, the commons consist of these heterogeneous elements hybridized by collective actions. These assets exist as a “community” with a peculiar domain (e.g., the “basin” of a river, an “occupational” (functional) classification, or an industrial “branch” classification) and members who share peculiar relationships and norms. From this unique perspective, this book succeeds in seeing the commons not as something existing outside the world of capitalism and, therefore, the last frontier for capital to subsume, but as something collective that has been and is created or organized by both capitalistic and other social actions in the past, present, and future. The book highlights that, historically, business (the state) has at least unconsciously engaged in creating and developing something collective in the capitalistic world. Thus, it implies that the reason for the current rash of discussion of the commons is not because current businesses (and policymakers) have started to look at and subsume the last external resources but because they have started to consciously manage, exploit, and foster something collective for the sake of profit making (or the realization of public purposes). The book consists of an editor’s introduction, 15 chapters, and a concise general conclusion from the editors. The chapters are categorized into three main parts—“Property,” “Work,” and “Value”—which could be elements characterizing and structuring our capitalistic world. One of the editors, Christian Bessy, director of the IDHES and a leading researcher of the second generation of EC, presents a long introduction to the book (pp. 5–17). This chapter connects the issue of common goods with issues in the EC (e.g., justification, conflict, compromise over multiple values, “transvaluation,” and the powers of valuation “intermediaries”) by highlighting the double meaning of “common goods.” This is a beautiful way to introduce readers interested in the topic of the commons to the conventionalist approach to common goods. The chapter evaluates Ostrom (1990), which views individuals not as rational and calculative beings, each of whom makes decisions in isolation, but as communicative beings who deliberate with each other. Its presupposition shows that it is possible to realize the effective management of the commons. Ostrom regards the commons not only as goods and services with characteristics of nonrivalness and/or nonexcludability but also as norms in which goods and services are arranged. Here, “goods” has a double meaning: accumulated common assets (below, the term “commons” and “the first meaning” of common goods express this meaning) and the value system shared by the members of the community (below, the term “common goods” and “the second meaning” of common goods express this meaning). This opens up the possibility of connecting the issue of the commons with our institutional approach. A collaborative actor with a “sense of fairness” tries to foster the common good by justifying her actions and advocating or criticizing the actions of others by referring to the norm (i.e., the common good) they share. The chapter criticizes Ostrom (1990) and other preceding studies—for instance, Coriat (2015)—because they pay little attention to the fact that there must be multiple (or plural) norms, values, or common goods within a community. When we investigate the real problem of common resource management, we confirm that multiple common goods coexist, compete, and are coordinated by collective actions. Highlighting the multiple common goods leads us to conflicts between multiple value systems, the existence of valuation powers and authorities (Bessy and Chauvin 2013), and the entanglement of economic and political powers with ethical power (i.e., public justification).Footnote 1 The first part, “Property,” consists of a concise “Introduction,” written by C. Bessy (pp. 21–26), and three main chapters. Anne Conchon, Dominique Margairaz, and Éric Szulman’s chapter, “Transportation infrastructure under the ancien régime” (pp. 27–44), mainly discusses the infrastructure of river transportation. Whereas, in the ancien régime, such infrastructure was seen as a common resource, from the eighteenth century, public intervention had been justified and the infrastructure was regarded as a public good. However, Conchon, Margairaz, and Szulman stress that the thought justifying public investment in the infrastructure is that the transportation brings society abundance, that is, a common good (a shared value); therefore, investment is the expression of a collective willingness to achieve the common good. Michela Barbot’s chapter, “What common goods were there in the communes?” (pp. 45–58), evaluates one commune, Milan, in the ancien régime to reconsider the current issue of “urban commons.” One of the issues was “simultaneous properties” in the commune. Barbot shows that the commune was sustained through the simultaneous properties reconstituted by conflict resolutions of the involved parties and mediations or interventions of the public and private powers. Public authorities, private groups, and relevant individuals collectively governed them. Consequently, they cannot be categorized under the modern concept of commons, which refers to the rest of private and public goods. C. Bessy’s chapter, “Patent law between exclusivity and sharing” (pp. 59–71), presents historical and current cases of patent use and its governance among firms, communities, and states to show dynamic complementarity between “proprietary ideology” and commons ideology in their practices. From these cases, Bessy derives the importance of conventions that repress excessive proprietary practices and historical effectiveness, as well as the contemporary possibility of “collective management” of a patent by the community to coordinate the two ideologies. Antonella Corsani’s chapter, “Free software and open source” (pp. 73–88), shows that there are two different sets of thought over software fabrication: The first is a combination of “free software,” the norm of “free access” to the commons, and communities of individual programmers, and the second is a combination of “open source software,” the norm of “utilitarianism pursuing economic efficiency,” and communities of firms and “free” or unpaid workers. This part treats natural and artificial resources as “property” of which various actors negotiate the meaning and range. This part breaks the trinomial conflicts between the market, the commons, and the state by focusing on multiple common goods (norms or values) with the state as the umpire or conciliator of conflicts between norms. Because there are multiple norms (for instance, traditional groups share some norms, while others are brought in by new “visitors” to the community), the umpire must resolve conflicts between them or hybridize them. This is a moment when the state, as an umpire, intervenes to support the community in conflict. The chapter by Conchon, Margairaz, and Szulman does a good job of explaining the historical processes by which the state gradually came to play the role of including (or, I imagine, sometimes excluding) new categories of relevant players by resolving conflicts over the use of a river. This chapter highlights the state as the conciliator of multiple public and common goods (norms). By focusing on the aspect of goods (norms) of the commons, this part also explores the relations between norms, property management, and the form or character of a community. For instance, as noted above, the chapter by Corsani shows that there are conflicts between the two sets of ideas about software fabrication: free software and open source. This implies that if we stand in the position of an umpire (for instance, the state), how we assign weight to the norms affects the future forms of communities. The second part, “Work,” consists of a short “Introduction,” written by M. Simonet (pp. 91–95), and four main chapters. Claude Didry’s “From trade to industry: From one common to another?” (pp. 97–111) examines the historical transformations of units and ranges of common goods in France from 1800 to 1936, to which labor law and social security law have referred. The unit of the common good has been transformed from a “family” community to a collective comprising of “individuals.” The range of common goods has also been changed from “trade” (occupation) to “industry.” These were transformed by collective actions combining associative and public actions, such as organizing activities of labor movements, participation in law making, collective bargaining under labor law, and collective administration of security laws. Marc Loriol and Line Spielmann’s chapter investigates “The social worlds of contemporary music” (pp. 113–128). While we can easily imagine that these worlds belong to the category of the commons, Loriol and Spielmann reveal that there are profound conflicts because common resources such as venues and funds are limited, and there are disagreements among contemporary musicians regarding their music ideologies and their stances in relation to the public service sector and private profit and nonprofit organizations. Loriol and Spielmann imply that it is challenging to create a common discourse that can maintain and foster conflicting social worlds as common goods and coordinate well with other parties (the public and the involved bodies). Delphine Corteel’s chapter, “The work under the park” (pp. 129–142), investigates free workers in the park (as common goods) of the former Tempelhof airport in Belin. Policy changes by public authorities easily affected their circumstances, meaning that they had to keep engaged in “political and militant” activities. However, Corteel sharply indicates that “maintenances have partly constituted a sense of freedom” in the park (e.g., cleaning, disposal, and grass cutting) provided by inconspicuous workers hired by a publicly funded company. This implies a complex relationship between the voluntary management of common goods and public authorities. Mathieu Cocq’s chapter, “Putting gamers to work? Community and free work in the video game industry,” examines a firm (a video game maker) and consumers (game players) who collaboratively created a commons through management by a firm. Cocq sharply indicates that game players who voluntarily participate in the community of game production and its progress perform substantial “work,” with the firm exploiting the common outcomes of the community for its profits. This part expresses the unique viewpoints of researchers from the IDHES: the profound conflict between norms that cannot be repressed, varieties of “rewards” for labor, and opportunities for exploitation. The conflict discussed in the chapter by M. Loriol and L. Spielman seems more profound than in the previous part. The multiple norms (e.g., individualism and competition, cooperation and collective sharing, and public interventionism) of the parties involved in a contemporary music venue cannot be integrated, compromised, or hybridized into a convention of contemporary music that coordinates their musical activities and directs them to provide shared interests. The pluralism of norms leads to various rewards for labor. Fulfillment for unpaid workers does not come from money such as salaries or other payments but from other rewards, such as prestige in their community. This fulfillment may not come from individual psychological rewards (for example, fulfillment that arises from a sense of competence or accomplishment within an individual mind) but social psychological rewards, such as feelings of respect from other members. Thus, the peculiar qualities of rewards within the community are derived from (or refer to) common goods (here, this means peculiar norms shared within the community). This part shows that, from the standpoint of profit-making enterprises, these multiple meanings of rewards provide opportunities to continually (or sustainably?) exploit community members. The third part, “Value,” consists of a short “Introduction,” written by Michel Margairaz (pp. 161–164), and four main chapters. M. Margairaz’s chapter “The three successive phases of money as a common good in the twentieth century” (pp. 165–185) focuses on the historical dynamics of common goods (shared values) of local, national, and international supranational communities (e.g., local financial interests, treasury departments, meetings of the governors of central banks, and the European Central Bank) to which the French currency system, especially the Bank of France, refers in managing its currencies. Margairaz shows that from the nineteenth century to today, the Bank of France has changed its priorities among conflicting common goods (e.g., currency stabilization and economic expansion) seen in the communities, with priorities being changed as a result of international events (e.g., the world wars), domestic events (e.g., the reconstruction since 1945), and embeddedness into the Euro system. Thus, Margairaz sees currencies as representations of common goods ordered according to specific priorities. Philippe Verheyde’s chapter, “State money, common goods and public faith in France (19th–twentieth century)” (pp. 187–199), considers the institutional grounds of “public faith” in state money. While money as a social institution has typically been categorized as a public institution or a private institution, Verheyde’s perspective implies that it is essential to foster the common goods (e.g., pension funds and public faith) of state money to maintain and stabilize the communities in which money mediates their internal, horizontal, and hierarchical transactions. Verheyde focuses on state pension funds that sold their pension insurance as public debt and paid their benefits as part of the state budget. By means of good coordination of the future benefits (interests) for private bodies and persons involved and the current advantage (current interest) to the state (sale of public debts in advantageous conditions for the state), pension funds have fostered public faith in them as a common good that connects private future interests and current state (public) interests. Thus, Verheyde shows that not only is public faith a common good but also that state pension funds themselves are common goods because they foster public faith in state money through their autonomous and transparent governance. Jean-Luc Mastin’s chapter, “Money: A common good among the great families of the North” (pp. 203–215), shows that each local ruling family in the Lille region between 1850 and 1914 maintained its own industrial and financial systems and saw them as commons. It was networked, accumulated, and governed on the basis of the value (logic) of blood ties and was commonly owned and managed by the entire ruling family, not by an individual dominator. Such a local common good resisted the financial centralization imposed by the nation-state, the Bank of France, and the financial center in Paris. Thus, with its local perspective, Mastin’s chapter complements the discussion of Margairaz and Mastin’s chapters on the conflicting common goods of communities. Marta Torre-Schaub’s chapter, “The commons and the law” (pp. 217–231), focuses on the valuation process of common goods (shared values) of the natural environment. These have been diversified and expanded (from economic values, such as productive resources and economic growth, to noneconomic ethical values) depending on the development and diversification of the social requirements for the natural environment, and the newly emerging values have been brought into legal rights and monetized (see, for instance, the polluter-pays principle and emissions trading). Torre-Schaub emphasizes that the monetization by which noneconomic values translate into the economic exchange value, that is, price, enables public agencies to intervene in economic activities involving the natural environment to maintain the newly emerging values. This part deals with the institutionalized and evolutionary relationships between values and money.Footnote 2 Normally, money embodies a single value (i.e., price), and a certain amount of that value (price) expresses market efficiency. Thus, money tends to be contrary to the world of the commons or multiple common goods. However, this part overcomes this prevailing dichotomy. It sees money as a quantified and institutionalized creditor–debtor relationship and shows that the governance of institutions that manage debt or money consists of conflict, compromise, or hybridization of multiple values. The institutionalization of money—that is, the genesis, development, and transformation of the institution—is processed as a contest and cooperation of multiple public and common goods (e.g., the solidarity of members of a pension system, the unity and stability of a national currency, or the stability, coordination, and solidarity mediated by the monetary system at the European Union level) driven by the collective actions of heterogeneous actors (e.g., the Bank of France, the state, the community of the financial center of France, or the EU). This implies that if we reconsider (investigate) how institutions have hybridized common goods, it might be possible to find a “better” way of governance for the institutions of debt and money that respect every public and common good. The short “General Conclusion” of the editors (pp. 233–235) implies that demands for pluralistic (especially qualitative and historical) approaches to common goods have grown as EU unification progressed for two reasons. First, the EU common rule has expanded to include the common goods of the environment and knowledge. Second, as connections between communities of different types and levels have been tightened and complicated, complex connections have generated further possibilities to create common goods. In addition, the editors stress that strong economic and political actors, typically big firms, have learned to create and use common goods and capture their benefits; consequently, it is important to reveal alternative perspectives and governance methods for common goods by investigating the current realities of collective and complex possession, the use and maintenance of common goods, and their historical varieties and dynamics. As stated above, this book evaluates a variety of perspectives on common goods that various contributors present with a view to opening up the possibilities of finding new methods of cultivation and governance of common goods. This reviewer, an investigator who employs qualitative research, strongly supports this project. However, the variety of perspectives does not allow readers to understand what the practical or ethical impact on society an EC approach to common goods might have or what the desired perspective and role conventionalists (ought to) take.Footnote 3 This review adopts the viewpoint of the American institutionalist John R. Commons (1862–1945) as an auxiliary approach to reveal the practical and ethical effects of the EC. Adopting his institutional economics (Commons 1934) to review this book is justified because Commons’ perspective is an unspecified but influential source from which one editor of the book, C. Bessy, elaborates his version of the EC. This review will outline Commons’ perspective and evaluate the chapters of the book. In other words, the review will clarify an unspecified source by the leading researcher of the second generation of the EC and use it to review his edited book (with another editor, M. Margairaz). This book review aims to illuminate the significance of the EC from the perspective of J. R. Commons, an important but inconspicuous source of the EC. In a significant theoretical work on the EC by Bessy and O. Favereau (a leading researcher of the first generation of the EC), which revisits various theoretical sources of the EC, they see in Commons’ institutional economics the seeds of the “contemporary theoretical framework” that they seek to construct (Bessy and Favereau 2003, p. 126), although they do not provide any further details with regard to Commons’ theoretical implications for the EC. It is not clear whether the other editors and contributors knew of Commons or were affected by him, but Bessy has a strong interest in Commons’ institutional economics even today (Bessy and Diaz-Bone 2013).Footnote 4 This review will evaluate the outcome (this book) of contemporary institutionalism, the EC, from the perspective of the origin of the EC. When we see overlaps in these contemporary and original institutional approaches, we discover the practical and ethical position of institutionalists, which has yet to be explicated. It will be referred to as pragmatic idealism. We have confirmed the outline and explicit significance of this book in Sect. 1. Therefore, in the following sections, to concentrate on achieving the purpose of this book review with the minimum number of steps, we select several chapters of the book for our review from the perspective of J. R. Commons’ institutional economics: the “Introduction” by C. Bessy (pp. 5–17) and the chapters by C. Bessy (pp. 59–71), C. Didry (pp. 97–111), and M. Cocq (pp. 144–154).",

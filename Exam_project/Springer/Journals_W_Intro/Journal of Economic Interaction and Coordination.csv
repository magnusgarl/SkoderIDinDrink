Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,Journal of Economic Interaction and Coordination,11 May 2006,https://link.springer.com/article/10.1007/s11403-006-0007-6,Welcome to JEIC,May 2006,Akira Namatame,Thomas Lux,Robert Axtell,,Male,Male,Mix,,
1.0,1.0,Journal of Economic Interaction and Coordination,06 May 2006,https://link.springer.com/article/10.1007/s11403-006-0001-z,Aggregation of Heterogeneous Interacting Agents: The Variant Representative Agent Framework,May 2006,M. Gallegati,A. Palestrini,E. Scalas,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Journal of Economic Interaction and Coordination,09 May 2006,https://link.springer.com/article/10.1007/s11403-006-0006-7,Determinants of Economic Interaction: Behavior or Structure,May 2006,Shyam Sunder,,,Male,Unknown,Unknown,Male,,13
1.0,1.0,Journal of Economic Interaction and Coordination,06 May 2006,https://link.springer.com/article/10.1007/s11403-006-0002-y,The Microfoundations of the Keynesian Multiplier Process,May 2006,Peter Howitt,,,Male,Unknown,Unknown,Male,,21
1.0,1.0,Journal of Economic Interaction and Coordination,10 May 2006,https://link.springer.com/article/10.1007/s11403-006-0003-x,Stock Prices and the Real Economy: Power Law versus Exponential Distributions,May 2006,Masanao Aoki,Hiroshi Yoshikawa,,Male,Male,Unknown,Male,,6
1.0,1.0,Journal of Economic Interaction and Coordination,06 May 2006,https://link.springer.com/article/10.1007/s11403-006-0004-9,White Flight or Flight from Poverty?,May 2006,Charles Jego,Bertrand M. Roehner,,Male,Male,Unknown,Male,,7
1.0,1.0,Journal of Economic Interaction and Coordination,12 May 2006,https://link.springer.com/article/10.1007/s11403-006-0005-8,Heterogeneity in Economics,May 2006,Alan Kirman,,,Male,Unknown,Unknown,Male,,50
1.0,2.0,Journal of Economic Interaction and Coordination,21 October 2006,https://link.springer.com/article/10.1007/s11403-006-0015-6,Developments in experimental and agent-based computational economics (ACE): overview,November 2006,Sheri M. Markose,,,Female,Unknown,Unknown,Female,,2
1.0,2.0,Journal of Economic Interaction and Coordination,12 October 2006,https://link.springer.com/article/10.1007/s11403-006-0012-9,Multi-unit auction format design,November 2006,Atakelty Hailu,Sophie Thoyer,,Unknown,Female,Unknown,Female,,22
1.0,2.0,Journal of Economic Interaction and Coordination,21 October 2006,https://link.springer.com/article/10.1007/s11403-006-0009-4,Can boundedly rational sellers learn to play Nash?,November 2006,Roger Waldeck,Eric Darmon,,Male,Male,Unknown,Male,,2
1.0,2.0,Journal of Economic Interaction and Coordination,07 September 2006,https://link.springer.com/article/10.1007/s11403-006-0014-7,Evolving densities in continuous strategy games through particle simulations,November 2006,Julide Yazar,,,Female,Unknown,Unknown,Female,,2
1.0,2.0,Journal of Economic Interaction and Coordination,07 September 2006,https://link.springer.com/article/10.1007/s11403-006-0011-x,Impact of investor’s varying risk aversion on the dynamics of asset price fluctuations,November 2006,Baosheng Yuan,Kan Chen,,Unknown,,Unknown,Mix,,
1.0,2.0,Journal of Economic Interaction and Coordination,07 September 2006,https://link.springer.com/article/10.1007/s11403-006-0013-8,Social support among heterogeneous partners: an experimental test,November 2006,Sonja Vogt,Jeroen Weesie,,Female,Male,Unknown,Mix,,
2.0,1.0,Journal of Economic Interaction and Coordination,14 September 2006,https://link.springer.com/article/10.1007/s11403-006-0010-y,Agglomeration in an innovative and differentiated industry with heterogeneous knowledge spillovers,June 2007,Klaus Wersching,,,Male,Unknown,Unknown,Male,,7
2.0,1.0,Journal of Economic Interaction and Coordination,21 October 2006,https://link.springer.com/article/10.1007/s11403-006-0016-5,A comparison of different trading protocols in an agent-based market,June 2007,Paolo Pellizzari,Arianna Dal Forno,,Male,Female,Unknown,Mix,,
2.0,1.0,Journal of Economic Interaction and Coordination,10 March 2007,https://link.springer.com/article/10.1007/s11403-007-0018-y,Technological proximity and the choice of cooperation partner,June 2007,Uwe Cantner,Andreas Meder,,Male,Male,Unknown,Male,,72
2.0,1.0,Journal of Economic Interaction and Coordination,29 March 2007,https://link.springer.com/article/10.1007/s11403-007-0019-x,When does defection pay?,June 2007,Kerstin Press,,,Female,Unknown,Unknown,Female,,
2.0,1.0,Journal of Economic Interaction and Coordination,13 December 2006,https://link.springer.com/article/10.1007/s11403-006-0017-4,A note on interactions-driven business cycles,June 2007,Frank Westerhoff,Martin Hohnisch,,Male,Male,Unknown,Male,,11
2.0,2.0,Journal of Economic Interaction and Coordination,25 July 2007,https://link.springer.com/article/10.1007/s11403-007-0024-0,"Structural holes, innovation and the distribution of ideas",December 2007,Robin Cowan,Nicolas Jonard,,,Male,Unknown,Mix,,
2.0,2.0,Journal of Economic Interaction and Coordination,05 October 2007,https://link.springer.com/article/10.1007/s11403-007-0026-y,Patterns of dominant flows in the world trade web,December 2007,M. Ángeles Serrano,Marián Boguñá,Alessandro Vespignani,Unknown,Male,Male,Male,,133
2.0,2.0,Journal of Economic Interaction and Coordination,15 June 2007,https://link.springer.com/article/10.1007/s11403-007-0020-4,An objective function for simulation based inference on exchange rate data,December 2007,Peter Winker,Manfred Gilli,Vahidin Jeleskovic,Male,Male,Male,Male,,68
2.0,2.0,Journal of Economic Interaction and Coordination,27 June 2007,https://link.springer.com/article/10.1007/s11403-007-0022-2,The two-parameter Ewens distribution: a finitary approach,December 2007,U. Garibaldi,D. Costantini,P. Viarengo,Unknown,Unknown,Unknown,Unknown,,
2.0,2.0,Journal of Economic Interaction and Coordination,19 July 2007,https://link.springer.com/article/10.1007/s11403-007-0021-3,Asset allocation and multivariate position based trading,December 2007,Bernd Pape,,,Male,Unknown,Unknown,Male,,6
2.0,2.0,Journal of Economic Interaction and Coordination,07 July 2007,https://link.springer.com/article/10.1007/s11403-007-0023-1,Self-organization of R&D search in complex technology spaces,December 2007,Gerald Silverberg,Bart Verspagen,,Male,Male,Unknown,Male,,4
2.0,2.0,Journal of Economic Interaction and Coordination,21 September 2006,https://link.springer.com/article/10.1007/s11403-006-0008-5,Self-organization of R&D search in complex technology spaces,December 2007,Gerald Silverberg,Bart Verspagen,,Male,Male,Unknown,Male,,8
3.0,1.0,Journal of Economic Interaction and Coordination,13 March 2008,https://link.springer.com/article/10.1007/s11403-008-0030-x,Editorial,June 2008,Akira Namatame,Taisei Kaizoji,Enrico Scalas,,Unknown,Male,Mix,,
3.0,1.0,Journal of Economic Interaction and Coordination,14 March 2008,https://link.springer.com/article/10.1007/s11403-008-0029-3,Dispersion of growth paths of macroeconomic models in thermodynamic limits: two-parameter Poisson–Dirichlet models,June 2008,Masanao Aoki,,,Male,Unknown,Unknown,Male,,4
3.0,1.0,Journal of Economic Interaction and Coordination,14 March 2008,https://link.springer.com/article/10.1007/s11403-008-0028-4,Factor vector autoregressive estimation: a new approach,June 2008,Fabio C. Bagliano,Claudio Morana,,Male,Male,Unknown,Male,,2
3.0,1.0,Journal of Economic Interaction and Coordination,03 April 2008,https://link.springer.com/article/10.1007/s11403-008-0037-3,On the plausibility of sunspot equilibria,June 2008,Shu-Heng Chen,Chung-Chih Liao,Pei-Jung Chou,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Journal of Economic Interaction and Coordination,21 March 2008,https://link.springer.com/article/10.1007/s11403-008-0034-6,Social networks and labour productivity in Europe: an empirical investigation,June 2008,C. Di Guilmi,F. Clementi,M. Gallegati,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Journal of Economic Interaction and Coordination,21 March 2008,https://link.springer.com/article/10.1007/s11403-008-0036-4,Frankfurt Artificial Stock Market: a microscopic stock market model with heterogeneous interacting agents in small-world communication networks,June 2008,Oliver Hein,Michael Schwind,Markus Spiwoks,Male,Male,Male,Male,,11
3.0,1.0,Journal of Economic Interaction and Coordination,21 March 2008,https://link.springer.com/article/10.1007/s11403-008-0035-5,Correlated performance of firms in a transaction network,June 2008,Yuichi Ikeda,Hideaki Aoyama,Wataru Souma,Male,Male,Male,Male,,7
3.0,1.0,Journal of Economic Interaction and Coordination,13 March 2008,https://link.springer.com/article/10.1007/s11403-008-0031-9,Dynamics of clustered opinions in complex networks,June 2008,Woo-Sung Jung,Hie-Tae Moon,H. Eugene Stanley,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Journal of Economic Interaction and Coordination,21 March 2008,https://link.springer.com/article/10.1007/s11403-008-0032-8,Self-organized criticality in a herd behavior model of financial markets,June 2008,Makoto Nirei,,,,Unknown,Unknown,Mix,,
3.0,1.0,Journal of Economic Interaction and Coordination,27 March 2008,https://link.springer.com/article/10.1007/s11403-008-0033-7,Dynamics of quote and deal prices in the foreign exchange market,June 2008,Takaaki Ohnishi,Hideki Takayasu,Misako Takayasu,Male,Male,Female,Mix,,
3.0,1.0,Journal of Economic Interaction and Coordination,21 March 2008,https://link.springer.com/article/10.1007/s11403-008-0038-2,Dynamics of a market with heterogeneous learning agents,June 2008,Tatsuo Yanagita,Tamotsu Onozaki,,Male,Male,Unknown,Male,,3
3.0,2.0,Journal of Economic Interaction and Coordination,20 September 2007,https://link.springer.com/article/10.1007/s11403-007-0025-z,Organizations undertaking complex projects in uncertain environments,December 2008,Jason Barr,Nobuyuki Hanaki,,Male,Male,Unknown,Male,,5
3.0,2.0,Journal of Economic Interaction and Coordination,28 February 2008,https://link.springer.com/article/10.1007/s11403-007-0027-x,Quantitative determination of the level of cooperation in the presence of punishment in three public good experiments,December 2008,D. Darcet,D. Sornette,,Unknown,Unknown,Unknown,Unknown,,
3.0,2.0,Journal of Economic Interaction and Coordination,18 April 2008,https://link.springer.com/article/10.1007/s11403-008-0040-8,Nurturing breakthroughs: lessons from complexity theory,December 2008,D. Sornette,,,Unknown,Unknown,Unknown,Unknown,,
3.0,2.0,Journal of Economic Interaction and Coordination,20 November 2008,https://link.springer.com/article/10.1007/s11403-008-0041-7,Community connectivity and heterogeneity: clues and insights on cooperation on social networks,December 2008,Sergi Lozano,Alex Arenas,Angel Sánchez,Male,Male,Male,Male,,10
3.0,2.0,Journal of Economic Interaction and Coordination,25 November 2008,https://link.springer.com/article/10.1007/s11403-008-0042-6,Incorporating positions into asset pricing models with order-based strategies,December 2008,Reiner Franke,Toichiro Asada,,Male,Unknown,Unknown,Male,,11
3.0,2.0,Journal of Economic Interaction and Coordination,01 April 2008,https://link.springer.com/article/10.1007/s11403-008-0039-1,"M. Aoki and H. Yoshikawa: Reconstructing macroeconomics, a perspective from statistical physics and combinatorial stochastic processes",December 2008,Hans Gottinger,,,Male,Unknown,Unknown,Male,,
4.0,1.0,Journal of Economic Interaction and Coordination,23 January 2009,https://link.springer.com/article/10.1007/s11403-008-0043-5,Analysing tax evasion dynamics via the Ising model,June 2009,Georg Zaklan,Frank Westerhoff,Dietrich Stauffer,Male,Male,Male,Male,,61
4.0,1.0,Journal of Economic Interaction and Coordination,13 February 2009,https://link.springer.com/article/10.1007/s11403-009-0044-z,Global recessions as a cascade phenomenon with interacting agents,June 2009,Paul Ormerod,Amy Heineike,,Male,Female,Unknown,Mix,,
4.0,1.0,Journal of Economic Interaction and Coordination,18 February 2009,https://link.springer.com/article/10.1007/s11403-009-0045-y,The norm game: punishing enemies and not friends,June 2009,K. Kułakowski,,,Unknown,Unknown,Unknown,Unknown,,
4.0,1.0,Journal of Economic Interaction and Coordination,27 February 2009,https://link.springer.com/article/10.1007/s11403-009-0046-x,"Description, modelling and forecasting of data with optimal wavelets",June 2009,Oriol Pont,Antonio Turiel,Conrad J. Perez-Vicente,Male,Male,Male,Male,,13
4.0,1.0,Journal of Economic Interaction and Coordination,18 March 2009,https://link.springer.com/article/10.1007/s11403-009-0047-9,Analysis of a network structure of the foreign currency exchange market,June 2009,Jarosław Kwapień,Sylwia Gworek,Andrzej Górski,Male,Female,Male,Mix,,
4.0,1.0,Journal of Economic Interaction and Coordination,26 March 2009,https://link.springer.com/article/10.1007/s11403-009-0048-8,Industry dynamics with knowledge-based competition: a computational study of entry and exit patterns,June 2009,Myong-Hun Chang,,,Unknown,Unknown,Unknown,Unknown,,
4.0,2.0,Journal of Economic Interaction and Coordination,09 April 2009,https://link.springer.com/article/10.1007/s11403-009-0049-7,Stocks of information in personal consumption: a network model with non-rival borrowing and content overlap,November 2009,Steven Silver,Phillip Cowans,,Male,Male,Unknown,Male,,1
4.0,2.0,Journal of Economic Interaction and Coordination,23 April 2009,https://link.springer.com/article/10.1007/s11403-009-0050-1,Non-self-averaging of a two-person game with only positive spillover: a new formulation of Avatamsaka’s dilemma,November 2009,Yuji Aruka,Eizo Akiyama,,Male,Male,Unknown,Male,,5
4.0,2.0,Journal of Economic Interaction and Coordination,23 April 2009,https://link.springer.com/article/10.1007/s11403-009-0051-0,Smart predictors in the heterogeneous agent model,November 2009,Jozef Barunik,Lukas Vacha,Miloslav Vosvrda,Male,Male,Male,Male,,12
4.0,2.0,Journal of Economic Interaction and Coordination,17 May 2009,https://link.springer.com/article/10.1007/s11403-009-0052-z,Limits to the equilibrating capabilities of market systems,November 2009,Axel Leijonhufvud,,,Male,Unknown,Unknown,Male,,1
4.0,2.0,Journal of Economic Interaction and Coordination,30 April 2009,https://link.springer.com/article/10.1007/s11403-009-0053-y,The shared reward dilemma on structured populations,November 2009,Raúl Jiménez,José A. Cuesta,Angel Sánchez,Male,Male,Male,Male,,5
4.0,2.0,Journal of Economic Interaction and Coordination,29 May 2009,https://link.springer.com/article/10.1007/s11403-009-0054-x,Business fluctuations and bankruptcy avalanches in an evolving network economy,November 2009,Domenico Delli Gatti,Mauro Gallegati,Joseph E. Stiglitz,Male,Male,Male,Male,,75
4.0,2.0,Journal of Economic Interaction and Coordination,10 June 2009,https://link.springer.com/article/10.1007/s11403-009-0055-9,Microscopic spin model for the stock market with attractor bubbling on scale-free networks,November 2009,Andrzej Krawiecki,,,Male,Unknown,Unknown,Male,,5
4.0,2.0,Journal of Economic Interaction and Coordination,29 July 2009,https://link.springer.com/article/10.1007/s11403-009-0057-7,The Naming Game in social networks: community formation and consensus engineering,November 2009,Qiming Lu,G. Korniss,Boleslaw K. Szymanski,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Journal of Economic Interaction and Coordination,13 March 2010,https://link.springer.com/article/10.1007/s11403-010-0061-y,The international-trade network: gravity equations and topological properties,June 2010,Giorgio Fagiolo,,,Male,Unknown,Unknown,Male,,108
5.0,1.0,Journal of Economic Interaction and Coordination,03 April 2010,https://link.springer.com/article/10.1007/s11403-010-0063-9,"Productivity dispersion: facts, theory, and implications",June 2010,Hideaki Aoyama,Hiroshi Yoshikawa,Yoshi Fujiwara,Male,Male,Male,Male,,19
5.0,1.0,Journal of Economic Interaction and Coordination,27 June 2009,https://link.springer.com/article/10.1007/s11403-009-0056-8,Monopolistic competition and new products: a conjectural equilibrium approach,June 2010,Francesco Bogliacino,Giorgio Rampa,,Male,Male,Unknown,Male,,3
5.0,1.0,Journal of Economic Interaction and Coordination,16 September 2009,https://link.springer.com/article/10.1007/s11403-009-0058-6,Stochastic models of resonating markets,June 2010,Carlo Lucheroni,,,Male,Unknown,Unknown,Male,,5
5.0,1.0,Journal of Economic Interaction and Coordination,12 November 2009,https://link.springer.com/article/10.1007/s11403-009-0059-5,"Environmental degradation, self-protection choices and coordination failures in a North–South evolutionary model",June 2010,Angelo Antoci,Simone Borghesi,,Male,Female,Unknown,Mix,,
5.0,1.0,Journal of Economic Interaction and Coordination,06 March 2010,https://link.springer.com/article/10.1007/s11403-010-0060-z,Erratum to: Non-self-averaging of a two-person game with only positive spillover: a new formulation of Avatamsaka’s dilemma,June 2010,Yuji Aruka,Eizo Akiyama,,Male,Male,Unknown,Male,,
5.0,2.0,Journal of Economic Interaction and Coordination,05 June 2010,https://link.springer.com/article/10.1007/s11403-010-0064-8,Complex agent-based macroeconomics: a manifesto for a new paradigm,December 2010,Domenico Delli Gatti,Edoardo Gaffeo,Mauro Gallegati,Male,Male,Male,Male,,62
5.0,2.0,Journal of Economic Interaction and Coordination,25 August 2010,https://link.springer.com/article/10.1007/s11403-010-0070-x,Modeling collective adaptive agent design and its analysis in Barnga game,December 2010,Yuya Ushida,Kiyohiko Hattori,Keiki Takdama,Female,Male,Male,Mix,,
5.0,2.0,Journal of Economic Interaction and Coordination,31 July 2010,https://link.springer.com/article/10.1007/s11403-010-0069-3,Determining the optimal market structure using near-zero intelligence traders,December 2010,Xinyang Li,Andreas Krause,,Unknown,Male,Unknown,Male,,3
5.0,2.0,Journal of Economic Interaction and Coordination,10 October 2010,https://link.springer.com/article/10.1007/s11403-010-0072-8,Preface to the special issue,December 2010,Misako Takayasu,Tsutomu Watanabe,Hideki Takayasu,Female,Male,Male,Mix,,
5.0,2.0,Journal of Economic Interaction and Coordination,06 June 2010,https://link.springer.com/article/10.1007/s11403-010-0066-6,Network motifs in an inter-firm network,December 2010,Takaaki Ohnishi,Hideki Takayasu,Misako Takayasu,Male,Male,Female,Mix,,
5.0,2.0,Journal of Economic Interaction and Coordination,05 June 2010,https://link.springer.com/article/10.1007/s11403-010-0067-5,The dynamics and distribution of the area price in the Nord Pool,December 2010,Tsuruhiko Nambu,Takaaki Ohnishi,,Unknown,Male,Unknown,Male,,3
5.0,2.0,Journal of Economic Interaction and Coordination,10 July 2010,https://link.springer.com/article/10.1007/s11403-010-0068-4,Ownership and control in shareholding networks,December 2010,Giulia Rotundo,Anna Maria D’Arcangelis,,Female,Female,Unknown,Female,,32
5.0,2.0,Journal of Economic Interaction and Coordination,30 May 2010,https://link.springer.com/article/10.1007/s11403-010-0065-7,Macroscopic and microscopic statistical properties observed in blog entries,December 2010,Yukie Sano,Misako Takayasu,,Female,Female,Unknown,Female,,6
5.0,2.0,Journal of Economic Interaction and Coordination,21 March 2010,https://link.springer.com/article/10.1007/s11403-010-0062-x,Adopt the euro? The GME approach,December 2010,Paulo Ferreira,Andreia Dionísio,Cesaltina Pires,Male,Female,Female,Mix,,
5.0,2.0,Journal of Economic Interaction and Coordination,27 October 2010,https://link.springer.com/article/10.1007/s11403-010-0073-7,Prospect theory and risk appetite: an application to traders’ strategies in the financial market,December 2010,Shi-Nan Cao,Jing Deng,Honggang Li,Unknown,,Unknown,Mix,,
6.0,1.0,Journal of Economic Interaction and Coordination,14 September 2010,https://link.springer.com/article/10.1007/s11403-010-0071-9,"Transaction taxes, greed and risk aversion in an agent-based financial market model",May 2011,Markus Demary,,,Male,Unknown,Unknown,Male,,11
6.0,1.0,Journal of Economic Interaction and Coordination,21 October 2010,https://link.springer.com/article/10.1007/s11403-010-0074-6,The role of executives in hostile takeover attempts,May 2011,Mohd Irfan,,,Unknown,Unknown,Unknown,Unknown,,
6.0,1.0,Journal of Economic Interaction and Coordination,04 January 2011,https://link.springer.com/article/10.1007/s11403-010-0075-5,Volatility clustering and herding agents: does it matter what they observe?,May 2011,Ryuichi Yamamoto,,,Male,Unknown,Unknown,Male,,6
6.0,1.0,Journal of Economic Interaction and Coordination,04 January 2011,https://link.springer.com/article/10.1007/s11403-010-0076-4,The impact on the pricing process of costly active management and performance chasing clients,May 2011,Ron Bird,Lorenzo Casavecchia,Paul Woolley,Male,Male,Male,Male,,2
6.0,1.0,Journal of Economic Interaction and Coordination,15 January 2011,https://link.springer.com/article/10.1007/s11403-010-0077-3,A link between random coefficient autoregressive models and some agent based models,May 2011,Mamadou Abdoulaye Konté,,,Male,Unknown,Unknown,Male,,1
6.0,2.0,Journal of Economic Interaction and Coordination,10 July 2011,https://link.springer.com/article/10.1007/s11403-011-0078-x,Response of double-auction markets to instantaneous Selling–Buying signals with stochastic Bid–Ask spread,November 2011,Takero Ibuki,Jun-ichi Inoue,,Unknown,Unknown,Unknown,Unknown,,
6.0,2.0,Journal of Economic Interaction and Coordination,09 September 2011,https://link.springer.com/article/10.1007/s11403-011-0079-9,Market clearing by maximum entropy in agent models of stock markets,November 2011,Friedrich Wagner,,,Male,Unknown,Unknown,Male,,2
6.0,2.0,Journal of Economic Interaction and Coordination,30 July 2011,https://link.springer.com/article/10.1007/s11403-011-0080-3,The construction of choice: a computational voting model,November 2011,Luigi Marengo,Corrado Pasquali,,Male,Male,Unknown,Male,,5
6.0,2.0,Journal of Economic Interaction and Coordination,09 August 2011,https://link.springer.com/article/10.1007/s11403-011-0081-2,Oligopoly firms with quantity-price strategic decisions,November 2011,Tong Zhang,B. Wade Brorsen,,,Unknown,Unknown,Mix,,
7.0,1.0,Journal of Economic Interaction and Coordination,26 January 2012,https://link.springer.com/article/10.1007/s11403-012-0088-3,Non-self-averaging in macroeconomic models: a criticism of modern micro-founded macroeconomics,May 2012,Masanao Aoki,Hiroshi Yoshikawa,,Male,Male,Unknown,Male,,13
7.0,1.0,Journal of Economic Interaction and Coordination,12 November 2011,https://link.springer.com/article/10.1007/s11403-011-0085-y,"An evolutionary model of industry dynamics and firms’ institutional behavior with job search, bargaining and matching",May 2012,Sandra Tavares Silva,Jorge M. S. Valente,Aurora A. C. Teixeira,Female,Male,Female,Mix,,
7.0,1.0,Journal of Economic Interaction and Coordination,17 September 2011,https://link.springer.com/article/10.1007/s11403-011-0082-1,On the performance of voting systems in spatial voting simulations,May 2012,Anghel Negriu,Cyrille Piatecki,,Male,,Unknown,Mix,,
7.0,1.0,Journal of Economic Interaction and Coordination,02 November 2011,https://link.springer.com/article/10.1007/s11403-011-0083-0,"Identity, reputation and social interaction with an application to sequential voting",May 2012,Emilio Barucci,Marco Tolotti,,Male,Male,Unknown,Male,,4
7.0,1.0,Journal of Economic Interaction and Coordination,03 November 2011,https://link.springer.com/article/10.1007/s11403-011-0084-z,Ethnic polarization orderings and indices,May 2012,Satya R. Chakravarty,Bhargav Maharaj,,Male,Unknown,Unknown,Male,,6
7.0,2.0,Journal of Economic Interaction and Coordination,29 December 2011,https://link.springer.com/article/10.1007/s11403-011-0086-x,Innovation diffusion with heterogeneous networked agents: a computational model,October 2012,Rui Leite,Aurora A. C. Teixeira,,Male,Female,Unknown,Mix,,
7.0,2.0,Journal of Economic Interaction and Coordination,13 January 2012,https://link.springer.com/article/10.1007/s11403-012-0087-4,An agent-based model of innovation diffusion: network structure and coexistence under different information regimes,October 2012,Giovanni Pegoretti,Francesco Rentocchini,Giuseppe Vittucci Marzetti,Male,Male,Male,Male,,15
7.0,2.0,Journal of Economic Interaction and Coordination,18 March 2012,https://link.springer.com/article/10.1007/s11403-012-0089-2,Comprehensive analysis of market conditions in the foreign exchange market,October 2012,Aki-Hiro Sato,Takaki Hayashi,Janusz A. Hołyst,Unknown,Unknown,Male,Male,,6
7.0,2.0,Journal of Economic Interaction and Coordination,27 March 2012,https://link.springer.com/article/10.1007/s11403-012-0090-9,Exchangeability and non-self-averaging,October 2012,U. Garibaldi,P. Viarengo,,Unknown,Unknown,Unknown,Unknown,,
7.0,2.0,Journal of Economic Interaction and Coordination,10 April 2012,https://link.springer.com/article/10.1007/s11403-012-0092-7,Responders’ dissatisfaction may provoke fair offer,October 2012,Wenxin Xie,Yong Li,Keqiang Li,Unknown,,Unknown,Mix,,
7.0,2.0,Journal of Economic Interaction and Coordination,04 July 2012,https://link.springer.com/article/10.1007/s11403-012-0097-2,Agent based model of a simple economy,October 2012,Vladimír Gazda,Marek Gróf,Tomáš Rosival,Male,Male,Male,Male,,8
7.0,2.0,Journal of Economic Interaction and Coordination,05 August 2012,https://link.springer.com/article/10.1007/s11403-012-0099-0,A re-examination of the “zero is enough” hypothesis in the emergence of financial stylized facts,October 2012,Olivier Brandouy,Angelo Corelli,Roger Waldeck,Male,Male,Male,Male,,9
8.0,1.0,Journal of Economic Interaction and Coordination,05 August 2012,https://link.springer.com/article/10.1007/s11403-012-0100-y,Agent-based financial markets and New Keynesian macroeconomics: a synthesis,April 2013,Matthias Lengnick,Hans-Werner Wohltmann,,Male,Unknown,Unknown,Male,,43
8.0,1.0,Journal of Economic Interaction and Coordination,25 August 2012,https://link.springer.com/article/10.1007/s11403-012-0101-x,Global networks of trade and bits,April 2013,Massimo Riccaboni,Alessandro Rossi,Stefano Schiavo,Male,Male,Male,Male,,17
8.0,1.0,Journal of Economic Interaction and Coordination,29 August 2012,https://link.springer.com/article/10.1007/s11403-012-0103-8,"On the distributional properties of size, profit and growth of Icelandic firms",April 2013,Einar Jón Erlingsson,Simone Alfarano,Hlynur Stefánsson,Male,Female,Male,Mix,,
8.0,1.0,Journal of Economic Interaction and Coordination,22 September 2012,https://link.springer.com/article/10.1007/s11403-012-0104-7,Null models of economic networks: the case of the world trade web,April 2013,Giorgio Fagiolo,Tiziano Squartini,Diego Garlaschelli,Male,Male,Male,Male,"In the last years an increasing number of contributions have been addressing the study of economic systems and their dynamics in terms of networks (Schweitzer et al. 2009). The description of an economic system as a network requires to characterize economic units (e.g., countries, industries, firms, consumers, individuals, etc.) as nodes and their market and non-market relationships as links between them. Successive snapshots of these interacting structures can give us a clue about how networked systems evolve in time. Heterogeneity of agent and link types can be easily considered. Nodes can be tagged with different characteristics or properties (e.g., economic size) and links may be defined to be directed or undirected, binary or weighted, etc., according to the focus of the analysis. The study of economic networks has recently proceeded along three main complementary directions. First, a large body of empirical contributions have investigated the topological properties of many real-world economic and social networks (Caldarelli 2007), ranging from macroeconomic networks where nodes are countries and linkages represent trade or financial transactions, all the way to firm and consumer networks where links represent knowledge or information exchange. This empirical-research program has generated a very rich statistical evidence, pointing to many differences and similarities in the way economic networks are shaped. As a consequence, a very fertile ground for theoretical work has emerged. Second, a stream of theoretical research has explored efficiency properties of equilibrium networks arising in cooperative and non-cooperative game-theoretic setups, where players have the possibility to choose both their strategy in the game and whom to play the game with (Goyal 2007; Vega-Redondo 2007; Jackson 2008). Despite such models have been very successful in highlighting the role of network structure in explaining aggregate outcomes, they fell short from providing a framework where observed network regularities can be reproduced and explained. Third, a large number of contributions coming from a complex-network perspective have been developing simple stochastic models of graph evolution where nodes hold very stylized and myopic probabilistic rules determining their future connectivity patterns in the network (Newman 2010). The two foremost examples of such an approach are Watts and Strogatz (1998) small-world model and Albert and Barabási (2002) preferential-attachment model. Despite this family of stochastic models are able to reproduce observed economic-network patterns, the extent to which such stylized representations can be employed to understand causal relations between incentive-based choices made in strategic contexts and the overall efficiency of the long-run equilibrium networks is still under scrutiny. All that hints to the dramatic need for theoretical models that are able to reproduce and economically explain the observed patterns of topological properties in real-world networks. Despite we know a great deal about how economic networks are shaped in reality and what that means for dynamic processes going on over networked structures (e.g., diffusion of shocks and contagion effects, cf. for example Allen and Gale 2001; Battiston et al. 2009), we still lack a clear understanding of why real-world network architectures looks like they do, and how all that has to do with individual incentives and social welfare. This paper contributes to the aforementioned debate by exploring an alternative approach to the trade-off between explanation and reproduction of topological properties, grounded in the generation of null (random) network models. The idea is not new. Instead of building economically- or stochastically-based micro-foundations for explaining observed patterns, one tries to ask the question whether observed statistical-network properties may be simply reproduced by simple processes of network generation that only match some (empirically-observed) constraints, but are otherwise fully random. If they do, then the researcher may conclude that such regularities are not that interesting from an economic point of view, as no alternative, more structural, model would pass any test discriminating against the random counterpart. Conversely, if observed regularities cannot be reproduced by the null random model, we are led to argue that some more structural economic process may be responsible for what we observe. Null random-network models may therefore serve as a sort of sieve that can help us to discriminate between interesting and useless observed-network properties. Exactly as in statistics and econometrics one performs significance tests, null network models are very helpful to understand the distributional properties of a given network statistics, under very mild null hypotheses for the underlying network-generation process.Footnote 1
 Null (random) network models have been extensively used in the recent past (see for a review Squartini and Garlaschelli 2011). Since the seminal work of Erdős and Rényi (1960) on random graphs, many alternative null network models have been proposed.Footnote 2 A useful way of classifying them is according to the constraints they pose in the way the otherwise-random mechanism of network construction works. A large number of contributions, for example, have been focusing on generating random networks able to control (exactly or on average) for the degree sequence in binary graphs, or for the strength sequence in weighted ones.Footnote 3 This is reasonable, as degree and strength sequences are one of the most basic statistics characterizing graphs. It is therefore very important to study the properties of network statistics (other than degrees and strengths) in ensembles of otherwise fully-random graphs preserving those basic topological quantities (and thus looking somewhat similar to the observed one). However, most of the existing network null-model methods suffer from important limitations. A large class of algorithms generates randomized variants of a network computationally, through iterated “moves” that locally rewire the original connections in such a way that the desired constraints remain unchanged (Shen-Orr et al. 2002; Maslov and Sneppen 2002; Maslov et al. 2004). These approaches are extremely demanding in terms of computation time. In order to obtain expectations from the null model, one has indeed to constructively build many alternative random graphs belonging to the desired family, then measure any target topological property on each of such randomized graphs, and finally perform a final sample average of this property. At the opposite extreme, analytical approaches have been proposed in order to obtain mathematical expressions characterizing the expected properties, thus avoiding time-consuming randomizations (Newman et al. 2001; Chung and Lu 2002; Serrano and Boguñá 2005; Bargigli and Gallegati 2011). The problem with the latter approaches is that they are only valid under specific hypotheses about the structure of the original network. For instance, methods based on probability generating functions are generally only valid for sparse and locally tree-like (thus with vanishing clustering) networks (Newman et al. 2001). Similarly, models predicting factorized connection probabilities in binary graphs (Chung and Lu 2002) or factorized expected weights in weighted networks (Serrano and Boguñá 2005; Bargigli and Gallegati 2011) make (either explicitly or implicitly) the assumption of sparse networks, as has been shown recently (Squartini and Garlaschelli 2011). Additionally, each method or algorithm is generally designed to generate random networks satisfying a specific set of constraints (e.g., degree sequence) and cannot be easily extended to cover different sets of constraints (e.g., strength sequence, possibly in directed-graph contexts). For instance, a problem that inherently pervades random models of weighted networks is the simplifying assumption of real-valued edge weights (Bhattacharya et al. 2008; Ansmann and Lehnertz 2011; Fronczak and Fronczak 2011). When made in models that specify the strength sequence, this assumption leads to randomized ensembles of networks where edges with zero weight have zero probability, so that the typical networks are fully connected (Ansmann and Lehnertz 2011; Fronczak and Fronczak 2011). This actually makes the original network an unlikely outcome of the model, rather than one with the same probability as all other instances with the same sufficient statistics (e.g. with the same strength sequence). In this paper we employ a recently-proposed method that overcomes all the above restrictions simultaneously (Squartini and Garlaschelli 2011). The method is analytical and therefore does not require simulations to generate the family of all randomized variants of the target network. This important property makes the method very fast and strongly facilitates exhaustive analyses which require the analysis of many networks, e.g. in order to track the temporal evolution of a particular system or to study all the individual components of a multi-network with many layers, or both (Squartini et al. 2011a, b). At the same time, the method does not make assumptions about the structure of the original network, and therefore works also for dense and clustered networks. Furthermore, the method can deal with binary graphs and weighted networks in a unified fashion (in both cases, edges can be either directed or undirected). In the weighted case, it exploits the natural notion of a fundamental unit of weight to treat edge weights as discrete and integer-valued, preventing randomized networks from becoming fully connected. This property ensures that, even when dealing with randomized weighted networks, the expected bare topology is nontrivial and allows comparisons with that of the original network. In general, the method allows to set any given target topological property of interest and to obtain the expected values and standard deviations of the corresponding quantity over the family of all randomized variants of the network that preserve some arbitrary local structural properties. We apply the method to the world trade web (WTW) network, also known as the international trade network (ITN). The WTW is a weighted-directed network, where nodes are countries and directed links represent the value trade (export) flows between countries in that year. We also study the binary projection of this network, where a directed link between country \(i\) and country \(j\) is in place if and only if \(i\) exports to \(j\). Therefore, the binary WTW maps trade relationships, whereas the weighted WTW accounts for heterogeneity of bilateral trade flows associated to trade partnerships. The study of the WTW has received a lot of attention in the last years.Footnote 4 Despite we know a great deal about statistical regularities of the WTW, we still lack a clear understanding of whether such regularities can be really meaningful, or, conversely, whether they are just the effect of randomness, i.e. whether a simple null-network model could easily explain that evidence. This issue was already tackled in Squartini et al. (2011a, b), who show that, for the 1992–2002 period, much of the binary WTW architecture (both at the aggregate and product-specific level) can be reproduced by a null model controlling for in- and out-degree, whereas weighted-network regularities cannot be fully explained by node-strength sequences. More specifically, observed patterns of network disassortativity and clustering can be fully predicted by degree sequences, whereas they become non-trivially deducible from null-network models controlling for node strengths. These results have important consequences for international-trade issues. Indeed, controlling for in- or out-degree and strength means fixing local-country properties (e.g., involving direct bilateral relations only) that give us information about the number of trade partnerships and country total imports and exports. These are statistics that are traditionally employed by international-trade economists to fully characterize country-trade profiles. Conversely, higher-order network properties like assortativity or clustering are non-local, as they refer to indirect trade relations involving trade partners of a country’s partners, and so on. The fact that higher-order properties cannot be explained by random-network models controlling for local-properties only implies that a network approach to the study of the WTW is able to discover fresh statistical regularities. In turn, this suggests that we require more structural models to explain why such higher-order property do emerge. In this paper, we extend the analysis in Squartini et al. (2011a, b) and we analyze a longer time frame (1950–2000). This allows us to better understand if subsequent globalization waves have changed the structure of the WTW and whether local properties like node degrees and strengths have been playing the same role in explaining higher-order properties. We compare observed and expected directed-network statistics in both binary and weighted aggregate WTW for the period under analysis. Our results show that, in the binary WTW, knowing the sequence of node degrees, i.e. number of import and export partners of a country, is largely sufficient to explain higher-order network properties related to disassortativity and clustering-degree correlation, especially in the last part of the sample (i.e., after 1965). We also find that in the first part of the sample (before 1965) local binary properties badly predict the structure of the network, which however does not present any clear evident structural correlation pattern. We interpret this result in terms of pre-globalization features of the web of international-trade relations, mostly ruled by geographical constraints and political barriers. Our weighted network analysis conveys instead an opposite message: observed local properties (i.e. country total imports and exports) hardly explain any observed higher-order weighted property of the WTW. This implies that in the binary case node-degree sequences (local properties) become maximally informative and higher-order properties of the network turn out to be statistically irrelevant as compared to the null model. Conversely, in the weighted case, the observed sequence of total country imports and exports are never able to explain higher-order patterns of the WTW, making the latter fresh statistical properties in search of a deeper explanation. The rest of the paper is organized as follows. Section 2 briefly reviews the recent literature on the WTW. Section 3 introduces the null model. Data and methodology are described in Sect. 4. Sections 5 and 6 present and discuss the main results. Finally, Sect. 7 concludes.",66
8.0,1.0,Journal of Economic Interaction and Coordination,18 November 2012,https://link.springer.com/article/10.1007/s11403-012-0106-5,Diffusion of competing innovations in influence networks,April 2013,Jeehong Kim,Wonchang Hur,,Unknown,Unknown,Unknown,Unknown,,
8.0,1.0,Journal of Economic Interaction and Coordination,04 January 2013,https://link.springer.com/article/10.1007/s11403-012-0107-4,Do price limits hurt the market?,April 2013,Chia-Hsuan Yeh,Chun-Yi Yang,,Unknown,,Unknown,Mix,,
8.0,1.0,Journal of Economic Interaction and Coordination,24 January 2013,https://link.springer.com/article/10.1007/s11403-013-0108-y,Modeling the International-Trade Network: a gravity approach,April 2013,Marco Dueñas,Giorgio Fagiolo,,Male,Male,Unknown,Male,"The International Trade Network (ITN), aka World-Trade Web (WTW) or World Trade Network (WTN), is defined as the graph representing in each year the web of bilateral-trade relationships between countries in the World. The statistical properties of the ITN, and their evolution over time, have been recently received a lot of attention in a number of contributions.Footnote 1
 Understanding the topology of the ITN is important for two related reasons. First, trade is one of the most important channels of interaction among countries (Helliwell and Padmore 1985; Krugman 1995; Galvandatilde et al. 2007; Forbes 2002). The knowledge of macroeconomic phenomena such as economic globalization and internationalization, the spreading of international crises, and the transmission of economic shocks, may be improved by looking at international-trade patterns in a holistic framework, where indirect as well as direct linkages between countries are explicitly taken into consideration (Fagiolo 2010).Footnote 2 Second, ITN topological properties can help to statistically explain macroeconomics dynamics. For example, Kali et al. (2007) and Kali and Reyes (2010) have shown that country position in the trade network has substantial implications for economic growth and a good potential for predicting episodes of financial contagion. Furthermore, Reyes et al. (2010) suggest that country centrality in the ITN may help to account for the evolution of international economic integration better than what standard statistics, like openness to trade, do. The statistical properties of the ITN, in its undirected/directed or binary/weighted characterizations, have been extensively studied and today we know a great deal about the topological architecture of the web of international-trade flows. For example, Serrano and Boguñá (2003) and Garlaschelli and Loffredo (2004) show that the binary-directed representation of the ITN exhibits a disassortative pattern: countries with many trade partners (i.e., high node degree) are on average connected with countries with few partners (i.e., low average nearest-neighbor degree). Furthermore, partners of well connected countries are less interconnected than those of poorly connected ones, implying some hierarchical arrangements. Remarkably, Garlaschelli and Loffredo (2005) show that this evidence is quite stable over time. This casts some doubts on whether economic integration (globalization) has really increased in the last 30 years. Furthermore, node-degrees appear to be very skewed, implying the coexistence of few countries with many partners and many countries with only a few partners. These issues are taken up in more detail in a few subsequent studies adopting a weighted-network approach to the study of the ITN. The motivation is that a binary approach, by treating all relationship equally, might dramatically underestimate the impact of trade-linkage heterogeneity. This seems indeed to be the case: Fagiolo et al. (2008, 2009, 2010) find that the statistical properties of the ITN viewed as a weighted undirected network crucially differ from those exhibited by its binary counterpart. For example, the strength distribution is highly right-skewed, indicating that a few intense trade connections co-exist with a majority of low-intensity ones. This confirms the results obtained by Bhattacharya et al. (2007, 2008), who find that the size of the group of countries controlling half of the world’s trade has decreased in the last decade. Furthermore, weighted-network analyses show that the ITN architecture has been extremely stable in the 1981–2000 period and highlights some interesting regularities (Fagiolo et al. 2009). For example, countries holding many trade partners and/or very intense trade relationships are also the richest and most globally central; they typically trade with many partners, but very intensively with only a few of them, which turn out to be very connected themselves; and form few but intensive-trade clusters (i.e., triangular trade patterns). Most of existing network literature on the ITN, however, has been focusing on a purely empirical quest for statistical properties, largely neglecting the issue of exploring whether theoretical models are able to explain why the ITN is shaped the way it is.Footnote 3
 This paper is a preliminary attempt to fill this gap. We extend the work in Fagiolo (2010) to ask whether the gravity model (GM) can provide a satisfactory theoretical benchmark able to reproduce the observed architecture of the ITN across time. The GM (van Bergeijk and Brakman 2010) aims at explaining international-trade bilateral flows using an equation obtained as the equilibrium prediction of a large family of micro-founded models of trade (more on that in Sect. 2). The term “gravity” comes about because the predicted relation between trade flows and explanatory variables is similar to Newton’s formula: the magnitude of aggregated trade flows between a pair of countries is proportional to the product of country sizes (e.g. the masses, as proxied by country GDPs) and inversely proportional to their geographic distance (interpreted as proxies of trade-resistance factors, e.g. tariffs). From an econometric perspective, the original model-driven prediction can be augmented with a set of country-specific explanatory variables (e.g., population, area, land-locking effects, etc.), as well as with a set of bilateral variables (i.e., geographical contiguity, common language and religion, colony relation, bilateral trade agreements, etc.). The GM can be fitted to the data using different econometric techniques, ranging from simple ordinary least squares (OLS) applied to the log-linearized equation, to zero-inflated two-stage non-linear estimation, employed to correctly deal with the large number of zero trade flows characterizing the data. Overall, the GM is very successful: independently on the technique employed, it typically achieves a very high goodness of fit, e.g. in terms of R-squared coefficients. Motivated by the well-known empirical success of the GM, we fit data on bilateral-trade flows to estimate GM-predicted weighted-directed representations of the ITN, which we then compare to the observed one, constructed using original bilateral-flow data. We employ both a static and a dynamic approach. In the static approach, we assume that a GM holds in each subsequent year and we estimate a series of predicted ITN snapshots. In the dynamic approach, we control for time dummies in the estimation to account for change over time and get a unique predicted ITN from the unbalanced panel of predicted flows. In both cases, we end up with estimates for both the probability that a link is present and for the probability of any given level of bilateral-trade flow occurring between any two countries in a given year (given that a link is present). We complement this information with standard errors of estimated quantities, so as to evaluate the precision of GM-based predictions. In a nutshell, our results suggest that a necessary (but not sufficient) condition for the GM to well predict weighted ITN properties is to fix the binary structure equal to the observed one. Even if one conditions trade-flow estimation to the true binary architecture, the GM may badly predict higher-order statistics that, like clustering, require the knowledge of triadic link-weight topological patterns. Finally, the performance of the GM is very poor when asked to predict ITN weighted properties together with its binary architecture, or when one employs a GM specification to estimate the presence of a link only. The rest of the paper is organized as follows. Section 2 discusses the gravity model and presents data and related methodologies. Our main results are reported in Sect. 3. Finally, Sect. 4 concludes and flags some of the challenges facing ITN modeling in the future.",86
8.0,1.0,Journal of Economic Interaction and Coordination,17 February 2013,https://link.springer.com/article/10.1007/s11403-013-0109-x,Information dissemination in an experimentally based agent-based stock market,April 2013,Jakob Grazzini,,,Male,Unknown,Unknown,Male,"The aim of this paper is to use the flexibility of agent-based models to formalize the subjects’ behavior in an experimental stock market. Computational agents will trade in a realistic continuous double auction using simple strategies. The objective is to understand how experimental markets aggregate the traders’ private information. Experiments and agent-based models are “natural allies” (Duffy 2006) and can complete each other in a prolific way (Contini et al. 2007). In experiments it is possible to control the economic environment and the market structure but not the motives and the subjects’ individual characteristics (Chan et al. 2001). A model can use the experimental environment and results to formalize the subjects’ behavior using the generative approach proposed by Epstein and Axtell (1996). The experiment provides the foundation and the results needed for the model, while the model provides the formalization of the subjects’ behavior. According to Smith (1982) it is possible to define a microeconomic system, \(S=(e,I)\), as a microeconomic environment \((e)\) and a microeconomic institution \((I)\). The microeconomic environment is defined by Smith (1982) as a list of \(N\) economic agents, \(K+1\) commodities and the characteristics of each agent \(i,\,e = (e^1,\ldots , e^i,\ldots , e^N)\). The environment is thus a set of initial conditions that cannot be altered by the agents. The microeconomic institution defines the rules under which the agents act and interact. In a stock market the institution is represented by a trading mechanism, e.g. continuous double auction, that imposes a set of rules which regulate the agents’ actions: how they issue orders and how contracts are made. The behavior of agent \(i\) is a function \(\beta ^i(e^i|I)\). Given the institution, the environment and the behavior of the agents, the macro-behavior of the system is defined. The aim of the experimentally based model is to use the environment, the institution and the outcome of the experiments to formalize a plausible set of micro-behaviors. As stated by Gode and Sunder (1993, p. 120): “It is not possible to control the trading behavior of individuals. Human traders differ in their expectations, attitudes toward risk, preferences for money versus enjoyment of trading as a game, and many other respects. The problem of separating the joint effects of these variations, unobservable to the researcher, can be mitigated by studying market outcomes with participants who follow specified rules of behavior. We therefore replaced human traders by computer programs.” Once the behavior is defined, it is possible to test the effect of different hypotheses and isolate the influence of different factors on the outcome of the system. In the literature there are several examples of modeling experiments using heterogeneous agent models or agent-based models (Hommes 2011; Hommes et al. 2005; Anufriev and Hommes 2012; Chan et al. 2001; Gode and Sunder 1993; Gjerstad and Dickhaut 1998; Boero et al. 2010a; Bravo et al. 2012; Duffy and Ünver 2006; Zhang and Brorsen 2011). This paper will contribute to the literature by building an agent-based model using experimental stock markets and analyzing the plausible behaviors of the agents. The paper particularly focuses on how the market aggregates information. The market is simply defined by a set of boundedly rational agents operating in a continuous double auction. How the information is incorporated into the prices is a crucial question in financial economics. The problem was dealt with experimentally in Plott and Sunder (1982, 1988) and in the subsequent literature (Camerer and Weigelt 1991; Copeland and Friedman 1986, 1991; Forsythe and Lundholm 1990; Barner et al. 2005). The experimental results show that markets can aggregate information but sometimes they make mistakes (Plott 2000). Using the experimental data published by Barner et al. (2005), this paper builds an agent-based model of the experimental market and tries to understand how the market aggregates the information and what happens when the mechanism fails. The agent-based model tool was chosen because of the complexity arising from the behavior of boundedly rational agents interacting through a continuous double auction. Starting from the simple market described in Smith (1962), an agent-based model was developed following the model proposed by Cliff and Bruten (1997). Particularly in Sect. 2 the model described by Cliff and Bruten (1997) is reproduced to show how the behavior of the price in Smith (1962) can be replicated using very simple heuristics. Two basic incentives drive traders’ behavior: maximizing the profits for each trade and maximizing the probability of trading. Considering this behavior as a basic behavior, the model is extended to a more general type of market to replicate the experiment on information dissemination in Barner et al. (2005). In Sect. 3 the experiment described by Barner et al. (2005) is presented. In Sect. 4 the Cliff and Bruten (1997) model is modified to meet the experimental environment described by Barner et al. (2005), and then—using the experimental data—extended to more complicated cognitive tasks (Sect. 5) where the agents have to evaluate a random variable (representing the value of the traded asset) and learn the information held by the insiders in the market (Sect. 6). The aim is to identify the conditions under which the information is correctly disseminated into the market. The result is that markets tend to disseminate the information correctly due to the trading incentives and the transparency of the continuous double auction. Failures in information dissemination seem to depend on the idiosyncratic features of the traders and are therefore difficult to prevent. Section 7 will conclude and propose future research.",2
8.0,2.0,Journal of Economic Interaction and Coordination,26 October 2012,https://link.springer.com/article/10.1007/s11403-012-0105-6,Major trends in agent-based economics,October 2013,Leonardo Bargigli,Gabriele Tedeschi,,Male,Female,Unknown,Mix,,
8.0,2.0,Journal of Economic Interaction and Coordination,12 April 2012,https://link.springer.com/article/10.1007/s11403-012-0091-8,Income distribution and debts in a fragile economy: market processes and macro constraints,October 2013,Piero Ferri,,,Male,Unknown,Unknown,Male,,3
8.0,2.0,Journal of Economic Interaction and Coordination,24 August 2012,https://link.springer.com/article/10.1007/s11403-012-0102-9,Lending attitude as a financial accelerator in a credit network economy,October 2013,Daiki Asanuma,,,Male,Unknown,Unknown,Male,,7
8.0,2.0,Journal of Economic Interaction and Coordination,12 June 2012,https://link.springer.com/article/10.1007/s11403-012-0095-4,Coexistence of surplus labor and the Lewis turning point in China: a unitary household decision-making model study,October 2013,Huai Yu Liu,Shinan Cao,Jing Deng,,Unknown,,Mix,,
8.0,2.0,Journal of Economic Interaction and Coordination,05 May 2012,https://link.springer.com/article/10.1007/s11403-012-0094-5,Heterogeneity in the resistance to learning,October 2013,Haydée Lugo,,,Unknown,Unknown,Unknown,Unknown,,
8.0,2.0,Journal of Economic Interaction and Coordination,20 June 2012,https://link.springer.com/article/10.1007/s11403-012-0096-3,On the problem of calibrating an agent based model for financial markets,October 2013,Annalisa Fabretti,,,Female,Unknown,Unknown,Female,,45
8.0,2.0,Journal of Economic Interaction and Coordination,21 July 2012,https://link.springer.com/article/10.1007/s11403-012-0098-1,Expected utility theory with non-commutative probability theory,October 2013,Dino Borie,,,Male,Unknown,Unknown,Male,,2
9.0,1.0,Journal of Economic Interaction and Coordination,05 June 2013,https://link.springer.com/article/10.1007/s11403-013-0110-4,Agent-based modeling simulation under local network externality,April 2014,Arnut Paothong,G. S. Ladde,,Unknown,Unknown,Unknown,Unknown,,
9.0,1.0,Journal of Economic Interaction and Coordination,26 May 2013,https://link.springer.com/article/10.1007/s11403-013-0111-3,Adaptive learning in an asymmetric auction: genetic algorithm approach,April 2014,Kirill Chernomaz,,,Male,Unknown,Unknown,Male,"Introducing asymmetry of value distributions into the theory of auctions is a step towards bringing theoretical constructs closer to reality. Consider a procurement auction for a construction contract among several firms of different sizes. Suppose that it is common knowledge among the participants that there are economies of scale in the industry, i.e. construction costs decrease as the firm size increases. This situation can be described by means of ex ante asymmetric cost distributions such that larger firms are more likely to have lower costs than their smaller competitors (the size being an observable characteristic). In such situations the assumption of bidder symmetry may be too restrictive. However, allowing asymmetry complicates equilibrium calculations in some settings. In the second-price auction bidding your own value for the object (truthful bidding) remains a weakly dominant strategy. However, in the first-price auction the risk-neutral Nash equilibrium (RNNE) strategies are non-linear and distinct for each type of players.Footnote 1
 In general, a strategy in auctions is a function of one’s value. This fact alone makes it a difficult problem for a human decision maker. Asymmetry as well as non-linearity of the RNNE bidding functions are additional layers of complexity that cast doubts on the ability of human agents to discover and follow the equilibrium strategy. This statement is supported by the extensive experimental evidence for symmetric (Kagel 1995) and asymmetric auctions (e.g. Pezanis-Christou 2002; Guth et al. 2005; Chernomaz 2012; Chernomaz and Levin 2012). Human subjects’ behavior is often at odds with Nash equilibrium predictions. However, significant learning trends are often observed. These findings suggest that although theoretical results are extremely important benchmarks, they should be supplemented with models of learning. It seems necessary to evaluate the robustness of theoretical solutions by relaxing the assumption of perfect rationality and introducing explicit learning processes. This study is an effort to investigate the issues of learning and task complexity in the context of first-price auctions with asymmetrically distributed independent private values. In this paper we report results of agent-based simulations that employ a model of adaptive learning based on a genetic algorithm. The attractiveness of the algorithm is that it is easily interpretable and constitutes a plausible description of learning for human players who have little understanding of the auction game structure. The driving forces behind learning modeled by this algorithm are experimentation, imitation, and adoption of strategies that perform better than others. The algorithm has proven to be quite powerful in many situations and has been applied to a number of economic models (Arifovic 1996; Bullard and Duffy 1998; Dawid 1999). In a closely related paper, Andreoni and Miller (1995) use a genetic algorithm to study learning in various types of first- and second-price auctions including environments with independent private, common and affiliated private values. The authors focus on cases where bidders are symmetric. Their main conclusion is that with experience agents come close to following the RNNE. They also report that learning in independent private value auctions seems to be the most problematic. The approach of this study is similar to that of Andreoni and Miller (1995). However, we focus on the complexity of the learning task and sophistication of the agents rather than comparing outcomes in different auction mechanisms. The primary reason is that moving from symmetric to asymmetric settings by itself poses several new challenges to the long-run success of adaptive learning. It also raises interesting questions with respect to task complexity. Even for a simple case of uniformly distributed values the RNNE bidding functions are non-linear as opposed to the linear cases investigated in Andreoni and Miller (1995). This fact significantly expands the set of candidate strategies. Additionally, the agents of different types have to learn different strategies while in the symmetric case the equilibrium strategy is the same for all. First, we explore the impact of non-linearity of the RNNE strategies by contrasting two settings. In the first environment agents are endowed with the ability to manipulate quadratic functions, while in the second case, the sophistication of the agents is limited to linear approximations. This comparison is one way to explore the issue of task complexity and sophistication: on the one hand, quadratic functions offer a better fit (increase sophistication); on the other hand, quadratic functions are harder to learn. Our results suggest that these forces are indeed important and affect performance of the agents. Specifically, the agents with significantly non-linear equilibrium bidding function benefit from increased sophistication, while the agents whose equilibrium strategy is almost linear do not. In fact the latter perform worse when quadratic functions are allowed. To the best of our knowledge, this study is the first to explore the issue of sophistication and task complexity in relation to the degree of non-linearity of the RNNE strategies that varies across bidder types.Footnote 2
 Second, we approach the issue of task complexity from a different direction. A common distinction made in the literature is between individual and social learning (Arifovic 1994; Vriend 2000). Social (or single-population) learning typically corresponds to a multi-agent environment where during the learning process each agent evaluates and updates a single strategy but benefits from knowledge of the other agents’ strategies and their relative performance. In an individual (or multi-population) learning environment one or more agents evaluate and update a set of strategies based exclusively on their own experience. Only one of the strategies from the set is used at a time to determine the agent’s actions, however all strategies may be evaluated based on that experience. A key distinction between the two environments is in the amount of information that is shared between the agents. The social learning setting may be viewed as conducive to learning since the information flows freely between agents. Such free flow of information is rarely observed in the real world and even in the experimental lab.Footnote 3 Most of the time people have to (or choose to) rely on learning from their own mistakes. We compare individual and social learning in our simulations. We observe that the type of learning does not unambiguously determine some important free parameters that may have a strong effect on outcomes.Footnote 4 These parameters are: 1) the number of strategies that an agent has access to in the process of learning; 2) the number of interactions with the opponent population; and 3) the number of strategies that an agent can systematically compare, holding a number of factors fixed. We compare several learning environments to explore the relative importance of these forces. The paper proceeds as follows. In the following section, we describe the particular asymmetry considered in this study. Section 3 provides the details of the genetic algorithms used and develops the hypotheses about adaptive learning in the cases considered. Results are presented and discussed in Sect. 4. The last section concludes.",2
9.0,1.0,Journal of Economic Interaction and Coordination,07 June 2013,https://link.springer.com/article/10.1007/s11403-013-0112-2,An experimental investigation of insurance decisions in low probability and high loss risk situations,April 2014,Ozlem Ozdemir,Andrea Morone,,Unknown,Female,Unknown,Female,"A low probability and high loss (LPHL hereafter) events can be expressed as risky situations where probability of occurrence is low, but the harmful effect can be very dreadful (e.g., bankruptcy, insolvency, terrorism, and natural disasters). Both individuals and firms implement different risk reduction mechanisms, such as life insurance, credit insurance, home insurance, and storm shelters, against these events. However, how people decide to insure specifically towards LPHL hazards is questionable. Theoretical frameworks concerning protective measures chosen by individuals against LPHL risk situations have been developed over the past 30 years (e.g., Arrow 1996; Cook and Graham 1975; Dong et al. 1996; Kunreuther and Slovic 1978). Most have consistently demonstrated that insurance markets for high probability events can be expressed by standard expected utility theory (EUT); however, the EUT is inadequate to explain decision making processes in low probability risk situations (Hershey and Schoemaker 1980). As Morgenstern (1979) mentioned: [T]he domain of our axioms on utility theory is also restricted....For example, the probabilities used must be within certain plausible ranges and not go to 0.01 or even less to 0.001, then be compared to other equally tiny numbers such as 0.02, etc. (Morgenstern 1979, p. 178). Most survey studies indicate that some people perceive the risk as if no hazard exists, while others seem to react to the situation as if it is a frequent risk exposure condition (e.g. Camerer and Kunreuther 1989; McClelland et al. 1990; McDaniels et al. 1992; Slovic et al. 1980). The reasons why individuals behave in two opposite ways have not attracted enough attention in the literature. One possible reason might be that some people may take into account the low probability of occurrence and react to LPHL risks as if there is no such risk, while others may focus on the high loss and thus overreact (Etchart-Vincent 2004). In fact, Sjöberg (1999) finds that the demand for risk reduction is influenced by the severity of the hazard and not by the probability. Yet, some other scholars conclude that people make insurance decisions based on probability estimates (Slovic et al. 1977).Footnote 1 The lack of a definite answer to the question of whether individuals focus on low probability or high loss when they make insurance decisions in LPHL situations, stresses the necessity for further empirical research. The present study contributes to this research stream by conducting an experiment to examine a dominant risk assessing consideration, namely, the probability of loss versus loss amount on individuals’ insurance decisions in LPHL risky situations. During the course of this, we investigate whether using different elicitation methods induces any change in these decisions. In addition, we check the effects of individuals’ risk attitudes, their self-determined threshold probabilities, and their demographic characteristics (such as age, income and gender) on these insurance valuations. In our experiment, we use two different probabilities of loss (0.01 and 0.005)Footnote 2 and two loss amounts (all the income and half of the income) to test the dominance of probability and size of losses on the insurance decision. Furthermore, the design allows us to answer whether the two different elicitation mechanisms reported in the literature to detect the decision making process induce similar risk mitigation behaviour. Specifically, we try to examine the differences in individuals’ insurance decisions between the case when they are asked to decide whether they want to buy the insurance or not (asking dichotomous questions to elicit an individual’s choice), and the case when they are asked to state the amount of money they are willing to pay for the insurance (asking open-ended questions to elicit an individual’s valuation). The theoretical framework does not distinguish risk reduction mechanisms, such as insurance. However, according to the preference reversal phenomenon, individuals may consider different kinds of information when they make choice versus pricing/valuation decisions (Grether and Plott 1979; Holt 1986; Kagel and Roth 1995; Segal 1988; Tversky et al. 1988, 1990). More specifically, when people make choices they focus on the probability and when they assign values they look at the size of the outcome. Previous experimental studies about LPHL risks investigate either buying insurance decisions or paying for insurance decisions. To our knowledge there is no study that investigates both. According to the most recent experimental study that utilises buying decisions (through asking dichotomous questions) to examine insurance decisions in LPHL situations, the probability of occurrence of the risky event plays a dominant role in valuing insurance (Ganderton et al. 2000). However, another experimental study by McClelland et al. (1993) investigates insurance paying decisions (using open-ended questions), and concluded that some individuals are willing to pay zero while others are willing to pay much higher than the expected loss value. This contradiction necessitates further research. Moreover, our experimental design also allows us to elicit a threshold probability in individuals’ minds and gauge their risk attitudes towards the effects of these insurance decisions. A “threshold probability” is presented as the minimum probability in an individual’s mind for a given amount of loss for which he/she buys insurance. We also determine subjects’ risk attitudes by following the calculation used by McClelland et al. (1993). This enables us to test the consistency of our results with the well-known fourfold patterns of risk attitudes (Mauro and Maffioletti 2004). According to the well-known fourfold patterns of risk attitudes as suggested by Prospect Theory (Kahneman and Tversky 1979), people are risk averse for gains and risk seekers for losses in high probability events, and risk averse for losses and risk seekers for gains in low probability events. Finally, we examine how an individual’s endowment in the experiment, gender, age, and income influences his/her insurance buying and paying decisions. The results of the current study are important for academicians and practitioners in the insurance market in many aspects. It contributes to the literature by answering whether using different methods to elicit individuals’ insurance valuations changes their decisions and affects the dominant consideration of probability of loss and amount of loss. In other words, the study answers the question “do individuals consider different kinds of information when they make choice versus pricing insurance decisions?”. Further, it explores individuals’ threshold probabilities in low-probability and high-loss risk context. The practitioners in the insurance industry may use information about (1) the dominant consideration of probability of loss versus size of loss on consumers’ decisions, (2) the minimum probability of loss in their mind necessary to start considering insuring, (3) their risk attitudes and (4) the role of gender, income and age on decisions for motivating society to mitigate LPHL risks (such as natural disasters).",2
9.0,1.0,Journal of Economic Interaction and Coordination,09 June 2013,https://link.springer.com/article/10.1007/s11403-013-0113-1,Cognitive capacity and cognitive hierarchy: a study based on beauty contest experiments,April 2014,Shu-Heng Chen,Ye-Rong Du,Lee-Xieng Yang,Unknown,,Unknown,Mix,,
9.0,1.0,Journal of Economic Interaction and Coordination,10 July 2013,https://link.springer.com/article/10.1007/s11403-013-0114-0,The effect of rating agencies on herd behaviour,April 2014,Giovanni Ferri,Andrea Morone,,Male,Female,Unknown,Mix,,
9.0,1.0,Journal of Economic Interaction and Coordination,31 August 2013,https://link.springer.com/article/10.1007/s11403-013-0115-z,Common-value auction versus posted-price selling: an agent-based model approach,April 2014,Christopher N. Boyer,B. Wade Brorsen,Tong Zhang,Male,Unknown,,Mix,,
9.0,2.0,Journal of Economic Interaction and Coordination,28 September 2013,https://link.springer.com/article/10.1007/s11403-013-0117-x,Showing or telling? Local interaction and organization of behavior,October 2014,Zakaria Babutsidze,Robin Cowan,,Male,,Unknown,Mix,,
9.0,2.0,Journal of Economic Interaction and Coordination,26 November 2013,https://link.springer.com/article/10.1007/s11403-013-0119-8,Interlocking directorates in Italy: persistent links in network dynamics,October 2014,Lucia Bellenzier,Rosanna Grassi,,Female,Female,Unknown,Female,"Interlocking directorates is defined by Mirzuchi (1996) as the situation where a person affiliated with one organization sits on the board of directors of another organization. The topic raises great interest in scientific literature and various reasons have been put forward in order to explain the interlocks phenomenon (see Mirzuchi 1996 for an extensive review of the effect of interlocks; see also Croci and Grassi 2013; Ferris et al. 2003; Non and Franses 2007; Santella et al. 2009). Part of the literature proposes an approach based on the network theory, since the ties between firms and directors is well represented by a network in a rather natural way (see Battiston and Catanzaro 2004; Caldarelli and Catanzaro 2004; Milaković et al. 2009, 2010; Piccardi et al. 2010) and “interlocks remain a powerful indicator of network ties between firms”, as affirmed by Mirzuchi (1996). Most real complex systems can be modeled by complex networks and network tools are widely employed in various fields. Examples, initially found in sociology and in Social Network Analysis (Wasserman and Faust 1994), are further extended to other areas of applications, such as physics, biology, economy and finance (see Barabasi and Reka 2002; Barabasi and Oltvai 2004; Garlaschelli et al. 2005; Rotundo and D’Arcangelis 2010a, b; Stefani and Torriero 2013). The network tool is useful especially in studying the dynamical evolution of a complex system, therefore the researcher’s interest has recently been addressed to study networks evolving over time, known in literature as temporal networks. Our work fits in with this framework. In these networks, the time dimension explicitly appears in their representation. The complex system may be represented through a contact sequence, or through the so called interval graphs, i.e. graphs in which links are present over a set of time intervals rather than in a set of time. Other representations of temporal networks take into account the time dimension by aggregating in a unique (weighted) edge the links (between two vertices) which are present in different sets of time. This corresponds to projecting the time dimension on a single static network (see Carley 2003). Often, data is segmented into adjacent time windows, where links are aggregated into edges and the evolution of the network structure is studied within these windows. On the one hand, this allows to simplify the pattern, reducing the analysis to a sequence of static networks. On the other hand, this methodology is often not the best way to represent the time dimension, because projecting a dynamic network into a static structure may cause a loss of information (see Holme and Saramaki 2012). However, all of these representations are commonly used in the study of network evolution. In this work we intend to study the temporal evolution of the network of the interlocking directorates with reference to the Italian case. Several empirical studies on various countries have shown that this kind of network is characterized by a topological structure that tends to persist over time, nonetheless maintaining its structural properties. The Italian case falls within this framework: empirical studies on Italy, even those that refer to recent years, have observed the topological structure of type small world . In our analysis we pay particular attention to the long-term stability. We do not only establish whether the governance of the Italian companies is based on a stable structure over time, but we also want to understand where the links between companies originate from, thoroughly investigating the role of the directors in this networks. We examine an extensive database to find out how the structure evolves over time: in particular, we are interested in determining the role played by directors in the network structure. The question is relevant, indeed interlocking directorships have been an important characteristic of Italian capitalism for a long time (Rinaldi and Vasta 2005, 2008, 2009) and they still prevail, even after the recent improvements in corporate governance. Using the same methodology proposed by Milaković et al. (2009), we would like to assess if, as in Germany, Italy is formed by a connected and stable structure, created by the presence of directors with multiple mandates. As a result, we find a very cohesive network structure, due to the presence of a few directors with multiple assignments and, unlike the German case, this structure is stable, but not connected. Also, we investigate the nature of the stable links over time; to this end we propose an alternative approach based on temporal networks. The analysis is performed by quantifying the variation of links in a set time period. Therefore we construct a unique cumulative network, where nodes are companies and the existence of an edge is related with the persistence over time of an interlock between two companies. We think it is of interest to determine whether the ties persisting between firms are due to a stable presence of directors with the same mandate for years, or if the persistence occurs despite a turnover effect. Our methodology allows us to investigate the stability of the link also identifying how much persistent with respect to time this stability is. Looking at the stability in all time period (with 100 % of stability) the result that emerges is a structure with few connected components, each one of them very cohesive. On the one hand this states the fact that Italy has been characterized for many years by a governance structure of ownership, which can not be separated from the shareholding control of a few important family groups; on the other hand the large Italian firms, both in the financial sector and in industry, are connected to each other by sharing their directors. This paper contributes to the mainstream literature on country interlocks providing an analysis of the Italian situation over a wide range of time. Unlike previous studies focused on Italy, where the dynamic aspect is analyzed by following the temporal evolution of global and structural network indicators, in our work we propose a model which captures the variability of every single link over time. This allows us to better quantify the link stability of the interlock that seems to play such an important role in the Italian scenario. The paper is organized as follows: Sect. 2 presents a review of the literature on interlocking directorates using the network approach, focusing on previous studies of the Italian case. Section 3 recalls some basic definitions on graph theory. Section 4 provides a general descriptive analysis of the board network. Section 5 presents the evolution of the interlocking directorates in the Italian case with the same methodology proposed in Milaković et al. (2009). Section 6 presents the theoretical model of network dynamics; in Sect. 7 the model is applied to the board network in order to assess the link stability over time. The conclusions are in Sect. 8.",25
9.0,2.0,Journal of Economic Interaction and Coordination,05 February 2014,https://link.springer.com/article/10.1007/s11403-014-0125-5,Strong random correlations in networks of heterogeneous agents,October 2014,Imre Kondor,István Csabai,Máté Cs. Sándor,Male,Male,Unknown,Male,"The view of the economy as a complex adaptive system pioneered by Föllmer (1974) and developed in Anderson et al. (1988) and Arthur et al. (1997) and a large number of subsequent papers is gaining increasing significance in economics. An important approach to such a complex adaptive system is offered by agent based modelling (ABM) (Tesfatsion 2003). In its ambition to overcome the unrealistic simplifications of general equilibrium theories, it tries to model the behaviour of the individual agents by taking into account their heterogeneity, and to generate the phenomena observed on the macro level as the manifestations of the collective behaviour of these heterogeneous interacting agents. As a consequence of the wide variety of the characteristics of the agents and their interactions, ABM’s are necessarily very high dimensional: they depend on a large number of variables. The calibration and validation of these models is therefore a very difficult endeavour, which is the source of frequent, and justified criticism, see e.g. Windrum et al. (2007). A closely related issue is the high degree of instability of these models: very small modifications of the details may lead to unforeseeable changes in the overall behaviour. The high dimensionality of agent based models may be seen either as a damning fault or a virtue, depending on whether one believes that knowledge about society or the economy should be possible to be compressed into a few postulates, or is willing to accept the irreducible complexity of economic and social activity. Agents are often represented as the nodes of a complex network or graph, and their interactions as the links of this network. In a typical setup, the agents have idiosyncratic goals and specific strategies to reach them, and they are also invested with the capability of learning and adaptation. This is a great virtue of the ABM approach, but further aggravates the calibration and the instability problem. In this paper we are going to consider a hugely simplified version of such a model: binary agents having a simple yes–no alternative in front of them, and linked to each other by fixed interactions, symmetric in the interacting pair. So our agents do not learn, but try (according to a probabilistic rule) to react to the influence of their partners, and possibly also to that of an external source. This model retains an important aspect of ABM’s in that the interactions between the agents are supposed to be heterogeneous. However, for the sake of numerical simplicity, we assume that even this heterogeneity consists merely in some of the interactions being positive (friends or collaborators), some negative (enemies or competitors), but the absolute values are the same. (A richer realization of the interactions, such as drawing them from a continuous probability distribution, could be easily accommodated). This model is the same as the standard model used to describe a class of disordered magnets called spin glasses (Mezard et al. 1987). The way we use the model will however be rather different from the approach of statistical physics; the differences will be explained later. Social interaction models inspired by statistical physics, sometimes especially spin glasses, have been proposed in the context of various socioeconomic problems for quite some time now (Durlauf 1996; Brock and Durlauf 2001). Durlauf (1999) writes: ...  this approach introduces an explicit sociological perspective on individual behavior. By moving away from market-mediated interrelationships to direct interdependencies in behavior, this new approach to analysis represents an “interactions-based” approach to socioeconomic behavior in which individual decisions are explicitly understood as determined by one’s social context. A large class of these interactive decisions are binary: Staying in or dropping out of school, the decision to have an out-of-wedlock birth, the use of drugs, and entry into illegal activity all have this characteristic. Sequential voting in a two-party election system, e.g. Barucci and Tolotti (2012), can also be added to the above examples. In a different context Krugman (1994) argues that in order to model complex landscapes in economic geography, a useful metaphor is provided by spin-glass models. Incidentally, he notes: “The residential segregation model introduced [\(\ldots \)] by Schelling (1978) bears a strong resemblance to simple spin-glass models”. Spin glasses appear also in the context of rational decision making (Galluccio et al. 1998), portfolio selection under nonlinear constraint (Gábor and Kondor 1999), optimization of capital charge under international financial regulation (Kondor 2000), coalition formation and fragmentation (Galam 2008), in the dynamics of social networks (Shafee 2005), or the description of financial markets (Rosenow et al. 2002; Bury 2013). Our goal in this paper is not to describe another application of spin glass theory to a concrete social or economic problem, but to display the remarkable richness of behaviour this family of models can exhibit, including the sensitivity to fine details and emergence of long range correlations that make the system “more than the sum of its parts”. Given the socioeconomic context of this work, we deviate from the standard statistical physics approach in some important respects. We do not seek to reach very large system sizes because the effects we wish to exhibit can be demonstrated already on moderate sized systems. Furthermore, most of the applications we have in mind are concerned with systems that would qualify at most as mesoscopic, rather than macroscopic from the point of view of physics. Except for the smallest sizes, we do not necessarily attempt to equilibrate our samples either. The reason is that heterogeneous agent models relax extremely slowly after a shock, and the next shock may well arrive before they could settle down into equilibrium. A recent paper by Gualdi et al. (2013) gives an excellent demonstration of this type of behaviour—in perfect agreement with what we perceive as the true dynamics of markets. To consider the limit of infinitely large systems is well justified in physics that is concerned with macroscopic systems consisting of, say, \(10^{24}\) particles. In the limit of large particle numbers the macroscopic thermodynamic quantities self-average, i.e. become independent of the realization of the interaction matrix. This allows one to average over the random samples (quenched averaging), which simplifies the treatment tremendously. It must be stressed, however, that not all quantities become insensitive to the details of the interactions: the spin-glass overlap and two-spin correlations, for example, show random variations from sample to sample (Mezard et al. 1987; Pastur and Shcherbina 1991). [The importance of the lack of self-averaging has been analysed in the context of the relationship between micro- and macroeconomics by Aoki and Hawkins (2010), Aoki and Yoshikawa (2012) and Garibaldi and Viarengo (2012)]. One may feel that such sample dependent, randomly varying quantities carry no meaningful information. We wish to show, however, that some typical features do emerge, and they must be explored if we wish to associate these models with some real world systems, such as a group of human actors, or a network of institutions. One of the typical features we observe is that a large cluster of strongly correlated agents is formed for almost any realization of the interaction matrix. We believe that strong correlations are a fundamental feature of complex systems, but we are not aware that this would have been emphasized in the context of agent based models. From the point of view of physics, there is nothing mysterious about the emergence of strong correlations: they are the direct consequence of the interaction between the agents, and the manifestations at the mesoscopic level of what would become a genuine phase transition with its associated long range order in a macroscopic system. We argue that the high sensitivity to small details of the models studied in this paper is a consequence of the presence of the large correlated clusters. This sensitivity means these systems are effectively irreducible: they depend on so many details that a reduced description in terms of a small number of variables is impossible. [Parisi (1999) once gave a deceptively innocent looking definition of a complex system as one whose behavior crucially depends on small details]. Another aspect of the same property is the nonlocal character of these systems. With the large strongly correlated clusters spanning the whole system, a local disturbance at one point may have an effect which extends to the whole system. A further typical feature is that these systems exhibit a complicated attractor structure: depending on the actual arrangement of the interactions between the agents, the system’s evolution may be arrested in many more or less deep “valleys” (basins of attraction) in which the system will be trapped for long time periods before it escapes via lower or higher passes, only to be trapped in the next valley. This induces an extremely slow, punctuated equilibrium-like dynamics which is quite reminiscent of the quasi-equilibria separated by regime changes in biosystems or societies or markets. The existence of many attractors makes the system’s dynamics dependent on initial conditions: trajectories started at different microscopic configurations may end up in different basins of attraction, and even trajectories which are initially very close may evolve very far from each other. In other words, we have strong path dependence in these models. Moreover, even the landscape itself may be strongly rearranged in response to small changes in the interaction structure, in the control parameters or in the boundary conditions. This leads us to the problem of equilibration. Heterogeneous, glassy physical systems may take an extremely long time to equilibrate. As a matter of fact, laboratory spin glasses never reach true thermodynamic equilibrium, i.e. a state in which no macroscopic quantity would change any more, and all the previous history would be forgotten.Footnote 1
 The numerical simulation of sufficiently large spin glass models in physics is a challenging task that requires tremendous numerical effort and programming ingenuity. The context of the present work is, however, entirely different: the social, economic or finance systems for which our schematic binary agent model can be considered at least as a metaphor are even less likely to come into equilibrium than their spin glass counterparts, therefore we do not need try to reach ideal equilibrium in our simulations. Sometimes we will consider, mainly for the sake of illustration, small enough systems that allow us to scan the whole of phase space (i.e. the complete set of microscopic configurations) and compare the results thus obtained with the results extracted from Monte Carlo simulations. Generally, however, we will adopt the position that our Monte Carlo simulations are a kind of history of the artificial society of interacting agents, and content ourselves with following this history for a sufficiently long time to draw some conclusions. The slow dynamics and the existence of quasi-equilibrium states allow us to measure various quantities by averaging over long, but finite periods of time. In the context of statistical physics, going to the limit of very large system sizes and very long time spans is not only natural, but also a powerful means of reduction: many small details lose their significance and become forgotten along the way. Accordingly, the results will be stable and smooth. In renouncing the thermodynamic limit and equilibrium as powerful tools of simplification, we are evidently sacrificing some reproducibility (results will depend on the sample, on initial and boundary conditions, etc.), also some of the beauty of smooth results, and may also run into difficulties of interpretation. As an example of the latter, we mention that whereas in a large, equilibrium system the distinction between a stable structure and fluctuations about it is clear-cut, in finite size, out of equilibrium systems it is blurred. On the other hand, our “mesoscopic” approach also has some benefits: we are relieved of the impossible task of simulating huge systems for infinitely long times. We know from statistical physics that the nature, in fact even the existence, of collective phenomena and ordering depend on the underlying topology. In this paper we are going to consider two kinds of underlying geometries: the complete graph and 2-dimensional regular lattices. Agents occupying the nodes of a complete graph directly interact with every one of their peers: this could correspond to a close-knit group. In contrast, agents occupying the lattice sites on, say, a square lattice form something like a sparsely populated flat country: agents have only a small number (4) of partners with whom they are directly linked. In a sense, these cases represent two extremes: on the complete graph the distance between any pair of agents is 1, everybody is nearest neighbour to everybody else. In contrast, on a 2-dimensional regular lattice of \(N\) lattice sites the average distance between two randomly chosen agents is of the order of \(N^{\frac{1}{2}}\), i.e. it grows rather fast with system size. More realistic multi agent models would be implemented on some kind of complex random graphs. The structure of these falls somewhere in between the above two cases: complex random graphs tend to display the small world property (Watts and Strogatz 1998), with the average distance between agents growing as the logarithm of the system size. It is rather natural to expect the direct coupling between each pair of partners on the complete graph to induce strong correlations between them, and we will indeed find that large, strongly correlated clusters readily emerge. It is less obvious that the same phenomenon should arise in low dimensional lattices, but we will see that extended correlated clusters, spanning the whole sample build up in low dimensional lattices too. A similar study of correlations on complex random graphs poses no difficulty in principle, but would introduce, in addition to the random distribution of the interactions, further elements of complication, related to the random network on which the agents would be placed. Therefore we decided to postpone the analysis of correlations on random graphs to a later work, but there is no doubt in our minds that these graphs, falling between the two extremal structures of low dimensional lattices and the complete graph, will also be found to yield extended correlations. The same is true also for extending the model in other directions, such as allowing a richer structure for the interactions between agents and/or for the set of choices available for them. To summarize, our approach consists in studying the behaviour of not necessarily large [sizes going from \(\mathcal {O}(1)\) through \(\mathcal {O}(100)\), in a few exceptional cases up to \(\mathcal {O}(10^4)\)] schematic (spin glass-like) heterogeneous agent systems on various geometries, for times that are not necessarily long enough to reach equilibrium, and looking for regularities, typical features in the idiosyncratic and casual behaviour of these miniature societies of binary agents. The paper is organized as follows. In Sect. 2 we specify the model and lay down the rules of its dynamics. In Sect. 3 we show the results of some long Monte Carlo simulations that follow the evolution of a relatively large, \(N=10^4\), sample and display the type of slow dynamics where the results depend on the length of observation time. In Sect. 4 we display a few examples of the cost function landscape and the network of low lying (low cost) states (very different from, but depending on, the network of interactions) on which the dynamics unfolds. Section 5 gives a description of the correlations emerging between the agents on the complete graph in the absence resp. presence of an external field, while Sect. 6 displays correlation results in 2-dimensional regular lattices. Section 7 demonstrates the role of boundary conditions. Section 8 is a summary of the results.",7
9.0,2.0,Journal of Economic Interaction and Coordination,17 May 2014,https://link.springer.com/article/10.1007/s11403-014-0133-5,Using difference equations to find optimal tax structures on the SugarScape,October 2014,Matthew Oremland,Reinhard Laubenbacher,,Male,Male,Unknown,Male,"In many agent-based models (ABMs), even the simplest local interactions can give rise to complicated and unpredicted global dynamics, indicating both the power of agent-based modeling and the care that must be taken in any attempted analysis. A mathematical description of an ABM has particular appeal because it brings the desired rigor to the form of the model, and allows access to a plethora of theory and tools. For many ABMs, questions amenable to investigation arise naturally: for example, what is the best investment strategy in order to maximize profit? How should resources be allocated in order to maximize customer satisfaction? What is the ideal tax policy for a given scenario? We refer to these as optimization problems, because they are focused on determining the optimal way to achieve a particular goal. There are methods for solving optimization problems for ABMs directly—this area of study is referred to as simulation optimization. However, the approximation of ABM dynamics by mathematical equations is another method for solving optimization problems, and one which brings to light another advantage: rather than relying on results from simulation—which can be computationally expensive and unreliable—the equation systems are rigorous and can be evaluated very quickly. Additionally, if the equations are analytical rather than phenomenological, the solving of optimization problems can offer additional insight into model dynamics, as results can be interpreted in terms of the model parameters. To illustrate this technique, we introduce a simplified version of a well-known ABM, pose an optimization problem focused on the determination of optimal tax structures, and develop analytical equations to capture pertinent model dynamics. Finally, we use the equations to solve the optimization problem and then verify that the solutions faithfully translate back to the ABM. The results indicate the power of this approach, as well as informing directions for future research. Many studies have examined similarities and differences between ABMs and equation-based models (EBMs). In their user’s guide, Parunak et al. (1998) contrast ordinary differential equation (ODE) models with ABMs, concluding that ABMs are more appropriate in models with highly local features. Fahse et al. (1998) use an ABM of birds in an attempt to extract a population growth rate from ABM data; a similar goal is examined in a model by Duboz et al. (2003). A spatially heterogeneous ABM is used to derive a series of EBMs in a study by Picard and Franc (2001); here, local spatial effects are shown to be critically important. Mean-field aggregations of ABM dynamics are shown to work well by Edwards et al. (2003), while Ovaskainen and Cornell (2006) emphasize the importance of local densities in such models. Huet et al. (2007) examine the issue of local neighborhoods in the development of mean-field approximations. A broader study on the use of ABMs versus EBMs discusses circumstances for which a particular model type is preferable (Rahmandad and Sterman 2008). Cecconi et al. (2010) use a model from game theory to generate empirical data in order to fit an EBM to that data; they emphasize the complementary nature of the two modeling paradigms. A formal methodology for translation of ABMs to polynomial dynamical systems (PDS) over a finite field is presented by Hinkelmann et al. (2011); this technique demonstrates that in theory, every ABM can be represented as a PDS. In addition to general studies on agent- and equation-based modeling, simulation optimization techniques have been investigated in order to solve problems similar to those posited in Sect. 1. Okell et al. (2008) examine the effects of therapy and treatment in an ABM of malaria transmission, while Kasaie et al. (2010) focus on optimal resource allocation. Strategies for mitigating influenza outbreaks are the focus of studies by Mao (2011) and Yang et al. (2011). A textbook chapter by Laubenbacher et al. (2013) examines optimal control theory for ABMs, offering examples of simulation optimization strategies. In this study, a system of discrete difference equations is developed analytically from ABM rules; the equations are subsequently used to solve an optimization problem. While optimal control theory exists for the mathematical system, it is often limited to cases of very few equations or a very short time horizon. Ding et al. (2007) describe such theory for a model containing several equations, but simulations are limited to fewer than ten time steps. Lenhart and Workman (2007) dedicate a textbook chapter to the topic but also focus on small systems. An infinite-time discrete optimal control problem is investigated by Hayek (2011); it is not clear if results are generalizable to finite-time systems. Existing optimal control theory is intractable for models containing many equations over a long (but finite) time horizon, and translation of spatially heterogeneous ABMs often results in precisely these types of systems. Hence, heuristic methods are employed in order to solve optimization problems. Heuristic methods have been used in conjunction with ABMs to solve optimization problems related to cancer vaccination (Lollini et al. 2006; Pennisi et al. 2008) and HIV treatment (Castiglione et al. 2007), for example. Different heuristic methods for determining general strategies are contrasted by Pappalardo et al. (2010). Hence, there is precedent for the use of such methods in solving the same optimization problem in an equation-based model.",4
9.0,2.0,Journal of Economic Interaction and Coordination,25 May 2014,https://link.springer.com/article/10.1007/s11403-014-0134-4,Animal spirits and unemployment: a disequilibrium analysis,October 2014,Robert Jump,,,Male,Unknown,Unknown,Male,"The past decade has seen a number of advances in modelling disequilibrium dynamics, which can be divided into four main groups. The first builds on the Neo-Keynesian approach, with notable contributions including Bignami et al. (2003), Raberto et al. (2006), and van der Hoog (2008), and original work including Benassy (1976), Lorie (1978), and Picard (1983). The second is based on a more Classical framework, particularly Goodwin’s early formalisation of Marxian dynamics, with notable contributions including Franke et al. (2006), Velupillai (2006), and Charpe et al. (2014). The third group, agent based modelling, might better be described as modelling non-equilibrium dynamics, with influential contributions including Wright (2005), Gintis (2007), Dawid et al. (2014), Dosi et al. (2008), Dosi et al. (2010), and Lengnick (2013). The final group removes the ex ante coordination assumption from New Keynesian DSGE models, with notable contributions in Branch and McGough (2010), and Lengnick and Wohltmann (2013). A particularly interesting example of the latter is De Grauwe (2011), which reintroduces the notion of “animal spirits” into formal Keynesian models. The model adds heterogeneous predictor selection into an otherwise standard New Keynesian 3-equation model, and “animal spirits”, precisely defined as the proportion of agents over-estimating output and/or inflation in any period, are identified as an endogenous property correlated with business cycles. This paper presents results concerning the formal relevance of “animal spirits”, with a model that draws from the first three approaches to disequilibrium dynamics. Specifically, the model is a non-tâtonnement analogue of the cross-dual variant of the traditional tâtonnement process for studying the stability of production economies. The traditional tâtonnement process, formalised in Samuelson (1941), can be conceptualised as a process of negotiation between a large number of agents and a central auctioneer. At the beginning of a single period of the process, the auctioneer announces the price vector at which commodities are to be traded. Each agent then communicates the amount of each commodity that it would like to trade at that price vector, which defines the extent of excess demand in each market. The auctioneer then adjusts prices in the direction of excess demand going forward, and the process can repeat itself. With constant returns to scale technology, however, there is not a unique level of output associated with each price vector, as firms have supply correspondences rather than supply functions. In this case, the traditional tâtonnement process is inapplicable, and the cross-dual variant must be used.Footnote 1 Assume a constant returns to scale technology in which labour is the only input, such that a unit cost function \(c(w)\) exists, and a labour demand function \(l(x)\) exists. Finally, suppose that there exists a level of employment \(L\) consistent with full employment. A simple cross-dual adjustment process is then as follows, where \(\alpha ,\beta \), and \(\gamma \) are speed of adjustment parameters: In the system described by Eqs. 1–3, output is adjusted in the direction of excess profitability, and the wage and price levels are adjusted according to excess demand in their respective markets. Importantly, the adjustment equations are at an aggregate level, specified in terms of representative agents (or agent types). Competitive markets are assumed, such that the number of households and firms is large, and the steady state is characterised by zero profits. Although this may seem like an overly restrictive assumption, specifying the disequilibrium behaviour of monopolistically competitive markets is an extremely demanding task. The main difficulty here is the need for monopolistically competitive firms to know the demand curve they face: either this is known with certainty by each firm, which arguably renders the situation one of equilibrium rather than disequilibrium, or each firm faces the difficult problem of learning their demand curve. A good discussion of these issues can be found in Drazen (1980).Footnote 2
 An interesting aspect of the system described by Eqs. 1–3 is that both \(\partial \dot{x} / \partial x = 0\) and \(\partial \dot{w} /\partial w = 0\), such that the state variable that ensures the stability of the Walrasian equilibrium is the commodity price \(p\). However, as the system is a tâtonnement system, the rationing implicit along the traverse does not affect the adjustment process. To see this, note that the demand for labour in Eq. 2 may be higher than the full employment level, and that the real wage in Eq. 3 may be higher than output. In these states, firms demanding labour and households demanding commodities will be rationed on their respective markets. As implied by the auctioneer narrative, a tâtonnement system is one in which the adjustment process takes place prior to trade: agents have the ability to “recontract”, such that contracts are only fulfilled when the general equilibrium price and output vectors have been found. In a non-tâtonnement process, production and trade at non-equilibrium prices are permitted, and the associated rationing affects agents’ decisions. The stability properties of a partial equilibrium cross-dual process have been studied in Veendorp (1972), and the stability properties of a highly stylised general equilibrium process in Lorie (1978). In the latter, firms are aware of the Walrasian equilibrium ex ante, and adjust their output plans in that direction. In the model presented here, by contrast, the information set of firms is restricted to price signals and realised trades, such that commodity prices remain the stabilising state variables, as above. The approach follows that of van der Hoog (2008), which considers a non-tâtonnement process in the context of an exchange economy. In this approach, markets must open and close sequentially, such that a sequence of events that can be described as a “trading process” forms the basis of the dynamics. In this sense, the approach is very similar to that of the agent based models referred to above, which invariably rely on a sequence of events occurring in each period (which may or may not be stochastic). In the latter approach, however, there is not usually an equilibrium that can be identified prior to the trading process. In the van der Hoog (2008) paper, as with this paper, the Walrasian equilibrium to which the trading process may or may not converge can be identified analytically via the economy’s fundamental characteristics (preferences and technology). The main result of the paper can be considered as a computational demonstration of an idea associated with the original Neo-Keynesian research program. Franklin Fisher, in his canonical treatment of disequilibrium processes, suggested that the theory of unemployment equilibrium might require a theory of trading process stability (Fisher 1983: 9). His argument follows Leijonhufvud, thus: 
\(\ldots \) pure price adjustment behaviour in the short-run is implied only by models which postulate some version of the “recontract” mechanism. If this very restrictive assumption is relinquished, the generation of the information needed to coordinate economic activities in large systems where decision making is decentralized is seen to take time and involve economic costs \(\ldots \) No other assumption, we argue, need be relinquished in order to get from the Classical to Keynes’s Theory of Markets. (Leijonhufvud 1968: 38) As the “recontract” mechanism underlies the tâtonnement assumption of no trade at non-equilibrium prices, and if “Keynes’s Theory of Markets” is interpreted as the theory of unemployment equilibrium, then the above quote amounts to an argument that it is rationing and the associated quantity signals which underlie the theory of unemployment equilibrium. In the model presented in this paper, a parameter associated with the “animal spirits” of firms governs whether or not the trading process converges on the Walrasian, full employment equilibrium. Particularly, the extent of transitory unemployment following a shock decreases in this parameter, and in situations of extremely depressed “animal spirits”, the Walrasian equilibrium is not stable at all. Instead, the economy converges on an unemployment equilibrium. First, the fundamental structure of the economy is described, and the trading process is described at a high level. The market structure of the process, agent behaviour, and price adjustment mechanisms are then described in more detail. Section 3 presents the results, and Sect. 4 concludes the paper.",
9.0,2.0,Journal of Economic Interaction and Coordination,17 July 2014,https://link.springer.com/article/10.1007/s11403-014-0137-1,A multi-agent model of a low income economy: simulating the distributional effects of natural disasters,October 2014,Ali Asjad Naqvi,Miriam Rehm,,Male,Female,Unknown,Mix,,
10.0,1.0,Journal of Economic Interaction and Coordination,17 March 2015,https://link.springer.com/article/10.1007/s11403-015-0153-9,Special Issue in Honor of Masanao Aoki,April 2015,Yuji Aruka,Mauro Gallegati,Hiroshi Yoshikawa,Male,Male,Male,Male,"Masanao Aoki (1931, Hiroshima; 1953 BA and 1955 MSc in Physics, the University of Tokyo; 1960 PhD in Engineering, UCLA; Professor of Engineering, 1960–1974 UCLA and Berkley; Professor of Economics, 1974–2002 Illinois and UCLA) is an outstanding scholar in several fields of Economics. He is very special to the JEIC community because he is one of the founder—fathers of the ESHIA and a pioneer of the HIA approach. His contributions to Economics could be classified into the following five topics: The dual and adaptive theory of control and the applications in dynamic programming, where the idea of stochastic approximation is applied to economic modeling (On Sufficient Conditions for Optimal Stabilization Policies, Review of Economic Studies, vol. 40(1), 1973; Approximation scheme for evaluating some terminal capital stock, Journal of Economic Theory, vol. 6(3), 1973). The control theory and parameters estimation of large-scaled and decentralized system, where he successfully introduces the controllability and the related ideas in the system theory into the macroeconomic policy (On a Generalization of Tinbergen’s Condition in the Theory of Policy to Dynamic Models, Review of Economic Studies, vol. 42(2), 1975; Local Controllability of a Decentralized Economic System, Review of Economic Studies, vol. 41(1), 1974; Non-interacting control of macroeconomic variables: Implications on policy mix considerations, Journal of Econometrics, vol. 2(3), 1974); The applications of control theory and system theory to the economic system as well as the neoclassical economic models (Optimal control and system theory in dynamic economic analysis, North-Holland, 1976; Dynamic analysis of open economies, Academic Press, 1982). The development of a new algorithm for the time series model and the applications of them to the economic data where the models were also featured by Aoki’s method on aggregation in a dynamic system, which virtually happened to be the same method “co-integration” developed by C. Granger (Note on economic time series analysis: system theoretic perspective, Springer Verlag, 1982; State space modeling of time series, Springer Verlag, 1997; State space modeling of time series, Springer Verlag, 2002). At this stage he argues that Instrumental Variable Method is not only precisely argued in the context of stochastic realization due to Lindquint and Picci but also is developed completely as a software package. This method can be applied to the multivariable time series. These works were highly appreciated and obtained a good popularity. He thus had invited lectures in the American Statistical Association and International Forecasting Society. Econometric Review also arranged the special issue on this subject in 1991. Amid an abundance of ARMA, the models of innovation could be estimated by the state space modeling. This algorithm was commercialized also as a software package for sales. The 5th stage is the most exciting stage to construct a new perspective for economic science in line with the Society for Economic Science for Heterogeneous Interacting Agents he co-founded in 2006. This is represented by the three books: New approaches to macroeconomic modeling: evolutionary stochastic dynamics, multiple equilibria, and externalities as field effects. Cambridge University Press, New York, 1996; Modeling aggregate behavior and fluctuations in economics: stochastic views of interacting agents, Cambridge University Press, New York, 2002; (with H. Yoshikawa) Reconstructing macroeconomics: a perspective from statistical physics and combinatorial stochastic processes, Cambridge University Press, Cambridge, New York, 2006. The new method is featured by Statistical Physics and Combinatorial Stochastic Processes. Equilibria is not treated as a fixed points. The system may be subject to non-self averaging and mutant could emerge internally. These studies can rightly establish a set of theoretical foundations to socio-econophysics. Masanao proposes a new, alternative approach to economics, based on the statistical physics. For the mainstream economist this comes as an eresia. He still believes in determinism, equilibrium and reductionism (as Newton did). Equilibrium in particular is considered an aberration for a statistical physicist. The macro behavior of a system does not reflect those of its constituents. In economics it means that a, e.g., a sector can be in equilibrium even if some firm overproduces the good and other underproduces it. So there can be disequilibrium at the individual level, but equilibrium at the aggregate one. Equilibrium thus becomes a statistically distributed concept. Aoki suggests this can be studied in Economics with the Master Equation approach, which, at the same time, overcomes the Representative Agent approach and provides a proper tool for analyzing a system with many heterogeneous interacting agents. It is worth noting that the modern micro-founded macroeconomic theory actually emphasizes heterogeneity. However, the Lucas rational expectations theory, the Mortensen search model, and DSGE (Dynamic Stochastic General Equilibrium) models all presume one distribution of variable of interest common to all micro agents in model. As such, they effectively presume the representative agent whose optimization is analyzed in detail. Heterogeneity in standard models is not “true” heterogeneity. Masanao’s breakthrough works have opened the door to analyzing true heterogeneity in economic models. This issue of the JEIC is dedicated and honoring Masanao. Some of the contributors are personal friends to him, most of them are his followers, all of them love him. The list of papers can be divided into 3 categories. Macroeconomics Sorin Solomon, Natasa Golo: Microeconomic Structure determines Macroeconomic Dynamics; Aoki defeats the Representative Agent Hiroshi Yoshikawa: Stochastic Macro-equilibrium: A Microfoundation for the Keynesian Economics Hideaki Aoyama, Hiroshi Iyetomi, Hiroshi Yoshikawa: Equilibrium Distribution of Labor Productivity: A Theoretical Model Yoshi Fujiwara, Hideaki Aoyama, Mauro Gallegati: Micro-Macro Relation of Production: Double Scaling Law for Statistical Physics of Economy Enrico Scalas, Tijana Radivojevic’, Ubaldo Garibaldi: Wealth distribution and the Lorenz curve: A finitary approach Agent-based modeling Simone Landini, Mauro Gallegati, Joseph Stiglitz: Economies with Heterogeneous Interacting Learning Agents Daniel Fricke, Thomas Lux: The Effects of a Financial Transaction Tax in an Artificial Financial Market Macroeconomic policy Raymond Hawkins: Okun’s Law and Anelastic Relaxation in Economics Willi Semmler, Christian R. Proano, Matthieu Charpe, Peter Flaschel, Hans-Martin Krolzig, Daniele Tavani: Credit-Driven Investment, Heterogeneous Labor Markets and Macroeconomic Dynamics Raymond Hawkins, Jeffrey Speakes, Daniel Hamilton: Monetary Policy and PID Control All the papers aim to move small steps toward a new economic theory by following Masanao’s approach. Readers will judge if they reached this goal. Now, let’s enjoy Masanao’s pathbreaking ideas.",1
10.0,1.0,Journal of Economic Interaction and Coordination,14 June 2014,https://link.springer.com/article/10.1007/s11403-014-0135-3,Microeconomic structure determines macroeconomic dynamics: Aoki defeats the representative agent,April 2015,Sorin Solomon,Nataša Golo,,Male,Female,Unknown,Mix,,
10.0,1.0,Journal of Economic Interaction and Coordination,25 September 2014,https://link.springer.com/article/10.1007/s11403-014-0142-4,Stochastic macro-equilibrium: a microfoundation for the Keynesian economics,April 2015,Hiroshi Yoshikawa,,,Male,Unknown,Unknown,Male,"The purpose of this paper is to present a new concept of stochastic macro-equilibrium which provides a micro-foundation for the Keynesian theory of effective demand. Cyclical changes of aggregate economic activity, namely quarter to quarter or year to year changes of real GDP are basically determined by changes of aggregate demand. This is the central message of Keynes (1936). Keynes argued that real demand rather than factor endowment and technology determines the level of aggregate production in the short-run because the rate of utilization of production factors such as labor and capital endogenously changes responding to changes in real demand. Keynes maintained that this proposition holds true regardless of flexibility of prices and wages; he, in fact, argued that a fall of prices or wages would aggravate, not alleviate the problems facing the economy in deep recession because it may lower aggregate demand. Following Tobin (1993), let us call this proposition the Old Keynesian view. The challenge is to clarify the market mechanism by which aggregate demand conditions the allocation of production factors in such a way that total output follows changes in real aggregate demand. A decrease of aggregate output is necessarily accompanied by lower utilization of production factors, and vice versa. Since the days of Keynes, economists have taken unemployment as the most important sign of possible under-utilization of labor. However, unemployment is by definition job search, a kind of economic activity of worker, and as such calls for explanation. Besides, unemployment is only a partial indicator of under-utilization of labor in the macroeconomy. The celebrated Okun’s law which relates the unemployment rate to the growth rate of real GDP demonstrates the significance of under-utilization of employed labor other than unemployment.Footnote 1 In this paper, we consider not only unemployment but also on productivity dispersion in the economy. To consider Keynes’ principle of effective demand, we must obviously depart from the Walrasian general equilibrium. The most successful example of “non-Walraian economics” which analyzes labor market in depth is equilibrium search theory surveyed by its pioneers Rogerson et al. (2005), Diamond (2011), Mortensen (2011), and Pissarides (2011). The standard general equilibrium abstracts itself altogether from the search and matching costs which are always present in the actual markets. By explicitly exploring search frictions, search theory has succeeded in shedding much light on the workings of labor market; see also Tobin (1972) for macroeconomics of labor market. While acknowledging the achievement of equilibrium search theory, we find several fundamental problems with the standard theory. In particular, the theory fails to provide a useful framework for explaining cyclical changes in effective utilization of labor in the macroeconomy. Section 2 points out limitations of standard search theory. After brief explanation of the concept of equilibrium based on statistical physics in Sects. 3, 4 presents a model of stochastic macro-equilibrium. The model explains how the distribution of productivity is determined together with unemployment. Section 5 then explains that the stochastic macro-equilibrium provides a micro-foundation for Keynes’ principle of effective demand. It also presents a suggestive evidence supporting the model. The final section offers brief concluding remarks.",5
10.0,1.0,Journal of Economic Interaction and Coordination,07 November 2013,https://link.springer.com/article/10.1007/s11403-013-0118-9,Equilibrium distribution of labor productivity: a theoretical model,April 2015,Hideaki Aoyama,Hiroshi Iyetomi,Hiroshi Yoshikawa,Male,Male,Male,Male,"The concept of equilibrium plays a central role in economics. Of all, the most influential is the Walrasian general equilibrium as represented by Arrow and Debreu (1954). Though it is a grand concept, and well established in the profession, it cannot be more different from the real economy. The Walrasian theory specifies preferences and technologies of all the consumers and firms, and defines the equilibrium in which micro-behaviors of all the economic agents are precisely determined. This Walrasian definition of equilibrium is analogous to analyzing gas, for example, comprised of many particles, by determining the equations of motion for all the particles. Physicists know that this approach though it may look reasonable at first sight, is actually infeasible and on the wrong track. To better understand the behavior of heterogeneous agents in economics, a suitable approach is offered by statistical physics. Curiously, despite of the fact that the macroeconomy consists of many heterogeneous consumers and firms, the basic method of statistical physics has had almost no impact on economics. The notable exception is a series of works by Aoki (1996, 2002) and Aoki and Yoshikawa (2007). In the Walrasian equilibrium, the marginal productivity of production factors such as labor and capital are equal in all the firms, industries, and sectors. Otherwise, there exists inefficiency in the economy, and it contradicts the notion of equilibrium. However, in the real economy, we actually observe significant productivity dispersion. That is, there is a distribution rather than a unique level of productivity. Search theory has attempted to explain such distribution by considering frictions and search costs which exist in the real economy (Diamond 2011; Mortensen 2011; Pissarides 2011). However, it is still based on representative agent assumptions in the sense that all the workers and firms make their decisions based on the common job arrival rate, job separation rate, and wage distribution (Yoshikawa 2011). To explain equilibrium distribution, the most natural and promising approach is to eschew pursuit of precise micro-behavior on representative agent assumptions, and resort to the method of statistical physics. Foley (1994) is a seminal work which applies such statistical method to the general equilibrium model. Yoshikawa (2003) argues that the study of productivity dispersion provides correct micro-foundations for Keynesian economics, and that to explain distribution of productivity we should apply the method of statistical physics. In a series of papers, we have attempted to establish the empirical distribution using a large data set covering more than a million firms in the Japanese manufacturing and non-manufacturing industries (Fujiwara et al. 2009; Souma et al. 2009; Aoyama et al. 2010a, b). To explain this empirically observed distribution of productivity, Iyetomi (2012) introduced the notion of negative temperature. Based on this notion of negative temperature, Yoshikawa (2011) also made a similar attempt with the help of grandcanonical partition function. In this paper, we explore the problem from a different angle than the standard entropy maximization. Before doing so, we first update our empirical investigation of distribution of labor productivity in Sect. 2. Most of theoretical works exploring distribution of productivity resort to the straight-forward entropy maximization. Instead, Garibaldi and Scalas (2010) suggest that we study the same problem using the Ehrenfest–Brillouin model, a Markov chain which describes random creations and destructions in system comprising many elements moving across a finite number of categories. Following their lead, we present such a model in Sect. 3. By considering detailed balance, we derive the stationary distribution of the model which explains the empirically observed distribution. Section 4 offers brief concluding remarks.",5
10.0,1.0,Journal of Economic Interaction and Coordination,30 January 2014,https://link.springer.com/article/10.1007/s11403-014-0124-6,Micro-macro relation of production: double scaling law for statistical physics of economy,April 2015,Hideaki Aoyama,Yoshi Fujiwara,Mauro Gallegati,Male,Male,Male,Male,"Economics is in crisis. Although there exists a mainstream approach Kydland and Prescott (1982); Galí (2008), its internal coherence and ability in explaining empirical evidences are increasingly questioned. The causes of the present state of affairs go back to the mid of the XVIII century, when new figures of social scientist (economists) borrowed the method (mathematics) of the most successful hard science (physics) allowing for the mutation of political economy into economics. It was, and still is, the Newtonian mechanical physics of the XVII century, which rule economics. From then on, economics lived its own evolution based on the classical physics assumptions (reductionism, determinism and mechanicism). Quite remarkably, Keynes adopted the approach of statistical physics, which deeply affected physical science at the turn of the XIX century by emphasizing the difference between micro and macro, around the mid 1930s Keynes (1936). However, after decades of extraordinary success it was rejected by the neoclassical school around the mid 1970s, which framed the discipline into the old approach and ignored, by definition, any interdependence among economic agents (firms, banks, households) and difference between microscopic individual behavior and macroscopic aggregate behavior. The ideas of natural laws and equilibrium were transplanted into economics sic et simpliciter. As a consequence of the adoption of the classical mechanics paradigm, behavior of macro-economic systems are treated as a scaled-up version of one individual agent, who is called representative agent (RA), and complexity that emerges from aggregation was lost. Any learned physicist knows that this is entirely wrong for physical systems with many constituents: macroscopic behavior of gas is qualitatively different from that of a single molecule. Likewise, economy of a country is not explained by analysing a single RA as if he is on a deserted island all by himself, like Robinson Crusoe without even Friday, and multiplying the number of population to the results. Another quite a dramatic example is the concept of equilibrium. In many economic models equilibrium is described as a state in which (individual and aggregate) demand equals supply. The notion of statistical equilibrium, in which the aggregate equilibrium is compatible with stochastic behavior of the constituents, is outside the box of tools of mainstream economists. Again, physics teaches us that the equilibrium of a system does not require that every single element be in equilibrium by itself, but rather that the statistical distributions describing macroscopic aggregate phenomena be stable. What modern physics can do for economics is, then, to open a way to a proper treatise of macro economy as an aggregation of individual economic agents, which is one main theme of econophysics on real economy Delli Gatti et al. (2008); Aoyama et al. (2010). Such a thought is not totally unfamiliar to open-minded economists, whose keyword is Heterogeneous Interacting Agents (HIA) Blume and Durlauf (2006), where ‘heterogeneity’ implies that each has different characteristics; different financial profile like different energy and momentum, and ‘interaction’ is trade with exchange of money, goods, workers, information, etc. just as physical particles interact with each other. In this paper, we show that a system populated by many HIA generates equilibrium distribution in the form of scaling laws. In particular, economic literature has shown the existence of large and persistent differences in labour productivity across industries and countries Aoyama et al. (2009); Ikeda and Souma (2009); Aoyama et al. (2010). Productivity is often measured in terms of the ratio between firms’ revenues and the number of employees. It can be expected to be a unique value only within a very straightjacket hypothesis, such as the RA. If agents are heterogeneous and interact, then scaling laws emerge, and dispersion is nothing but a consequence of it. Using Japanese data we empirically demonstrate it. The conclusive remarks points out that a thermodynamical approach (see also Foley (1994); Aoki and Yoshikawa (2007)) may be what economics needs if HIA are the actors of the drama.",2
10.0,1.0,Journal of Economic Interaction and Coordination,27 June 2014,https://link.springer.com/article/10.1007/s11403-014-0136-2,Wealth distribution and the Lorenz curve: a finitary approach,April 2015,Enrico Scalas,Tijana Radivojević,Ubaldo Garibaldi,Male,Female,Male,Mix,,
10.0,1.0,Journal of Economic Interaction and Coordination,21 January 2014,https://link.springer.com/article/10.1007/s11403-013-0121-1,Economies with heterogeneous interacting learning agents,April 2015,Simone Landini,Mauro Gallegati,Joseph E. Stiglitz,Female,Male,Male,Mix,,
10.0,1.0,Journal of Economic Interaction and Coordination,13 September 2013,https://link.springer.com/article/10.1007/s11403-013-0116-y,The effects of a financial transaction tax in an artificial financial market,April 2015,Daniel Fricke,Thomas Lux,,Male,Male,Unknown,Male,"For several decades the financial transaction tax (FTT) has been discussed as an instrument to curb financial market volatility, cf. Keynes (1936), chapter 12, and Tobin (1978). Only recently -given the surging government deficits from responses to the global financial crisis- the focus has shifted to the FTT’s large potential monetary revenues.Footnote 1 In this paper we investigate the effects of a FTT in an agent-based artificial financial market. The FTT’s appeal stems from its potential to limit short-term speculative behavior, and thus transaction volumes, on financial markets. This seems a reasonable aim given the divergence of financial market and ‘real’ activity during the last decades, when increases in financial market transaction volumes continuously exceeded those of the real economy. The exponential growth of financial transaction volumes was fueled by a continuous fall in transaction costs for many assets due to the technological progress in computer-based trading and an increased competition between stock exchanges. One result of this development is the increased presence of so-called high-frequency trading (HFT), which is predominantly employed by large hedge funds.Footnote 2 Indeed, higher liquidityFootnote 3 seems to have come along with higher fragility in the sense that financial crises, i.e. the build-up and bursting of speculative bubbles, became more frequent.Footnote 4 In this way, a FTT that favors longer-term investments could have the effect of reducing the decoupling of financial markets from real activity and could additionally free resources from the financial sector for more productive uses.Footnote 5
 Critics of the FTT, most importantly from the financial industry, usually bring forward the following arguments: (1) market liquidity will dry up, (2) volatility may thereby in fact increase, (3) banks will pass on the tax burden to firms and other bank customers, raising capital costs in general, and (4) there is a danger of capital flights from a taxed market towards untaxed markets. In this paper we are concerned with the first two (interrelated) points.Footnote 6
 High liquidity, i.e. small transaction costs, fuels excess volatility (compared to ‘fundamentals’) as it makes round-trips relatively cheap, cf. Shiller (1981). Empirical evidence suggests that FTTs, despite applying for all market participants, harm short-term speculators disproportionately more. For example, it has been found that an increase of a transaction tax has increased asset holding periods, while transaction volumes have decreased.Footnote 7 However, this does not imply that volatility will decrease as well. In theory, there could be a U-shaped relationship: for small tax rates volatility should decrease, since (destabilizing) short-term oriented speculation becomes unprofitable. However, larger tax rates will affect (stabilizing) longer-term strategies as well, thereby reducing liquidity and potentially increasing volatility. Empirical evidence on point (2) is therefore rather mixed: some studies find that volatility decreases, increases or does not react at all in response to a tax increase.Footnote 8
 Given these contradicting results, simulations of artificial financial markets are a promising way to non-invasively evaluate the effects of regulatory measures in general, see Westerhoff (2008) for a discussion. More detailed (realistic) models are usually hard to tackle analytically, so numerical simulations are needed. Agent-based models are such computerized simulations, containing a number of components (agents) interacting with each other through prescribed rules, thus taking all the necessary ingredients for modelling complex systems into account, cf. Aoki (2002). Numerous agent-based models, usually within the chartist-fundamentalist framework, are able to replicate many of the stylized facts characterizing financial market data.Footnote 9 When used to evaluate regulatory policies, however, using overly simplified models could affect the conclusions. For example, many authors assume that a market-maker provides infinite liquidity, in which case FTTs are potentially stabilizing for small tax rates. For a single asset market, see Ehrenstein (2002) and Westerhoff (2003), and Westerhoff (2004). However, Giardina and Bouchaud (2003) find that only substantial trading costs will actually stabilize the market, while a small tax (of the order of a few basis points) would have no real effect.Footnote 10 However, since liquidity is a major determinant of volatility in real markets, cf. Mike and Farmer (2008), it is crucial when discussing the effects of FTTs. In fact, the findings from both laboratory experiments and simulation studies indicate that the effects of a FTT may depend on the structure of the market, see Kirchler et al. (2012) and Pellizzari and Westerhoff (2009). Therefore we explicitly take the microstructure of real markets and provision of liquidity into account by simulating an order-driven continuous double-auction (CDA). While the models based on the marker-maker setting typically incorporate important psychological factors that drive the system’s properties, e.g. through herding and imitation, CDA models work at shorter time-scales where psychological factors are either hard to model or simply assumed to be absent. For example, Bak et al. (1997) treat the limit order book (LOB) as a system of particles with each particle (order) having a mass (order size) and a price (spatial position). Price variations stem from diffusion and annihilation of particles, well-known processes in physics, which allows to obtain analytical results. Even though many important insights can be gained from such approaches, the usual criticism is that these models operate within a zero intelligence framework.Footnote 11 This is the exact opposite to ‘homo oeconomicus’ in mainstream economics, but traders are unlikely to be either fully rational or plainly stupid. Another problem is that, by avoiding detailed behavioral assumptions, these models typically ignore budget constraints and wealth dynamics. Nevertheless, since these models are able to replicate certain stylized facts of LOB data, the structure of the trading protocol is likely to have a significant effect on the data-generating process. To date, few attempts have been made to model the LOB based on detailed strategic interactions between many boundedly rational agents, while incorporating economic constraints.Footnote 12 Our model aims at bridging the gap between models with short and long time-scales. To our knowledge, only few studies have been dealing with FTTs in detailed order-driven markets. Two examples are Mannaro et al. (2008) and Pellizzari and Westerhoff (2009). Mannaro et al. (2008) use a zero intelligence framework combined with a once-a-day supply-demand based market-clearing rule, deleting all orders not executed during the clearing session and thus strongly limiting their impact. In this setting the FTT is found to be destabilizing. Pellizzari and Westerhoff (2009) compare the effects of the FTT in different market settings. The main finding is that the FTT destabilizes a CDA market, while it stabilizes a dealership market where specialists provide abundant liquidity. One important assumption in the dealership setup is that the dealer (or market-maker) is exempt of the tax, which is hardly the case for a general FTT that should apply to all market participants. Moreover, these studies suffer from the assumption that all agents act with equal probability, i.e. they neglect the importance of heterogeneous investment horizons.Footnote 13 In this way, the FTT’s effect of more severe taxation of short-term speculation is missed.Footnote 14 Another novelty compared to the existing literature is that the limit orders in our model emerge from a rule-based decision process, rather than from a pure zero intelligence framework. In our model two groups of agents compete in the market: Noise traders act as liquidity providers, by posting random orders. Informed traders use information about past prices and the fundamental value when forming their price expectations. As in Youssefmir et al. (1998), their price expectations depend on three different time horizons (Fig. 1): the investment horizon (denoted by \(H^w\)) basically defines how often a particular agent acts and how long his planning horizon is when making investment decisions. Two different trend horizons model the trend chasing behavior of agents: the backward trend horizon \((H^b)\) defines how many past price observations are relevant when calculating the trend. The forward trend horizon \((H^t)\) defines how long the agent expects his calculated trend to last before the price will start returning to the fundamental value. This setting is very flexible concerning the strategies and we essentially allow for all combinations of time horizons within a certain set. Most importantly, the relative size of forward trend and investment horizon defines whether an agent is a chartist, a fundamentalist or something in between.Footnote 15
 Time horizons in the model. In order to reduce the complexity of the model, we will set \(H^b=H^t\) in the following. More details can be found below Our main conclusions can be summarized as follows: First, the model is able to replicate certain stylized facts of real financial time-series for several parameter combinations, e.g. the model replicates the building up and bursting of price bubbles. Second, we find the usual trade-off between monetary revenues (a kind of Laffer curve) and stability, as higher tax revenues come along with higher volatility. This finding is in line with the results from the existing literature. However, we find somewhat different results for very small and large tax rates, indicating that the effects of the tax may not be entirely negative. In any case, the tax allows to generate substantial tax revenues, which could be used for a number of more productive purposes. The remainder of this paper is structured as follows: Sect. 2 introduces the structure of the model. Section 3 presents pseudo-empirical results and Sect. 4 concludes.",11
10.0,1.0,Journal of Economic Interaction and Coordination,15 March 2014,https://link.springer.com/article/10.1007/s11403-014-0128-2,Okun’s law and anelastic relaxation in economics,April 2015,Raymond J. Hawkins,,,Male,Unknown,Unknown,Male,"Okun’s law is remarkable both for its global reach and its parsimony. Originating in Okun’s research on the potential output of the United States (Okun 1962), subsequent confirmationFootnote 1 of this simple linear relationship between output and unemployment across economies and time established it as a macroeconomic law. Linking, as it does, two key macroeconomic issues, Okun’s law continues to draw the attention of academics and policymakers interested in the dynamics linking these issues in general and of the microfoundations of this relationship in particular.Footnote 2
 Three versions of Okun’s law—gap, difference, and dynamic—dominate the current discussion of output and unemployment. Studies using the gap version have established that unemployment is driven by output and the difference and dynamic version have demonstrated that the response of unemployment to output is not instantaneous.Footnote 3 The time-dependence of the unemployment response is typically represented by partial-adjustment models that, while describing adequately the observed dynamics, largely lack a theoretical basis with which to link macroeconomic policy with the model.Footnote 4 Implicit in the three versions of Okun’s law are three formal assumptions: (i) that for every level of the output gap there is a unique equilibrium unemployment gap, and vice versa, (ii) the equilibrium response of unemployment to a change in output is achieved only after the passage of sufficient time, and (iii) the relationship between the output gap and the unemployment gap is linear.Footnote 5 In this paper we develop the observation that these formal assumptions are identical to assumptions underlying the formal treatment of macro-adjustment processes in other complex systems.Footnote 6 All these systems show time-dependent adjustment, or relaxation, toward newly established equilibria that follow from a change in a driving force and can be described in terms of linear-response theory. Since these systems share a common linear-response description of relaxation and since they and unemployment–output dynamics share the three underlying assumptions listed above (and from which as we shall see linear-response dynamics follow), we make the ansatz that Okun’s law shares this linear-response description. We develop the anelastic basis of Okun’s law below in Sect. 2 by deriving the dynamical relationship between output and unemployment that follows from the basic economic principles that underly the three well-known versions of Okun’s law. In this section we also show that this representation of Okun’s law describes well quarterly observations of output and unemployment in the U.S. We then show in Sect. 3 how our anelastic derivation of Okun’s law allows for a direct link to the underlying microeconomics of unemployment adjustment as expressed by a master-equation treatment of labor mobility. Our anelastic approach also has the added feature of a well-developed formalism for friction—known as internal friction—which comes directly from the dissipation of a change in output as it is translated over time into a change in unemployment as shown in Sect. 4. Finally we close with a discussion and summary in Sect. 5.",3
10.0,1.0,Journal of Economic Interaction and Coordination,08 March 2014,https://link.springer.com/article/10.1007/s11403-014-0126-4,"Credit-driven investment, heterogeneous labor markets and macroeconomic dynamics",April 2015,Matthieu Charpe,Peter Flaschel,Daniele Tavani,Male,Male,Female,Mix,,
10.0,1.0,Journal of Economic Interaction and Coordination,07 March 2014,https://link.springer.com/article/10.1007/s11403-014-0127-3,Monetary policy and PID control,April 2015,Raymond J. Hawkins,Jeffrey K. Speakes,Dan E. Hamilton,Male,Male,Male,Male,"Although over a century has passed since their articulation by Wicksell (1898, 1907), monetary-policy rules and their role in monetary policy continue to engage the economics community.Footnote 1 Current expressions of these rules focus on interest-rate rules which gained heightened prominence following the publication of the parsimonious Taylor rule (Taylor 1993a). Since then a considerable literature has developed in which variations on the Taylor rule have been proposed in response to particular models of the macroeconomy.Footnote 2 This literature has been characterized generally by rules that are essentially variations of the Taylor rule and characterized recently by a view towards assessing rules less on their ability to perform for the macroeconomic model from which they originated and more on how they perform across macroeconomic models (Wieland et al. 2011; Taylor and Wieland 2012). These characteristics of monetary-policy rule development are strikingly similar to controller development in engineering (Bennett 1993; Åström and Murray 2008) and suggest that these fields share a common framework: a goal of this paper is to demonstrate that this is so. When a central bank implements monetary policy by changing an interest rate in response to the deviation between desired and actual macroeconomic performance (e.g. inflation or output) the central bank is providing feedback into the economy designed to reduce, and hopefully eliminate, that deviation. This principle of feedback—“base correcting actions on the difference between desired and actual performance”—is found extensively in natural and technological systems (Åström and Murray 2008). Of the forms of feedback one can use, PID control is by far the most frequently encountered and also remarkably parsimonious: corrective action is based on (1) a proportion, P, of the deviation, (2) the history of the deviation represented by the integral, I, of the deviation over time and (3) the forecast or expectation of future deviations represented by the derivative, D, of the deviation with respect to time. Over 95 % of industrial applications and a wide range of biological systems employ elements of PID control (Åström and Murray 2008).Footnote 3
 In addition to this ubiquity and parsimony, decades of PID control across a wide range of applications have shown that “in the absence of any knowledge (in terms of a dynamical model) of the process to be controlled, the PID controller is the best form of controller” (Bennett 1993). This feature recommends PID control strongly as a way to address an issue that plagues the traditional approach to monetary-policy rule development: the intrinsic uncertainty in the equations of motion of the macroeconomy. Given a set of equations that describe a macroeconomy one can, in principle, derive a monetary-policy rule (or controller) for that economy using well-known engineering techniques (Aoki 1967) that derive, ultimately, from the work of Maxwell (1867–1868), and this approach is used in monetary-policy rule development. As the complexity of a system increases, however, analytical tractability is often soon lost and system control can be realized only by using feedback control tuned to the system to be controlled, and the most popular approach to this problem in the field of control engineering is, as mentioned above, the proportional-integral-differential (PID) control (Bennett 1993; Åström and Murray 2008). Since recent work to assess monetary-policy rules across macroeconomic models (Wieland et al. 2011; Taylor and Wieland 2012) is a form of tuning, a natural question is whether PID control can be viewed as a monetary-policy rule. As we shall see, a subset of PID control—proportional-integral, or PI control—is nearly identical in mathematical form to the versions of the Taylor rule emerging from tuning work based on a global macroeconomic-model database (Wieland et al. 2011), and the PI-version of extended Taylor rules can be motivated on a purely economic basis. Thus, a contribution of this paper is a new PI-based monetary-policy rule that embraces the intrinsic uncertainty in our evolving understanding of macroeconomic dynamics. We continue in Sect. 2 with the derivation of our monetary policy rule and a comparison of its formal structure with that of policy rules currently in use. The tuning of our rule and its performance are discussed in Sect. 3. In assessing rule performance we compare the performance of our policy rule against that of the original Taylor rule (Taylor 1993b) and of a rule recently proposed by Taylor and Wieland (2012) across 37 current-generation models of the macroeconomy. We close with a discussion and summary in Sect. 4.",2
10.0,2.0,Journal of Economic Interaction and Coordination,03 June 2014,https://link.springer.com/article/10.1007/s11403-014-0132-6,Agent-based modeling and economic theory: where do we stand?,October 2015,Gerard Ballot,Antoine Mandel,Annick Vignes,Male,Male,Female,Mix,,
10.0,2.0,Journal of Economic Interaction and Coordination,08 January 2014,https://link.springer.com/article/10.1007/s11403-013-0120-2,Central bank policy instrument forecasts,October 2015,Hyuk Jae Rhee,Nurlan Turdaliev,,,Male,Unknown,Mix,,
10.0,2.0,Journal of Economic Interaction and Coordination,12 January 2014,https://link.springer.com/article/10.1007/s11403-013-0122-0,Long cycles in a modified Solow growth model,October 2015,Fátima Fabião,João Teixeira,Maria João Borges,Female,,Female,Mix,,
10.0,2.0,Journal of Economic Interaction and Coordination,18 January 2014,https://link.springer.com/article/10.1007/s11403-014-0123-7,Monetary policy experiments in an agent-based model with financial frictions,October 2015,Domenico Delli Gatti,Saul Desiderio,,Male,Male,Unknown,Male,"Macroeconomic agent-based models have been around for at least a decade. By and large, they have been remarkably successful in replicating the empirical “stylized facts” of aggregate business cycles and the cross-sectional evidence (e.g. firms’ size distribution) by means of artificial data. Only a handful of papers, however, has explored the effects of monetary policy, the channels of monetary transmission and the performance of alternative monetary policy rules in these models. 
Delli Gatti et al. (2005b) show, by means of genetic algorithms, that an adaptive, discretionary Taylor rule outperforms a commitment strategy. Similar results can be found in Raberto et al. (2008), where a policy rule targeting the output gap over-performs a random monetary policy both in improving social welfare and in stabilizing inflation. Monetary policy is also effective in Oeffner (2008) when the economy is not stuck in a liquidity trap. Mandel et al. (2010) find that a Taylor rule may increase macroeconomic volatility. Finally, Cincotti et al. (2010) employ the EURACE simulator to show how quantitative-easing coupled with expansionary fiscal policy improves the overall macroeconomic performance but speeds up inflation and output volatility. The original goal of the present paper was to carry out monetary policy experiments in a streamlined and simplified variant of the model presented in Macroeconomics from the Bottom Up (Delli Gatti et al. (2011); MBU hereafter).Footnote 1 The model is described in Sect. 2 and is labelled the “baseline model”. Before experimenting, our belief—grounded in the results of the papers summarized above—was that monetary policy is effective in steering the macroeconomy in the direction aimed at by the policy makers. The baseline model, in fact, is characterized by built in non-linearities and is characterized by asymmetric information and nominal wage rigidity, features that are generally conducive to monetary non-neautrality. From the simulations it turned out, to our surprise, that the emergent properties of the baseline model are robust to monetary policy shocks of a realistic size. In other words, the long run tendency of GDP (towards a quasi full employment equilibrium) of the baseline model is not affected in a noticeable way by monetary policy. Therefore, we have explored an extension of the original framework in search of potential sources of non-neutrality. The “extended model” we present in Sect. 3 is characterized by a demand for credit which may respond to changes in the interest rate and therefore incorporates a credit channel of monetary transmission. From the simulations of the extended model it turns out that in the presence of the credit channel monetary policy is indeed an effective macro-stabilization tool. We explore two regimes. In the first one (interest rate pegging) the central bank sets the policy rate, which is therefore exogenous. In the second one the central bank follows a Taylor rule, so that the policy rate becomes endogenous. In the pegging regime, the timing of the removal of the shock is key to its effectiveness. If a policy rate increase is offset by an equivalent reduction “too late”, the economy falls into a low activity-low leverage rate. A Taylor rule, on the other hand, works surprisingly well to stabilize the macroeconomy. These results can be traced back to changes in the distribution of firms’ activity and leverage. Therefore, by construction, they cannot be achieved by means of the usual impulse response functions in DSGE models. The remainder of the paper is organized as follows. In Sect. 2 the baseline model is described and the relative simulation results are presented. Section 3 first introduces the extended model and shows its basic properties emerging from simulations, then it presents the monetary policy experiments performed on the extended model. Section 4 concludes.",27
10.0,2.0,Journal of Economic Interaction and Coordination,15 March 2014,https://link.springer.com/article/10.1007/s11403-014-0129-1,Markets connectivity and financial contagion,October 2015,Ruggero Grilli,Gabriele Tedeschi,Mauro Gallegati,Male,Female,Male,Mix,,
10.0,2.0,Journal of Economic Interaction and Coordination,14 March 2014,https://link.springer.com/article/10.1007/s11403-014-0130-8,An agent based decentralized matching macroeconomic model,October 2015,Luca Riccetti,Alberto Russo,Mauro Gallegati,Male,Male,Male,Male,"In recent years many economists have developed agent-based models to investigate the working of a macroeconomic system composed of heterogeneous interacting entities (Tesfatsion and Judd 2006; LeBaron and Tesfatsion 2008). In general, the idea is to start from simple (adaptive) individual behavioral rules and interaction mechanisms and to simulate a model in order to reproduce the emergence of aggregate regularities and endogenous crises. In a sense, this is a generative approach according to which we construct the macroeconomy from the “bottom-up” (Epstein and Axtell 1996). In what follows, we report some examples of agent-based models showing emergent macroeconomic features based on the interplay between two fundamental characteristics of agent-based models: heterogeneity and interaction. We also provide a discussion on early contributions, particularly in the field of microsimulation, which provided the bases for future developments of heterogeneous interacting agent models. Moreover, we describe the possible use of the agent-based methodology in order to assess the role of different economic policies. One of the basic characteristics of agent-based models is heterogeneity. Agents may differ in many dimensions such as income, wealth, size, financial fragility, location, information, and so on. One of the main advantages of considering agent heterogeneity is that aggregate regularities are not approximated by the behavior of a “representative agent” (Kirman 1992; see also Gallegati and Kirman 1999). The latter assumption, indeed, may lead to some inconsistencies, for instance because real-world data are often not Gaussian distributed, showing in many cases a power law shape. Consequently, it is usually not possible to reduce the complexity of macroeconomic dynamics to the behavior of a single agent with an “average” behavior, because the average does not represent the behavior of the system. As a consequence, macroeconomic models based on the representative agent hypothesis suffer from a “fallacy of composition”. However, even in mainstream economic models it is possible to introduce a certain degree of heterogeneity. For instance, the recent debate in the DSGE (Dynamic Stochastic General Equilibrium) models community is focused on the introduction of financial factors and agents’ heterogeneity. However, another basic feature that can be hardly introduced in mainstream models and that is instead at the root of agent-based models is the “direct interaction” among heterogeneous agents. As for the specific topic regarding the interaction among agents, earlier contributions are based on a stochastic approach. Even in the case of stochastic interaction among individual agents, aggregate regularities may emerge. Indeed, the aggregation of simple interactions at the micro level can generate sophisticated behavior at the macro level. An early contribution on this topic is the one proposed by Föllmer (1974): he studies the dynamics of an exchange economy with random preferences. By applying the physics tools from interacting particle systems, Föllmer (1974) shows that even weak interaction may propagate through the economy and give rise to aggregate uncertainty causing large price movements and even a breakdown of price equilibria. Kirman (1991, 1993) proposes an analysis of the effects of local interactions in an exchange model with fundamentalists and chartists; in particular, Kirman (1993) provides an interesting analogy between biology and economics, focusing on the similarities between the behavior of ants and the dynamics of financial markets. See Brock (1993) and Kirman (1999) for a review of models with interacting agents. See also the reviews proposed by Gallegati and Kirman (1999) and Delli Gatti et al. (2000). However, agent-based macro modelling has a 50 years tradition and at least a seminal paper can be cited, that is Orcutt (1957), which founded the microsimulation approach. “Microsimulation is a methodology used in a large variety of scientific fields to simulate the states and behaviors of different units—e.g. individuals, households, firms—as they evolve in a given environment—a market, a state, an institution. [...] Compared to other methodologies based on representative agents or aggregate level analysis, e.g. computable general equilibrium or macroeconomic models, the main strength of [microsimulation] is indeed to simulate how a certain policy change may differently affect heterogeneous individuals (or other entities). Furthermore, modeling at the micro level allows macro phenomena to emerge “from the bottom up” without the aggregation bias deriving from the use of statistical average” (Baroni and Richiardi 2007, p. 2). Further developments of such a model led to the dynamic microsimulation model DYNASIM (Wertheimer et al. 1986). For a comprehensive review of microsimulation studies, see Baroni and Richiardi (2007). Two (independently developed) contributions in which the foundations of “bottom up” macroeconomics based on boundedly rational agents can be traced back to the microsimulation approach, are those proposed by Bergmann (1974)—see also Bennett and Bergmann 1986, and Eliasson (1977)—see also Eliasson 1984). Indeed, Neugart and Richiardi (2012, p. 3), which refer in particular to the analysis of the labor market, maintain that the roots of agent-based models (ABMs) “must be traced back to two early studies that are generally not even recognized as belonging to the AB tradition: Barbara Bergmann’s microsimulation of the US economy (Bergmann 1974) and Gunnar Eliasson’s microsimulation of the Swedish economy (Eliasson 1976)”. The “transactions model” by Bergmann (1974) is a development of Orcutt’s microsimulation approach, in which the behavior of individual agents is based on decision rules, rather than consisting of transition probabilities from one state to another. As noted by Neugart and Richiardi (2012, p. 8), “in the early 1974 version, only one bank, one financial intermediary and six firms, ‘representative’ of six different types of industrial sectors / consumer goods (motor vehicles, other durables, nondurables, services and construction) are simulated. In the labor market, firms willing to hire make offers to particular workers, some of which are accepted; some vacancies remain unfilled, with the vacancy rate affecting the wage setting mechanism. Unfortunately, the details of the search process are described only in a technical paper that is not easily available anymore (Bergmann 1973). Admittedly, the model was defined by Bermann herself as a ‘work in progress’, and was completed only years later (Bennett and Bergmann 1986)”. The “micro-to-macro” model by Eliasson (1976), which we can also refer to as MOSES (“Model of the Swedish Economy”) is a dynamic microsimulation model in which firms and workers are the basic unit of analysis. While a concise description can be found in Eliasson (1977), a developed version of such a model is that proposed in Eliasson (1991). MOSES is particularly suited for analyzing industrial growth; therefore, manufacturing is the most detailed sector in the model. The manufacturing sector is divided into four industries/markets (raw materials, processing, semi-manufacturing, durable goods manufacturing), and the manufacturing of consumer nondurables. In particular, the manufacturing sector is populated by 225 firms, a bank, the government, an exogenous foreign sector; then, there are an entry-exit process and the markets replacing mechanical input-output coefficients between selling and buying firms.Footnote 1 In such an experimentally organized market environment, competition does not lead to a “competitive equilibrium”. Indeed, the MOSES model considers Smithian competition, involving rivalry and an entry-exit process related to innovation dynamics. According to Eliasson (1991), this creates a continuous state of disequilibrium, leads to economic growth, and generates systematic divergences between ex ante plans and ex post realizations, providing a synthesis of Schumpeterian and Wicksellian ideas in the spirit of the Stockholm School. The MOSES model has been used to study the characteristics of both business fluctuations and endogenous growth.Footnote 2 This model also has assumptions about money and credit. Although the interest rate is set exogenously, it can depend on firms’ risk and credit rationing. Moreover, the framework has been extended to include venture capital by capitalists of heterogeneous competence as a source of economic growth (Ballot et al. 2006). However, the model does not have multiple banks; consequently, bank failures are not considered. Moreover, the model does not consider the equity market. During the 1990s, the MOSES model has been further extended in order to better analyze some economic growth issues such as the effects of many types of endogenous technical progress, the process of learning by doing, the introduction of incremental and radical innovations based on R&D and “human capital” investments, and the role of incremental and radical imitations. These features yield endogenous waves of innovation but also the possibility of extended “lock-in” periods (Ballot and Taymaz 1998). A recent extension with 16 countries shows that the combination of endogenous competencies and spillovers can reproduce “club convergence” and “global divergence”. Finally, a central feature of the MOSES model is that major costs of macroeconomic growth are due to agents’ mistaken plans which generate non-linear dynamics and path-dependent trajectories. Consequently, model simulations may exhibit an unpredictable and disorderly behavior. According to Eliasson (1991), this means that the complexity of such a model, although less complex than reality, prevents the external observer from predicting the behavior of the economic system. This also holds for policy makers which should behave quite cautiously, so to avoid to do more harm than good through policy interventions. We will provide a comparison between some aspects of this model and some characteristics of the one we propose in the present paper. Let’s now discuss some other recent contributions in the field of agent-based computational economics, with a particular emphasis on the analysis of macroeconomic dynamics. Howitt and Clower (2000) show that economic organization may emerge from the bottom-up through decentralized interactions. Fagiolo et al. (2004) investigate labor market dynamics and the evolution of aggregate output. In particular, they model a decentralized matching process to describe the interaction between workers and firms in context characterized by endogenous price formation and stochastic technical progress. Delli Gatti et al. (2005) show that the interaction of financially fragile firms generates power law distributions and may lead to large crises. Along these lines, Delli Gatti et al. (2009, 2010) analyze the role of financial factors in a credit network economy in which even small shocks may lead to bankruptcy avalanches. Russo et al. (2007) present an agent-based model in which bounded rational firms and workers interact on fully decentralized markets both for final goods and labor. The model is used to analyze the role of fiscal policy in promoting R&D investments that may increase economic growth. This model has been further developed by Gaffeo et al. (2008) through the introduction of a similar matching protocol for the credit market. Haber (2008) investigates the effects of fiscal and monetary policies in a macroeconomic framework. Westerhoff (2008) analyzes the role of regulatory policies on financial markets. Neugart (2008) evaluates the role of labor market policies. Deissenberg et al. (2008) provide a massively parallel agent-based model, named EURACE, which they use to simulate the European economy. Based on the EURACE framework, Cincotti et al. (2010) investigate the interplay between monetary aggregates and the dynamics of output and prices by considering both the credit extended by commercial banks and the money supply created by the central bank. In particular, they study the effects of quantitative easing as a monetary policy. Building upon Dosi et al. (2006, 2010, 2012) analyze the interplay between income distribution and economic policies. They find that more unequal economies are exposed to more severe business cycles fluctuations, higher unemployment rates, and higher probability of crises. They also find that fiscal policies dampen business cycles, reduce unemployment and the likelihood of large crises, and may affect positively long-term growth. Westerhoff and Franke (2012) analyze the effectiveness of various stabilization policies, so using the agent-based approach for economic policy design. Hence, agents-based macroeconomic models show that an alternative formulation of microfoundations is possible for complex environment and this has relevant implications for policy advice (Dawid and Neugart 2011). Fagiolo and Roventini (2009, 2012) review both methodological and policy-oriented papers employing agent-based simulation techniques. Our aim is to develop a macroeconomic framework with heterogeneous agents that interact through a decentralized matching process presenting common features across markets. The framework is basic since we propose a minimal macroeconomic model and it is flexible because this baseline setup is thought to be enriched by adding new modules with different agents, markets, and institutions. Indeed, in this paper we propose an agent-based macroeconomic model in which there are three classes of computational agents-individuals, firms, banks-interacting in four markets-goods, labor, credit and deposit-according to a fully decentralized matching mechanism. Moreover, we build a model in which stocks and flows are mutually consistent. Stock-flow consistency is a very important feature (Godley and Lavoie 2006) that economists are applying also in the field of agent-based macroeconomics as, for instance, in Cincotti et al. (2010, 2012), Kinsella et al. (2011), Seppecher (2012). This paper is just a first step towards a complex task that is the development of a micro-founded general (dis)equilibrium macroeconomic model based on heterogeneous interacting agents. Although the model is populated by many heterogeneous agents which interact in a truly decentralized way in different markets, various features of a macroeconomic framework have still to be introduced, for instance technological progress, human capital, the foreign sector, etc. For these reasons, it is quite difficult to compare our result with those emerging from other models which instead include long-run growth factors, as for instance the previously discussed MOSES model. Moreover, our model is much more focused on the relationship between financial factors and the real economy, considering the possibility of firm and bank defaults. In our framework, indeed, even small shocks can lead to large fluctuations due to financial contagion. Thus we focus on some characteristics such as the dynamics of financial variables—firms’ leverage, banks’ exposure—and their interplay with the business cycle. Indeed, many papers recently try to understand the leverage process both for firms and banks: Adrian and Shin (2008, 2009, 2010), Brunnermeier and Pedersen (2009), Flannery (1994), Fostel and Geanakoplos (2008), Greenlaw et al. (2008), He et al. (2010), Kalemli-Ozcan et al. (2011).Footnote 3
Geanakoplos (2010) finds that leverage is pro-cyclical, while Kalemli-Ozcan et al. (2011), as well as Adrian and Shin (2008, 2009), find that the leverage pattern for non-financial firms is acyclical (instead this is pro-cyclical for investment banks and large commercial banks). The leverage level is a component of a more general discussion on firm and bank capital structure, such as in Booth et al. (2001), Diamond and Rajan (2000), Gropp and Heider (2010), Lemmon et al. (2008), Rajan and Zingales (1995). In the economic literature there are many theories on capital structure but almost all previous papers in the agent-based macroeconomic approach assumed a “pecking order” theory (Donaldson 1961; Myers and Majluf 1984), based on information asymmetry, according to which investments are financed first with internally generated funds, then with debt if internal funds are not enough, and equity is used as a last resort. A different perspective on the firms’ financial structure was proposed by the “trade-off” theory, firstly observed in a paper concerning asset substitution (Jensen and Meckling 1976), and in a work on underinvestment (Myers 1977). This theory is based on the trade-off between the costs and benefits of debt and implies that firms select a target debt-equity ratio. The empirical literature found at first contrasting evidence to support these theories. Then, a refined version of the trade-off theory was proposed: the “dynamic trade-off theory” (Flannery and Rangan 2006). In this theory firms actively pursue target debt ratios even though market frictions temper the speed of adjustment. In other words, firms have long-run leverage targets, but they do not immediately reach them, instead they adjust to them during some periods. Dynamic trade-off seems to be able to overcome some puzzles related to the other theories, explaining the stylized facts emerging from the empirical analysis and numerous papers conclude that it dominates alternative hypotheses: Hovakimian et al. (2001), Mehrotra et al. (2003), Frank and Goyal (2008), Flannery and Rangan (2006). Moreover, Graham and Harvey (2001) conduct a survey where they evidence that 81 % of firms affirm to consider a target debt ratio or range when making their debt decisions. Then, one of the major innovations we introduce compared to the agent-based macroeconomic framework delineated in the literature is that firms’ financial structure is derived from the Dynamic Trade-Off theory. According to this theory, we assume that firms have a “target leverage”, that is a desired ratio between debt and net worth, and they try to reach it by following an adaptive rule governing credit demand. This capital structure is already investigated in the agent-based model proposed by Riccetti et al. (2013) that builds upon the previous work by Delli Gatti et al. (2010), which is based on a firms’ capital structure given by the Pecking Order theory. The Dynamic Trade-Off theory has a relevant role in influencing the leverage cycle, with important consequences on macroeconomic dynamics.Footnote 4
 Another important point in the model is the presence of an acyclical sector, here represented by the government that hires public workers so providing a fraction of the aggregate demand. In this way the government partially stabilizes the economy by reducing output volatility. Nevertheless, our model also demonstrates that large and extended crises with large unemployment and a lacking aggregate demand may endogenously emerge. This also means that the model can exhibit unpredictable behaviors and path-dependent trajectories which depend on historical accidents. As in the case of the MOSES model (Eliasson 1991), the modeler as well as the external observer (and also policy makers) cannot predict the realization of a large crisis, due to the complexity of individual and collective behaviors. However, one can detect a tendency of the economic system towards the crisis, as we will see when we discuss model simulations. The paper is organized as follows. In Sect. 2 we explain the basic aspects of the modeling framework such as the sequence of events and the matching mechanism. Section 3 presents the working of the four markets which compose our economy. The evolution of agents’ wealth is described in Sect. 4, while the behavior of policy makers is discussed in Sect. 5. Model dynamics are studied in Sect. 6 in which we report the simulation results. Moreover, in Sect. 7 we develop some Monte Carlo experiments in order to: (i) investigate the relationship between financial factors and the real economy, (ii) analyze the peculiar aspects of extended crises. In Sect. 8 we analyze the role of the government as an employer of public workers: in particular, we study the influence of the public sector’s size (and then the interplay between the private economy and the public sector) on macroeconomic dynamics. Section 9 concludes.",101
10.0,2.0,Journal of Economic Interaction and Coordination,27 March 2014,https://link.springer.com/article/10.1007/s11403-014-0131-7,"Share price formation, market exuberance and financial stability under alternative accounting regimes",October 2015,Yuri Biondi,Pierpaolo Giannoccolo,,Male,Male,Unknown,Male,"The global financial crisis—triggered by the breakdown of interbank loan market during the summer of 2007—has resulted in partial and temporary suspension of fair value accounting and given impetus for its reassessment. Hearings held before committees of the US House of Representatives in October 2007 led to the drafting of a report by the “Financial Stability Forum” at the G7 meeting of April 2008. This report recommended strengthening the prudential supervision of leverage, liquidity and risk, clarifying and limiting the use of fair value accounting, improving off-balance-sheet accounting and increasing the resilience of financial and banking systems to tensions and crises (Bignon et al. 2009; Banque 2008; Banca d’Italia 2009; Henry and Holzmann 2009). On 2nd October 2008, the US Parliament adopted the so-called “Paulson plan,” which, in sections 132 and 133, granted the Securities and Exchange Commission (SEC) with power to suspend application of fair value accounting for reasons of “public interest” and consistent with “protection of investors”.Footnote 1 In January 2009, a report by the “Group of Thirty” (G30) condemned fair value for its role in creating systemic risks, low resilience and financial instability. These triggering events have renewed long-standing debates on suitable modes of accounting and prudential regulations for financial markets, questioning not only fair value accounting, but also its overarching reference to financial market-based regulation (Bignon et al. 2004; Acharya et al. 2011; Stout 2011; Biondi 2011a). Contrary to other economic regulations and policies, recent literature has neglected economic consequences of accounting regulatory regimes. In the aftermath of the global financial crisis, however, accounting ceased to be relegated among obscure, irrelevant technicalities to be included into the core of financial market architecture. Accounting has then become a major financial regulatory issue (Enria 2004; Magnan 2009; Pozen 2009; Acharya et al. 2011). Generally speaking, accounting plays two main roles in securities and exchange regulation (Pinnuck 2012; Henry and Holzmann 2011): On the one side, information provision to prospective investors, financial analysts and other gatekeepers; on the other side, corporate stewardship for and accountability to shareholders and other holders of listed securities, which are issued by business firms and financial institutions, and traded on regulated exchanges. In this way, corporate accounting systems make securities-issuing entities accountable for their financial performance. Already in 1943, George May (1943: 21) argues that “the present ferment in accounting thought is very largely due to conflicting objectives of those who would continue to regard financial statements as reports of progress or of stewardship, and those who would treat them as being in the nature of prospectuses”. Concerned with both accounting roles, policy-makers and regulatory bodies have recently realized that accounting numbers are not straightforward “natural” measurements, but socioeconomic “artificial” constructions that are framed and shaped by standards and conventions, which fundamental accounting principles of reference underpin (Biondi 2011b). Let us label every whole of accounting techniques, standards and principles as an “accounting regime” hereafter. To assess alternative accounting regimes (Bignon et al. 2004; Ijiri 2005; Laux and Leuz 2009; Barth and Landsman 2010; Yuan and Liu 2011; Kusano 2012), some students try to develop empirical tests exploiting available data. These empirical analyses face serious theoretical and practical limits. Concerning the global financial crisis, fair value accounting was factually suspended before that its application would trigger its main effects. Although suspension itself can be considered as evidence of accounting relevance for financial crisis dynamics (at least in actors’ perception), its empirical assessment is irremediably undermined. In addition, only one series of market prices—that generated under the accounting regime that is currently in place— can be assessed by econometric methods, while possible series under alternative regimes cannot be obtained under identical identifiable conditions that are under experimenter’s control. Moreover, it seems difficult to disentangle and assess the distinctive contribution of fair value accounting since corporate accounting policies—which interpret and apply accounting standards—constitute a further autonomous dimension that prevents straightforward transmission between accounting regimes and accounting numbers. In this context, other students aim to identify mode and channel of interaction between accounting numbers and actors’ behaviors. Most contributions point then to sudden illiquidity of financial assets, or unexpected lack of funding for actors involved in distressed financial market dynamics. Although relevant, these cases remain special ones. Focus on them may eventually justify inconsistent accounting rules which depend on peculiar circumstances: Fair value accounting may be and have been considered as the most suitable mode of accounting as long as share market is “liquid” and “active” (or going-up, cynically speaking), while it would require suspension once share market becomes “illiquid” or “inactive” (or going-down). Opportunistic behavior, structuring opportunities, moral hazard and regulatory capture may be reinforced by such an intermittent theoretical position, while the comprehensive relationship between accounting and financial market dynamics remains unaddressed. Our paper aims to comprehensively address the relationship between accounting regimes and the dynamics of share market price formation over time. Share market price cycles and their aggregate characteristics in terms of market volatility and market exuberance shall be connected to alternative accounting regimes in a comparative analysis that applies the theoretical model developed by Biondi et al. (2012) and Biondi (2013). The latter provide a complex systems approach to financial market dynamics based upon common knowledge and social opinion dynamics. Embedded in a financial system, interacting heterogeneous investors are imagined to trade entity shares through a collective device (a Share Exchange, or share price system), under collective provision of accounting information on financial performance of a share-issuing entity (through a corporate accounting system). This approach develops a model of the role of accounting in financial market dynamics that allows an assessment of the relative capacity of alternative accounting regimes to enhance financial market resilience while explicitly recognizing the socio-economic context that underlines the formation of share market prices over time. This context involves social interaction, processes and institutions. In particular, this model purports to improve understanding of and provide insights into the effects of alternative institutional configurations, striving then for simplicity with the ultimate goal of incorporating only the features that are necessary to generate the phenomenon of interest. This model consists of a population of heterogeneous investors, an environment in which the investors interact (a financial system), and some sets of rules (or minimal institutions, in Shubik’s words) that frame and shape the interaction among investors. It considers two minimal institutions that constitute a dual institutional architecture for the financial system under investigation: An accounting system related to the congeries of the business firm; and a market price system related to the Share Exchange. Both institutions provide collective mechanisms that enable investors’ interaction, common knowledge discovery and transmission, and collective action over space and time. From this perspective, discovery and processing of entity-specific accounting information is expected to play a specific role in the making of individual expectations and related investment decisions, influencing the formation of aggregate market prices over time. According to the conceptual framework of US Financial Accounting Standards Board (FASB, CON 2—par 98), “accounting information cannot avoid affecting behavior, nor should it,” for accounting does integrate modes of management, governance, and regulation. This implies that alternative accounting representations cannot be “neutral” with respect to the underlying socioeconomic activities, i.e., they cannot rest “without influence on human behavior” (ibidem). Information set available to investors is then jointly composed by market-driven and firm-specific information. On the one side, share market (or Share Exchange) constitutes an institution that collectively generates an aggregate share price over time. On the other side, accounting system (and regulation) conveys a specific representation of corporate affairs that defines accrued performance and payments of the business entity to shareholders (influencing share investment pays-off in this way), providing a collective signal to current and prospective investors interested in trading entity shares. This concept of a dual information set expands upon semi-strong form of market efficiency (Fama 1970, 1991), which Fama and French (1992) relate to firm-specific information driven by fundamental analysis. Fama (1970) distinguishes three forms of share market efficiency depending on alternative information sets available to investors. Weak form includes only history of market prices; semi-strong form includes all publicly available information; strong form includes all publicly and privately existing information. Our approach delves into publicly available information set to disentangle two distinctive subsets: one driven by share market pricing (essentially, a history of market prices), another one comprising firm-specific information made available to investors by another institution that complements the share market itself. The first subset is generated by a price system; the second subset is provided by an accounting system of reference. This dual structure fits the duality that characterizes the share pricing process, making it dependent on a monetary and an epistemic dimension. Concerning the monetary dimension, each investor forms his own expectations (or guesses) on the dividend flow (earnings) and capital gains or losses (equity premium) from share market prices over time. Individual investor’s financial return (pay-off) depends then on the market price he may obtain by selling his shares (or the market price he should pay for buying the entity shares), and on dividend flow (earnings) that is distributed by (accrued to) share-issuing entity. This dividend flow is defined and represented by an accounting regime of reference. Concerning the epistemic dimension, individual investor’s decision-making deals with two information flows provided by distinctive institutions. One flow of information is generated by the Share Exchange and subsumed by aggregative (collective) pricing processes through time (Phelps 1987; Kirman 1999). Another flow comes from the accounting system that generates collective information from outside the market trading. Together with other institutions external to share market trading (Frydman 1982), accounting system facilitates then the working of the share market over time (Sunder 1997; Biondi 2008, 2011a, b). Following Biondi et al. (2012) and Biondi (2013), in presence of heterogeneous individual mindsets, price system and accounting system complement each other in driving share market price formation trough time. This financial system (which is no longer an equilibrium)Footnote 2 consists in, and depends upon the coherence and universal diffusion of relevant and reliable knowledge by means of both a price system and an accounting system publicly determined and announced. The current period in-between ex ante and ex post locates here among future time, submitted to individual guesses and intentions, hopes and fears, and past time, a history of reporting that, in principle, may be partly public, consistent, and conventionally agreed (Shackle 1967). In this context, accounting reporting and disclosure provide public common knowledge (Sunder 2002) through relevant and reliable signals on financial performance generated by share-issuing entity over time (entity-specific information). Share Exchange provides aggregate pricing of entity shares through trading between interacting heterogeneous investors, which are potentially informed on entity-specific information that is reported under an accounting regime of reference. Drawing upon this theoretical framework (Biondi et al. 2012; Biondi 2013), our paper introduces alternative accounting regimes that provide distinctive fundamental information signals of accounting performance. In this way, each accounting regime frames and shapes an imagined world in which investors are embedded and make share trading and investment decisions. Numerical simulation shall provide comparative assessment of aggregate performance of various financial systems submitted to these ’imagined worlds of accounting’ (Sunder 2011). A systemic statistical analysis shall then address the following three issues: market volatility, related to occurrence and impact of speculative bubbles (fluctuations) generated by herd behavior by individual investors; market exuberance, related to formation of market price fluctuations over intrinsic fluctuations driven by underlying economic fundamentals; and the statistical correlation between accounting signal series and market price series, which relates to the so-called “value-relevance” of accounting information for share investment decision-making. This latter analysis may shed light on “accounting anomalies” discovered by Sloan (1996) and recently discussed by Richardson et al. (2010) and Lewellen (2010). The rest of the paper is organized as follows. The next three sections summarize our model, which comprises alternative accounting regimes (Sect. 2); formation of individual expectations and decision-making (Sect. 3); and evolution of market clearing price generated by matching aggregate share demand and supply through time (Sect. 4). Section 5 provides numerical simulation findings of aggregate performance of financial systems under alternative accounting regimes. In particular, it investigates economic consequences of accounting systems which do either replicate information generated by the share market (so-called fair value accounting regime), or constitute an autonomous source of firm-specific information (so-called historical cost accounting regime). Theoretically informed implications and recommendations are then derived regarding cyclical effects of accounting information on share market dynamics and allocative efficiency of share market price formation (Boyer 2007; Rochet 2008), including interpretation and occurrence of speculative bubbles, as well as “value relevance” of accounting information. A brief summary concludes.",11
11.0,1.0,Journal of Economic Interaction and Coordination,20 August 2014,https://link.springer.com/article/10.1007/s11403-014-0140-6,Why a simple herding model may generate the stylized facts of daily returns: explanation and estimation,April 2016,Reiner Franke,Frank Westerhoff,,Male,Male,Unknown,Male,"In the last two decades numerous models with heterogeneous interacting agents and simple heuristic trading strategies have been designed that in this way seek to contribute to an explanation of the behaviour of financial markets.Footnote 1 Guided by questionnaire evidence (Menkhoff and Taylor 2007), this literature focusses on the behaviour of fundamental and technical traders.Footnote 2 The latter, also called chartists, employ trading methods that attempt to extract buying and selling signals from past price movements (Murphy 1999). By contrast, fundamentalists bet on a reduction in the current mispricing with respect to some fundamental value of the asset (see already Graham and Dodd 1951). Small models with extremely simple versions of these two strategies have proven to be quite successful in generating dynamic phenomena that share central characteristics with the time series from real financial markets, such as fat tails in the return distributions, volatility clustering and long memory effects. Two features are particularly useful in this respect. First, a device that permits the agents to switch between fundamentalist and technical trading, so that the market fractions of the two groups endogenously vary over time. Second, the concept of structural stochastic volatility (SSV henceforth). By this, we mean a random term that is added to the deterministic “core demand” of each of the two strategies, which is supposed to capture some of the real-life heterogeneity within the groups. Given that the two noise terms may differ in their variance, the variations of the market fractions will induce variations in the overall noise level of the asset demand, which then can carry over to the price dynamics. Several models with these features have been put forward and (partly) also successfully estimated by Franke (2010) and Franke and Westerhoff (2011, 2012a, b). The present paper reconsiders a model of this origin that emphasizes a herding mechanism. Here we wish to provide an in-depth inquiry into its dynamic properties, which takes place in the phase plane of a majority index and the asset price. Integrating analytical and numerical methods, this framework allows us to study the conditions of a stochastic switching between a tranquil fundamentalist regime of relatively long duration and a more volatile chartist regime of shorter duration. In this way, we are able to go beyond the mere observation of a simulation outcome and obtain a better understanding of why the model performs so effectively. We also take up the issue of estimating this model once again, albeit with two new aspects. First, the computation of the weighting matrix for the objective function is based on an alternative bootstrap procedure, which we have not seen applied before and which we believe is superior to the block bootstrap used in previous work. Apart from this improvement, we wish to make sure that the resulting parameter estimates are nevertheless robust. Second, complementary to the measures of a model’s goodness-of-fit discussed in other contributions, we propose the concept of a more straightforward \(p\) value. This statistic is derived from a large number of re-estimations of the model which, in particular, give us a distribution of the minimized values of the objective function under the null hypothesis that the model is true. The model fails to be outright rejected if this \(p\) value exceeds the five per cent level; and the higher it is, the better the fit. The estimation approach itself, which proves most suitable for our purpose of reproducing the aforesaid stylized facts, is the method of simulated moments (MSM). “Moments” refers to the time series of one or several variables and means certain selected summary statistics computed from them, the empirical values of which the model-generated moments should try to match. In our case, the latter have no analytical expressions but must be simulated. Hence the estimation searches for the parameter values of a model that minimize the distance between the empirical and simulated moments, where the distance is defined by a quadratic loss function (specified by the weighting matrix mentioned above). In the present context, the moments that we choose will reflect what is considered to be the most important stylized facts of the daily stock returns from the S&P 500 stock market index, in particular, volatility clustering and fat tails. After all, this is what the evaluation of the models in the literature usually centres around. It thus also goes without saying that the MSM estimation approach may equally be applied to other financial market models of a similar complexity.Footnote 3
 The remainder of the paper is organized as follows. The model is introduced in the next section. In Sect. 3 its dynamic properties are studied in the phase plane, first in a deterministic and then in the full stochastic setting. Section 4 briefly recapitulates the MSM approach, carries out the estimation on the empirical moments and then applies the econometric testing of the model’s goodness-of-fit. At the same time these computations provide us with the confidence intervals of the estimated parameters. Section 5 concludes. Several appendices are added for the discussion of finer details. Appendix 1 summarizes the value added of the present paper vis-à-vis previous work. Appendix 2 contains a few remarks on the technical treatment of our herding mechanism in the earlier literature. The mathematical proofs of two propositions in the main text are relegated to Appendices 3, 4 and 5 collect some estimation details.",53
11.0,1.0,Journal of Economic Interaction and Coordination,30 July 2014,https://link.springer.com/article/10.1007/s11403-014-0138-0,Growth in total factor productivity and links among firms,April 2016,Enrico Guzzini,Antonio Palestrini,,Male,Male,Unknown,Male,"Total factor productivity (hereafter TFP), as is well known, is defined as the part of output not explained by the inputs used in production. One of the most common procedures to measure the TFP-growth is through the so-called Solow residual. According to Solow (1957) TFP-growth is estimated by the rate of growth of output not explained by the growth of inputs. It can be considered as a measure of technical progress. About 35 years later, Kydland and Prescott (1982) introduced the idea that business cycle models and growth models can be considered together in a unified approach (Rebelo 2005). Following the Kydland and Prescott (1982) seminal paper, a huge amount of models suddenly appeared. This stream of literature was labeled Real Business Cycle theory (hereafter RBC) because of the importance of real (technology) shocks in shaping economic business fluctuations. Following the classical survey of King and Rebelo (1999) many of the most important macroeconomic indicators (consumption, investment, inventories and import) are pro-cyclical. Moreover, as concerns the persistence of economic data, the most important macroeconomic indicators (output, consumption, investment, total factor productivity) display a significant level of auto-correlation. Because of these macro properties, the literature on RBC specifies the stochastic component of the productivity shock as a first-order autoregressive process at the micro level (Prescott 1986; Plosser 1989). Standard RBC models require “large and persistent productivity shocks” (King and Rebelo 1999, p. 963). “Large” refers to the fact that the innovation component of the productivity dynamics has to exhibit a high volatility in order to explain output and consumption volatility. “Persistence” means that the parameter which links shock at time \(t\) with shock at time \(t-1\) should be close to one, in order to have a propagation mechanism of some consistence. One of the weaknesses of RBC models, probably the most important one, is the central role played by productivity shocks. From a historical perspective, as we said above, TFP-growth was considered by Solow (1957) as a measure of technical progress. According to this ‘tradition’ Prescott (1986) interprets TFP-growth as a measure of technology shock. However, the idea that the most important driving factor in explaining cycles are technology shocks gave rise to many controversies. For example, Summers (1986) and Mankiw (1989) highlight that Solow residual is a poor dummy for technology shocks and leads to excessively volatile productivity shocks. Indeed, Hall (1988) and Evans (1992) find that the Solow residual is highly correlated with, respectively, military expenses and monetary indicators which seem not to be particularly close to the technology shocks. Burnside et al. (1996) find that when Solow residual is used as an indicator of technology shocks, the probability of a technological regress in US industry sector should amount to 37 % which appears to be too high. As a consequence many authors try to find ‘endogenous’ components in the Solow residual. In order to reduce the volatility of the TFP-growth, for example, Burnside et al. (1996) consider the capacity utilization as an explanatory variable (the proxy they use for the capacity utilization is the “electricity utilization”). In this way they find that the volatily of Solow residual decreases by about 70 %. Other authors (Jaimovich and Floetotto 2008) consider the variation of the number of firms in the market as a trigger which give rise to changes in markup rates. Also in this way, technology shocks are less volatile than the Solow residual. In this paper we identify a new component which may potentially explain a part of the TFP-growth. We test the hypothesis that idiosyncratic shocks among firms may produce an amplified and auto-correlated aggregated shock because of the network structure of the firms. In other terms, these idiosyncratic shocks do not simply average out, but may be amplified by the existence of links among firms and their weights. Our work is also related to a series of papers studying the relationship between idiosyncratic shocks and aggregate volatility. Among the most recent ones, Horvath (1998, 2000) shows that aggregate volatility is generated by idiosyncratic sectoral shocks which are amplified by the interaction among sectors. Acemoglu et al. (2012) generalize Horvath’s results by analyzing which configuration of network structures of sectors tends to amplify sectoral idiosyncratic shocks, and which one tends to average them out. Differently from Horvath (1998, 2000) and Acemoglu et al. (2012) whose analysis are based on economic sectors, our paper considers individual firms as the unit of the analysis. Moreover, our work explains the way the auto-correlation of the aggregate productivity shock is affected by the network structure. Another recent paper considering shocks at the firm level is Gabaix (2011). This work shows that idiosyncratic large firms’ shocks contribute (do not average out) to the aggregate fluctuations when the distribution of firm size is fat-tailed (i.e., when few firms ‘dominate’ the economy). Differently from our paper, Gabaix (2011) does not consider, however, the network structure of firms. Finally, our paper is related with other works which use an agent-based framework to derive aggregate behavior. These papers include Delli Gatti et al. (2005) and Ormerod and Heineke (2009). The first paper builds up an agent-based model in which the interaction of heterogeneous financially fragile firms seems to reproduce many industrial dynamic stylized facts. Ormerod and Heineke (2009) develop a model of recession among interacting agents (countries). The key idea is that if the number of neighbors (weighted by their size) of a given agent are in recession, then also the agent falls into recession. In the next section, we analyze the relationship between the network structure and the aggregate productivity shock by assuming that links between firms may be represented as an adjacency matrix (matrix of links), which is an analytical tool developed in graph theory. In order to generalize the results, in Sect. 2 (Sect. 2.1) we perform a Monte Carlo simulation using random graphs adjacency matrices; i.e., graphs in which links are generated randomly with a given probability. In the following two sections, we carry out an empirical investigation with Italian micro level data. In particular, Sect. 3 describes the dataset and in Sect. 4 we carry out a panel data analysis of the statistical relationships. Section 5 concludes.",3
11.0,1.0,Journal of Economic Interaction and Coordination,10 August 2014,https://link.springer.com/article/10.1007/s11403-014-0139-z,Rule-based modeling of labor market dynamics: an introduction,April 2016,Clemens Kühn,Katja Hillmann,,Male,Female,Unknown,Mix,,
11.0,1.0,Journal of Economic Interaction and Coordination,23 September 2014,https://link.springer.com/article/10.1007/s11403-014-0141-5,Does real wage converge in China?,April 2016,Yang Chen,Hsu-Ling Chang,Chi-Wei Su,,Unknown,Unknown,Mix,,
11.0,1.0,Journal of Economic Interaction and Coordination,27 November 2014,https://link.springer.com/article/10.1007/s11403-014-0143-3,Robustness of banking networks to idiosyncratic and systemic shocks: a network-based approach,April 2016,Matjaž Steinbacher,Mitja Steinbacher,Matej Steinbacher,Male,Male,Male,Male,"We develop a network-based structural model of credit risk to examine the robustness of various banking networks to the systemic and idiosyncratic shocks, and show how the shocks propagate across the system. After the demise of Lehman Brothers in September 2008, the banking community went through really tough times that largely surpassed all expectations (Brunnermeier 2009). A disrupted mortgage market forced many banks to substantial writedowns and the consequent capital losses pushed many banks into capital shortages and the bankruptcy. In a very short period of time, the credit event spread throughout the banking system, ending in the severest crisis since the Great Depression.Footnote 1 The crisis did not only demonstrate that certain circumstances can make the banking system very fragile, but, what seems to be relevant from the scientific perspective, that a fundamentally new approach to modeling credit risk is needed, which would adequately assess the systemic complexity and the network effects. The counterparty risk, in particular, was not assessed properly; it was missed. Yet, the severity of a crisis was caused by a combination of the fall in the housing prices and the subsequent cascading failures. This has given the credit risk analysis a new perspective. For good reason. Credit risk can be defined as a risk of unexpected changes in the credit quality of the entity and ranges from the changes in credit ratings and variations in credit spreads to the default event. Starting with the early structural and reduced-form models (Merton 1974; Black and Cox 1976; Vasicek 1977; Jarrow and Turnbull 1995, Duffie and Singleton (2012) for an overview of these models), the credit risk research has proceeded towards the network-based models which focus on the microscopic perspective (Leitner 2005; Egloff et al. 2007; Nier et al. 2007; Gai and Kapadia 2010; Gai et al. 2011; Haldane and May 2011; Ladley 2013; Montagna and Lux 2014; Sachs 2014; Steinbacher et al. 2014).Footnote 2 A fundamental feature of this network-based approach is that it attributes a great part of the credit risk to counterparty risk and the externalities which come from the network of interconnected banks. Each bank is a single entity within the banking system. It is defined by the node which represents its balance sheet. Banks are then connected with one another with a set of directed and weighted links, representing mutual exposures. The resulting banking network is a highly complex and intertwined system of interdependent balance sheets (Allen and Gale 2000; Furfine 2003; Boss et al. 2004; Dasgupta 2004; Cifuentes et al. 2005; Lelyveld and Liedorp 2006; Degryse and Nguyen 2007; Iori et al. 2008; Allen and Babus 2009; Schweitzer et al. 2009; Huang et al. 2013; Fricke and Lux 2013; Craig and Peter 2014; Fricke and Lux 2014). This implies the endogenous nature of the credit risk analysis, which for single banks evolves conditionally independent; subject to the bank’s microspecifics within the system’s structure and credit positions of counterparty banks. The paper contributes to the literature on interaction-based methods in economics and finance.Footnote 3 Specifically, it integrates a structural credit risk model, originally proposed by Merton (1974), and social networks. We construct a banking network from heterogeneous banks, which differ in size and the balance sheet structure. Banks possess various types of assets, while the liabilities side of the balance sheet consists of the bank Tier 1 capital and other liabilities.Footnote 4 We employ a solvency principle and assume that a bank defaults when its capital is wiped out. An idiosyncratic shock is represented as a sudden default of an individual bank, while the systemic shock is represented as a reduction in housing values by a certain percent. A principal characteristic of the systemic shock is that it is widespread and not bound to a single bank. All shocks are one-time events and to banks unexpected events. Any loss directly reduces the level of the bank capital. A bank defaults when its capital is wiped out. After a bank defaults, the counterparty banks are repaid the recovery rate (RR) proportion of the exposure at default (EAD). Bank defaults impose negative externalities to the rest of the banking system through the increased counterparty risk. We assume that troubled banks cannot raise additional capital or be bailed out. In addition, credit events do not exacerbate uncertainty or loss of confidence or a panic. An \(\alpha \)-criticality index is then constructed to evaluate the cost of the shocks to the banking system. The index measures the proportion of defaulted assets (capital) of the system caused by the shock. Through a series of simulation runs, we show that the stability of the banking system is determined by a combination of systemic and bank-specific components. We demonstrate that idiosyncratic shocks cannot substantially disturb the system. On the contrary, systemic shocks of even moderate magnitudes are likely to become contagious and can be highly detrimental. This means that a severe crisis has to be initiated by a systemic shock of at least moderate magnitude. The result is similar to Gai and Kapadia (2010), who argue that, generally, the banking system is not very susceptible to contagion although the effects can be widespread when problems occur. As the number of failed banks increases, the banking system’s stability tends to decrease disproportionately. We have shown that as the shock magnitude increases, the post-shock time until the banking system collapses shortens. Further, we show that the structure of the banking network may have a significant impact on the propagation of a shock across the network and the consequences it produces. Capital ratio and recovery rates are two additional factors that contribute to the stability of the banking system. When the system hits the critical point, even small changes in parameter values can induce large changes in shock consequences. Finally, we have found a substantial nonlinear character of the shock propagation. Methodologically, the network-based approach is similar to epidemic models in which the state of a node progresses between different types, from susceptible to infected and then to either recovered, immune or ceased (Pastor-Satorras and Vespignani 2001 and Newman 2002). The two models differ in consequences which are induced by connectivity. In a banking network, connectivity can work either contagiously or as a channel of risk-sharing, while the epidemic networks generally do not have a risk-sharing potential. The remainder of the paper is organized as follows. The model is presented in Sect. 2 and the data in Sect. 3. The results are presented in Sect. 4 and discussed in Sect. 5. Section 6 concludes.",12
11.0,1.0,Journal of Economic Interaction and Coordination,21 January 2015,https://link.springer.com/article/10.1007/s11403-015-0144-x,International automotive production networks: how the web comes together,April 2016,Leticia Blázquez,Belén González-Díaz,,Female,Female,Unknown,Female,"Trade theories based on New Economic Geography (NEG) have stressed the importance of taking into account the interconnection of regions in the analysis of the spatial distribution of economic activities. They highlight how any change that directly involves only two regions are unlikely to leave the remaining regions unaffected. Additionally, NEG has emphasized that the relative position of the region/country within the whole network of interactions is a key issue faced by firms when choosing where to locate and thus also influences the way they organize their production, management and outsourcing (Krugman 1993; Thomas 2002; Fujita and Thisse 2009). One of clearest signs of how the complex interactions between countries and regions influence the way companies organize their production, and vice-versa, has been the emergence of international production networks in most manufacturing sectors. In this sense, the main objective of this paper is to study the structural evolution of the spatial distribution of economic activities for one of the sectors with the highest incidence of global production sharing—the automotive industry, in the light of some of the assumptions and results obtained from New Economic Geography. As an alternative to more traditional theoretical models, in this analysis we will use an empirical approach by applying the methodology proposed by Social Network Analysis. The extent to which these sharing strategies have been implemented in the auto industry is reflected by the spectacular increase in the world’s automotive trade in general during the past decade (from 1996 to 2009, it grew at a cumulative annual rate of 5.4 % in nominal terms) and by the particularly high dynamism of its intermediate commodity flows, with an annual cumulative growth rate of 6.2 % (almost 2 percentage points more than final goods), increasing its share of the overall world auto trade from 50 to 56 %. These highly intense international trade flows give us a clear idea of the complexity of the auto industry’s organization demonstrating the relevance of considering its networks characteristics when analysing the sector. According to NEG, the spatial distribution of economic activities can be viewed as the outcome of a complex balance between two types of opposing and mutually reinforcing forces: agglomeration (or centripetal) and dispersion (or centrifugal) forces (Baldwin et al. 2003; Fujita and Thisse 2009).Footnote 1 In this regard, since the automotive industry can be considered fragmented and to operate under increasing returns to scale in a globally imperfect competitive market, the location of auto companies across space should presumably be explained primarily in terms of the search for privileged access to large, wealthy markets and the desire to relax the competitive pressures imposed by other firms. Therefore, we expect larger economies in terms of population and purchasing power to attract a more than proportional share of auto companies; in other words, for the home market effect to be one the most significant agglomeration forces in the conformation of the auto network. Intermediate transportation costs incurred by the auto industry would lead firms to seek to locate close to their end markets, thus reinforcing the market effect. As a result, we also expect deeper integration within the auto network to actually lead to more regional imbalance in the spatial distribution of the industry and for some degree of economic specialization to arise. Additionally, in a highly fragmented sector like the auto industry, the presence of input-output linkages between firms is expected to be one strong agglomeration force (Krugman and Venables 1995). On the other hand, traditional auto markets are fairly mature and the competition between companies in their territories could be labelled as fierce. Therefore, we would expect a dispersion force generated by each firm’s desire to avoid market crowding (corresponding to the price effect in spatial competition) to emerge. Fostered by this dispersion force, some firms would relocate from the traditional core to the periphery, yielding a bell-shaped curve of spatial development à la Kuznets. In order to evaluate the extent to which the evolution of the auto industry structure is in accordance with these assumptions set by the NEG, we will apply Social Network Analysis tools and graph theory to the world trade web rather than the traditional general equilibrium models applied by the NEG (e.g. Venables 1996; Venables and Gasiorek 1999). The reason is that, although the contribution of NEG to introducing spatial factors into rigorous models is unquestionable, its own precursors have pointed out that these models are too stylized and simple to adequately represent the real economic geography and thus become relevant from a policy-making standpoint (Krugman 2000, p. 59). In analysing these cross-border production blocks, numerous factors should be taken into account. It is not only a question of intensifying the openness of countries, but also of developing networks of direct and indirect relations between individuals, companies and countries at a distance from each other (Arribas et al. 2009). The network approach is an appropriate method for studying such issues as it provides a methodology that enables us to measure the nature of these relations, and how and to what extent they evolve over time in ways that other measurements do not capture. By applying network analysis to international trade we can complement other empirical analyses of trade that put countries’ characteristics at the forefront (e.g. the gravity model of international trade), since it places more emphasis on the relationship between units in the graph and on the structure of the system itself than on the units’ attributes, which are generally left in the background. Although these techniques have not been very extensively used in economics to date, the approach is not new in international economics (see, for instance, the recent paper by Chinazzi et al. 2013) and specifically in trade analysis. Recent examples include Garlaschelli and Loffredo (2005), Kali and Reyes (2007), Kali et al. (2007), Arribas et al. (2009), Fagiolo et al. (2008), Fagiolo (2010), and De Benedictis and Tajoli (2011). All of these studies focus on analyzing the world trade network and accurately analyze the properties of the system in terms of trade flows, partners and links. Most of them suggest that there is strong heterogeneity between countries, with nations playing very different roles in the network structure, but there is only a very limited effort in these studies to explain these findings on the basis of trade or location theories. Moreover, none of them analyzes specifically the characteristics of sharing production networks nor considers the differentiated features of sectors which use these strategies as we do in this paper by focusing on one specific sector. The structural characteristics of the World Automotive Trade Network have been analyzed in several meritorious studies (e.g., Humphrey and Memedovic 2003; Sturgeon et al. 2008, 2009; Amighini and Gorgoni 2013; Sturgeon and Van Bieserbroeck 2010; Sturgeon and Van Biesebroeck 2011). However, most of these papers do not offer an empirical analysis with a solid analytical framework that makes it possible to discover stronger affinities with trade theories and international economics. In this sense, this paper complements these more descriptive contributions. The article is organized as follows. In Sect. 2, we briefly explain the main network analysis tools and the data source used in the research. Section 3 describes the evolution of structural features of the World Automotive Trade Network and analyzes the agglomeration and dispersion forces behind this evolution. Section 4 offers some concluding remarks.",9
11.0,1.0,Journal of Economic Interaction and Coordination,29 January 2015,https://link.springer.com/article/10.1007/s11403-015-0145-9,A case study for a new metrics for economic complexity: The Netherlands,April 2016,Andrea Zaccaria,Matthieu Cristelli,Luciano Pietronero,Female,Male,Male,Mix,,
11.0,2.0,Journal of Economic Interaction and Coordination,02 September 2016,https://link.springer.com/article/10.1007/s11403-016-0179-7,Introduction to the special issue,October 2016,Einar Jón Erlingsson,Marco Raberto,Hlynur Stefánsson,Male,Male,Male,Male,,
11.0,2.0,Journal of Economic Interaction and Coordination,20 May 2015,https://link.springer.com/article/10.1007/s11403-015-0159-3,"What does the financial market pricing do? A simulation analysis with a view to systemic volatility, exuberance and vagary",October 2016,Yuri Biondi,Simone Righi,,Male,Female,Unknown,Mix,,
11.0,2.0,Journal of Economic Interaction and Coordination,09 June 2015,https://link.springer.com/article/10.1007/s11403-015-0162-8,Toward an understanding of market resilience: market liquidity and heterogeneity in the investor decision cycle,October 2016,Richard Bookstaber,Michael D. Foley,Brian F. Tivnan,Male,Male,Male,Male,"Just as the day-to-day movement of prices gives little insight into the potential for the dislocations that emerge during a fire sale, so also the day-to-day liquidity of the market gives little indication for the drying up of liquidity that often occurs during those events. Rather than lower prices eliciting greater supply, it can seem as if supply is not forthcoming at any price. Indeed, it often seems to be the contrary; the rapid drop in prices as those who face the urgent need to liquidate try to elicit the other side of the trade actually reduces the supply in the market. The key feature of the market that leads to illiquidity during times of urgent demand is that the frequency with which agents arrive at the market is heterogeneous among market participants. Some agents, such as high frequency and statistical arbitrage traders, are continuously engaged in the market and can take action very quickly based on changes in prices. Others, such as longer-term fundamental investors, will not enter the market immediately in reaction to changes in the market price because they have a slower decision cycle. Due to their approach to risk taking or simply their institutional structure, their decision cycle might involve consulting with others in their firm or reformulating a broader investment plan before taking action. Furthermore, as a practical matter, these longer-term investors tend to be the ones with the deepest pockets in terms of liquidity supply. Because of the heterogeneous arrivals, and more to the point because the liquidity demand has a greater frequency of arrival than does the deep-pocket supply, liquidity will appear to have dried up during periods of high immediacy. Impatience reduces the effectiveness of price as a signal. What is more, the very attempt to use prices as a signal can lead to perverse results. When those with a lower frequency finally do arrive, the price might have dropped to the point that they are not willing to supply liquidity without regrouping, reducing their frequency of participation as they huddle with their analysts to assess the market action and become more timid with the liquidity they do supply. The heterogeneous decision cycle dynamic was evident during the October 19, 1987 market crash (Presidential Task Force on Market Mechanisms 1988). Early on that Monday morning, computer-generated sell orders from portfolio insurance programs hit the futures market. Program traders then bought these positions in the index and simultaneously put in orders to sell the underlying stocks, leading to a flood of sell orders into the New York Stock Exchange. But, as described in Bookstaber (2007), the heterogeneity of the decision cycle for those in the futures pit and those operating in the stock market led to illiquidity: “The specialists at the NYSE tried to elicit more buyers by dropping the price, but there was a limit to how much more buying interest they could attract. No matter how quickly the price was dropped, the decision made by the equity investors took time; unlike the twitch-quick futures pit traders, they made portfolio adjustments only after reasoned consideration\(\dots \) The root dynamic was what I call time disintermediation—the time frame for being able to do transactions in the futures market was substantially different from the time frame in the equity market.” The heterogeneity of decision cycles led to a similar result during the Flash Crash on May 6, 2010 (U.S. Commodity Futures Trading Commission and U.S. Securities and Exchange Commission 2010). As confirmed by Kirilenko et al. (2011), the replenishment of the order book could not keep up with the essentially instantaneously triggered market stop loss orders, even with the microsecond speed of the high frequency traders. And, once again, the heterogeneity of the decision cycle came with a one-two punch: first prices dropped precipitously as market stop loss orders ran through the order book, and then, with the prices in disarray, many liquidity providers pulled away from the market to recalibrate their market activity, leading prices to continue their cascade, in some cases to absurd levels. As Kirilenko et al. (2011) concluded, while heterogeneity in investors’ decision cycles did not trigger the Flash Crash, that heterogeneity certainly exacerbated market volatility. 
Duffie (2010) presents a model highlighting the impact of inattentive investors with particular interest on its implications for the 2008 financial crisis. The effect of heterogeneous decision cycles is like that of the inattentive investors in Duffie’s model; in either case price signals will fail to transmit to a subset of market participants who are not actively in the market at the moment the price signal is made. As Duffie shows, this will have implications for price dynamics even during typical day-to-day market activity. But it will have particularly adverse effects during times of forced selling. If the liquidity demander takes the lack of response to his price concessions as being an indication that the price level is still too high rather than understanding that the lack of response is due to a lack of attentiveness or a lag in the ability of the other side of the trade to make a decision, it will lead to a further drop in prices that in both unnecessary and ineffective because those who might be willing to take on the other side of the trade might literally be out to lunch. As observed by Bookstaber (2007), referring to the 1987 market crash, “Supply dried up because of the difference in time frames between the demanders and suppliers. By the time equity investors could have reacted to the prices and done some bargain hunting the specialists had moved prices so precipitously that these potential liquidity suppliers were scared away. The key culprit was the difference in liquidity because of the different trading time frames between the demanders and the suppliers in the two markets. If the sellers could have waited longer for the liquidity they demanded, the buyers would have had time to react and the market would have cleared at a higher price.” The objective of this paper is to detail the development and testing of a model which can capture essential characteristics of asset price dynamics through the use of heterogeneous decision cycles that range along the spectrum from intra-day traders to the long-term, fundamental investors. These market dynamics fit a number of important stylized facts and are particularly important in understanding the drying up of liquidity and resulting cascades that occur during market breaks and crises along the lines of events like the 1987 Crash and the 2010 Flash Crash, and similar events such as the LTCM-related fire sale in late 1998, and the August, 2007 “Quant Quake”.",8
11.0,2.0,Journal of Economic Interaction and Coordination,19 May 2015,https://link.springer.com/article/10.1007/s11403-015-0160-x,Phase transition in the S&P stock market,October 2016,Matthias Raddant,Friedrich Wagner,,Male,Male,Unknown,Male,"In this paper we analyze the structure of the U.S. stock market. We show that the influence of stocks on the market is changing and that this influence is related to trading volume and the stocks’ betas. The analysis of the structure of stock markets is dominated by two research approaches. The first one tries to explain the differences between the rates of return of stocks and relates to the seminal work by Lintner (1965) and Sharpe (1964) and the CAPM model. The second approach takes the investor’s point of view, and is hence mostly focused on the choice of a portfolio and the analysis of risk. Both are related by the need to evaluate the comovement of stocks with each other and some index or market proxy. The original version of the CAPM is in fact a one-factor-model, which postulates that the returns \(r_i\) of the stocks should be governed by the market return \(r_M\) and only differ by the an idiosyncratic component \(\beta _i\) for each stock \(i\), such that In this setting \(\alpha \) can also be interpreted as the risk free rate of interest. Hence, stocks differ by the amount of volatility with respect to the market, and economic rationale necessitates that higher stock volatility is compensated by higher absolute returns. Empirical tests of this model had rather mixed results and have let to the conclusion that beta values are not constant but time-varying, see Bollerslev et al. (1988). The Fama and French (1992) model extends this approach to a three-factor model, incorporating firm size and book-to-market ratio. Several other extension of the original models have been suggested, mostly building on some kind of conditional CAPM, where the entire model follows a first-order auto-regressive process, see Bodurtha and Mark (1991). The reasons for the change of the betas are manifold. They could change due to microeconomic factors, the business environment, macroeconomic factors, or due to changes of expectations, see, e.g., Bos and Newbold (1984). Adcock et al. (2012), Harvey and Siddique (2000) and Plerou et al. (2002) have discussed that the non-normality of stock returns, conditional skewness, and mainly the long memory in returns can lead to distorted estimations of the CAPM. There is also a strand of literature that tries to capture the effects of heterogeneous beliefs of investors in a CAPM framework, e.g., Brock and Hommes (1998) and Chiarella et al. (2010). In order to manage the risk of a portfolio, one can derive optimal portfolio weights from the spectral decomposition of the covariance matrix of stock returns. Many studies show that the non-normality of stock returns can lead to an under-estimation of risk. A common way to describe the properties of stock comovements is to look at the eigenvalue spectrum of the correlation matrix. Random matrix theory suggests that a market that behaves like a one-factor model should result in one dominant eigenvalue. Both, the non-normality in the data and any influence from other factors will result in deviations from this simplified model, see Citeau et al. (2001) and Livan et al. (2011). Approaches which utilize the spectral properties of correlations matrices have their limits once the number of variables becomes large relative to the number of observations. Networks approaches, which derive dependency networks from the correlation matrix can be useful, as long as one does not need explicit portfolio weights for each single stock, see, e.g., Kenett et al. (2012a) and Tumminello et al. (2005). A related approach is to try to identify different states of the stock market, either by an analysis of the correlation matrix like in Münnix et al. (2012), or by the analysis of transaction volumes as in Preis et al. (2011). Recent studies like Kenett et al. (2012b) show that the correlation structure in stock markets are rather volatile, and partly mirror economic and political changes. Kenett et al. (2011) for example shows that a structural break seems to happen in the U.S. market around 2001. This strand of literature is also related to approaches from econometrics. Beine and Candelon (2011) for example argue that correlations increase in times of crisis, which has profound implication for portfolio choice and hedging of risks. Other studies like Ahlgren and Antell (2010) analyze if correlations in and between markets have increased due to more openness and tighter economic relations between countries. Since financial markets tend to react very fast on any change in the economy, but also inhibit a lot of noise, we found that a look at longer time horizons is a worthwhile contribution to the field, since many of the above mentioned studies look at time horizons of months or a few years. We found that the S&P 500 contains around 170 stocks with a history of price quotes of 25 years (the number drops rapidly with much more than this 25 years). We analyze the long-run development of the stocks influence upon the market. We derive both, a market index and the stocks’ influence, from the spectral decomposition of the covariance matrix. We show that for most of the period under consideration the market was in a ordered state, characterized by a disproportionate influence of stocks from the IT sector. While some changes in the market seem to happen in 1995, the collapse of this regime starts with the burst of the dot.com bubble. A disordered state is found after 2005. We will show that from here the market develops into a new (although weaker) ordered state, where the driving sector is the financial industry. The paper is organized in the following way. In Sect. 2 we describe the subset of stocks in the S&P market used in this analysis. Section 3 contains the definition of market indices derived from the covariance matrix. In Sect. 4 we explain why we prefer the latter over the usually applied correlation matrix, and that the average return \(r_{av}\) and market return \(r_M(t)\) may be almost equal when calculated from a large number of stocks. Section 5 contains our results for the phase transition and Sect. 6 some conclusions.",3
11.0,2.0,Journal of Economic Interaction and Coordination,22 July 2015,https://link.springer.com/article/10.1007/s11403-015-0163-7,Too dynamic to fail: empirical support for an autocatalytic model of Minsky’s financial instability hypothesis,October 2016,Nataša Golo,David S. Brée,Sorin Solomon,Female,Male,Male,Mix,,
11.0,2.0,Journal of Economic Interaction and Coordination,16 May 2015,https://link.springer.com/article/10.1007/s11403-015-0154-8,"Endogenous fluctuations, markups, capacity and credit constraints",October 2016,Piero Ferri,Annalisa Cristini,Anna Maria Variato,Male,Female,Female,Mix,,
11.0,2.0,Journal of Economic Interaction and Coordination,07 February 2015,https://link.springer.com/article/10.1007/s11403-015-0147-7,How much should an investor trust the startup entrepreneur? A network model,October 2016,Anna Klabunde,,,Female,Unknown,Unknown,Female,"Deciding whether or not to become an entrepreneur is a situation under true–Knightian–uncertainty: There is no way to quantitatively assess one’s chances of success because by definition an innovation creates a new market situation for which no data is available. Often, an entrepreneur has to obtain external finance, a common source being the market for business angel investment. From an investor’s perspective, the uncertainty he faces regarding a potential startup investment is even larger. To the same extent as the entrepreneur, he is confronted with uncertainty about the demand for the new product. In addition to that, an investor is confronted with uncertainty regarding the entrepreneur’s characteristics: skills and capabilities that have an impact on his market success, as well as his personality, which can ease or complicate the investment relation. This comes down to a principal-agent relationship: The investor wants to increase his income not by founding a firm himself, but by delegating this to a startup. He can only hope that the entrepreneur will act in his best interests. Here, the investor faces both adverse selection—the entrepreneur himself knows his abilities better than the investor does and might have an incentive to deceive him—and moral hazard—the entrepreneur might exert an effort that is just enough to satisfy his own goals, but not necessarily to maximize the investor’s income.Footnote 1
 How do business angel investors decide whether or not to make a particular investment? Lewis and Weigert (1985) relate situations of high complexity to the need for trust as a substitute for a rational probabilistic decision rule: “Trust begins where prediction ends.” (p. 976). Wong et al. (2009), Sudek (2006), Kelly and Hay (2003) and Prowse (1998) indeed find that trust is one of the most important determinants of investment decisions in business angel finance, whereas Bottazzi et al. (2011) show empirically that trust matters even for the provision of venture capital. I therefore model an agent-based angel investor market where sufficient trust is a precondition for investment, but where investors are otherwise as rational as possible given the available information. Initial trust depends on a distance measure, but then trust is updated based on received returns only. Investors communicate with other investors in a network-like structure. Comparing returns with others, investors determine what level of returns constitutes an acceptable level for them. If this level is not reached, an investor becomes disappointed by an entrepreneur and his trust in him decreases. Entrepreneurs are homogenous, apart from an idiosyncratic productivity shock each period. They produce according to a linear production function that takes as inputs capital provided by investors and an entrepreneur’s own capital contribution that is paid out of savings. Each period entrepreneurs decide how to use their revenue. First, they have to pay back what they received from the investor. Then, they can use the remaining profit to pay an interest rate to the investors in the hope of attracting more funds in the future, or they can save it and invest in the firm themselves next period, or they can add the profit to their private wealth. They use heuristics to determine what proportion of their profit is returned as interest, what proportion is invested, and what proportion is added to their private wealth. Investors are assumed to adjust their trusting behavior so that their returns are maximized. It is often found that business angel investors follow altruistic motives as well, such as wanting to help young entrepreneurs with their own expertise. Furthermore, the exit options of a particular investment–i.e. whether the startup firm will eventually go public so that the angel can easily sell his shares–play an important part in making an investment choice as well (Sudek 2006). Angel investors are often also more fully involved in a firm’s monitoring than other types of investors, while they exert control through contractual and factual veto or approval rights rather than through board seats (Wong et al. 2009; Kelly and Hay 2003). A fuller analysis of the business angel market would have to include these behaviors as well. For the sake of simplicity, tractability and generalizability of the model I refrain from modeling these aspects here and stick to return maximization under imperfect information. I therefore investigate what an optimal trusting strategy is if all investors behave in the same way. In a second step I study whether it is rational for an individual investor to deviate from this strategy by trusting more, or less. Relevant decision parameters are the following: First, investors decide how easily they are disappointed, i.e., how tolerant they are if the return from one of the entrepreneurs they invested with is lower than the average return the other investors received (disappointment threshold). Second, they decide how many disappointments they are willing to accept before they end the relation with an entrepreneur (trust decrease). The model was calibrated so that it roughly fits criteria of the US business angel market. However, the model could easily be calibrated to match other settings, or it could be extended to model e.g. a banking network based on trust. The paper is structured as follows. Section 2 provides a more detailed explanation of the trust concept used here and a literature review. Section 3 introduces the model and its calibration. Section 4 presents results, Sect. 5 concludes.",7
11.0,2.0,Journal of Economic Interaction and Coordination,23 January 2015,https://link.springer.com/article/10.1007/s11403-015-0146-8,"Social norms, costly punishment and the evolution of cooperation",October 2016,Tongkui Yu,Shu-Heng Chen,Honggang Li,Unknown,Unknown,Unknown,Unknown,,
11.0,2.0,Journal of Economic Interaction and Coordination,04 March 2015,https://link.springer.com/article/10.1007/s11403-015-0151-y,Hierarchical economic agents and their interactions,October 2016,Ted Theodosopoulos,,,Male,Unknown,Unknown,Male,"The primary goal of this paper is to describe the potential of a new economic modelling environment, populated by multi-layered agents, hierarchical objects that probe the boundary between individual and group, institution and society. Bypassing questions of aggregation, the proposed paradigm seeks coordination through hierarchical, heterogeneous agents, influencing one-another through their opinions and actions. Importantly, the agents’ limited rationality permits pockets of inconsistent allegiances to percolate through their interaction network. The proposed modeling environment extends work over the past decade on agent-based models of the economy (Johnson et al. 2003; Horst and Rothe 2008; Brock and Hommes 1998; Ghoulmie et al. 2005). Progressively, such models have shown how heterogeneities in the agents’ endowments, preferences and interactions can persist and lead to observable deviations from the efficient market hypothesis, a collection of so-called stylized facts (Gabaix et al. 2003; Farmer et al. 2004; Gillemot et al. 2006; Bouchaud et al. 2002; Potters and Bouchaud 2003; Alfarano et al. 2005). The extension proposed here invites us to broaden our notion of heterogeneity to encompass attributes that aren’t reducible to individuals, but instead arise at different levels of aggregation. But instead of seeking to extract them from properties of the individuals, we posit them as part of the evolving state of ‘meta-individualist’, multi-layer agents that populate the economy (Horst 2010; Tedeschi et al. 2009). To illustrate this broader modelling paradigm, we proceed to extend a specific agent-based model of the economy, first proposed by Bornholdt and subsequently studied both numerically and analytically by different authors (Kaizoji et al. 2002; Theodosopoulos 2005). This model is based on an interaction potential that trades off two components, the desire to belong to a local majority and simultaneously to the global minority. These two terms are balanced by a coupling constant
\(\alpha \). The study of the statistical mechanics of this model led to the identification of an explicit phase transition (Theodosopoulos 2005), controlled by \(\alpha \), whereby sufficiently strong coupling leads to non-self-averaging behavior (Aoki 2004, 2008) and persistent opinion mixing. Furthermore, this framework allowed us to identify a fundamental, irreducible limit to the observability of various measures of excess demand (Theodosopoulos 2005). In the original Bornholdt model, states of the economy were represented by spin configurations of a fixed lattice, or network more generally. Configurations of this kind can be denoted by vectors of \(-1\)s and \(+1\)s, \(\eta \in \{-1,+1\}^N\), for some \(N\). These vectors are then propagated following a Markov process, driven by an interaction potential, \(h(\eta )\). The resulting stochastic evolution is seeking minimum energy states, which represent equilibria. This minimization is controlled by a ‘temperature’ parameter, in an analogy to the simulated annealing process of non-equilibrium statistical mechanics. At high temperatures, state transitions are largely random. As the temperature is lowered, transitions that locally reduce the interaction potential are progressively favored. In the frozen phase, the system picks out some equilibrium state, and is subsequently trapped there. This framework is often interpreted as describing the evolution of individual agents, represented by the different sites on the lattice or network, with the spins at each site denoting the evolving opinions or actions (buys vs. sells) of the agent on that site. However, such an interpretation, which attempts to reduce the resulting market dynamics to the interactions of individual agents, has come repeatedly under fire, from different perspectives (Kirman 1992; Horst 2010). Most problematic, from the point of view of the current paper, is the inability of this framework to reproduce any of the myriad intermediate structures, from coalitions to firms, that populate the real economic landscape. More precisely, present instantiations of this paradigm reserve heterogeneity for individual agents, relegating any higher structures to the realm of transient epiphenomena. Here, we propose to extend the standard spin market framework in an explicit effort to bring out the irreducible relevance of structures in the economy. We choose to see these intermediate structures as endowed properties of the economic state, largely indecomposable to their constituents, albeit spontaneously evolving, in interaction among themselves and their constituents. To help visualize our proposed scheme, we propose the following abstraction: agents are analogous to simplicial complexes in topology, consisting of locally matching components of different dimensionality and degree of complexity (Singer and Thorpe 1976). Such an object is generally indecomposable to a listing of its constituents. Instead, it depends crucially on details of the ‘gluing map’ that put it together. Extending the analogy further, we posit a generalized interaction potential, which allows such hierarchical objects to ‘act’ on one-another, without this action being describable as the interaction between individual components, e.g. an edge interacting with an edge or a tetrahedron interacting with a triangle. The goal of this work is to introduce this new modeling paradigm, in which the agents and the network on which their opinions evolve are indissolubly coupled. Unlike earlier agent-based studies of economic interactions, we don’t attempt to generate a price process that can be calibrated against empirical statistics. The translation of agents’ opinions to observable aggregates depends sensitively on market microstructure, from details of the double-auction to explicit market making (Chiarella et al. 2009; Tedeschi et al. 2009; Alfarano and Lux 2007; Benediktsdottir 2006). Instead, the current work focuses on the rich array of stochastic convergence effects that arise in models of heterogeneous economic agents, particularly when we endogenize the evolution of the interaction network (Föllmer et al. 2005; Cirillo et al. 2014). Specifically, while much emphasis has been placed on conditions for guaranteeing ergodicity (Barnsley et al. 1988; Steinsaltz 1999), the focus here is on the effective lack of ergodicity under certain parametric regimes for our model, and its economic consequences, both theoretical and empirical. Along the way, we introduce techniques from symbolic dynamics and the spectral analysis of Markov chains to enrich the economic toolkit. We begin with an introduction to our hierarchical agent model, including a description of the interaction potential that couples the spin configurations on the nodes and arcs of our evolving network. This interaction potential drives a Markov process whose hypergeometric state transitions are described in Sect. 3, along with some sample paths that hint at the non-ergodic behavior we are after. The following section introduces the contingent submartingale representation, a technique that allows us to extract a deterministic skeleton underlying our stochastic dynamics. We then proceed to investigate the invariant measure of the Markov process and its sensitivity to the model’s parameters. The discrepancies between the limiting distribution and the deterministic attractors we identified earlier leads us to pursue a spectral analysis of the underlying Markov chain, which uncovers and quantifies a source of persistent path dependence. The paper concludes with a set of phenomenological conjectures that govern the paths of our hierarchical agent model, as well as a putative socioeconomic interpretation (Cioffi-Revilla 2010) for the three distinct dynamic regimes that the model exhibits. All along, we relegate the more technically demanding details of our exposition to an appendix.",1
12.0,1.0,Journal of Economic Interaction and Coordination,21 February 2015,https://link.springer.com/article/10.1007/s11403-015-0148-6,Scale-free distribution as an economic invariant: a theoretical approach,April 2017,Anindya S. Chakrabarti,,,Unknown,Unknown,Unknown,Unknown,,
12.0,1.0,Journal of Economic Interaction and Coordination,26 February 2015,https://link.springer.com/article/10.1007/s11403-015-0149-5,Heterogeneous wage setting and endogenous macro volatility,April 2017,Orlando Gomes,,,Male,Unknown,Unknown,Male,"Empirical evidence does not corroborate the idea that exogenous shocks affecting technology or preferences occur with the frequency and with the intensity that are required to generate the type of business cycles that are observed in practice. The values of the main economic aggregates change persistently over time and these variations are hardly compatible with the sporadic occurrence of shocks that by hypothesis hit an otherwise inherently stable economy. Kocherlakota (2010) highlights this point and concludes that one needs to look beyond exogenous shocks to explain the true foundations of short-term economic fluctuations. Without dismissing the role that shocks over fundamentals undeniably have in partially explaining observed fluctuations, macro literature began, in recent years, exploring other pathways. The focus has been re-centered on the local interaction across individual agents and on the psychological drivers of human action, features that once introduced in canonical macro models are likely to turn fluctuations into an intrinsic characteristic of the economic system. This paper designs and develops a macroeconomic model that integrates and combines behavior heterogeneity, decentralized decisions, local interaction, evolutionary rules and animal spirits in a simple theoretical structure, with the objective of explaining the component of business cycles that does not depend on shocks to fundamentals. The model will be able to address how endogenous business cycles are formed and perpetuated over time. In this study’s framework, the source of heterogeneity is wage setting. Producers of different varieties of an intermediate good may choose one of two strategies to set wages. Some intermediate producers will choose, after negotiating with workers’ representatives, to set an efficiency wage, such that the real wage depends positively on the economy’s employment level. The adopted efficiency wage rule is adapted from Mankiw and Reis (2003). The other group of firms also considers efficiency wages, but these are just one of the components of the selected target wage; the other concerns an inertia element, according to which to some extent the current wage is indexed to the wage of the previous period. There will be, in this way, a component of wage stickiness, which captures the existing empirical evidence on wage setting rigidity (see Sect. 2.2 for a brief discussion on this empirical evidence). After describing the macroeconomic scenario and having highlighted the distinct approaches to wage setting, one must introduce the interaction rules that potentially lead to a systematic change of perspective by firms. These rules combine the direct interaction across agents with a global assessment of which of the rules conducts to a most profitable decision. Animal spirits are incorporated into the analysis by assuming that each follower of one of the strategies, the efficiency wage strategy, can fall in one of two distinct categories: enthusiastic and non-enthusiastic adopters of the efficiency wage rule. Therefore, firms are separated into those that do not follow the pure efficiency wage rule and those which intending to follow it may adopt an enthusiastic or a non-enthusiastic perspective about it. These differences on perspective give place to differences on behavior and to distinct reactions in the contact with others, generating an interaction process that for reasonable parameter values implies a perpetual cyclical motion on the trajectories of the output gap and of the inflation rate. In this framework, the values of macro variables fluctuate independently of shocks to fundamentals. They change as firms acquire distinct perceptions about the best strategy for setting wages. In some time periods, pure efficiency wages are preferred by the majority (in this case, the output gap approaches its steady-state value); in other time periods, the strategy involving sticky wages dominates (in this case, the output gap departs from its long-term equilibrium level). It is important to highlight that despite of the above reasoning, the intention of this study is not to rule out external shocks as a source of business cycles. Eventual disturbances over fundamentals are of primary importance to determine aggregate fluctuations. Nevertheless, one intends to emphasize that there is an endogenous component on the trajectories followed by macro variables, that must be dissected in its causes and consequences. The regular cycles that the mechanics of the model will generate constitute a more plausible foundation over which one can add exogenous shocks than the trivial fixed-point steady-state outcome of the deterministic version of conventional macro models. In this way, it is possible to claim that observable irregular business cycles are the result of two main forces: (i) unpredictable and random motion originating in stochastic shocks, and (ii) an underlying propensity of the economic aggregates to display cyclical motion, due to local interaction among heterogeneous players. The adopted source of fluctuations is necessarily simple and partial; there are, obviously, in the economy, other sources of business cycles. However, mixing heterogeneity with local interaction, decentralized decisions and sentiment changes in a minimal analytical structure, it is possible to obtain unconventional dynamic results that put us closer to real-world observed economic processes. The remainder of the paper is organized as follows. In Sect. 2, the analysis in the paper is placed in due context, through a brief exploration of recent and relevant literature on the theory of decentralized interaction and on the empirics of wage setting and wage stickiness. Section 3 launches the discussion of the model, as it characterizes the behavior of the intermediate producers, explains the alternative wage setting strategies and derives the Phillips curve relation that holds in the proposed environment. Section 4 is dedicated to a conventional description of the demand side of the economy and of monetary policy. In Sect. 5, aggregate supply and aggregate demand are put together to assess the properties of the macroeconomic equilibrium. Section 6 introduces the mechanism of individual decision based on local interaction and overall profits evaluation. Section 7 makes a numerical assessment of the model’s results, highlighting the endogenous fluctuations outcome. Finally, Sect. 8 concludes.",3
12.0,1.0,Journal of Economic Interaction and Coordination,01 March 2015,https://link.springer.com/article/10.1007/s11403-015-0150-z,Coordination in the El Farol Bar problem: The role of social preferences and social networks,April 2017,Shu-Heng Chen,Umberto Gostoli,,Unknown,Male,Unknown,Male,"The El Farol Bar problem, introduced by Arthur (1994), has over the years become the prototypical model of a system in which agents, competing for scarce resources, inductively adapt their belief models (or hypotheses) to the aggregate environment they jointly create. The bar’s capacity is basically a resource subject to congestion, making the El Farol Bar problem a stylized version of the central problem in public economics represented by the efficient exploitation of common-pool resources. Real-world examples of this problem include traffic congestion and the congestion of computer networks. On the one hand, we hope that the resources can be utilized without too much idle capacity left; on the other hand, we do not want them to be overused which leads to congestion. When, for some reasons, solving this problem by means of central intervention is either infeasible or undesirable, then it has to be solved in a bottom-up manner as the El Farol problem describes. In the literature, most of the studies addressed this problem from the perspective of learning; hence, the answers depend on how agents learn. Briefly put, there are two kinds of learning mechanism being studied in the literature. The first one is best-response learning (Arthur 1994; Edmonds 1999; Fogel et al. 1999; Challet et al. 2004; Atilgan et al. 2008) and the second one is reinforcement learning (Bell and Sethares 1999; Franke 2003; Zambrano 2004; Whitehead 2008).Footnote 1 The typical results are as follows. The best-response learning model tends to have fluctuations, sometimes quite severe, around the threshold (switching between the idle and congestion states), but the steady state where the aggregate bar’s attendance is always equal to the bar’s maximum capacity is very hard to reach. The reinforcement learning model, however, shows that perfect coordination is possible and that it is, indeed, the long-run behavior to which the system asymptotically converges (Whitehead 2008). However, it is an equilibrium characterized by complete segregation (a bimodal distribution of participation): the population split into a group of agents who always go or frequently go (filling the bar up to its capacity at all times) and another group of agents who seldom go or never go. This latter result has led to a new problem which has rarely been addressed in the literature, namely, the inequity issue. The group of people who have been discouraged by previous unsuccessful attempts and decide to quit obviously share very little or none of the public resources, which may further make them the disadvantaged class in the society. In fact, if we consider attending a bar as a social-engagement opportunity to gain more information and social connections, then the quitting can imply social exclusion or isolation. Therefore, the El Farol Bar problem is not narrowly just an economic distribution problem, it may become a social segregation problem characterized by a group of people who fully occupy the available public resource and a group of people who are discouraged, ignored and completely marginalized.Footnote 2
 In this paper, we continue the pursuit of the self-coordination mechanism of the El Farol Bar problem. However, in addition to the efficiency concern (the optimal use of the public facility), we are also interested in the distribution of the public resources among citizens. Hence, we study two variants of the El Farol Bar problem, which are distinguished from many earlier studies that are only concerned with efficiency. We ask whether self-coordinating solutions can exist in these variants of the El Farol Bar problem so that the public resources can be optimally used with neither idle capacity nor congestion being incurred and, in the meantime, the resources can be well distributed among all agents. We may call this ideal situation with both efficiency and equity the El Farol version of a “good society”.Footnote 3
 Through agent-based simulation, we shall show in this paper that the answer is surprisingly yes,Footnote 4 but the likelihood of the emergence of a good society, in addition to learning, also depends on two other elements which have not yet been incorporated in the El Farol Bar literature. These two additional elements are social networks and social preferences. Before we move further, let us first make one remark on these two elements. It should come to us as no surprise that these two elements can be significant in agent-based modeling. In fact, more and more agent-based models have taken these two elements explicitly into account, realizing their importance in emergent dynamics. The former one, social networks, is obvious because agent-based modeling relies heavily on interactions, and social network topologies are what underpin these interactions.Footnote 5 The latter one, social preference, is less obvious but can be well expected when agent-based modeling is extended to the areas involving various pro-social behaviors, which have been examined under intensive interdisciplinary studies across evolutionary biology, the humanities and the social sciences (Chen 2008; Xianyu 2010). Nevertheless, recognizing their potential significance does not automatically imply that we can predict what will happen. In the vein of the “new kind of science” or computational irreducibility (Wolfram 2002), we can probably only learn the rest from computer simulation, and it is this part where surprise may show up. In this paper, it is the combined force of social networks and social preferences which can solve the even harder efficiency-and-equity El Farol problem that surprises us. In this paper, we sequentially introduce two variants of the original El Farol Bar model, both of which represent a step towards the development of a ‘socially oriented’ version of the El Farol Bar problem. Through a series of simulations, we assess the effect of these socially-grounded assumptions on the macro-dynamics of the El Farol Bar problem and on the kind of equilibria that the system eventually reaches. The first of these variants concerns the structure of the agents’ interaction and is represented by the introduction of a social network connecting the agents and through which the agents can access the information regarding their neighbors’ choices and strategies. Therefore, in addition to the bar’s aggregate attendance, in this variant, agents can also obtain access to local information. While in the original setup the agents base their decisions on global information, represented by the bar’s aggregate attendance, a feature that is likely to cause herding behavior, making it very difficult for them to coordinate their activities, we may wonder whether coordination will be improved if, instead, the agents base their decisions on only local information, represented by the attendance of their closest neighbors.Footnote 6 As we shall see later, this alteration motivates the simulations of bar attendance dynamics through cellular automata. It is found that the introduction of social networks coupled with neighbor-based decision rules, in a form of cellular automata, allows the system to always reach an equilibrium characterized by perfect coordination, that is, a state where the bar’s attendance is always equal to the bar’s capacity, but there is a great diversity of these equilibria. The one of most interest to us, the “good society” (all the agents going to the bar with equal accessibility), is one of them. The one normally found with reinforcement learning (a group of agents always going and another group always staying at home) is also one of them. These two, however, are not exhaustive; as we shall see, there are many others, which, to the best of our knowledge, have never been found in the literature before. The effect of social networks can then be concisely represented by the resultant empirical distribution over these equilibria. After having assessed the effect of this first variant, we introduce a second variant concerning the agents’ preferences for fairness. In the original version of the El Farol Bar problem the agents did not care about their attendance frequency (that is, how often they were going to the bar). The only thing that mattered to them was to make the right choice, even if it implied staying all the time at home. In this paper we assume, instead, that some or all agents are characterized by a preference for fairness or inequity aversion. With this fairness or inequity-averse preference, agents expect a fair share or a minimum attendance frequency and take it as one of their satisfying criteria. If their decision rule does not lead to this fair share, they will react upon it and search for changes. The inequity-averse preference is one of the essential ideas in the recent literature on the study of pro-social behavior. The agent with the inequity-averse preference does not only care about his own payoff, but also cares about the distribution of payoffs among agents. This idea originated from the attempt to have a coherent framework which can explain the co-existence of selfish behavior in some games but other-regarding behavior in some other games, as repeatedly demonstrated in human-subject experiments. Our answer to this question is affirmative if one is willing to assume that, in addition to purely self-interested people, there are a fraction of people who are also motivated by fairness consideration\(\ldots \) We show that in the presence of some inequity-averse people “fair” and “cooperative” as well as “competitive” and “noncooperative” behavioral patterns can be explained in a coherent framework. (Fehr and Schmidt (1999), pp. 818–819.) The economic model of the inequity-averse preference was initiated by Bolton (1991), and was refined and established by Fehr and Schmidt (1999) and Bolton and Qckenfels (2000).Footnote 7 There is still a heated discussion on the reconciliation between inequity preference and the conventional rational choice theory, but that is beyond the scope of the paper.Footnote 8 In this paper, we simply take this preference either as exogenously given or as endogenously evolving. For the former case, the inequity-averse agents are characterized by a parameter, called the minimum attendance threshold; for the latter case, the awareness of inequity is endogenously formed through interactions with neighbors in the familiar ‘keeping-up-with-the-Joneses’ manner, i.e., the agents’ minimum attendance threshold is represented by the average of their neighbors’ attendance frequencies. The incorporation of social preference significantly increases the likelihood of the emergence of the “good society” to the extent that its appearance is always the most likely outcome. The likelihood increases with the size (number) of inequity-averse agents and the degree of their inequity-aversion. However, the emergence of the “good society” does not require all agents to be sensitive to inequity. Our simulation shows that for even a minority of them, up to 20 or 25 %, having this kind of awareness, the emergence of the good society is already guaranteed. The remainder of the present paper is organized as follows. In Sect. 2, we will present a brief review of the literature on the El Farol Bar problem. Section 3 describes our locally-interacted El Farol Bar model, a model with two-dimensional cellular automata, and the resultant adaptive behavior of agents. Section 4 presents the simulation results with respect to different settings of social networks and social preferences. Section 5 provides a simple analysis and a detailed look at the formation of the perfect coordination in light of our simulation results. Section 6 then concludes the paper with remarks on its current limitations, implications and future work.",13
12.0,1.0,Journal of Economic Interaction and Coordination,26 March 2015,https://link.springer.com/article/10.1007/s11403-015-0152-x,Is the service sector different in size heterogeneity?,April 2017,Junho Na,Jeong-dong Lee,Chulwoo Baek,Unknown,Unknown,Unknown,Unknown,,
12.0,1.0,Journal of Economic Interaction and Coordination,28 April 2015,https://link.springer.com/article/10.1007/s11403-015-0155-7,Dynamic patterns in similarity-based cooperation: an agent-based investigation,April 2017,Caterina Cruciani,Anna Moretti,Paolo Pellizzari,Female,Female,Male,Mix,,
12.0,1.0,Journal of Economic Interaction and Coordination,24 April 2015,https://link.springer.com/article/10.1007/s11403-015-0156-6,A comparison of endogenous and exogenous timing in a social learning experiment,April 2017,Lukas Meub,Till Proeger,Hendrik Hüning,Male,Male,Male,Male,"Studies on social learning emphasize the dismal effects of herding in information markets. Following the seminal papers by Banerjee (1992) and Bikhchandani et al. (1992), a number of studies show how rational subjects do not just follow private information but rather use public information constituted by prior decisions, thus frequently eliciting informational cascades. With individuals “following the crowd”, private information is left unrevealed. Although subjects are on average reluctant to rationally follow cascades in experiments (Weizsäcker 2010), socially non-optimal aggregation of information represents the core result of experimental studies following the seminal paper by Anderson and Holt (1997).Footnote 1
 Recent studies by Sgroi (2003), Ziegelmeyer et al. (2005), Çelen and Hyndman (2012) and, most recently, Ivanov et al. (2013) have furthered the analysis of social learning by allowing for endogenous ordering of decisions.Footnote 2 They point to fairly efficient observational learning, as well as deviations from rational timing that result in informational inefficiency. However, none of these studies allow for a quantification of the effect on informational efficiency and overall welfare, which requires comparison to a benchmark setting with exogenous decision order and the use of a continuous action space instead of binary action sets. In this paper, we make a direct experimental comparison of three endogenous with an exogenous benchmark setting on a continuous action space, building on the theoretical framework of Gul and Lundholm (1995). Comparing these two structures in terms of informational efficiency and welfare is worthwhile not only for the experimental and theoretical literature but to policy-makers as well. For instance, consider a situation in which firms have to decide on a new investment opportunity, e.g. a novel product, technology, or a new market. Firms might hold some private information on the profitability of the investment and might earn more by taking action before their competitors by realizing first mover advantages. However, strategically delaying the investment decision might also provide valuable information through the observation of competitors, who, in turn, could also wait or make their (observable) investment decision. In this situation, both delaying and taking the investment decision reveals the firms’ private information, given that private information and the costs of waiting are correlated, i.e. the more profitable you expect an investment to be, the faster you would want to act. Firms then face the tradeoff between acting early, yet ill-informed or delaying the decision, thus foregoing first mover advantages but gathering additional information to potentially improve their decisions. Since this set of incentives could lead to a “war of attrition” with excessive delay of investments and an overall loss of welfare, a social planner could seek to influence the respective ordering structure of investment decision to maximize welfare. Examples for potential interventions could include paying subsidies for investments, directly acting as an investor, granting exclusive rights in an auction at a specific point in time, or fostering the dissemination or publication of private information. Ultimately, the social planner has to decide whether to change the ordering of decisions, resulting in an exogenous ordering regime, or leave the timing of the investment decision to firms and thus enable an endogenous ordering regime. For the policy maker, the costs of delaying the investment and the efficiency of firms’ observational learning conditional on the ordering regime need to be considered. However, previous empirical research on social learning has focused on testing theoretical predictions derived by assuming perfect Bayesian rationality, using either exogenous or endogenous orderings of sequential decisions. No attempt has yet been made to compare the two basic structures of decision ordering in terms of welfare and informational efficiency. We offer such an immediate comparison of the two structures within a uniform experimental framework. While building on Gul and Lundholm (1995), we refrain from implementing a well-defined equilibrium strategy for optimal timing of decisions conditional on private information (signal) strength. This obviously precludes an analysis of subjects’ deviations from rational timing derived from Bayesian optimality considerations given an endogenous ordering of decisions, which, we argue, has been done convincingly before. Instead, players are essentially required to gamble on the best period to decide as they have the incentive to always outwait each other regardless of private information, which could induce a war of attrition situation. This puts subjects in a complex decision situation more closely resembling actual investment decisions, which often compels firms to choosing between fast or accurate decisions. Consequently, our analysis is concerned with individuals dealing with uncertainty in a social learning environment and the resulting differences in informational efficiency and welfare between an exogenous or endogenous ordering of decisions. Our study thus constitutes an extension to the studies on observational learning that consider agents sequentially making binary choices in a fixed order, building on the seminal papers by Banerjee (1992) and Bikhchandani et al. (1992). Private information informs both agents imperfectly about the better alternative. Agents observe all preceding decisions. In the Nash Equilibrium (NE), subsequent agents might rationally discard their private information and information is aggregated inefficiently. The binary action set limits the transmission of information (Bikhchandani et al. 1998).Footnote 3 Chamley and Gale (1994) extend the models for the endogenous ordering of decisions and waiting cost. There is strategic timing, given that prior decisions are public and have informational value; however, delaying a decision leads to waiting cost. In the NE, information is revealed imperfectly as there is either excessive delay (“war of attrition”) or no investments.Footnote 4 Most relevant for our investigation of informational efficiency is the model of Gul and Lundholm (1995), who consider two agents predicting a value that is the sum of their distinct private information in continuous time. Private information is the realization of a uniformly distributed random variable, and thus the action set is continuous. The strength of the private signal is inversely related to waiting cost. This determines the optimal time of decision as both agents face a trade-off between accuracy of their prediction and delay costs. Individual predictions become public information. The resulting equilibria depend on the agents’ strategies. Firstly, in a unique symmetric NE, both agents act sensitively to their private signal, according to the trade-off between accuracy and delay costs. Due to the inverse relationship of private signals and delay costs, the timing of decisions reveals information about the signal strength, which improves the agents’ predictions. Secondly, in an asymmetric equilibrium, the first agent waits indefinitely for the other prediction regardless of her own signal. Given that excessive waiting is uninformative for the second agent, she predicts immediately and the first players’ decision ensues. As both agents are insensitive to their signals, the result is similar to an exogenous decision sequence. In the symmetric NE, no informational cascades occur, but predictions of both agents are clustered due to two effects. The first mover anticipates that the other agent’s signal is lower as she has not yet acted; the second agent in turn infers a higher signal of the first mover from her earlier prediction. The perfect mapping of the signal space to the action space allows for a perfect transmission and revelation of information.Footnote 5 However, due to waiting cost, the sum of agents’ expected utility, i.e. overall welfare, is lower compared to exogenous ordering. 
Sgroi (2003) presented the first experimental study implementing endogenous timing, adding non-informative signals to the seminal urn game by Anderson and Holt (1997). Facing constant waiting cost, subjects have 15 periods to pick an urn, and face a trade-off between waiting cost and potentially better predictions through the observation of prior decisions. Subjects receiving informative signals optimally decide in the first period to avoid waiting cost, while subjects receiving non-informative signals rationally decide immediately afterwards, using public information. As subjects’ ordering works fairly well in Sgroi (2003), overall informational efficiency should be close to optimal. Nonetheless, normal and reversed cascades continue to occur, and thus no perfect revelation of information is achieved. In Ziegelmeyer et al. (2005), two subjects receive an integer signal as a realization of a random variable. Both subjects are asked to assess whether the sum of both signals is either positive or negative and face constant waiting cost. Both subjects are able to anticipate the strength of the other’s signal depending on the respective period of decision, which should lead to information efficiency. However, subjects deviate from rational behavior by acting too early according to their signals, which in turn reduces delay costs. The authors interpret this as an internalization of informational externalities to reduce welfare-damaging delay. In Çelen and Hyndman (2012), two subjects make a binary choice between an investment with fixed pay-off and a risky alternative with an unknown pay-off, and have three periods to take a decision. Decisions for the non-risky investment are reversible, while the choice of the risky option is not. Additionally, subjects receive private information on the actual payoff of the risky alternative. Subjects delay their decision in order to gather additional information, particularly when their signal does not favor the risky investment. Excessive waiting is partly explained by risk aversion when the accuracy of the private signal is low. Considering herding in financial markets with a flexible pricing mechanism, Park and Sgroi (2008) use an exogenous timing of decisions, while Park and Sgroi (2012) implement an endogenous decision sequence. Herding and contrarianism are observed in both studies, yet endogenous timing leads to more pronounced effects and the clustering of decisions. Based on the model by Levin and Peck (2008), Ivanov et al. (2013) ask subjects to decide in discrete time whether to invest, not to invest or wait and decide later. Once a subject takes a decision, it becomes public information. Contrary to previous experiments, subjects receive two kinds of private information: a private signal about the return and a private signal concerning the cost of investment. Subjects generally use information correctly, yet deviate from rational timing. Following Ivanov et al. (2013), it is well established that individuals fail to apply rational timing. Therefore, we refrain from investigating deviations from optimal timing of decision and take a different angle on the timing of choices. While we implement a two-player game with a simple rational strategy for predictions in any given situation, we adjust costs of delay and accuracy rewards so there is no NE for the timing of decisions. Being second mover is always preferred unconditional on signal strength, yet one could end up bearing waiting cost and still be first mover as the time horizon is finite and the other player might act symmetrically. Accordingly, the timing of a choice does not perfectly convey private information and simultaneous decisions that preclude observational learning are possible. We thus implement a multi-dimensional decision situation that resembles a gamble on gathering additional information by strategically delaying decisions. We argue that this resembles real world decisions and renders our comparison of fixed-order efficiency and endogenous ordering more interesting in terms of external validity compared to studies that investigate deviations from rational timing. Implementing this concept, we find that endogenous timing on average increases the degree of rationality of predictions and thus their accuracy when compared to an exogenous setting. This increase is smaller when higher waiting cost are implemented, which leads to earlier and often simultaneous decisions that preclude observational learning. Therefore, subjects are sensitive to changes in waiting cost. In addition to observational learning, first movers correctly infer signal strength from the waiting time of the co-player when waiting cost are designed in a simple way, which adds to the overall increase in prediction accuracy for the endogenous setting. However, the gains in informational efficiency are compensated by waiting cost, resulting in no positive net welfare effect of endogenous timing. Our results suggest that there are no positive welfare effects of introducing endogenous rather than exogenous ordering, yet improvements in overall informational efficiency. We find neither excessive waiting, i.e. waiting when no additional information can be obtained, nor accuracy maximizing behavior, i.e. waiting to always become second mover. In turn, there are many subjects minimizing waiting cost by making their prediction in the very first period, thereby passing on the opportunity of observational learning. The remainder of this paper is structured as follows. Section 2 derives our theoretical framework, while Sect. 3 contains our experimental design. Section 4 presents the results and Sect. 5 concludes.",
12.0,2.0,Journal of Economic Interaction and Coordination,28 April 2015,https://link.springer.com/article/10.1007/s11403-015-0157-5,Exploring issues of market inefficiency by the role of forecasting accuracy in survivability,July 2017,Ya-Chi Huang,,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Journal of Economic Interaction and Coordination,07 June 2015,https://link.springer.com/article/10.1007/s11403-015-0161-9,Corporate social responsibility and financial performance: event study cases,July 2017,Yu-Shan Wang,Yi-Jie Chen,,,,Unknown,Mix,,
12.0,2.0,Journal of Economic Interaction and Coordination,31 July 2015,https://link.springer.com/article/10.1007/s11403-015-0164-6,Effects of limit order book information level on market stability metrics,July 2017,Mark Paddrik,Roy Hayes,Peter Beling,Male,Male,Male,Male,"On May 6, 2010, the U.S stock market experienced one of the most severe price drops in its history. Known as the May 6th Flash Crash, the Dow Jones Industrial Average fell 5 % in less than 5 min. The bulk of losses were recovered in the next 15  min of trading. The crash raised concerns about the stability of capital markets and resulted in a joint investigation by the U.S. Securities and Exchange Commission (SEC) and the U.S. Commodity Futures Trading Commission (CFTC) (CFTC & SEC 2010). The May 6th Flash Crash seems to be singular in the sense that no following events have rivaled its depth, breadth, and speed of price movement. Flash crashes on a smaller scale, however, occur frequently. Between 2006 and 2012, there were 18,520 incidents that we term mini flash crashes, in which a single security experienced abrupt and severe price change over a short period of time (Johnson et al. 2012). Although the underlying causes of mini flash crashes vary (Golub et al. 2012), they share some uniform markers of stress that, if recognized in advance, could be the basis for market intervention aimed at increasing price stability. The metrics TR-VPIN (Easley et al. 2011), and BV-VPIN (Easley et al. 2012) were both developed to try to predict flash crashes. These metrics have considerable limitations because they were constructed using in-sample metric calibration with no out-of-sample testing, have high false positive rates, and are general ineffective at predicting short term volatility once accounting for volume dynamics (Andersen and Bondarenko 2014). Moreover, the VPIN metrics use only transactional information and do not take into account even basic information on about the limit order book, which is the predominant trading mechanism in today’s electronic equities and futures markets. However, it has been observed that the limit order book offers a structure that makes it possible to examine strains on the state of price formation and overall market stability (Duong and Kalev 2014). In this paper, we examine the hypothesis that the quality of market stability metrics can be improved by making use of order flow information. A central point of investigation is to assess the increases in metric quality associated with moving from publically available data (such as Level 2 quotes) to increasingly more detailed data such as that held by regulators, which may contain trader identification or other restricted information. This hypothesis is difficult to test empirically, because much of the historical data about the inner workings of the limit order book, often called microstructure data, is available only to regulators and exchanges and, in many cases, is too complex and time consuming to examine. But regulatory attention to this process is needed, because price discovery is viewed as the most important product that security or futures markets offer and must be treated as a public good, essential to financial stability (Hasbrouck 1995). A number of stability metrics are considered in this paper, including several designed specifically to make use of fine-grained information such as Level 6 data, a form of regulatory order flow data which includes unique market participants identifiers and non-publicly visible order type actions. Stability metrics are assessed on their ability to predict impending destabilization in the form of flash crashes. We tested market data produced by an agent-based simulation model that incorporates a limit order book mechanism and associated microstructure data at all levels. The simulation model has zero-intelligence agents, and so is limited in fidelity but is calibrated to a set of historically parameters of a market consistent with traditional market pricing behavior models (LeBaron 2001). Our key conclusion is that metrics using more detailed market microstructure data had the greatest power to predict flash crashes in the simulated market. To validate the conclusions from the simulation, we used a small set of actual market data to show that the metrics most effective on the simulated data were also most effective in predicting the May 6, 2010, Flash Crash of the E-Mini S&P 500 futures market and the September 17, 2012, mini flash crash of the Light Sweet Crude Oil futures market 1 min in advance. The paper is organized as follows: Sect. 2 gives general background on the electronic order book and what occurs when it becomes unstable in advance of and during flash crashes. Section 3 describes the agent-based simulation model used to produce the principal datasets used to test the stability metrics. Section 4 covers how different data levels may be used to determine possible measures of market stability. Section 5 presents results for testing of the stability metrics on simulated data and the degree of extra information each level of data may add. Section 6 examines the ability of each stability metric to predict two historical flash crashes. Section 7 summarizes our results and conclusions, including a discussion of the opportunities that this method may offer to operational risk management and data selection.",5
12.0,2.0,Journal of Economic Interaction and Coordination,08 September 2015,https://link.springer.com/article/10.1007/s11403-015-0165-5,Persistence in corporate networks,July 2017,Matthias Raddant,Mishael Milaković,Laura Birg,Male,Unknown,Female,Mix,,
12.0,2.0,Journal of Economic Interaction and Coordination,07 September 2015,https://link.springer.com/article/10.1007/s11403-015-0166-4,Network position and health care worker infections,July 2017,Troy Tassier,Philip Polgreen,Alberto Segre,Male,Male,Male,Male,"Vaccines are a primary way to stop or slow the spread of many infectious diseases, perhaps most notably, influenza. The lack of appropriate vaccination levels is a major health problem. For instance, influenza is a major cause of morbidity and mortality throughout the world despite the availability of a highly effective and inexpensive vaccine. In the US alone, influenza causes an estimated 36,000 deaths and 120,000 hospitalizations annually yet only around 1/3 of healthcare workers are vaccinated each year (Thompson et al. 2003). Efficient provision of vaccinations poses a difficult problem in that the positive externality associated with a vaccination is the product of the probability of infection, the cost of the infection, and the marginal infections generated by an agent if infected (all of which may vary across agents). There is great concern over the spread of infectious diseases in hospitals, but little knowledge is available to identify healthcare workers who are most likely to acquire and transmit infectious diseases in hospitals. The problem is especially difficult because the transmission of many infectious diseases is not observable. For instance if someone in your household acquires influenza, you likely do not know which of the potentially hundreds of people you come in contact with each day that may have caused the infection. Further if a vaccine is available for an infectious disease and the vaccine is in short supply or is expensive, it is imperative to know which individuals should have the highest priority in vaccine campaigns. In this paper we use a newly collected data set on hospital worker contacts in order to identify hospital worker groups that have the potential to create the largest number of infections based on their location in a hospital contact network. To achieve this goal, we have collected person-to-person contact information on 140 individuals belonging to one of 15 types of healthcare workers at the University of Iowa Hospitals and Clinics (UIHC). The data contain information on the contacts between healthcare workers and patients and between healthcare workers and other healthcare workers at the hospital. With this information we develop a network model describing the spread of an infectious disease in a hospital. We estimate, using an agent-based model, the effect of network position of different hospital worker groups on the spread of infectious diseases in a hospital. Through this model we are able to identify the hospital worker groups that create the largest externality if removed from the network (perhaps through a vaccination or a quarantine). We argue that methods such as those used in this paper can help hospitals, health care professionals, and epidemiologists to design efficient programs for healthcare worker vaccinations. Of importance, we note that we only study the externality in terms of network position within the hospital. In this paper we do not consider other potential heterogeneity among agents such as differences in transmission rates across workers, or differences in behavior outside the hospital that may play a role.Footnote 1 While these effects are also important, the large differences across classes of workers shown below are worthy of independent study. This is the first paper to use specific micro-level contact data within a hospital that can be used to help guide policy makers and public health officials in the problem of efficiently allocating vaccines within hospitals. The data used in this paper is unique and detailed in comparison to other studies. The data consists of shadow data (where a research assistant follows a specific, randomly chosen hospital worker for an entire shift) for the 15 major groups of healthcare workers at the UIHC, a 700-bed major medical center. This results in over 600 h of direct hospital worker observations and the notation of over 6500 specific worker to worker or worker to patient contacts throughout the hospital. To the best of our knowledge, the data that we have collected comprises the most detailed micro-level healthcare worker contact data set in existence. As a comparison, Ueno and Masuda (2009) collect data on contacts between doctors, nurses, and patients. Their data is based on two calendar days from a small, 129 room, community hospital in Tokyo. They model contacts between nurses, physicians and patients. Their data does not consider contacts with and between other healthcare worker groups (other than nurses and doctors). Based on our data at the UIHC, these assumptions would ignore over 60 % of hospital staff, including most of the groups we identify as most crucial to the spread of an infection disease. We begin by discussing the background and motivation for our study and then move to develop a simple model of infectious disease transmission. In the model, we initially assume homogeneous contacts as in traditional epidemiological models. We then discuss a similar model with heterogeneous contacts and discuss the difficulties of achieving efficient vaccination levels. Following the theoretical discussion, we use our newly collected data on healthcare worker and patient contacts to model the spread of an infectious disease in a hospital setting. The model allows us to identify the healthcare worker groups that would be expected to play the largest role in the spread of infectious diseases, in terms of network position, in this hospital setting. Traditionally, epidemiology research has focused on well-mixed (randomly mixed) populations where agent contacts are homogeneous. In these models, every agent in a population may “bump into” any other agent with equal probability, much as a gas molecule may bump into any other gas molecule with an equal probability over a fixed time period. Only recently have epidemiologists and other researchers begun to study the heterogeneous contact structures between people over which infectious diseases spread (early studies include Comins et al. 1992; Grenfell and Harwood 1997; Wallinga et al. 1999). We focus this study on healthcare workers and a particular class of infectious disease, of which influenza is an example. Healthcare workers are at especially high risk of contracting influenza. One study of healthcare workers with a low rate of influenza vaccination demonstrated that 23 % of healthcare workers had evidence of influenza infection during a single influenza season (Elder et al. 1996). Two features of influenza make its spread difficult to control in hospitals. First, people with influenza are infectious 1–4 days before the onset of symptoms. Thus, they can spread the virus when they are still feeling well and are unaware of their own infectious state. Second, only about 50 % of infected persons develop classic influenza symptoms (CDC 2002, 2003) Consequently, restricting healthcare workers with influenza-like symptoms from the workplace will not completely prevent transmission of influenza because healthcare workers with atypical symptoms could continue working and spreading the virus. Furthermore, studies show that healthcare workers are more likely than other workers to return to work early or to keep working when they have influenza-related symptoms (Weingarten et al. 1989). Because of the ease with which influenza may be contracted and spread by healthcare workers, the Centers for Disease Control and Prevention (CDC) have, for the past two decades, recommended influenza vaccination for all healthcare workers. Yet, in the US, only 36 % of workers who have direct patient contact are immunized against influenza annually (Smith and Bresee 2006). Outside of concerns about traditional influenza, there are additional reasons to study the spread of infectious diseases in hospitals. First, healthcare-associated infections affect about 2 million patients in US hospitals each year (Jarvis 1996). Second, there is a growing fear that hospitals could become a breeding ground for new strains of influenza such as the recent H1N1 influenza outbreak, the potential emergence of person-to-person transmission of avian flu, or other “new viruses.” Much as SARS spread widely in hospitals to begin the SARS epidemic in Toronto (Chowell et al. 2004), person-to-person transmission of avian flu may start in hospitals as well, and, if a more lethal version of H1N1 were to develop, hospitals again could be a breeding ground for new infections. This last point is of particular salience. With the recent H1N1 outbreak and the subsequent work to develop a vaccine, controversies arose concerning which individuals to vaccinate first. Healthcare workers were high on the list. But, as we show below, not all healthcare workers are equal in terms of their importance in spreading infectious diseases. Thus, one primary focus of this paper is identifying the individual hospital workers who are most important to vaccinate should a similar outbreak occur again. There is a growing literature in economics on the vaccination choices of individuals and of the externalities associated with vaccinations. But scant attention is paid to the network effects determined by heterogeneous contacts that we focus on in this paper. For example, Francis (2004) solves for the optimal tax/subsidy policy for influenza in an SIR model with a constant contact rate and random mixing among the population. Geoffard and Philipson (1997) examine how the individual incentives for vaccination decrease as disease incidence decreases and thereby argue that relying exclusively on private markets is unlikely to lead to disease eradication. Boulier et al. (2007), the most similar paper to ours, investigate the magnitude of the externality associated with a vaccination as a function of the number of vaccinations in the population, the transmission rate of the disease, and the efficacy of the vaccination. They find non-montonic relationships between each of these items and the magnitude of the vaccine externality. More specifically, the externality and the number of infections prevented per vaccination initially increases before eventually decreasing. However, like Francis, they do not consider the case of heterogenous contact number or heterogenous sorting among the population. Finally, much of the recent literature on the economics of infectious disease is summarized in Philipson et al. (2000).",3
12.0,2.0,Journal of Economic Interaction and Coordination,12 September 2015,https://link.springer.com/article/10.1007/s11403-015-0167-3,Yield curve responses to market sentiments and monetary policy,July 2017,Markus Demary,,,Male,Unknown,Unknown,Male,"Demand in interest-sensitive sectors like housing, consumer durable goods and business fixed investment depends on interest rates with different maturities (Roley and Gordon 1995). Ellingsson and Söderström (1998), Kozicki and Sellon (2005) and Rudebusch et al. (2006) arrive at the result, that the response of long-term yields to changes in the central bank rate is covered with uncertainty. While long-term interest rates increased in 1994–1995 and 1999–2000 after the federal reserve increased its short-term policy rate, longer term interest rates declined after the Federal Reserve increased its target rate in August 2005 (Kozicki and Sellon 2005, p.1). Blinder (2013, p. 8) mentions that ”the empirical failure of the rational expectations theory of the yield curve [...] is a major practical headache for central bankers”. This paper explains these uncertain yield curve responses within a behavioral model of the yield curve with heterogeneous and bounded-rational agents. The results might be helpful for monetary policy making for the following reasons. First, monetary policy traditionally targets the short end of the maturity spectrum, while longer term yields on mortgages and corporate bonds are determined by the public’s expectations and sentiments about future short-term policy rates, inflation and the state of the business cycle (Estrella and Hardouvelis 1991; Mishkin 1990). These longer term interest rates determine private sector borrowing and investment decisions rather than the short-term policy rate (Bernanke et al. 2004, p. 8). Eggertson and Woodford (2003a, b) come to the result, that shaping the public’s expectations is an important tool in monetary policy making. Managing expectations, moreover, gained momentum as interest rates in the Eurozone, Japan, the United Kingdom and the United States recently reached the zero lower bound. Central banks have responded with forward guidance strategies that target longer term interest rates by influencing market expectations about future monetary policy (Gürkanak and Wright (2012), p. 333). Understanding how market sentiments and monetary policy interact will, thus, have important implications for the design of forward guidance strategies by central banks. De Grauwe (2010a, b, c, 2012) shows that macroeconomic models with heterogeneous and bounded-rational agents are able to generate correlated beliefs among agents that drive the business cycle and that respond to monetary policy. Combining his model with a yield curve model will result in a framework for analyzing the different yield curve movements in response to monetary policy cited above. Second, central banks recently added financial stability goals to their traditional price stability objective. Financial stability implications of yield curve movements arise, since institutional bond holders like banks, pension funds and insurance companies have large portfolios of sovereign and corporate debt contracts in their balance sheets that are sensitive to movements in interest rates. A behavioral model of the yield curve could help to quantify the risks associated with holding bonds, especially in periods when market participants become collectively pessimistic as during the climax of the banking and sovereign debt crisis in the Eurozone. Moreover, the model could help to predict if and how a central bank could tame destabilizing sentiments. Third, in academic research macroeconomic models often assume a financial sector with only one single interest rate and no yield spreads, while finance models of the yield curve abstract from a macroeconomic environment as highlighted in Rudebusch (2010). In the macro-finance models of Rudebusch and Wu (2008) and Bekaert et al. (2010), however, market sentiments and bounded-rationality play no role. Recent behavioral finance models with heterogeneous interacting agents are very successful in explaining empirical properties of asset price dataFootnote 1 (Lux 1997; Lux and Marchesi 2000; Westerhoff 2008). Moreover, this literature stresses the importance of market sentiments (Lux 2009). These studies, however, focus mostly on stock market prices and exchange rates (Westerhoff 2008; De Grauwe 2006). Behavioral macro-finance models of the yield curve would certainly be interesting contributions to this literature. My contribution to this literature consists of combining a yield curve model with the behavioral macroeconomic model of the type proposed by De Grauwe (2010a, b, c, 2012)Footnote 2. In contrast to the rational expectations theory of the term structure I develop a behavioral theory of the yield curve, in which yields across the maturity spectrum are determined by the expectations of heterogeneous and bounded-rational agents about inflation and the business cycle. Moreover, these expectations are influenced by how the central bank conducts monetary policy. Hence, we arrive at a model framework in which market sentiments and monetary policy interact. The behavioral model of the yield curve is then applied to the following research questions: (i) How do macroeconomic shocks drive the yield curve in a behavioral macroeconomic model and what is the contribution of each shock to the variation in the level, slope and curvatureFootnote 3 of the yield curve? (ii) Which role do inflation sentiments and business cycle sentiments play in determining the yield curve? (iii) How can a central bank influence the level, slope and curvature of the yield curve when agents are bounded-rational? (iv) How do the different policy parameters interact? Are there trade-offs implied by the choice of policy parameters and how are these trade-offs reflected in the shape of the yield curve? The first result is that the model replicates empirical facts of term structure data,Footnote 4 e.g. that long-term inflation expectations shift the level of the yield curve while monetary policy drives its slope. The second result is that the behavioral model overcomes one major deficiency of rational expectations models of the yield curve in explaining why the long-term interest rate moves sometimes in the same direction as the short-term central bank rate and in the opposite direction at other times.Footnote 5 While the rational expectations model can only explain the first response, the behavioral model explains both responses. In the behavioral model the median responses to macroeconomic shocks are of similar shape as those of the estimated rational expectations model of Bekaert et al. (2010), the behavioral model, however, generates an additional variation of yield curve responses across simulation runs that the rational expectations model cannot produce. The behavioral model thereby explains why long-term interest rates sometimes rise after a hike in the short-term central bank rate and why it does the opposite at other times by different market sentiments of inflation and the business cycle.Footnote 6 Further results can be used as policy advice on how central banks can influence the shape of the yield curve by influencing market sentiments about inflation and the business cycle. Summing up, central banks can stabilize the level of the yield curve by means of a clear and time-constant inflation target. Moreover, they can stabilize its slope and its curvature by giving inflation and the output gap sufficient high, but not too high weights in the policy reaction function for the short-term interest rate.Footnote 7 Central bank strategies that make its inflation target more credible should be favored over monetary policies that keep interest rates too low or too high for too long. In the next Sect. 1 present the behavioral model of the term structure of interest rates. Section three contains the analysis on how macroeconomic shocks drive the yield curve. Section four aims at getting insight into how market sentiments shape the yield curve, while section five analyses how a central bank can target these sentiments and thereby the yield curve by the selection of its policy parameters. Section six ends the paper with a discussion of the results and the points I abstracted from and left for future research.",2
12.0,2.0,Journal of Economic Interaction and Coordination,16 October 2015,https://link.springer.com/article/10.1007/s11403-015-0168-2,Using an artificial financial market for studying a cryptocurrency market,July 2017,Luisanna Cocco,Giulio Concas,Michele Marchesi,Unknown,Male,Female,Mix,,
12.0,2.0,Journal of Economic Interaction and Coordination,30 October 2015,https://link.springer.com/article/10.1007/s11403-015-0169-1,Endogenous business cycles caused by nonconvex costs and interactions,July 2017,Yoshiyuki Arata,,,Male,Unknown,Unknown,Male,"In this paper, we show that endogenous business cycles (inventory cycles) arise from a combination of nonconvex costs and economic interactions among firms. In particular, we show that the aggregate of randomly behaving microeconomic agents generates deterministic collective behavior via interactions. Economic fluctuations are certainly an important issue in economics, but what causes such fluctuations? This natural and fundamental question has not yet been answered in economics. For example, Cochrane (1994) demonstrates that popular economy-wide shocks (e.g., monetary shocks or oil prices) fail to explain the bulk of economic fluctuations. He writes, “What shocks are responsible for economic fluctuations? Despite at least two hundred years in which economists have observed fluctuations in economic activity, we still are not sure” (p. 295). We cannot resort to mysterious aggregate exogenous shocks to explain aggregate fluctuations. However, because an economy is composed of many firms, it might be expected that aggregate fluctuations stem from firm-specific shocks and inherit some properties from them. At the micro level, economic activities are characterized by lumpiness and discreteness. Managers temporarily shut down plants or change the number of shifts for inventory adjustment. This behavior clearly contradicts the standard production-smoothing theory in microeconomics textbooks. In fact, the production-smoothing theory has been empirically rejected (see Blinder and Maccini 1991). It is found that, when some fixed costs exist (e.g., ordering costs), the cost curve is kinked and nonconvexity emerges, which implies that the cost-minimizing strategy of firms is production bunching (or the bunching of orders). This theory can account for the stylized fact that production is more volatile than sales (e.g., Hall 2000). The aim of this paper is to investigate how these firm-level characteristics are related to aggregate fluctuations. There are two different views concerning the effect of microeconomic characteristics on aggregate fluctuations. One is that microeconomic characteristics disappear at the macroscopic level. Indeed, less attention has been paid to the role of idiosyncratic shocks in the macroeconomic literature simply because these shocks are considered to average out in the aggregate by the law of large numbers (LLN). Lucas (1977)’s argument is a typical one.Footnote 1 According to this view, the observed aggregate fluctuations must be explained by the presence of shocks that have a common origin across firms in the economy. By definition, they are aggregate shocks. On the other hand, another view, which has attracted much attention in recent years, emphasizes the effects of interactions between sectors (or firms), especially input–output linkages. In fact, positive comovement across sectors is a salient feature of the business cycle. In contrast to the LLN argument, it is emphasized that the effects of interactions between sectors (or firms) through input–output linkages, which propagate idiosyncratic shocks throughout the economy, cause the aggregate fluctuations that are unexplained by the usual aggregate shocks (e.g., Long and Plosser 1983; Carvalho 2010; Foerster et al. 2011; Acemoglu et al. 2012; Carvalho and Gabaix 2013; for a review, see Carvalho 2014). The key element of models used in these studies is the existence of sectors that have disproportional impacts on the entire economy. This is due to the heterogeneity of input–output linkages; that is, sectors are not equally intense material suppliers. Shocks to general purpose technologies such as oil, electricity, and iron and steel propagate to all sectors through the input–output linkages because most sectors rely on them. In this sense, the microeconomic shocks accounting for aggregate fluctuations in these studies can be regarded as “pseudo–macroeconomic” shocks. There are other strands of literature that are related to our analysis, for example, Bak et al. (1993) and Durlauf (1993), where nonconvex technology and (local) interactions are explicitly considered. Bak et al. (1993) demonstrate that small shocks to final goods can cause an “avalanche” of production increases via supply chains. Even though such interactions explain how aggregate fluctuations can be caused by microeconomic shocks, there exist broad distinctions between our model and previous studies. In contrast to Carvalho (2010) and Acemoglu et al. (2012), we assume that each firm is small compared to the economy as a whole and can hardly influence the outcome of the economy on its own. Furthermore, in contrast to Bak et al. (1993), in which shocks to final goods are assumed to be exogenous, we assume that demand for the products depends on the overall economic condition. We assume that on the one hand, the behavior of a firm is affected by the state of the economy as a whole, but on the other hand, the economy is composed of the firms themselves. In other words, the macroscopic state of the economy not only is an aggregation of the firms, it also prescribes the macroeconomic environment in which the firms engage in business activities. This feedback loop generates rich interesting phenomena. This idea is closely related to the “macro-micro loop” emphasized by Hahn (2002), where a macro variable acts as an externality. We show that this mechanism can generate collective behavior that is different from the motion of an individual firm. On this point, our approach is close in spirit to heterogeneous interacting agent models (see, e.g., Delli Gatti et al. 2009; Stiglitz and Gallegati 2011; for a survey, see Hommes 2006), especially to Aoki’s methods (Aoki 1996, 2002; Aoki and Shirai 2000; Aoki and Yoshikawa 2007). Aoki and coauthors have developed the application of jump Markov processes, where the evolution of the probability distribution is described by master equations. Although there is no doubt that Aoki’s methods expand the scope of macroeconomic analysis, there exist some difficulties and situations that cannot be dealt with in his framework (see Sect. 4.1). In particular, in our model, firms’ inventories are distributed continuously and affect firms’ choice of production. That is, the system is described by an infinite-dimensional random variable, which is the distribution of inventories (and production). By using the propagation of chaos instead of Aoki’s methods, we present an alternative method to investigate how the system (i.e., the probability distribution) behaves and changes its properties when parameters are changed. On the basis of cost-function nonconvexity and the feedback effect, we show that a regular cyclical movement at the macroscopic level emerges given that the effect exceeds a certain threshold. This cyclical movement is endogenous and is an explanation for the Kitchin cycle. The rest of the paper is organized as follows. Section 2 discusses the firm behavior characterized by nonconvexities, which can explain the empirical puzzle that the volatility of production is larger than that of sales. Section 3 discusses the importance of inventory movement for understanding business cycles. Section 4 contains our main results and shows that the simple LLN cannot be applied and that an endogenous movement emerges. Section 5 concludes.",1
12.0,2.0,Journal of Economic Interaction and Coordination,16 November 2015,https://link.springer.com/article/10.1007/s11403-015-0170-8,"Social networks, mass media and public opinions",July 2017,Haibo Hu,Jonathan J. H. Zhu,,Unknown,Male,Unknown,Male,"In real life our opinions are shaped by what our friend circle said and what we read or watched on mass media outlets like the television, radio, Internet, and print (DellaVigna and Kaplan 2007; Quattrociocchi et al. 2011). These opinions could be about topics such as consumer products, life style, or celebrities. Opinion dynamics in social networks has been widely studied, both from the perspective of analytic modeling and experimental psychology. It is well known that there are several complementary social influence processes that shape individual opinion formation (Deutsch and Gerard 1955; Burnkrant and Cousineau 1975) and have been widely studied in social psychology, economics and marketing science (Cialdini and Goldstein 2004). One is termed informational influence, where people lacking necessary information inquire the opinions of their friends to update their ideas. Such a process is common in opinion evolution for fashion, consumer products, and music. Many theoretical models of opinion dynamics in networks use informational influence as the underlying premise (Clifford and Sudbury 1973; DeGroot 1974; Holley and Liggett 1975), and many works use the models to characterize the evolution of opinions in a social network in terms of convergence time, and the emergence of a consensus or polarization. A well-known opinion dynamics model that is often studied on networks is the so-called voter model. Voter model was first considered by Clifford and Sudbury in the 1970s as a model for species competition (Clifford and Sudbury 1973), and the dynamical system that they introduced was dubbed the “voter model” by Holley and Liggett a couple of years later (Holley and Liggett 1975). In the model each agent is endowed with a binary state or opinion. The elementary step consists in choosing a first agent and one of her nearest neighbors, both randomly. Then the first agent sets her new opinion to be the same as the neighbor. Voter model has been widely and intensively studied from a multidisciplinary perspective (Castellano et al. 2009) and the binary states can also be extended to multiple states. Its key assumption is that the more others choose an option, the more one is apt to do so as well. Thus voter model is a kind of ordering dynamics where agents update their opinions according to some version of majority rule or imitation. In the field of stochastic processes, the voter model has been extensively studied in the framework of interacting particle systems (Holley and Liggett 1975; Liggett 1985, 1999). The main analysis tool for the state evolution of agents is the dual process, which is defined through coalescing random walks on networks. The dual process runs backward in time and allows one to identify the source of the opinion of each agent at any time instant (Holley and Liggett 1975; Liggett 1985). The coalescence time in a coalescing random walk is related to the expected time for voting to complete (reaching consensus) in voter model via duality (Cox 1989; Cooper et al. 2012; Oliveira 2013). In fact the consensus time of the voter model has the same distribution as the convergence time of the coalescing random walk process (Liggett 1985). The dual approach can also be applied to the voter model to study the average opinion of the society and variance of the average opinion (Yildiz et al. 2013). Most dynamics models based on voter model only consider the endogenous influence of social networks on opinion evolution. In fact mass media can also influence individual opinions (Crokidakis 2012), and the research on consumer behaviors has found that mass media can influence people’s brand preference and consumption decisions (Liebert and Schwartzberg 1977; Batinic and Appel 2013), and for different products the relative intensity of the effects of media and social network is also different. Utilizing a culture dissemination model on social networks, Axelrod’s model (Axelrod 1997), some works study the exogenous influence of mass media on opinion dynamics (Rodríguez and Moreno 2010; Peres and Fontanari 2011, 2012). In original Axelrod’s model, an agent is represented by a string of cultural features, where each feature can adopt a certain number of distinct traits. The interaction between any two agents takes place with probability proportional to their cultural similarity, i.e., proportional to the number of traits they have in common. When interacting, one agent influences another agent causing the latter to adopt the former’s trait on a feature randomly chosen from those that they do not share. The process continues until no cultural change can occur. This happens when every pair of neighboring agents has cultures that are either identical or completely different. Two final possible states are possible: only one cultural region is obtained or multiple cultures are obtained separated by a boundary. Axelrod’s model can be viewed as multiple coupled voter models since it features the main ingredients of a voter model. The introduction of mass media in the model means adding a virtual agent which can interact with all agents in the networks. A counterintuitive result derived from the analysis of Axelrod’s model is that the introduction of an external global media aiming at influencing the agents’ opinions actually favors polarization (Shibanai et al. 2001; González-Avella et al. 2005, 2006, 2007; Peres and Fontanari 2010; Flache and Macy 2011). This finding is at odds with the common-sense view that mass media are devices that can be effectively used to control people’s opinions and so homogenize society. Some extended Axelrod’s models considering the mass media effect have also been proposed (Kennedy 1998; Parisi et al. 2003; Candia and Mazzitello 2008); however for almost all models originating from the Axelrod’s model we can only study their characteristics by numerical simulations and it is still difficult to obtain analytical results for the models. In the paper based on a multi-state voter model, we study the impacts of endogenous social influence and exogenous media on public opinions in social networks. Except modeling opinion evolution, the voter model is also useful as a general framework for the analysis of diffusion of products, innovations, practices and consumption decisions, and its ability to model real opinion dynamics and product adoption has also been validated (Fernández-Gracia et al. 2014; Karsai et al. 2014; Das et al. 2014). Our goal of proposing a new model is not to fit empirical data exactly, since human behavior, even on simple questions, is not likely to admit a single model. Instead we aim to propose an extension to the multi-state voter model that captures the influences of social networks and mass media on people’s opinions. We focus on the problem of modeling how individuals update opinions based on their neighbors’ opinions and media views according to several realistic rules. We do not study the consensus time for the model, instead, we study the proportion of agents for each opinion at the final steady state. The paper is organized as follows. In Sect. 2 we present several basic assumptions of the opinion dynamics model based on realistic scenarios and describe the model rules. Empirical online user experiments show that some users show stubborn behavior (Das et al. 2014). The users do not change their opinions over time. Thus we assume that in a social network, except the regular agents whose opinions can be affected by both media and social circle, there also exist committed agents who always stick to their original opinions. We analyze the model in Sect. 3 and give the analytical results. In Sect. 4 numerical simulations on networks are implemented to verify the analytical solutions. Section 5 discusses several extensions to the model which consider more realistic scenarios. Finally in Sect. 6 we present our concluding remarks and give several directions for future research.",17
12.0,2.0,Journal of Economic Interaction and Coordination,19 December 2015,https://link.springer.com/article/10.1007/s11403-015-0171-7,Network analysis of inter-sectoral relationships and key sectors in the Greek economy,July 2017,Theodore Tsekeris,,,Male,Unknown,Unknown,Male,"Input–output tables constitute a valuable tool for supporting the analysis of the structural relationships among sectors in a national or regional economy. These tables depict the intermediate and final transactions; namely, they describe the supply and use of goods and services that are directly consumed or used up as inputs in the production process of the whole economic system. They can be utilized both in the planning and evaluation of public investment programs. Specifically, they can provide useful insights into the size of industries, development strategies with respect to different production structures, and the potential effect of the expansion of one sector on other ones (Rasmussen 1957; Hirschman 1958; Jones 1976). Typical analysis of input–output tables (Leontief 1951; Rasmussen 1957) employs the input or technical coefficients, calculated by dividing each sector’s column of inputs by the total national production, to calculate the inverse Leontief matrix and multiplier effects. Nonetheless, this analysis relies on the absolute or relative magnitude of sectors, without accounting for heterogeneous characteristics such as their particular role in the interconnectivity and stability of the whole economic system. Moreover, it assumes the existence of constant returns to scale and concentrates on the average (rather than marginal) effects of a change in one sector’s demand on the others and the national economy. The adoption of methods and metrics from graph theory and network science (Newman 2003; Boccaletti et al. 2006, 2014) can address some of those problems, through considering issues of centrality and clustering effects. In particular, the national economic system is modeled as a complex network, taking as nodes (agents) the sectors and edges (directed links) the transactions (sales/purchases) between them, weighted by the amount (monetary flow) of each transaction, as described in the input–output matrix. The proposed approach can suitably represent multi-sectoral interdependencies and the potential influence of one sector on other, significant sectors and/or groups of sectors, beyond the standard analysis of the pair-wise relationships between sectors. Besides, it can help to identify critical sectors associated with the robustness of the whole economic system. In this way, some intrinsic features pertaining to the complexity of the economic network are taken into account, as it is recognized that the outcome at the macro level cannot be inferred from an individual outcome on its own, but from varying interactions among several (groups of) sectors at different network scales. This paper focuses on the complex network analysis of the inter-sectoral relationships and the key sectors of the Greek economy, using input–output data. By employing network analysis tools, the main objectives of the paper are: (a) to identify main sectoral clusters, which accumulate interrelated economic activities, (b) to determine key sectors, which can potentially contribute to the restructuring of the economy, by favoring the production of internationally tradable goods and services, and (c) to determine critical sectors, which can mostly ensure the stability and resilience of the whole economic system. Especially, knowledge of key sectors (or industries) is crucial to design policies for economic recovery, since the expansion of activities of those sectors would lead to a general increase in the economical activity of all or most of the other sectors (Rasmussen 1957). Furthermore, knowledge about the formation of groups or activity clusters may allow better coordination of policies and more efficient allocation of resources and utilization of infrastructure, information sharing and improved business practices (Kali 2013), in order to promote innovation and knowledge transfer within and among sectors. As far as the organization of the rest of the paper is concerned, Sect. 2 reviews the use of network analysis in economic systems in the existing literature. Section 3 provides a description of the data. Section 4 presents the results of a procedure for identifying the main economic activity clusters of the country. Section 5 reports and discusses the results about the key sectors and critical sectors and links of the Greek economy network, and Sect. 6 summarizes and concludes.",18
12.0,2.0,Journal of Economic Interaction and Coordination,10 May 2017,https://link.springer.com/article/10.1007/s11403-017-0196-1,Heterogeneous trading and complex price dynamics,July 2017,Mengling Li,Huanhuan Zheng,,Unknown,Unknown,Unknown,Unknown,,
12.0,3.0,Journal of Economic Interaction and Coordination,09 February 2016,https://link.springer.com/article/10.1007/s11403-016-0172-1,Voting for the distribution rule in a Public Good Game with heterogeneous endowments,October 2017,Annarita Colasante,Alberto Russo,,Female,Male,Unknown,Mix,,
12.0,3.0,Journal of Economic Interaction and Coordination,04 March 2016,https://link.springer.com/article/10.1007/s11403-016-0173-0,Complexity and model comparison in agent based modeling of financial markets,October 2017,Alexandru Mandes,Peter Winker,,Male,Male,Unknown,Male,"Agent-based modeling (ABMFootnote 1) basically stands for bottom-up modeling, i.e. instead of directly modeling the global outcome, the focus is shifted towards the individual agents and on the interactions between them, which subsequently lead to the emergence of collective behavior. By this description of ABM, one can see that ABM might also be interpreted as a computational model suitable for describing complex systems. In the field of economics, this modeling framework is also known as Agent-based Computational Economics or Complexity Economics [see, e.g., Tesfatsion (2001), Arthur (2014)]. ABM has been applied in various fields of research where complex systems are involved, ranging from natural phenomena [e.g., ecological systems as in Reuter et al. (2011)] to socio-economical systems [see, e.g., the contributions in LeBaron and Winker (2008) for some examples]. This contribution focuses on applications to financial markets, but the approach taken might be useful also in other fields. Reviewing the literature on ABM in the financial markets context allows to identify two slightly different lines of research. The first one, which covers most of the existing studies, views ABM as a tool for testing abstract ideas or thought experiments by means of computer-based simulations. Such ABM are usually evaluated only qualitatively, if at all, e.g., by examining their ability to replicate some of the so-called “stylized facts” of financial markets (Cont 2001; Winker and Jeleskovic 2006, 2007). The second line takes a more quantitative (econometrical) approach and tackles calibration to real data (Winker et al. 2007; Chen and Lux 2015) or even forecasting. A calibration procedure—in the sense of parameter estimation—represents a relevant step in order to fit a model to a specific time series. However, given the uncertainty about the most adequate model, choosing parameter values alone is not sufficient. It has to interact with a further step, namely taking into account also the model specification, i.e. a model selection procedure. A first example is provided by Winker et al. (2007) who rejected the model proposed by Kirman (1991, 1993) for exchange rate data under the assumption of a constant fundamental value. The rejection was based on an objective function comparing properties of real and simulated data. The minimum obtained for this objective function was tested against the bootstrap distribution for the real data. Winker et al. (2007) also rejected the model suggested by Lux and Marchesi (2000). A further example is provided by Fabretti (2013) who set up an optimization problem for the ABM proposed by Farmer and Joshi (2002) using the objective function introduced by Winker et al. (2007) and both a Nelder–Mead with Threshold Accepting, as well as a Genetic Algorithm optimization. After fitting the model parameters and comparing the (optimized) simulated time series properties with the ones of real data, the authors conclude that the replication is not completely satisfactory, which is attributed to the design of the model itself. Model design is still quite an open problem in ABM. In the last two decades, a substantial number of ABM for financial markets have been proposed following quite different approaches. This diversity might be explained by the interdisciplinary character of ABM and, consequently, researchers with a different modeling background. Chen (2012) provides a historical perspective on the different lines of research in agent-based computational economics. Moreover, several surveys review the most prominent milestones of this development, e.g., LeBaron (2000), Tesfatsion (2001), Hommes (2006), LeBaron (2006), Lux (2009), Chiarella et al. (2009), Hommes and Wagener (2009), Cristelli et al. (2011), Iori and Porter (2012). However, a commonly agreed upon set of “core concepts” is still missing and also the issue of discriminating between various models has not been completely solved. Chen et al. (2012) propose an ABM taxonomy and order the different types of models with respect to their complexity.Footnote 2 According to the authors, there are essentially two main design classes of financial agents in ABM of financial markets: the N-type design and the autonomous-agent (AA) design. Within the first class, agents have either a behavioral forecasting rule, such as fundamentalists and chartists, or follow a more loosely-typed rule described by general—linear or non-linear—functions. The adaption strategy consists in a herding or a probabilistic switching mechanism. This class spans from few-type to many-type agents. Intuitively, the few-type models are considered less complex than the many-type models because of their limited heterogeneity. On the other side, the autonomous-adaptive agents are associated with a more complex learning algorithm, such as a Genetic Algorithm (GA) or Genetic Programming (GP), which does not only change the agents’ behavior by choosing between predetermined strategies, but also creates new, sometimes improved, trading rules. Again intuitively, this class is considered to be more complex than the N-type design, as the learning mechanism is able to generate a more heterogeneous population, which enables to search through a larger space of forecasting functions. 
Chen et al. (2012) classify 50 ABM with respect to their type (class membership: 2-type, 3-type, many-type, AA) and analyzes to what extent the type has an impact on the capability to replicate stylized facts. The authors find no significant evidence in favor of more complex design models having higher explanatory power (explaining more stylized facts).Footnote 3 From our perspective, it is interesting that Chen et al. (2012) define the concept of complexity for an ABM class as the degree of agents’ heterogeneity (diversity). Though, this heterogeneity is not independent of the learning algorithm. Following a different approach, Westerhoff (2009) considers the “level” of building blocks of ABM in order to find explanations for their ability to replicate stylized facts. The author tweaks around with different parametric forms and with stochastic factors of the agents’ trading rules, of the strategy selection mechanism, as well as of the price impact function, which defines the price adjustment process. He finds that the underlying structure of the endogenous dynamics is a result of the non-linear (deterministic or stochastic) interactions between positive and negative feedback rules. Our research question is how size and complexity of an ABM can be defined at the conceptual level, and how it could actually be measured. Thereby, we have to take into account that from the perspective of a complex system, at least two dimensions have to be distinguished: the size of the model itself (number of components, parameters, description length, non-linearity etc.) and the emergent complexity resulting from the aggregation of individual behavior in an ABM. We might consider the first approach as the more conventional input oriented definition of complexity, while the second one is focusing more on the qualitative properties of the resulting dynamics. We will not address the effective trade-off between complexity and fit, leaving this open question for further research. We will start by identifying the main building blocks of typical ABM for financial markets in Sect. 2. In the following Sect. 3, we will discuss some facets of complexity and how they could be accommodated with ABM. The approach is illustrated with an example of the foreign exchange market in Sect. 4. Section 5 concludes and provides an outlook to further research.",20
12.0,3.0,Journal of Economic Interaction and Coordination,25 May 2016,https://link.springer.com/article/10.1007/s11403-016-0174-z,Monetary policy and dark corners in a stylized agent-based model,October 2017,Stanislao Gualdi,Marco Tarzia,Jean-Philippe Bouchaud,Male,Male,Unknown,Male,"The Agent Based Model (ABM) studied in this paper is a generalization of the “toy-ABM” (dubbed Mark-0) recently introduced in Gualdi et al. (2015a), following previous work by the group of Gaffeo et al. (2008), Delli Gatti et al. (2008) and Delli Gatti et al. (2011).Footnote 1 Mark-0 considers a stylized economy with firms and households, but no banks, no interest rates on loans and deposits, and therefore no direct concept of “monetary policy”. As discussed at length in Gualdi et al. (2015a), the original motivation of Mark-0 was mostly to illustrate the importance of phase diagrams and phase transitions in the context of ABMs, in particular the sensitivity of the state of the (artificial) economy on a subset of parameters. Small changes in the value of these parameters were indeed found to induce sharp variations in aggregate output, unemployment or inflation. In other words, endogenous crises can occur in such economies, as the result of insignificant or anecdotal changes in the environment. This possibility is quite interesting in itself, and must be contrasted with more traditional economic models, such as the popular DSGE framework (Galí 2008; Fagiolo and Roventini 2012), where the dynamics is linear and only large exogenous shocks can cause havoc. As recently pointed out by Blanchard (2014) in a very inspiring piece: We in the field did think of the economy as roughly linear, constantly subject to different shocks, constantly fluctuating, but naturally returning to its steady state over time. [...]. The main lesson of the crisis is that we were much closer to “dark corners” – situations in which the economy could badly malfunction—than we thought.
 Because they can deal with non-linearities, heterogeneities and crises, ABMs are often promoted as possible alternatives to the DSGE models used in central banks as guides for monetary policy (Deissenberg et al. 2008; Dawid et al. 2011; Dosi et al. 2005, 2010; Mandel et al. 2009; Fagiolo and Roventini 2012). It is therefore clear that introducing interest rates monitored by a central bank in Mark-0 is mandatory for policy makers to develop any interest in the ABM research program in general and in our model in particular. The aim of the present paper is to parsimoniously extend Mark-0 as to capture the effects of monetary policy on the course of the economy. We first identify and model several channels through which interest rates can feed into the behavior of firms and households. We then study different policy experiments, whereby the “Central Bank” attempts to reach a target inflation and/or unemployment rate using a Taylor rule to set the interest rate (see Eq. (3) below). We find that provided the economy is far from phase boundaries [or “dark corners” (Blanchard 2014)] such policies can be successful, whereas too aggressive policies may in fact, unwillingly, drive the economy to an unstable state, where large swings of inflation and unemployment occur. Our Agent-Based framework is voluntarily bare bones. It posits a minimal set of plausible ingredients that are most probably present in the real world in one form or the other. For example, we assume reasonable heuristic rules for the hiring/firing and wage policies of firms confronted with over- or under-production, or with a rising level of debt. Similarly, our model encodes in a schematic manner the consumption behavior of households facing inflation and rising rates, that is in fact similar to the standard Euler equation for consumption in general equilibrium/DSGE models (Galí 2008). Our approach is therefore prone to the usual critique addressed to ABMs: the rules we implement are—although reasonable—to some degree arbitrary. The ABM community is of course aware of this weakness, with many attempts to resolve it, such as imposing consistency constraints on behavioral assumptions (see Hoog and Dawid 2015, Appendix 1), or, even better, relying on micro-panel data that reveal how firms and households actually make decisions under different macro-economic or specific conditions [see for example Lein (2010), Kothari et al. (2014) for the behavior of firms, and Gross and Souleles (2002), Souleles (2004), Ludvigson (2004), Nardi et al. (2012) for the behaviour of households]. However, reliable empirical data are still rather scarce and do not allow yet to answer all the questions needed to constrain and calibrate an ABM, even simple ones like Mark-0. Our philosophy, explained in detail in Gualdi et al. (2015a), is different. We argue that the qualitative, aggregate behavior elicited by the Mark-0 model is in fact robust and generic, although the actual quantitative aspects may not be (as, for example, the precise value of the parameters of the Taylor rule beyond which instabilities occur). In other words, if one forgoes the idea of quantitatively predicting the macro-behaviour of the economy but is satisfied (at least temporarily) with a qualitative description of the possible aggregate behaviour, some progress is possible without detailed knowledge of the micro-rules. Our belief is backed by the idea—pervasive in many areas of science—that the aggregate properties of interacting entities can be classified in different phases, separated by phase boundaries across which radical changes of the emergent behavior take place. This idea has a long history, in particular in economics—remember, for example, the title of Thomas Schelling’s famous book: Micromotives and Macrobehaviour (Schelling 1978); for more recent progress see e.g. Kirman (2010), Hommes (2013), Ehrhardt et al. (2006), Anand et al. (2010), Marsili (2014), Gai et al. (2011), da Gama Batista et al. (2015) and for recent reviews: Bouchaud (2013), Chakraborti et al. (2015). To bolster our belief that emergent aggregate properties are robust against micro-changes, we have tested many variants of the model presented below and indeed found that the overall behavior of our artificial economy is remarkably robust—in particular the presence of instabilities and crises. Following up on Blanchard’s lament (Blanchard 2014), the existence of large swaths of the parameter space where the economy is prone to violent crises seems to be an unavoidable fact that we have to learn to confront with (Kirman 2010; Caballero 2010). The work presented in this paper is part of a growing literature of macroeconomic agent-based models (Tesfatsion 2006; LeBaron and Tesfatsion 2008). In the recent years several such models have been developed, allowing both to reproduce macro-stylized facts and to study policy design (Dawid and Fagiolo 2008). Building upon their previous work (Dosi et al. 2005, 2010, 2013, 2015), Dosi et al. characterize the impact of fiscal and monetary policies on macroeconomics fluctuations. Fiscal policies are found to have a greater role in dampening business cycles, reducing unemployment and the likelihood of economic crises. Another example somewhat related to our work is Lengnick and Wohltmann (2013), which focuses on a simple ABM of financial markets coupled with a New Keynesian DSGE model, which allows one to reproduce endogenously stock price bubbles and business cycles and to study the introduction of financial taxes. Calibration and validation procedures have been explored in Haber (2008), Bianchi (2007), Fagiolo (2006). Although similar in spirit, the present study builds upon a slightly different perspective, following the framework and philosophy developed in Gualdi et al. (2015a). In particular, we model an idealized closed economy with linear production capabilities, no capital (labor is the only input for production), no innovation and growth, no financial sector, and only a minimal set of additional behavioural rules which couple the Central Bank monetary policy to firms’ and agents’ choices. In this sense, our model is much more parsimonious than what is found in the recent ABM literature and voluntarily overlooks important actors and processes at play in the real world. Our central concern is instead to characterize the qualitative behaviour of the emerging economy and investigate its dynamical stability with respect to the Central Bank’s policy. The outline of the paper is as follows. In Sect. 2 we give a brief description of the Mark-0 model proposed in Gualdi et al. (2015a) and detail the minimal additional rules we need to couple the monetary policy with firms’ and agents’ decisions. In Sect. 3, we investigate the “natural” state of our toy-economy, in the presence of interest rates but without any Central Bank intervention. In Sect. 4, we perform several policy experiments: an expansionary monetary shock, and the implementation of a Taylor-rule monitoring of the interest rate by the Central Bank to achieve given employement and inflation targets. We establish the phase diagram of our model in the presence of this Taylor-rule based intervention. We briefly compare our findings to the prediction of DSGE models. We conclude in Sect. 5. Appendix 1 gives further details about Mark-0; Appendix 2 investigates the continuous phase transition induced by the wary behaviour of indebted firms; finally, Appendix 3 gives a full pseudo-code of our model that should allow easy duplication of our results.",15
12.0,3.0,Journal of Economic Interaction and Coordination,01 August 2016,https://link.springer.com/article/10.1007/s11403-016-0175-y,Heterogeneity of expectations and financial crises: a stochastic dynamic approach,October 2017,Toshihiro Shimizu,,,Male,Unknown,Unknown,Male,"The subprime financial crisis has had a devastating impact on the world economy since 2007. What caused it and how policymakers responded to it will be widely debated among scholars, politicians, and business people for a long time to come. This study presents a stochastic dynamic model that describes the crisis as a resurgence of the “liquidity trap” postulated by Keynes (1936). A liquidity trap represents a situation in which the demand for money is ultra-sensitive to the interest rate when the majority of agents regard the interest rate as too low to decline further. It is noteworthy that the emergence of a liquidity trap is contingent on people’s expectations about the future interest rate. Particularly, a liquidity trap emerges when economic agents uniformly predict that the interest rate will not drop to a lower level. Keynes (1936, p. 172) remarks, “It is interesting that the stability of the system and its sensitiveness to changes in the quantity of money should be so dependent on the existence of a variety of opinion about what is uncertain.” Simply put, a liquidity trap is a situation in which the heterogeneity of opinions is absent in the economy. Following the lead of Keynes, Tobin (1958) developed models of the demand for money that take two approaches. One is his famous mean-variance approach, which laid the foundation of the theory of portfolio selection and has been influential for subsequent studies in financial economics. His other approach focuses on the heterogeneity of expectations about the interest rate. When each investor has a different expectation about the future interest rate, the demand for money can be traced on a smooth, downward-sloping curve, the shape of which depends on the distribution of expected interest rates. However, the second approach, in particular, the heterogeneity of expectations has received little attention in subsequent developments in macroeconomics and financial economics. Although several studies have identified the importance of the heterogeneity of opinions, few have incorporated it explicitly in theoretical models. In particular, the majority of mainstream macroeconomic models presuppose the representative agent and the rational expectations hypothesis. This study deploys a binary choice model, following Aoki (1998) and Aoki and Yoshikawa (2006). In this model, an economic agent stochastically changes her decisions. The transition rates from one state to the other vary depending on the degree of diversity in expectations. This model might possess several locally stable equilibria. Then, the equilibrium randomly moves from one state to another and oscillates asymmetrically. Applying this model to the money/bond choice, this study seeks to derive the money demand function discussed by Keynes and analyze how the heterogeneity of expectations affects it. It provides a heterogeneous agent-based microfoundation for the famous and influential discussions on money demand by Keynes, Tobin, and others. The study also examines a situation in which the relative “thickness” of money holders increases the attractiveness of money, thereby further enhancing the profitability of money holding. Although there are several studies that indicate such a possibility, this study distinguishes itself by showing that when the profitability of money increases with the thickness of money holders, the economy could have multiple equilibria, stochastically settle on one, but transit to the others over time. This study interprets a financial crisis as a leap from one equilibrium to the other and then conducts comparative statics to clarify the condition for the existence of multiple equilibria. The model helps to explain the recent financial crisis and provides practical implications for monetary policy. In particular, in analyzing the influences of heterogeneous expectations on the economy, the model offers a novel perspective on monetary economics, especially on the role of monetary policy, by demonstrating that an unconventional monetary policy (e.g., a credible long-term commitment to a zero interest rate policy) works better than a conventional one (e.g., continual short-term adjustment of the policy rate) in response to a crisis induced by a flight to liquidity. This paper proceeds as follows. Section 2 constructs a stochastic dynamic model, and Sect. 3 derives the money demand function from the model. Section 4 analyzes the case in which an increase in the proportion of money holders amplifies the profitability of money holding and interprets the results. Section 5 discusses implications of our findings for financial crises and monetary policy. Section 6 concludes the paper by suggesting possible extensions of the study.",1
12.0,3.0,Journal of Economic Interaction and Coordination,23 August 2016,https://link.springer.com/article/10.1007/s11403-016-0176-x,Multiscale correlation networks analysis of the US stock market: a wavelet analysis,October 2017,Gang-Jin Wang,Chi Xie,Shou Chen,Unknown,,,Mix,,
12.0,3.0,Journal of Economic Interaction and Coordination,16 September 2016,https://link.springer.com/article/10.1007/s11403-016-0177-9,The effect of communication channels on promise-making and promise-keeping: experimental evidence,October 2017,Julian Conrads,Tommaso Reggiani,,Male,Male,Unknown,Male,"In modern organizations, new communication channels are reshaping the way in which people get in touch, interact and cooperate. Emails, online polls, chat rooms, and conference calls are replacing face-to-face interactions in many situations. At the same time, non-binding and spontaneous cooperation between members of the staff has become a key factor in dealing with the increasing complexity in modern organizations (The Economist 2016a, b; Walther 2011). In addition to that, given the increasing size of corporations (e.g., in multinational companies), employees sometimes receive spontaneous requests from colleagues who are complete strangers, possibly sitting in an opposite corner of the world. Ellingsen and Johannesson (2004) found that just a simple indirect and anonymous written communication helps to reinforce promise-keeping in a stylized hold-up problem (gain-from-trade set-up). However, in their conclusions they asked “How would (promise) behavior be affected if interactions were oral and face-to-face rather than written and anonymous?” (pp. 417–418). Our study addresses this specific question raised by Ellingsen and Johannesson (2004)Footnote 1 and looks at its organizational implications. The effects of different channels of communication have already been analyzed in other fields of social interactions. Harbring (2006), for example, examines the impact of unilateral communication (announcements) versus bilateral communication (exchange of emails) in promoting agents’ performance. They find that bilateral communication increases the performance of the agents both under tournament- and team-based incentive schemes. Conversely, when the unilateral communication technology is put in place, detrimental effects in terms of cheating behavior arise only under the tournament-based incentive scheme. 
Brosig et al. (2003) focus on the effects of different communication channels on cooperation in several standard public good games. The authors vary the communication channel applied in pre-play communication, e.g., auditory or visual channels, either bidirectional or unidirectional. They find that bidirectional face-to-face communication is crucial for enhancing cooperation. 
Bicchieri and Lev-On (2007) and Bochet et al. (2006) also find that face-to-face communication increases cooperation in social dilemmas, but also that written communication through a chat room that preserves anonymity and excludes facial/verbal expression is almost as efficient.Footnote 2
 
Valley et al. (1998) study a bilateral negotiation game with asymmetric information, finding different degrees of trust, truth-telling and efficiency across communication channels. Higher levels of truth-telling allow subjects negotiating face-to-face to achieve higher joint benefits than those negotiating by telephone or in writing. In other studies, fixed- or free-form communication in trust games is analyzed, either in face-to-face or in written form. The studies find that pre-play communication increases trust and trustworthiness compared to no communication (Roth 1995; Charness and Dufwenberg 2010). In this vein, Ben-Ner et al. (2011) find that verbal communication in a chat room significantly increases trusting and trustworthiness compared to no or other fixed-forms of communication. Hoffman et al. (1996) and Bohnet and Frey (1999) assume that decreasing social distance increases pro-social behavior in dictator games. The latter authors argue that identification of the “other” causes more prosociality (see also Charness and Gneezy 2008; Gaechter and Fehr 1999). We investigate the effects of alternative communication channels on promise-making and promise-keeping. While Ellingsen and Johannesson (2004) analyze promises in a very abstract environment, we opt for a less stylized set-up that better resembles an organizational context where a broken promise can be a source of direct costs as well as delays or frictions in the organizational flow. Our controlled experiment employs a simple promise-making/promise-keeping task, in which subjects are asked about their willingness to voluntarily commit to taking part in a short online survey for scientific purposes within the next 24 h, without monetary compensation. A baseline face-to-face interaction is compared to a phone call, a chat room-based interactionFootnote 3 a computer-mediated interaction “office”, and a further computer-mediated interaction “remote”, i.e., online.Footnote 4
 Under face-to-face, phone call, and chat-based conditions—which are distinguished by a synchronous interaction between the parties—promise-making rates prove to be significantly higher than under the two non-synchronous and computer-mediated conditions. Despite these differences in promise-making, no significant differences in promise-keeping rates are observed across treatments. The paper is organized as follows: Sect. 2 introduces the experimental design; testable hypotheses are derived in Sect. 3, in light of the technical features of the different communication channels; results are presented in Sect. 4, and final considerations are found in Sect. 5.",18
12.0,3.0,Journal of Economic Interaction and Coordination,30 August 2016,https://link.springer.com/article/10.1007/s11403-016-0178-8,The effect of structural disparities on knowledge diffusion in networks: an agent-based simulation model,October 2017,Matthias Mueller,Kristina Bogner,Muhamed Kudic,Male,Female,Male,Mix,,
12.0,3.0,Journal of Economic Interaction and Coordination,17 September 2016,https://link.springer.com/article/10.1007/s11403-016-0180-1,On the coevolution of social norms in primitive societies,October 2017,Giorgio Negroni,Lidia Bagnoli,,Male,Female,Unknown,Mix,,
13.0,1.0,Journal of Economic Interaction and Coordination,24 March 2018,https://link.springer.com/article/10.1007/s11403-018-0221-z,Recent advances in financial networks and agent-based model validation,April 2018,Mauro Napoletano,Eric Guerci,Nobuyuki Hanaki,Male,Male,Male,Male,"This special issue combines contributions dealing with (i) the analysis of financial networks and (ii) the use of statistical techniques for the validation of agent-based models (ABMs). Although quite heterogeneous in terms of the topics they address, the papers in the issue provide a good overview of two recent and growing fields in the literature inspired by the complexity-based approach to economic analysis (see e.g. Kirman 2010a). The financial crisis of 2008 and the Great Recession that followed have led to a reconsideration of standard economic theories based on general equilibrium and on the representative agent hypothesis (Stiglitz 2011; Kirman 2010b), and to a call for new approaches that could better capture the fundamental features that generated the crisis. One of them is agents’ heterogeneity, which is is needed in order to properly account for any credit relation. Another one is financial contagion and systemic risk, which are very much related to externalities across different agents. Indeed, the transmission of externalities does not occur in a vacuum. On the contrary, it is mediated by the structure of financial and economic contracts existing among agents (Stiglitz 2011). Furthermore, the financial crisis was also triggered by the building-up of large debt imbalances across agents (as well as across countries), which can hardly be accounted for in a equilibrium framework. Finally, the crisis itself was not the result of an aggregate shock, as it is typically postulated in many standard models. It was instead endogenously generated by the interactions of the heterogeneous agents populating an economy. Network models provide a valid alternative to standard models to account for some of the above-mentioned issues (see Battiston et al. 2016). This is because these models allow one to represent the very structure of localized interactions across economic agents. Accordingly, they allow one to study under which conditions financial distress may propagate and systemic risk may emerge. They are therefore valid tools to identify early warning signals about an imminent crisis. In addition, they allow one to properly test the impact of regulatory measures or of new institutional arrangements in financial markets. The papers by Gaffeo and Molinari (2017), Hayakawa (2017) and Catullo et al. (2017) represent very good examples of the applications of network models to the foregoing issues. The paper by Clemente et al. (2017) instead provides a methodological contribution that can prove useful in many network-based analysis of financial contagion. Agent-based models (ABMs henceforth) are another valid alternative to standard models that has received increasing attention after the last crisis. The list of applications of ABMs is already very rich and it keeps growing (see Bargigli and Tedeschi 2013; Haldane and Turrell 2018; Fagiolo and Roventini 2017; Samanidou et al. 2007, for recent accounts). ABMs represent economies as dynamical systems of heterogeneous agents, interacting out of equilibrium (Tesfatsion 2006). In that, they constitute a generalization of the relatively simple framework usually present in network models, as they allow one to introduce richer sets of behavioural rules and institutional features of markets, and to deal with evolving networks. The paper of Catullo et al. (2017) in the issue is also interesting in this respect, as it integrates the network and the ABM approaches for the analysis of credit relations and of the impact of macro-prudential policies. At the same time, ABMs have also been criticized for their inadequate validation against empirical data or for their inability to assess how general their results are. This has so far prevented a full-fledged comparison of ABMs with other models, and accordingly their wider diffusion. Nevertheless, such a critique has started to receive an adequate response in the literature (see also Fagiolo et al. 2017), with the development of statistical techniques that allow one to analyse the sensitivity of the results of an ABM to alternative parameters’ values or to estimate the model’s parameters or, finally, to evaluate the performance of an ABM in explaining features of empirical data. In what follows, we discuss more in depth the papers of the special issue by grouping them into two categories, representing the main areas of contribution to the literature, namely the analysis of financial networks (Sect. 2) and the validation of agent-based models (Sect. 3).",2
13.0,1.0,Journal of Economic Interaction and Coordination,16 November 2017,https://link.springer.com/article/10.1007/s11403-017-0208-1,Does a central clearing counterparty reduce liquidity needs?,April 2018,Hitoshi Hayakawa,,,Male,Unknown,Unknown,Male,"In the recent financial crisis, the collapse of financial institutions, such as Bear Stearns and Lehman Brothers, demonstrated that the interconnected feature of bilateral exposure among financial institutions could lead to market disruption. In order to cope with this apparent vulnerability, the G20 leaders agreed at the 2009 Pittsburgh Summit that standardized over-the-counter (OTC) derivatives should be cleared through central clearing counterparties (CCPs). Central clearing is expected to help mitigate counterparty credit risk by removing the direct risk exposure between counterparties, thereby reducing the systemic risk of the “domino” of defaults and relevant firesales.Footnote 1
 However, the success and relevant cost of central clearing is never evident. For example, CCPs typically require margin themselves in order to bear the counterparty risk arising from cleared derivative transactions. In times of financial distress, margin requirements by CCPs could trigger firesales.Footnote 2 Here, we should note that multilateral netting by CCPs could have already reduced relevant exposure, and have contributed to reduce the required margin compared to settlements without CCPs. The effect of CCPs on overall liquidity needs is not apparent until the two aspects are investigated in a consolidated manner. We further argue about the operating cost of CCPs once the economy is no longer in financial distress. CCPs could affect overall liquidity needs in times of non-distress, and larger liquidity needs tend to imply larger costs, since liquidity is essentially scarce resource. In times of non-distress, when financial institutions are not in a rush to obtain liquidity, contracted trades would need less liquidity, or similarly, liquidity would circulate more efficiently among relevant financial institutions even without CCPs. Consequently, CCPs and their multilateral netting could have different effects on overall liquidity needs compared with those in times of financial distress. In order to assess the effects of CCPs and discuss further improvements of the clearing mechanism, it is important to understand the essential nature of CCPs regarding how they affect overall liquidity needs in times of both financial distress and non-distress. This study develops a stylized model to probe this issue. Our focus is on how the introduction of a CCP could alter the interconnected feature of the relevant network of financial obligations, and how the change of network topology could affect overall liquidity needs. From the perspective of network topology, the introduction of a CCP serves as an additional entity itself in the relevant network, while its multilateral offsetting serves to eliminate relevant obligations. We explicitly show that the effect of a CCP is decomposed into two effects: the central routing effect, and the central netting effect, such that the total effect is essentially the addition of the two effects. The effect of a CCP is examined on the basis of two polar liquidity scenarios. One is assumed as a situation in times of financial distress, by which liquidity circulates least efficiently. The other is assumed as a benchmark situation in times of non-distress, by which liquidity circulates most efficiently. We refer to the former situation as the bad environment and the latter as the good environment. We show that in the bad environment, the central routing effect is always negative, but the central netting effect is always positive. A negative central routing effect means that adding a CCP certainly increases the overall liquidity needs if there is no financial obligation to be offset by the CCP. A positive central netting effect means that larger offset amounts lead to smaller liquidity needs. This implies that the total effect of a CCP is positive in the bad environment when the offset amount is sufficiently large. By contrast, we show that it is possible for the central netting effect in the good environment to be negative, whereby, although counterintuitive, a larger offset amount could lead to larger liquidity needs. This is because eliminating financial obligations could effectively separate a connected network into multiple disconnected networks, thereby inhibiting the same liquidity from circulating through the whole network. We observe a trade-off of multilateral offsetting regarding the overall liquidity needs. It is possible for a CCP to reduce liquidity needs during times of financial distress, thereby reducing the risk of firesales and relevant systemic risk of the “domino” of defaults. However, it could conversely increase overall liquidity needs during times of non-distress. For our benchmark situations, in order for the total effect of a CCP to be always positive in the bad environment, more than two-thirds of the relevant trading needs to be offset. Since the overall liquidity needs in the good environment could become larger in proportion to the offset amount, the trade-off could become serious. There are two policy implications of this research. First, when central clearing is used, the expected offset efficiency for the relevant security should be examined with sufficient care, since insufficient netting efficiency could have an adverse effect. Second, possible severe trade-off associated with multilateral netting suggests conditional utilization of a CCP. Since our analysis shows that multilateral netting could be costly at the time of non-distress but helps mitigate liquidity needs at the time of financial distress, a CCP could be used as an emergency scheme. Although it is out of the scope of this study to argue about the whole cost of such an emergency scheme, our analysis indicates there is possible merit in resolving the underlying trade-off regarding overall liquidity needs. The role of CCPs has been examined largely focusing on how much relevant exposure is reduced through CCPs’ multilateral nettings, supposing that smaller amount of exposure to each counterparty implies smaller counterparty risk. This study departs from the literature by examining the roles of CCPs in overall liquidity needs. Focusing on the effect on exposure, Duffie and Zhu (2011) examine central clearing in derivative markets and point out the possible disadvantage of central clearing arrangements compared with bilateral clearing arrangements when central clearing is provided only within each class of derivatives. The roles of CCPs in derivative markets are debated in Bliss and Kaufman (2006), Bliss and Steigerwald (2006), and Pirrong (2009). Jackson and Manning (2007) argue about the effects of CCPs in relation to “tiering,” which refers to the ratio between the number of indirect and direct members of CCPs. The authors argue there is incentive for a “tiered” structure. Galbiati and Soramäki (2012) analyze the implications of “tiering” in terms of network topology. In view of relevant network topologies, they examine the tree structure, while more general structures matter in our liquidity context. Several recent studies have examined the role of CCPs in affecting overall liquidity needs, in the context of how CCPs set their margins. Murphy et al. (2014) investigate the procyclical nature of various margin models, proposing quantitative measures of procyclicality. Abruzzo and Park (2016) empirically analyze the margin-setting behavior of CCPs, and find that margin-induced procyclicality is a concern during recessions, but not during times of expansion. Miglietta et al. (2015) quantify the impact on the cost of funding in repo markets of the initial margins applied by CCPs. These studies have helped clarify the possible negative effects of CCPs on overall liquidity needs, but they have not explicitly shown how the existence of CCPs could have negative effects compared with cases without CCPs. This study provides a stylized model that enables us to compare a situation with a CCP with that without a CCP. Our focus is on the effects of CCPs on overall liquidity needs by affecting the relevant network topology. For this purpose, we utilize liquidity problems defined in the network, which are formally represented as problems of flow network. The model and liquidity problems used in this study are based on Hayakawa (2016), who investigates settlement efficiency of gross settlements in view of network topology.Footnote 3 The present study serves as an application of Hayakawa (2016) to examine the role of CCPs, utilizing several basic results of the study. Section 2 presents our model. Section 3 illustrates our analysis in a less formal manner. Section 4 provides an overview of the results. Section 5 shows our formal analysis. Section 6 concludes. The appendix includes proofs of the relevant results.",7
13.0,1.0,Journal of Economic Interaction and Coordination,02 December 2017,https://link.springer.com/article/10.1007/s11403-017-0210-7,A functional perspective on financial networks,April 2018,Edoardo Gaffeo,Massimo Molinari,,Male,Male,Unknown,Male,"Over the last two decades, a large literature based on the theory of networks has actively contributed to enhance our comprehension of contagion-related phenomena in modern financial systems (Chinazzi and Fagiolo 2015; Neveu 2016). Recognizing the pervasiveness of cyclical interdependencies (A has obligations to B, who has obligations to C, who in turn has obligations to A) and higher-order feedbacks, network models have been rightfully employed by both academics and regulators for the assessment of systemic risk, and as a guidance to gauge how such risk maps onto the topological structure of the complex web of transactions linking financial institutions together. A large body of empirical studies have indeed focused on the characterization of real-world financial networks,Footnote 1 whereas simulation models have been developed to assess the extent of system-wide disruptions under different capitalization scenarios, network topologies, resolution procedures, contagion channels, types of shocks, degrees of heterogeneity in interbank exposures and consolidation policies.Footnote 2
 In fact, the relevance of such a huge research effort goes well beyond the contingencies related to episodes of financial crises and market freezes, but points straightly to the core issue of how a financial system performs the task of facilitating the allocation of resources in an uncertain environment. As emphasized by Allen and Babus (2009), the network approach to financial systems proves useful for understanding other relevant economic phenomena, like the relation between investment decisions and the way information flows among decision makers, the very existence of intermediaries specializing in investment banking, or the role of social interconnectedness in solving group incentive problems in the enforceability of contracts. In this paper we aim at complementing this catalogue by offering a functional perspective on what financial institutions effectively do, and how this relates to the resilience of the whole system to external shocks. The material offered here is partly empirical and partly methodological. On the one hand, we provide evidence documenting that a functional perspective on financial intermediation can indeed represent an improvement of our understanding on how actual financial networks work, as well as on how policy actions impinge upon them. On the other hand, we propose a novel approach to comparatively assess the functional performance of alternative institutional structures. This is grounded on the idea of comparing the historical evolution of the actual institutional setting under alternative stress-testing scenarios, with those that one would have obtained if the structure of the financial system—in our case, the topology of the interconnections linking financial intermediaries—had been different but the whole amount of services delivered by the system was unchanged. Our starting point consists in recognizing that financial institutions represent a key component of an economic system, since they deliver services—or functions, according to the conceptual framework put forth by Merton and Bodie (2005, 1995) and Merton (1995)—that are vital for the saving-investment process, like the clearing and settlement of payments, the transfer of economic resources through time and space, the management and pricing of risk, and the deployment of mechanisms for dealing with incentive problems. While the institutional structure of a financial system—in terms of prevailing institutions, instruments and markets—can vary substantially over space and time, the basic functions it performs remain basically the same. An obvious corollary descending from such a perspective is that the analysis of how a financial system works should be primarily focused on what kind of functions are activated by each financial institution, how different institutions evolve over time as regards the particular type of function they offer, and how they interact and co-evolve with the functions performed by other actors. This view has also profound policy implications. Besides the conventional concern of regulating intermediaries according to their contribution to systemic risk—which is clearly related to their degree of interconnectedness—the focus on the actual functions provided by financial institutions viewed as heterogeneous nodes of a financial network speaks directly to the transmission mechanism of monetary policy. Malfunctions in one or more critical nodes ensuring critical services for the rest of the financial system exert an impact on the central bank’s ability to affect the real economy through the bank-lending or the risk-taking channels as it moves interest rates. Furthermore, the effectiveness with which a central bank acting as a lender-of-last-resort channels funds to restore normal market conditions during a crisis depends on the specialized function enacted by the class of institutions chosen as the insertion point of extra liquidity. Finally, long and interconnected chains of intermediation affect the link between money growth and price inflation, with far from obvious implications for monetary analysis (ECB 2012). We have access to a unique dataset that allows us to provide a thorough inspection and characterization of a financial system in which several economic functions can be analyzed with a good degree of accuracy. More in detail, we exploit a network approach to offer an empirical assessment of the double role played by Cassa Centrale Banca (henceforth CC)—a money center offering intermediation services to a large number of small cooperative credit banks in the North–East of ItalyFootnote 3—as a provider of services for other intermediaries on the one hand, and as an entry point for liquidity injections by the central bank on the other one. Due to their tiny size, most of the cooperative credit banks involved have no direct access to the official interbank market to accommodate their surpluses and deficits of liquidity. They use instead a sort of liquidity pool—CC itself—that, in addition to accepting deposits from and extending loans to banks, has direct access to the wholesale funding market and to the re-financing facilities of the Bank of Italy. Our dataset comprises previously unreleased biannual (overnight and non-overnight) records of interbank flows intermediated by CC over the period 2001–2011 among 224 credit cooperative banks and 69 shareholders banks, together with detailed information on the liquidity injections provided by the Bank of Italy. The time span covered by the data is particularly appealing, as it sequentially comprises a period of substantial tranquility, the local reverberation of the global financial turmoil of 2007–2008, and the outbreak of the European sovereign debt crises in 2010, which caused the monetary authority to activate a sizable program of Long Term Refinancing Operations (LTROs) to restore normal market conditions. The financial network under scrutiny is characterized by an invariant star-shaped topology. In spite of this, the system displays a richness of dynamic evolution. In the period of time considered, we can in fact identify three distinct stages: (i) at first, CC acts as an intermediary between surplus cooperative banks and deficit shareholder banks; (ii) during an intermediate phase, CC works as a liquidity distributing device also among cooperative banks, with excess liquidity deposited at the central bank; (iii) at the very end of our observation period, CC performs as a key node in the transmission of monetary policy. The set of our empirical findings can be complemented as follows. First, the degree of interconnectedness in the network has remained stable until the onset of the 2007–2008 financial crisis, and it has increased exponentially during the sovereign debt crisis. Second, the persistence of bilateral trading between the hub and leaf nodes of the star has been significantly higher and more stable for borrowing. A structural break appeared as the central bank started to provide liquidity to the market in the first half of 2010. Third, the role of CC has changed over time from being a net lender to becoming a net borrower. Fourth, until the end of 2005 CC operated as a maturity transformer by acting as a net borrower in the overnight market and a net lender in the non-overnight market. By the end of 2010 this function was completely reversed. Fifth, the presence of CC were also conducive to a volume transformation of interbank flows. It did so by intermediating liquidity from a large number of surpluses units to a small pool of deficit banks, whose liquidity requirements were much larger in volume. Clearly, none of these functions could have been executed if the liquidity exchanges flowing through the network were somehow interrupted, due for instance to external liquidity shocks causing serious disruptions of the interbank market through contagion-like cascades. When read in the light of the literature dealing with the relationship between network topology and the vulnerability of the financial system to systemic risk, therefore, a functional perspective on financial intermediation encourages us to advance additional questions, which we tried to answer by adopting a counterfactual history approach. Was a star-shaped topology really efficient in limiting the risk of aggregate disruptions due to liquidity shocks, or would other network architectures have been capable to ensure better results? What kind of outcomes would we have achieved if the central bank’s liquidity injections had been made through the money center, as it actually occurred, but banks were allowed instead to exchange funds also bilaterally? What if the central bank could inject liquidity without the intermediation of CC? In order to answer these questions we use computer simulations, by constructing a set of “what if” alternative histories. We artificially alter the underlying structure of the interbank network and compare the actual star-shaped one to alternative architectures, paying attention to maintain the total amount of interbank credit and external liquidity injections equal to the ones observed in real data. In simulations, contagion dynamics are triggered by exogenous (homogenous and targeted) shocks that take the form of unforeseen bank runs on deposits. Banks respond to these unexpected liquidity shortages by recalling their interbank assets. These orders work as an additional funding shock to their interbank borrowers, that are now asked to find the necessary liquidity to meet their lenders’ demand by calling in their interbank claims. These funding shocks propagate through the network and will eventually account for a series of complex knock-on effects that will engender losses to an extent greater than the initial exogenous shock. Such amplification arises because the exact magnitude of aggregate disruption will in fact be the sum of the exogenous shock, i.e. the initial deposit withdrawal determined ex-ante, and the contagion-driven erosion of interbank assets, which can only be assessed ex-post and depends on the distribution of links i.e. the topology of the network. We calculate final interbank losses by solving a fixed-point equation in a fashion similar to that presented in Lee (2013), that allows us to compute both first-order direct losses and contagion multipliers under a set of alternative network topologies. Results from our historical stress testing exercises suggest that the benefits associated with the presence of a money center, such as CC, has to be carefully pondered against the higher systemic liquidity risk owing through the interbank market. In particular, the relative efficiency of a star-shaped institutional structure in limiting damages if a shock hits the network depends on the metric the policymaker employs in measuring welfare losses. The remainder of the paper is organized as follows. In Sect. 2 we argue in favor of a functional perspective in analyzing financial networks. Section 3 presents an examination of the topological structure of our dataset, with a focus on the qualitative asset transformation services operated over time by CC. Section 4 discusses the role of the hub in a star-shaped network in transferring liquidity to the whole system, and sets the stage for the subsequent counterfactual experiments. Section 5 presents simulation results. Section 6 concludes.",4
13.0,1.0,Journal of Economic Interaction and Coordination,26 July 2017,https://link.springer.com/article/10.1007/s11403-017-0199-y,Early warning indicators and macro-prudential policies: a credit network agent based model,April 2018,Ermanno Catullo,Antonio Palestrini,Mauro Gallegati,Male,Male,Male,Male,"After the 2007 crisis, systemic risk and macro-prudential policies have been at the center of the economic debate (Basel Committee 2010; Yellen 2011; Ghosh and Canuto 2013; MARS 2014). Several studies on interbank markets and credit networks show that networks configurations play a crucial role in determining the vulnerability of the economic system (Allen and Gale 2000; Iori et al. 2006; Battiston et al. 2012; Caccioli et al. 2012; Allen et al. 2012; Palestrini 2013; Battiston et al. 2016). Simulating an endogenous evolving credit network, we aim at gaining insights into the relation between network configurations and systemic risk in order to select early warning indicators for crises and define policy precautionary measures. Following the network-based financial accelerator approach (Delli Gatti et al. 2010; Riccetti et al. 2013; Catullo et al. 2015), we constructed a macroeconomic agent-based model (Delli Gatti et al. 2005; Delli Gatti et al. 2010; Dosi et al. 2010) reproducing an artificial credit network, which evolves endogenously through individual demand and supply of loans of heterogeneous firms and banks. We modified both learning and credit matching mechanisms of the Catullo et al. (2015) model in order to increase the stability of individual leverage choices and to reduce the inertia of the network configuration dynamics. Moreover, adopting the methodology followed by Schularick and Taylor (2012), we isolated early warning indicators for crises (Alessi and Detken 2011; Babecky et al. 2011; Betz et al. 2014; Drehmann and Juselius 2014; Alessi et al. 2015) and we simulated different policy scenarios using capital-related macroprudential policiesFootnote 1 (IMF 2011; Claessens 2014; MARS 2014; Angeloni 2014). The model defines a simple interaction structure between firms and banks. Firms fund production through internal resources and by borrowing money from banks. By increasing the leverage, firms are able to raise their production level and thus boost the expected revenues. However, firm revenues are influenced by idiosyncratic shocks. Consequently, if a firm increases its leverage, its expected profits will augment. At the same time, however, the firm’s exposure to negative shocks will raise along with its failure probability (Greenwald and Stiglitz 1993; Delli Gatti et al. 2005; Riccetti et al. 2013). Moreover, high levels of target leverage are associated with high interest rates on loans and high probability of suffering credit rationing. Similarly, high levels of leverage result in the augmentation of the expected profits for banks, but also in the raising of their exposure to the firm’s failure. Therefore, firms and banks have to deal with the trade-off between increasing their leverage to augment expected profit, and reducing exposure to contain failure probability (Riccetti et al. 2013; Catullo et al. 2015). In the attempt of gaining satisfying levels of profits without being exposed to excessive risks, firms and banks choose their target level of leverage through a simple reinforcement learning procedure (Tesfatsion 2005; Catullo et al. 2015). Consequently, modifying individual credit demand and supply, agent target leverage choices determine the evolution of the credit network. We calibrated the model on a sample of firms and banks quoted on the Japanese stock-exchange markets from 1980 to 2012 (Marotta et al. 2015), reproducing the levels of leverage, connectivity and output volatility observed in the empirical dataset. Model simulations generate endogenous pro-cyclical fluctuations of credit and connectivity. Indeed, since they lend to relatively robust firms, banks are able to increase their net-worth during expansions: hence, they do not suffer from firm failures. Consequently, the bank’s supply of loans augments implying that borrowing money for firms becomes easier. Thus, both the firm’s leverage and the integration of the credit network tend to increase. However, the default risk increases with leverage, and high connectivity may foster the diffusion of negative effects of firms and banks failure (Delli Gatti et al. 2010; Riccetti et al. 2013; Catullo et al. 2015). In effect, aggregate credit leverage and connectivity are positively correlated with the number of firm failures. Therefore, during expansionary phases credit, leverage and connectivity growth may create the conditions for future recessions and crises (Minsky 1986). Indeed, following the methodology developed by Schularick and Taylor (2012), we found that both credit and connectivity growth rates are positively correlated with crisis probability and that they are effective early warning measures in both empirical and simulated data. Moreover, the model is suitable for designing macro prudential policies which exploit agent heterogeneity and the network’s interaction structure. Indeed, capital-related measures which force banks to avoid lending to more indebted firms may decrease the output’s volatility without causing consistent credit reductions and, thus, output contractions. We also tested permanent capital-related measures applied to larger and more connected banks only. When interventions target banks that are relatively central in the credit network in terms of size and connections, the vulnerability of the economic system may be substantially reduced without affecting aggregate credit supply and output. Thus, the analysis of credit network connectivity may be useful for assessing the emerging of system risk. Besides, agent-based models that endogenize credit network dynamics may be used for testing the effectiveness of early warning indicators and the effects of different macro-prudential policies. The paper is structured as follows. The next section describes the agent-based model: agents behavioral assumptions, matching mechanisms between banks and firms and leverage decisions. The third section illustrates simulation results. In first instance, we will focus on the patterns of calibrated simulations. Secondly, we test the effectiveness of connectivity measures as early warning indicators. After that, we will implement simple macro-prudential capital-related measures. The last section contains our conclusions.",4
13.0,1.0,Journal of Economic Interaction and Coordination,30 August 2017,https://link.springer.com/article/10.1007/s11403-017-0202-7,Structural comparisons of networks and model-based detection of small-worldness,April 2018,Gian Paolo Clemente,Marco Fattore,Rosanna Grassi,Male,Male,Female,Mix,,
13.0,1.0,Journal of Economic Interaction and Coordination,20 November 2017,https://link.springer.com/article/10.1007/s11403-017-0206-3,Empirical validation of simulated models through the GSL-div: an illustrative application,April 2018,Francesco Lamperti,,,Male,Unknown,Unknown,Male,"Empirical validation is crucial for all modelling frameworks providing support to policy decisions, independently of their theoretical background. Even though agent based models (ABMs) have often been advocated as promising alternatives to neoclassical models rooted in the dogmatic paradigms of rational expectations and representative agents, there are still some concerns about how to bring them down to the data (Windrum et al. 2007; Gallegati and Richiardi 2009; Grazzini and Richiardi 2015). In macroeconomics, for example, Giannone et al. (2006), Canova and Sala (2009) and Paccagnini (2009) provide details about how to estimate and validate dynamic stochastic general equilibrium models. However, their approach cannot be extended to settings where an analytical solution of the model (or an equilibrium) does not exist, which are typical cases in ABMs, system dynamics and complex systems more in general. Broadly speaking, these numerical models are validated through a comparison of the statistical properties emerging from simulated and real data. In many cases, this amounts at replicating the largest possible number of stylized facts characterizing the phenomenon of interest (see Dosi et al. 2010, 2013, 2015 for business cycle properties, credit and interbank markets or Pellizzari and Forno 2006; Jacob Leal et al. 2015 for financial markets). Recent attempts are trying to enrich empirical validation beyond the simple replication of empirical regularities, thereby requesting models to generate series that exhibit the same dynamics (Marks 2013; Lamperti 2017), conditional probabilistic structure (Barde 2016b) and causal relations (Guerini and Moneta 2016) as those observed in the real world data.Footnote 1 At least partially, such contributions have been motivated by the unsatisfactory results delivered by calibration. In general, it is difficult to justify the choice of one combination of model’s parameters over another, and calibration can be thought exactly as the exercise of selecting the values of the parameter set that best fit the real data. In this paper I present an application of the GSL-div developed in Lamperti (2017) to validate model’s output against real word data and explore the behaviour of the model quantifying the distance between the dynamics observed in the data and those simulated numerically. GSL-div stands for Generalized Subtracted L-divergence and constitutes an information theoretic criterion that builds on the L-divergence (Lin 1991) and measures the distance between distributions of patterns retrieved in time series data. Validation is achieved capturing the ability of a given model to reproduce the distributions of time changes (that is, changes in the process’ values from one point in time to another) in the real-world series, without the need to resort to any likelihood function or to impose requirements of stationarity. The GSL-div adds something that seems missing in the literature: a precise quantification of the distance between the model and data with respect to their dynamics in the time domain. On this side, my work builds on Marks (2013) and extend it by capturing and emphasizing the dynamical nature of time series models, which is, for example, loosely represented by the longitudinal moments used in many calibration exercises. The GSL-div is tested on the series produced by the well known asset pricing model with heterogeneous traders developed in Brock and Hommes (1998). The rest of the paper is organized as follows. Section 3 introduces the GSL-div, discusses its main properties and provides a simple example; Sect. 4 summarizes the mathematical structure of the model that will be used throughout the paper and validated against historical data; Sect. 5 constitutes the core of this contribution, it illustrates and discusses the results I obtained. Finally, Sect. 6 concludes the paper and provides some insights into future research.",25
13.0,1.0,Journal of Economic Interaction and Coordination,18 March 2017,https://link.springer.com/article/10.1007/s11403-017-0193-4,On the robustness of the fat-tailed distribution of firm growth rates: a global sensitivity analysis,April 2018,G. Dosi,M. C. Pereira,M. E. Virgillito,Unknown,Unknown,Unknown,Unknown,,
13.0,1.0,Journal of Economic Interaction and Coordination,09 June 2017,https://link.springer.com/article/10.1007/s11403-017-0197-0,Realistic simulation of financial markets: analyzing market behaviors by the third mode of science,April 2018,Damien Challet,,,Male,Unknown,Unknown,Male,,1
13.0,1.0,Journal of Economic Interaction and Coordination,21 December 2017,https://link.springer.com/article/10.1007/s11403-017-0213-4,"Economics with heterogeneous interacting agents: a practical guide to agent-based modeling, Edited by Alessandro Caiani, Alberto Russo, Antonio Palestrini, Mauro Gallegati",April 2018,Wei-Bin Zhang,,,,Unknown,Unknown,Mix,,
13.0,2.0,Journal of Economic Interaction and Coordination,28 October 2016,https://link.springer.com/article/10.1007/s11403-016-0181-0,Competitive moment matching of a New-Keynesian and an Old-Keynesian model,July 2018,Reiner Franke,,,Male,Unknown,Unknown,Male,"The paper brings together two strands of economic research on small-scale modelling in the context of the so-called new macroeconomic consensus. First, there is the New-Keynesian approach with its extensive estimation literature. While Bayesian likelihood methods have become dominant here in the past decade, a smaller number of studies has alternatively used moment matching procedures. They seek for parameter values such that a set of model-generated summary statistics, or ‘moments’, comes as close as possible to their empirical counterparts. The goodness-of-fit of a model, or a trade-off between the merits and demerits in matching specific moments, can thus also be assessed in finer detail than by just referring to the optimization of a single and relatively abstract objective function. In particular, moment matching has recently been applied to a hybrid three-equation model with forward- and backward-looking elements, which focusses on the quarterly output gap, the inflation gap and the interest gap (i.e., the deviations of these variables from a constant or possibly time-varying trend). Within mainstream macroeconomics, the model represents a sort of common-sense middle ground that preserves the insights of standard rational expectations with some sort of sluggish behaviour, while allowing for better empirical fit by dealing directly with a well-known empirical deficiency of the purely forward-looking models. As a result, this class of models has been widely used in applied monetary policy analysis, with the policy implications depending importantly on the values of the coefficients on the expected and lagged variables, respectively. When estimating the New-Keynesian model, the moments to be matched were the auto- and cross-covariances of the three variables with lags up to eight quarters. Admitting sufficiently backward-looking behaviour (in some contrast to the ordinary likelihood literature), the performance of these estimations on US data for the two sub-periods of the so-called Great Inflation and Great Moderation was so good that they were said to constitute a challenging yardstick for any macroeconomic model of a similar complexity (Franke 2011; Franke et al. 2015). The second strand of research that we address are macroeconomic theories that refuse the paradigm of the optimizing representative agents and their rational expectations. To face the task of providing strong alternatives, two new types of models have been advanced within the three-variables framework that put special emphasis on translating the idea of the famous ‘animal spirits’ into a formal canonical framework. They can thus be briefly characterized as models of sentiment dynamics (Franke 2008, 2012a, on the one hand and De Grauwe 2010, on the other hand).Footnote 1 Their cycle-generating properties have been demonstrated by numerical simulations with suitably calibrated parameters, but so far these models have not yet been subjected to serious econometric procedures. This is where the present paper sets in. It takes up the model by Franke (2012a), which is based on the notion of endogenously determined transition probabilities with which the individual firms switch between an optimistic and a pessimistic investment attitude. In addition, entering the Phillips curve is an inflation climate the adjustments of which are influenced by a parameter that represents the general credibility of the central bank. For a better contrast, this model is called an Old-Keynesian model. Because of its intrinsic nonlinearities, the second moments for the estimation can no longer be analytically computed as in the linear New-Keynesian case but have to be simulated over a long time horizon (which introduces the problem of sample variability). The obvious question is, of course, whether the general match of the Old-Keynesian model can compete with the New-Keynesian model (NKM). Three different versions of the Old-Keynesian model are studied. The first one is deterministic and the exact discrete-time analogue of Franke (2012a), except that it slightly extends the Taylor rule in order to have the same specification with interest rate smoothing as in NKM. The persistent cyclical behaviour in this model is mainly brought about by a sufficiently strong herding mechanism; it renders the long-run equilibrium unstable, while the nonlinearities prevent the dynamics from exploding. The goodness-of-fit that can here be achieved deserves already some respect, although it will clearly fall behind the stochastic NKM. Versions two and three of the Old-Keynesian model are stochastic, which, incidentally, will deemphasize the role of herding in the estimations. They introduce the analogous quarterly random shocks from NKM, i.e. demand shocks, supply shocks and monetary policy shocks, all of them being serially uncorrelated. In the second version they only take direct effect in an output equation, the Phillips curve and the Taylor rule, respectively. Being inspired by NKM where in the reduced-form solution each shock acts on each of the three variables, our third version additionally allows the demand shock to act on the adjustments of the inflation climate and the cost push shock to act on the adjustments of the firms’ investment attitude. When estimating the models with the second moments mentioned above one will note that the general pattern of the simulated time series exhibits a similar level of noise. This is in contrast to the empirical series where the quarterly inflation rates are much noisier than the other two variables. As an innovative feature we specify a measure of raggedness of the time series and add these statistics to the other moments. By and large it turns out that the Old-Keynesian model is better suited to match the new moments—without deteriorating the original second moments too much. A short and succinct characterization of the overall matching quality will then be that the second version is fairly similar to that of NKM, whereas the third version is markedly superior. The paper thus shows that, in the framework indicated, there is indeed an alternative ‘Old-Keynesian’ model that can well bear comparison with the workhorse model of orthodox macroeconomics. The remainder of the paper is organized as follows. The next section describes our estimation approach of the method of simulated moments. Section 3 introduces the two rival models, the New- and the Old-Keynesian one. The estimation results are presented in Sects. 4 and 5, where Sect. 4 deals the period of the Great Inflation and Sect. 5 with the Great Moderation. Section 4 is actually the main part of the paper as it also contains the discussion of many specification details when they are first applied. Section 6 concludes. Some less central parameter estimates are relegated to an appendix.",9
13.0,2.0,Journal of Economic Interaction and Coordination,03 November 2016,https://link.springer.com/article/10.1007/s11403-016-0182-z,A survey of network-based analysis and systemic risk measurement,July 2018,Andre R. Neveu,,,Male,Unknown,Unknown,Male,"The 2008 global financial crisis was the result of numerous failures across a highly interconnected dynamic network. This systemic event—which had no identifiable beginning and a number of potential causes—highlighted the importance of understanding financial network dynamics and characteristics. In the wake of the crisis, researchers have attempted to map financial networks and quantify the risk of a future systemic collapse (i.e., systemic risk). Network-based models offer one analytical framework to examine the issue of systemic risk in financial markets. This paper surveys the development of these network-based models, and discusses the ways in which the approach could be better integrated with other risk measures based on an equilibrium approach. In 2013, Janet Yellen presented a narrative of the financial crisis that described how issues of counterparty risk and contagion were seen as minimal risks by regulators prior to the crisis.Footnote 1 Additionally, much of the pre-crisis theoretical literature discussing financial networks had shown minimal dangers of counterparty risk and contagion (Upper 2011; Drehmann 2009; Eisenberg and Noe 2001). Early network-based models finding that these risks were of little importance led to certain policy prescriptions, including diversification, deregulation, reliance on market discipline, and the use of portfolio-based risk measures (Allen and Gale 2000; Aragonés et al. 2008). Interbank lending market models like those found in Allen and Gale (2000) were the foundation of the literature supporting these policy prescriptions, and remains the focus of many studies since the financial crisis. Recent financial innovations like credit default swaps—which were being sold by numerous unregulated firms—unearthed glaring holes in regulation and oversight which appeared only after the crisis began (Markose et al. 2010). This paper meets several ends through a survey of the literature on financial network modeling and systemic risk, with a focus on providing a summary of the successes and remaining gaps in the literature. Models of financial networks attempting to understand systemic risk have their roots in a traditional economic equilibrium (i.e., general equilibrium) approach. As described below, early theoretical models by Allen and Gale (2000) and Freixas et al. (2000) have overshadowed subsequent work on empirical networks, agent-based models, and other work which challenges the notion that financial markets gravitate towards a steady-state of stability or non-existence. The hallmark of the general equilibrium approach to networks often entails static agent behavior including rational expectations and representative agents. Historically network models of systemic risk are often overlooked for various reasons, including the types of networks considered, the algorithms used to solve models, and the scope of study.Footnote 2 In the context of network modeling, this paper reviews and critically assesses new systemic risk measures designed to detect or mitigate future financial crises. By linking research from different perspectives, we can provide a suitably wide foundation for studying systemic events. Examination of contrasting approaches, including general equilibrium, empirical, simulation, network, and agent-based modeling, reveals many areas where research is needed.Footnote 3 This survey also suggests changes to the current regulatory approach, including variable capital and liquidity requirements, greater transparency, and improved bankruptcy laws. Recent research on financial networks offers important insights into systemic risk by studying contagious links and fragile network structures. Regulators should expand their focus beyond firms that exhibit certain traits (e.g., size) to include entities where new contagious links might develop. Recent network research also shows considerations might need to be taken regarding mergers, acquisitions, and break-ups as they could weaken the network structure. Regulators cannot reasonably expect to prepare for every contingency, nor can they track every type of asset. However, a number of potential crises are currently being ignored by widespread use of methods that focus on only the investments and relationships of a few large institutions. There are numerous theories and empirical studies indicating regulators and researchers should focus on the interaction of network topology and regulations like capital and liquidity requirements. Agent-based models have helped deepen behavioral foundations, as seen in models including network-based financial and leverage accelerators which produce risk endogenously. These accelerators help explain why networks can become more or less conducive to contagion. An improved understanding of these agents’ behavioral foundations promises to help us better comprehend systemic risk. The remainder of this survey is organized as follows. Section 2 provides a taxonomy of systemic risk, a discussion of relevant externalities, and introduces the concept of endogenous risk. Section 3 reviews recent research on networks, complexity, and agent-based models as it relates to systemic risk. Section 4 reviews both traditional and network-based systemic risk measures. Section 5 discusses the reliability of systemic risk measures, and Sect. 6 concludes.",35
13.0,2.0,Journal of Economic Interaction and Coordination,17 November 2016,https://link.springer.com/article/10.1007/s11403-016-0183-y,Band or point inflation targeting? An experimental approach,July 2018,Camille Cornand,Cheick Kader M’baye,,,Unknown,Unknown,Mix,,
13.0,2.0,Journal of Economic Interaction and Coordination,28 November 2016,https://link.springer.com/article/10.1007/s11403-016-0184-x,Capability-based governance patterns over the product life-cycle: an agent-based model,July 2018,B. Vermeulen,A. Pyka,A. G. de Kok,Unknown,Unknown,Unknown,Unknown,,
13.0,2.0,Journal of Economic Interaction and Coordination,26 December 2016,https://link.springer.com/article/10.1007/s11403-016-0185-9,Mathematical models describing the effects of different tax evasion behaviors,July 2018,M. L. Bertotti,G. Modanese,,Unknown,Unknown,Unknown,Unknown,,
13.0,2.0,Journal of Economic Interaction and Coordination,29 December 2016,https://link.springer.com/article/10.1007/s11403-016-0186-8,Long-run consequences of debt,July 2018,Siyan Chen,Saul Desiderio,,Unknown,Male,Unknown,Male,"The 2008–2009 crisis has produced among the economists a renewed interest in the role of debt and financial variables in general. One important research question refers to the long-run consequences of a regime of persistent debt. Recent empirical investigations such as the controversial Reinhart and Rogoff (2010), Checherita and Rother (2010) and Cecchetti et al. (2011), find in general a negative relationship between growth and high levels of both public and private debt (for a survey see Panizza and Presbitero 2013). Strong evidence is also found on the negative role of high private debt levels on macroeconomic stability (Sutherland et al. 2012). Evidence is even stronger in the case of developing Countries (see, among the others, Pattillo et al. 2002; Clements et al. 2003). The theoretical literature on public debt offers different answers according to the perspective assumed: while at least since Keynes’ General Theory there is an acknowledgment for the short-run benefits brought about by public deficits during recessions (DeLong and Summers 2012), macro models tend to predict long-run negative effects for high and persistent public debts. For example, Modigliani (1961) and Diamond (1965) are classical studies relating public debt to a lower pace of capital accumulation because of crowding-out effects and raising taxes. More recent models lead to similar conclusions (Elmendorf and Mankiw 1999; Checherita-Westphal et al. 2012). When considering private debts, models emphasize the role of balance sheet conditions (debt and leverage, in particular) in transmitting and amplifying shocks from and to real sectors. Debt-deflation theory (Fisher 1933), the Financial Instability Hypothesis by Minsky (1982) and the Financial Accelerator theory by Bernanke and Gertler (1989, 1990) are foremost examples of this strand of research. Along the same lines are Greenwald and Stiglitz (1993) and Kiyotaki and Moore (1997). The adverse impact of external debt in developing Countries has been also highlighted (Krugman 1988). The role of debt and its complex interactions with the real sector has also been analyzed in recent agent-based macroeconomic literature (e.g. Raberto et al. 2012; Riccetti et al. 2013; Assenza et al. 2015). However, a unified theory fitting with any kind of debt does not exist. In fact, each of the above models focuses on one single typology of debt and relies upon very different hypotheses. Moreover, while they reveal how debt negatively impacts on the economy through its interactions with other mechanisms (such as the external finance premium), in general they do not consider its direct effects. The aim of this paper, therefore, is to fill in these gaps by developing a model that is able to describe the long-run direct effects of several kinds of debt abstracting from possible interactions with other factors such as asymmetric information or crowding-out effects. However, we will focus our analysis only on those typologies of debt that do not cause the money stock to increase, such as for example corporate bonds, Government bonds sold on secondary markets and commercial paper. Consequently, our framework is not suitable to analyze bank loans. We will model the economy as a network of interconnected agents characterized by state and control variables. The nodes of the network represent agents whereas the links represent trading relationships between pairs of agents. Debts are just special cases of such relationships. In addition, we introduce a form of stock-flow consistency assuring the closure of the model. This is particularly important when considering inter-temporal relationships such as debts. The steady-state solution yields three testable predictions, qualitatively in accordance with the empirical evidence. First, in the long run debts determine a redistribution of income and wealth from debtors to creditors. Thus, highly indebted agents will experience a lower capacity to spend: firms will reduce investment, households will reduce consumption and Government will reduce public expenditure. Second, the magnitude of the redistribution is amplified both by the level of the interest rate and by debt duration. This suggests that regimes of persistent debt should be avoided. Third, if debtors have a higher marginal propensity to spend than creditors, then debts will also reduce aggregate spending. This result, which straightforwardly explains the empirical evidence, may not come as a surprise to many, but we want to stress the strength and the added value of our modeling approach: generality and parsimony of assumptions. The rest of the paper is organized as follows. Section 2 introduces the baseline model without debts, proves the existence of its equilibrium and presents Lemma 1, which states that individual steady-state wealth stock is decreasing in one’s spending propensity. Section 3 shows that debts increase individual spending propensity. Thus, the main results are derived as a consequence of Lemma 1. Section 4 concludes.",1
13.0,2.0,Journal of Economic Interaction and Coordination,02 January 2017,https://link.springer.com/article/10.1007/s11403-016-0187-7,“Speculative Influence Network” during financial bubbles: application to Chinese stock markets,July 2018,Li Lin,Didier Sornette,,,Male,Unknown,Mix,,
13.0,2.0,Journal of Economic Interaction and Coordination,13 February 2017,https://link.springer.com/article/10.1007/s11403-017-0188-1,An agent-based model for financial vulnerability,July 2018,Richard Bookstaber,Mark Paddrik,Brian Tivnan,Male,Male,Male,Male,"Stress testing gained momentum among financial regulators after the 2007–2009 financial crisis because risk measures based on historical relationships failed to provide adequate insight. Although stress testing provides a better picture of a firm’s exposure in the face of proposed scenarios, the tests remain microprudential exercises that do not examine the impact of firms on one another beyond the initial stress event. Without incorporating the dynamics, feedback, and related complexities of financial intermediations, it is difficult to understand the impact that stress scenarios will have on lending, borrowing, and asset markets, and it is impossible to assess the stability risk to the aggregate financial system. Agent-based models (ABMs) are well suited for incorporating these transformations to explore crisis dynamics. ABMs follow the agents period by period, assessing their reaction to events and updating the macro system variables compiled from micro-level agent decisions. This paper develops an ABM to provide a macroprudential view of the transformations and dynamic interactions of agents in the financial system. The model extends from the suppliers of funding, such as money market funds, through the channels of bank/dealers to the financial institutions that use the funds, and the collateral that moves in the opposite direction. The ABM integrates various channels for crisis dynamics from specific failures in the transformations provided by the intermediaries. This paper builds on the literature about financial shocks and their impact on collateralization. We examine the impact of bank/dealers in financing and collateralizing, as done by Cifuentes et al. (2005), and additionally consider the bank/dealers’ multi-faceted roles as prime brokers in trading and market making, in capital-raising, and as a counterparty. We integrate the funding side of the market by incorporating money market funds and pension funds (Copeland et al. 2010) and hedge funds, which are major borrowers of cash and sources of leverage (Thurner et al. 2012). Our contribution integrates these related literatures into a multi-agent framework that incorporates the major participants typically seen in U.S. funding markets. Our model captures several interconnected, network-like relationships that can cause reverberations across a stressed financial system. The integration of market asset-price correlations through overlapping portfolios with funding networks provides the mechanisms for feedback cycles to occur. This is integral to explaining how the credit crises in 2008 unfolded for several financial institutions (Brunnermeier and Pedersen 2009; Tasca and Battiston 2016). This more detailed and integrated representation of the financial system depicts the various paths of fire sale cascades and contagion, including those that occur from leverage and funding-based fire sales, and from credit and redemption stress. Through the integration and use of ABMs, this paper moves various facets of the analysis of cascades and contagion closer to implementation. ABM can provide the macroprudential community, regulators and financial institution risk managers, with a flexible tool for enhanced stress and scenario analysis to discover vulnerabilities and forecast the potential implications of financial tail events. To demonstrate the value of the model, we test the effectiveness of critical regulatory risk measures during periods of market dislocation and crisis. We examine the ability of these risk measures to capture the propagation of asset and funding risks during systemic shocks such as sudden price declines, funding restrictions, erosions of credit, or investor redemptions. The remainder of the paper is organized as follows: Sect. 2 reviews the role of market participants in the dynamics of fire sales, and the function of the asset, funding, and credit channels. Section 3 presents an ABM to address these dynamics. Section 4 demonstrates the model’s response to market shocks, and includes tests of model validation and robustness. Section 5 provides additional details of the impact that leverage, liquidity and crowding play in market dynamics. Section 6 presents the performance of regulatory risk measures within the ABM during periods of shocks and their subsequent dynamics. The paper concludes with a summary in Sect. 7 of the current application of this analytical approach within the U.S. Treasury’s Office of Financial Research (2012) as an essential element of the Financial Stability Oversight Council’s macroprudential toolkit.",26
13.0,2.0,Journal of Economic Interaction and Coordination,21 April 2018,https://link.springer.com/article/10.1007/s11403-018-0222-y,Correction to: Heterogeneity in social values and capital accumulation in a changing world,July 2018,Pierre Gosselin,Aïleen Lotz,Marc Wambst,Male,Unknown,Male,Male,"In the printed version of the article, the conditions for the derivatives of Eq. (5) in Sect. 3.2 contained several misprints. The correct version reads as follows: so that the derivatives satisfy the following conditions: \( (\xi K_{2} )_{1} > 0,\;((1 - \xi )K_{1} )_{1} < 0,\;(\xi K_{2} )_{1,1} < 0,\;((1 - \xi )K_{1} )_{1,1} < 0,\;(K_{i} )_{3} > 0,\;(K_{i} )_{3,3} < 0 \) for i  = 1, 2.”",
13.0,3.0,Journal of Economic Interaction and Coordination,11 February 2017,https://link.springer.com/article/10.1007/s11403-017-0189-0,Quantifying invariant features of within-group inequality in consumption across groups,October 2018,Anindya S. Chakrabarti,Arnab Chatterjee,Anirban Chakraborti,Unknown,Unknown,Unknown,Unknown,,
13.0,3.0,Journal of Economic Interaction and Coordination,10 February 2017,https://link.springer.com/article/10.1007/s11403-017-0190-7,Estimating heterogeneous agents behavior in a two-market financial system,October 2018,Zhenxi Chen,Weihong Huang,Huanhuan Zheng,Unknown,Unknown,Unknown,Unknown,,
13.0,3.0,Journal of Economic Interaction and Coordination,20 February 2017,https://link.springer.com/article/10.1007/s11403-017-0191-6,Artificial stock markets with different maturity levels: simulation of information asymmetry and herd behavior using agent-based and network models,October 2018,Hazem Krichene,Mhamed-Ali El-Aroui,,Male,Unknown,Unknown,Male,"This paper presents Artificial Stock Markets (ASM) modeling some of the characteristics of immature and mature markets, and reproducing their stylized facts. This will contribute to a better understanding of the origins of immature stock markets microstructure and properties. Most of the existent Agent-Based Models (ABM) for stock markets deal with standard developed stock markets. The last decades have seen an increasing interest of investors in immature stock markets, due mainly to their rapid growth and their weak correlations with the international financial sphere giving high opportunities of portfolio diversification. Yet, recently, the situation on non-developed countries has become quite choppy due to several economical and political problems, as the political and economical tensions in China, the hyper inflation in Argentina and the Arab spring in the middle east, which increase the risk of these markets. In fact, immature stock markets are riskier and less efficient than developed markets and are more vulnerable toward local political and economical shocks discouraging foreign investors to invest in. Their poor performance is reflected by their high risk and by their low efficiency, mentioned respectively in De Santis and Imrohoroglu (1997), Bekaert and Harvey (2003) and Alagidede (2011). High risk and low efficiency of immature stock markets may derive from several macroeconomic problems, as low per capita incomes, low capitalization, low liquidity, administrative barriers for domestic and foreign investors, weak integration to international markets, etc. (see Bekaert and Harvey 2003). Other studies, as Yartey (2008), pointed out that non-developed countries suffer from a poor institutional quality as corruption and information opacity which stimulates insider trading and emergence of high information asymmetry. This information asymmetry yields stock markets populated by informed and uninformed investors, where the latter will try to herd the informed investors due to their low confidence toward public information, as shown by Kim and Wei (2002). Thus, it is assumed in this paper that the modeling of information asymmetry and herd behavior could reproduce the main stylized facts of immature stock markets, and consequently explain the microscopic determinants of their high risk and low efficiency. Some works tried to model information asymmetry by considering stock markets as complex social networks. Agents are represented by nodes and information spreads across their links. The information asymmetry was modeled in Ponta et al. (2011) by scale-free networks allowing the reproduction of some stylized facts (fat-tails, predictability and clustering). Herd behavior was also modeled in some previous works. LeBaron and Yamamoto (2008) employed a genetic algorithm to model herd behavior. Their results show that herd behavior promotes long memory of returns, volatility and trading volume. Furthermore, the herd behavior was modeled using random networks in Tedeshi et al. (2012). Their models are based on Chiarella et al. (2009) by integrating a simplified random network allowing communications between agents. They show that herding stimulates predictability, clustering and fat-tails on stock markets’ returns. All the previously cited works dealt with developed stock markets. Thus, we were inspired by these works to model different degrees of information asymmetry and herd behavior in order to simulate stock markets with different maturity levels. To model stock markets with different degrees of information asymmetry and herd behavior, several models from previous ABM’s are mixed here. Like Ponta et al. (2011), information flows of the proposed multi-assets ASM are modeled through network simulation and agents’ sentiment spread. The behavioral complexity is modeled using tools developed by Brock and Hommes (1998) and Chiarella et al. (2009) by considering heterogeneous behaviors agents who can learn based on their performance measure. Each agent has, in the model developed here, a trading behavior/strategy mixing three elements: fundamentalism (degree of available private information), chartism (degree of herding behavior), noisy (degree of noisy trading). The main technical contribution of this paper is the mix of these different works to construct a behavioral network able to reproduce information asymmetry and herd behavior through the assortative topologies of scale-free networks based on Bollobás et al. (2003) and Guo et al. (2006). Up to our knowledge, a part from the work of Krichene and El-Aroui (2016) who studied the single asset case, no works studied artificially non-developed stock markets. The paper is structured as follows: Sect. 2 will expose the modeling assumptions and principles. Section 3 will present the construction of the proposed multi-assets artificial stock market by exposing its technical details. Section 4 will expose the simulation protocol to generate several artificial market microstructures related to different maturity levels. Sections 5 and 6 will show how the developed artificial market could reproduce univariate and multivariate stylized facts for both mature and immature markets. It will be shown that information asymmetry and herd behavior, such as modeled here, allow the reproduction of the main immature stock markets properties. This gives a first validation of the proposed modeling approach. Finally, Sect. 7 will end by giving some concluding remarks and future extensions. Appendices A and B contain pseudo-codes with technical details about the network simulation and the artificial stock market design.",11
13.0,3.0,Journal of Economic Interaction and Coordination,08 March 2017,https://link.springer.com/article/10.1007/s11403-017-0192-5,Revisiting the issue of survivability and market efficiency with the Santa Fe Artificial Stock Market,October 2018,Chueh-Yung Tsao,Ya-Chi Huang,,Unknown,Unknown,Unknown,Unknown,,
13.0,3.0,Journal of Economic Interaction and Coordination,08 April 2017,https://link.springer.com/article/10.1007/s11403-017-0194-3,The creative response and the endogenous dynamics of pecuniary knowledge externalities: an agent based simulation model,October 2018,Cristiano Antonelli,Gianluigi Ferraris,,Male,Male,Unknown,Male,"This paper contributes the literature that impinges upon the approach elaborated by Schumpeter (1947) according to which innovation is the result of the creative reaction of firms, facing unexpected changes in product and factor markets, contingent upon the availability of knowledge externalities. The availability of knowledge externalities, in turn, is the stochastic result of the introduction of innovations. Its persistence depends upon the actual amount of knowledge externalities that are generated at each point in time. This dynamics is the result of the interaction between individual decision making embedded in a system and the changing conditions of the system (Antonelli 2011; Arthur 2014). The introduction of innovations requires the generation of technological knowledge. In turn, the generation and dissemination of technological knowledge can only take place in organized contexts characterized by appropriate levels of knowledge connectivity qualified in terms of viability of knowledge interactions and transactions among heterogeneous and creative agents that act intentionally to innovate when their individual performance is out of equilibrium. The generation of technological knowledge is, in fact, based on the interactive and collective recombination of internal and external knowledge through the intentional interaction and participation of a variety of learning agents embedded in a geographic and professional knowledge commons. Interaction is required for the acquisition and implementation of external knowledge, an essential input into the generation of new knowledge (Antonelli and David 2016). This process leads to the generation of knowledge stemming from internal research activities combined with knowledge externalities and strategic mobility across knowledge commons. The outcomes are determined by the structured contexts in which they are embedded, but they are also the cause of changes in the structure of the system, its knowledge connectivity and the pecuniary knowledge externalities available within the knowledge commons, likelihood of successful innovation and, thus, ultimately aggregate productivity. Innovation and changes to productivity levels affect the system’s price levels and the performance of firms, promoting new out-of-equilibrium conditions and new structures of the system (Antonelli 2008, 2011, 2015). This open-ended feedback system is based on continual interactions between individual acts and endogenous knowledge externalities related to the structure of the system and its levels of knowledge connectivity. In this context, the decisions to both generate technological knowledge and introduce technological innovations by exploiting the knowledge interactions and organized structures in which they take place, are endogenous and are determined internally by the dynamics of the system. The individual and intentional actions of creative agents are central to the system dynamics; however, no single agent is solely responsible for or is able to forecast the eventual results of his or her actions because of the effects on the organization of the system (Miller and Page 2007). The characteristics of the landscape in which knowledge interactions and transactions take place play a central role in assessing the viability of knowledge generation strategies. Thus, feasibility of knowledge generation depends upon the knowledge connectivity of the system as measured by the levels of knowledge externalities, which, in turn, depend upon the characteristics of the knowledge landscape. These characteristics are neither static nor exogenous. They change continuously through time as a consequence of the activities of agents, and their capabilities to generate knowledge and introduce innovations and freedom to search for new opportunities for the generation of new technological knowledge. Changes to the features of the landscape engender both positive and negative externalities, which affect the capability of firms to innovate. The changing capabilities of firms to generate new technological knowledge affect their mobility and, ultimately, the contours of the space. Moreover knowledge landscapes and knowledge externalities are not given, but emanate from an endogenous, path dependent collective process that includes institutional changes such as the introduction of new intellectual property right regimes (Sorenson et al. 2006). The present paper draws on the above to build a synthetic account of the role of externalities in the economics of technological knowledge, implementing the notion of endogenous knowledge externalities, showing the dynamic endogeneity of the emergence and decline of knowledge externalities at the system level, and exploring their implication for the rates of introduction of innovations and productivity increases in the system. Section 2 reviews the changing attitudes to knowledge externalities, and elaborates a theoretical framework to understand the endogenous dynamics of pecuniary knowledge externalities. Section 3 presents an agent-based model of the innovation system. Section 4 presents the results of the simulation focusing on the alternative hypotheses related to the institutional and architectural features of the innovation system. Section 5 concludes by summarizing the main results and discussing some policy implications of the analysis.",5
13.0,3.0,Journal of Economic Interaction and Coordination,09 April 2017,https://link.springer.com/article/10.1007/s11403-017-0195-2,The economics analysis of a Q-learning model of cooperation with punishment and risk taking preferences,October 2018,Nazaria Solferino,Viviana Solferino,Serena F. Taurino,Female,Female,Female,Female,"It is generally believed that public good can be produced only in the presence of repeated interactions (which allow for reciprocation, reputation effects and punishment) or relatedness. However, in some circumstances the production of public goods by individuals’ contribution represents a social dilemma due to the free-riding problem, which occurs when people can enjoy a good without paying anything or making a small contribution less than their benefit. In this respect, the occurrence and the maintenance of cooperative behaviours in public goods systems have attracted a great deal of research attention from multiple disciplines. Mechanisms that allow for the rise and the maintenance of cooperation have been analysed by a conspicuous amount of literature, also when in the presence of defectors (Hardin 1968; Dawes 1980; Kagel and Roth 1997). Boyd and Richerson (1988) describe how recurrent interactions among individuals in potentially cooperative situations are likely to evolve into a stable reciprocal cooperation. Also, numerical simulations of the infinitely iterated stochastic games (Hauert and Schuster 1998) provide evidence of the fact that stable cooperative solutions are strong strategies. Schuster and Sigmund (1983) obtain consistent results in several evolutionary models in distinct biological fields. There is a huge amount of literature analysing how cooperative behaviour may arise depending on several mechanisms, such as reputation and imaging scores (Nax et al. 2015), the possibility of moving away from bad neighbours (Efferson et al. 2016), evolutionary dynamics leading to own payoffs and directional (mis)learning of the benefits of joint payoffs when some players start to deviate from selfish behaviour (West et al. 2007; Burton-Chellew et al. 2015; Nax and Perc 2015). There are also studies based on evolutionary dynamics showing that learning leads to a declining cooperation (Nax et al. 2016) or to a not pro-social behaviour. In this work we aim go add a contribution to the literature on cooperation which analyses “learning” in public goods interactions with punishment mechanisms, when there is the possibility of defection and free-riding from others’ contributions to public goods. Theoretical (Fowler 2005; Brandt et al. 2006; Hauert et al. 2007, 2008; Nakamaru and Dieckmann 2009; Sigmund et al. 2010; Sasaki et al. 2012) and experimental (Fehr and Gachter 2002; Egas and Riedl 2008) papers show how the combination of voluntary participation and defectors’ punishment work together for the emergence and stabilization of cooperation. Among the authors, Boyd and Richerson (1992) show how the combination of punishment and ethical strategies lead to a stable and cooperative equilibrium. Another work by Brandt et al. (2006) grounds a bi-stable result in a microeconomic model showing the emergence of evolutionary dynamics towards a Nash equilibrium with punishment, non-punishment strategy, as well as towards an oscillating state without punishers. Punishment of defectors is the base for the beginning and constitution of cooperative behaviour, as in the work of Hauert et al. (2007). Also, they highlight that the free and common choice by all players of punishing non-cooperators is necessary to make such mechanisms work. The contribution from Nakamaru and Dieckmann (2009) points out that runaway selection can emerge from punishment and cooperation, leading to increased collaboration. 
Sigmund et al. (2010) find that pool-punishment is more efficient than peer-punishment in preventing second-order free-riders. It is so, because this type of free-rider is active, even if every single individual is contributing to the common good. Sasaki et al. (2012) show how the interaction between institutional incentives and voluntary participation can take off social traps and foster cooperation. The most important result of their work is the highlighting of a long-run effect: social learning will lead to a cooperative society, irrespective of the number of free-riders and cooperators playing at the beginning. Dercole et al. (2013) show that the moderate punishment tends to shrink the initial conditions and foster the fixation of cooperation. In their recent work Solferino and Taurino (2016) investigate the possible evolution of cooperation when individuals are not eager to cooperate initially, but are willing to “get back into the game” later on; these authors show that if the other players are willing to give them a second chance, then the “early stage defectors” will establish cooperation permanently. In this work, we contribute to this new strand of the recent literature on cooperation and punishment. We aim to investigate the probabilities of a stable cooperation in an environment where the agents take into consideration others’ behaviour to achieve their goal. In particular, we extensively apply the analytical results of the traditional Q-learning Model developed by Kianercy and Galstyan (2012) in a context of punishment and cooperation. In a Q-learning Model, people learn strategies based on the value of the related action itself and the possibly expected reward. 
Xie and Tachibana (2007) focus their work on “trash pickup”. They show the behaviour of agents interacting with the environment and learning how to perform a task (trash collection) as well as acquiring cooperative behaviour. For this purpose, the authors develop a Q-learning model as a representative technique of reinforcement learning. 
Waltman and Kaymak (2008) use the traditional Q-learning Model to study firms behaviour in a repeated Cournot oligopoly game. Their results show how in a situation with no punishment and no explicit communication, firms tend to collude with each other. In the present paper, we try to point out that individuals’ risk-taking preferences play a pivotal role in the set-up of a cooperative and stable equilibrium. Indeed, individuals with high exploration rates are keen to adapt their strategies and learn from others’ behaviour. Therefore, there is still room for cooperation by applying reinforcement learning strategies, depending on the use of strategic measures based on punishment. Our results suggest that a balanced duo of learning and punishment may help to preserve cooperation when there are not enough intrinsic motivations or utilities from cooperating. Cooperation is hence a “habit” that can be taught (and learned) whether or not there are intrinsic motivations. In particular, we show that it is possible to trigger a learning process that leads individuals to be equally cooperative, with a probability greater than \(\frac{1}{2}.\) This happens much more easily the more sensitive to the exploration rate people are. This result also depends on the appropriate punishment. The paper is organised as follows. Section 2 introduces the reinforcement learning framework and its implementation through the Q-learning model in the context of a punishment-and-learn scheme. Section 3 analyses the role of the “exploration rate” in the learning of cooperative strategies. Section 4 adds on exploring the role of intrinsic motivation for long-term cooperation. Section 5 concludes.",
13.0,3.0,Journal of Economic Interaction and Coordination,13 June 2017,https://link.springer.com/article/10.1007/s11403-017-0198-z,Zero-intelligence agents looking for a job,October 2018,André Veski,Kaire Põder,,Male,Female,Unknown,Mix,,
13.0,3.0,Journal of Economic Interaction and Coordination,03 August 2017,https://link.springer.com/article/10.1007/s11403-017-0200-9,An agent-based model of the observed distribution of wealth in the United States,October 2018,Hunter A. Vallejos,James J. Nutaro,Kalyan S. Perumalla,,Male,Male,Mix,,
14.0,1.0,Journal of Economic Interaction and Coordination,17 January 2019,https://link.springer.com/article/10.1007/s11403-019-00235-8,Alternative approaches for the reformulation of economics,March 2019,Simone Alfarano,Eva Camacho,Gabriele Tedeschi,Female,Female,Female,Female,"In the last decades most of advanced and developing economies have undertaken a deep structural transformation. This profound structural change, caused by the transition from a manufacturing economy to a service-based one, is among the causes of the current crisis (see Delli Gatti et al. 2012). The dereculation of the banking system with the consequent redirection of banking activity from the credit sector to the financial one,Footnote 1 and the liberalization of financial markets, the globalization and the delocalisation of production with the resulting labor market flexibility are just some of the many transformations affecting the socio-economic system in the recent decades. All these serious changes have been poorly described by mainstream economics. Emblematic is Queen Elizabeth’s question at the London School of Economics during a discussion on the 2008 financial crash: “Why did nobody notice it?”, asked the Queen. As reported by Catullo et al. (2015), “economists, after some time, explained why no one foresaw the timing and severity of the crisis by laying responsibility on the failure of the collective imagination of many bright people. But why was the imagination of so many economists so limited? According to the eight contributions presented in this special edition, this was not an “imagination failure”, but rather the failure of a paradigm (Lakatos 1976) unable to explain and predict crises of these dimensions”. Based on the lessons taken from the recent socio-economic transformations, the Society for Economic Science with Heterogeneous Interacting Agents (ESHIA) tries to re-formulate and integrate the pillars of mainstream economics.Footnote 2 Therefore, what our scientific community proposes is the formalization of a descriptive and non-normative theory, able to grasp the complex dynamics of social systems. The economic crisis, in fact, has been accompanied by a profound dissatisfaction with the dominant paradigm. The Nobel Laureate Paul Krugman has recently commented that most macroeconomics of the past 30 years was “spectacularly useless at best, and positively harmful at worst”. In line with this view, in his opening address to the ECB Central Banking Conference on 18 November 2010, the ECB President, Trichet said that “in the face of the crisis, we felt abandoned by conventional tools”, and went on to call for the development of complex systems based approaches to augment existing ways of understanding the economy.",2
14.0,1.0,Journal of Economic Interaction and Coordination,05 February 2019,https://link.springer.com/article/10.1007/s11403-019-00238-5,Macroeconomic implications of mortgage loan requirements: an agent-based approach,March 2019,Bulent Ozel,Reynold Christian Nathanael,Silvano Cincotti,Unknown,Male,Male,Male,"The financial crisis of 2007–2009 highlighted the central role played by the housing market and mortgage securities; it is now universally recognized that monitoring systemic risk can be misleading if the housing market sector is not considered adequately. In this paper, we address the role of the housing market in the economy (Muellbauer and Murphy 2008), by designing a housing sector and a mortgaging mechanism in the Eurace agent-based macroeconomic simulator (Cincotti et al. 2010; Dawid et al. 2008; Raberto et al 2012; Teglio et al. 2012). A recent article published in Science (Battiston et al. 2016) highlights the promising potentials of agent-based models (ABM) as an experimental macroeconomic approach. The article also suggests that such models are underused. A relevant overview by Richiardi (2015) of the state-of-the-art agent-based models in economics reports that one of the major problems of current models is that they are not modular or scalable when it comes to adding new features or to deactivating existing components. In this respect, the housing market model within the artificial economy of Eurace represents a seamless extension to an ABM model that is already highly advanced, designed as a new module that can be easily switched off. The calibration of the housing market within the Eurace base model is driven by recent surveys (Anderson et al. 2014; Borsch-Supan 1994; Deloitte 2014; Dubecq and Ghattassi 2009; ECB 2013; FI 2014; Gharaie et al. 2012). In spite of its acknowledged importance, there are only a few other ABM models on housing markets: Gilbert et al. (2009), Ge (2014), Axtell et al. (2014) and Baptista et al. (2016). These housing market models are standalone models, i.e. they do not interact with the rest of the economy. Their common and main focus is on the mechanisms of housing prices. The simple model by Gilbert et al. (2009) consists of sellers, buyers and real estate agents. Household income and any other variables are provided exogenously. A more advanced model by Ge (2014) demonstrates that a lax debt-to-income (DTI) constraint for households leads to highly volatile house prices. However, unlike with our model, shocks to the model are applied externally. The model by Axtell et al. (2014) is specifically tailored to the housing market in the city of Washington, D.C. The model can generate a housing bubble of approximately the same size as that which occurred previously in the area. The model has a micro-level focus on the real estate purchasing behavior of households. The study by Baptista et al. (2016) examines price dynamics, given the ratio of renters as well as speculators in a housing market. Their results suggest that a growing size of renters and speculators increases price volatility, while a regulated market decreases volatility. They consider a DTI type macro-prudential regulation. This extended Eurace model includes all major agents of an economy: households, firms, banks, a central bank and a government. Households may assume the role of consumers, workers, financial investors, shareholders and home owners. There are two types of firms: consumption good producers and capital good producers. Agents interact in different types of market, namely, the consumption good market, the capital good market, the housing market, the labor market, the credit market, and the financial market for stocks and government bonds. With the exception of the financial market, all markets are characterized by decentralized exchange with price-setting behavior on the supply side. Agents’ decision processes are characterized by bounded rationality and limited information gathering and computational capabilities (Tesfatsion 2003; Tesfatsion and Judd 2006); thus, agents’ behavior follows adaptive rules derived from the management literature about firms and banks, and from experimental and behavioral economics of consumers and financial investors. The extended model enables us to address the crucial issue of the interplay between mortgages to households, loans to firms, and business cycles in the economy (Catte et al. 2005; Muellbauer and Murphy 2008). In this new context, in addition to bank loans to firms, bank mortgages to households are considered among the factors connecting the banking sector to the real economy (Gallegati et al. 2008). The dynamics of credit money is endogenous and its supply depends on the banking system, which is constrained by Basel capital adequacy regulatory provisions (Blum and Hellwig 1995; Santos 2001), while on the demand side it depends on firms’ necessity to finance production activity and on households’ house purchase needs. The motivational framework for our investigation on mortgage regulation policies is based on the following observations: (i) Historically, housing has been the single largest asset in the economy (Dagher and Fu 2011; FI 2014). Given it is a leading source of financial crises and economic fluctuations, housing poses a big regulatory challenge; (ii) Mortgage regulation should operate in a prudential mode rather than as a response to a bubble after it has burst (FSA 2011); (iii) Regulatory measures should act at the mortgage level. In other words, regulations targeting only banks or other financial institutions may not be sufficiently effective. For instance, mortgage risk can move from regulated banks to unregulated shadow banking institutes (Dagher and Fu 2011); (iv) Access to mortgage should not be too restrictive. A very strict regulation may inhibit the stimulative effect of mortgage credits on business cycles (Kydland et al. 2012). Two common methods of assessing the creditworthiness of individual mortgage requests are the loan-to-value (LTV) and loan-to-income (LTI) measures (FSA 2011). LTV is the ratio of the requested mortgage to the present value of the house. An LTV value of 1 would mean that the purchase is fully covered by credit. Commonly adopted LTI related measures are the Debt-To-Income ratio (DTI) and the Debt-Service-To-Income ratioFootnote 1 (DSTI). Following the mortgage-led financial crisis of 2007–2009, these ratios have been probed extensively by surveys conducted by regulatory bodies such as the Bank of England (Anderson et al. 2014; Gareth et al. 2014), the Swedish financial supervisory board Finansinspektionen (FI 2014), and the Bank of Canada (Meh et al. 2009a). The main policy conclusion drawn by these studies is that mortgage crediting is one of the main factors of house price inflation. This is often followed by shocks in financial markets and the real economy, amplifying feedback loops which may cause the economy to collapse. The multi-country study by Muellbauer (2012) highlights that “credit supply conditions in the mortgage market are the ‘elephant in the room’. Without taking them into account, one simply cannot understand the behaviour of house prices, household debt and consumption.” (p. 36). These survey-based studies also raise questions regarding the aptness of LTV and LTI measures as enforced policy instruments.Footnote 2 For instance, one of the key observations of the paper published by the Financial Service Authority of the UK (FSA 2011) is that LTV and LTI ratios, when employed as the only measure, are not very good at detecting and preventing mortgage crises. Anderson et al. (2014) state that allowing mortgage contracts with a high DSTI ratio does not always trigger mortgage arrears (p. 422). Gareth et al. (2014) point out that the use of DSTI alone may not be reliable in the long term, especially when interest rates and house prices are highly volatile. In line with other researchers, Bubb and Krishnamurthy (2015) point out that, although a strict LTV ratio as a single regulatory measure may prevent mortgage crises to a large extent, a strict LTV ratio has several drawbacks: “An important objection to a leverage limit is that it might reduce access to homeownership by less wealthy households in contravention of a long standing policy commitment to expanding homeownership” (p. 67). They discuss how a strict LTV ratio would create a barrier to homeownership for first-time buyers with limited resources to make a downpayment. The book by Case et al. (2009) dedicates two chapters on mortgage access by poorer social classes. They stress the necessity for a generic regulatory criterion to enable lower income households to gain mortgage access. 
Meh et al. (2009a) raise a rather more technical concern about the use of LTI type measures. The authors use microdata to identify changes in household debt, and discuss their potential implications for monetary policy and financial stability. They examine sub-components of the assets and liabilities of household balance sheets. The rationale of their criticism is due to a mixture of stock and flow variables within an LTI type measure, where in general a household’s total mortgage debt, a ‘stock’ entry on its balance sheet, is related to the household’s disposable income, a ‘flow’ entry in its monthly income statement (Skingsley 2007). The concerns around LTV and LTI measures have led to a new debate calling for the identification of relevant measures to evaluate the risk of individual household mortgage debt.Footnote 3 Earlier, Meh et al. (2009a) suggested comparing a stock entry to another stock entry of the household balance sheet (e.g. the debt-to-asset ratio) to assess long-term vulnerability. Other authorities (FSA 2011) conclude that there is no simple quantitative rule, they propose exploring other measures where, for instance, consumption expenditures and current household payments are considered. Svensson (2014b) proposes using the net-worth ratio on a household’s balance sheet in to assess its credibility. This is qualitatively the same measure as the one suggested by Meh et al. (2009a). The rest of the paper is organized as follows. Section 2 presents the housing market model designed within the Eurace framework. Section 3 presents the computational results. In particular, Sect. 3.1 discusses the implications of the presence of the housing market in the simulated economy. Following the path of recent debate about stock and flow risk measures, we design policy experiments that adopt two different mortgage lending criteria: a flow control measure and a stock control measure. The flow control measure, namely DSTI, checks household incomes and debt payments for the upcoming quarter. The stock control measure, namely equity-to-assets (ETA), is the ratio of net-worth to the household’s total assets. It is qualitatively the same measure suggested by Meh et al. (2009a) and Svensson (2014b). We have employed the DSTI measure earlier in ABM simulations of the Icelandic economy (Bjarnason et al. 2015; Erlingsson et al. 2014, 2016). Section 3.2 presents the results of these policy experiments, where we examine separate and combined impacts of stock and flow controls. Section 4 concludes the paper.",11
14.0,1.0,Journal of Economic Interaction and Coordination,21 March 2018,https://link.springer.com/article/10.1007/s11403-018-0220-0,Heterogeneity in social values and capital accumulation in a changing world,March 2019,Pierre Gosselin,Aïleen Lotz,Marc Wambst,Male,Unknown,Male,Male,"Capital theory has been the subject of intense debates among economists since the early nineteenth century (Solow 1957). In the latest of these debates, the Cambridge capital controversy, neoclassical economistsFootnote 1 defended the orthodox treatment of interest, determined by supply and demand, whereas neo-Ricardians, led notably by Joan Robinson,Footnote 2 assumed it to be determined by the conjunction of technological conditions of production and by the distribution of income (Samuelson 1987; Gehrke and Lager 2000; Garegnani 2009). The root of the debate lies in the aggregation problem: using microeconomic concepts to understand the entire society’s production could prove to be a fallacy of composition. Indeed, it has since been established that identical micro-production functions obeying all the standard assumptions of neoclassical production theory cannot be aggregated to give a well-behaved aggregate production function, even as an approximation (Fisher 1992; Felipe and Fisher 2003; McCombie and Pike 2013). Yet general macroeconomics models developed so far by the neoclassical school, such as growth theory models, and notably the Solow model, all assume a single production function for the entire economy. Subsequent theories of endogenous growth and real business cycles, likewise, used aggregate production functions. In a recent paper, Romer (2015) considers that the technical criticism of marginal productivity theory may have been blurred by ideological considerations Robinson (1952, 1956). By comparison, the mathematization of growth theories initiated by Solow (1957) represented a simplification of the notion of capital, allowing for a clear formalization of the problems raised by capital accumulation and its mechanisms.Footnote 3 Yet Joan Robinson’s criticism remains open. The complex nature of capital requires a richer formalism involving multiple agents and environment-dependent factor productivities. The description of a disaggregated capital whose valorization depends on multiple factors would avoid the problem of price of capital, even if it should later be aggregated again to recover macro concepts. This paper is a first step towards such a formalism. Our approach considers an economy with a large number of different agents and goods. Each good has a specific social value, and heterogeneous personal values. We call “value” of a good a price that can be either objective, for example its exchange value, or subjective. Each agent is both a consumer and a producer, and may leave a capital stock of goods to his heirs, allowing for “disaggregated” capital accumulation. He can consume and produce several goods depending on his preferences and skills, given his economic environment encoded in the social value vector. This set up is suited to study the dynamics of heterogenous production sectors and the conditions for producers’ takeoff and capital accumulation, depending on producers’ individual characteristics and economic and social environment. Reciprocally, it allows to model the impact of capital accumulation on social values. How heterogeneity in personal values impacts social values and capital accumulation? What are the conditions that favor capital accumulation for precursors? Can several sectors with distinct values coexist, and how are social values and capital accumulation impacted by their coexistence? These are the questions raised by our model. They will be addressed by considering several variations of the above set up, starting from a homogenous society, then studying the consequence of heterogeneity in individual values, the dynamics for precursor agents, to ultimately endogenize the social value as a combination of individual ones. We show that, in all these cases, a threshold appears in the dynamics of agents’ capital stock. Below this threshold, the initial stock will quickly fade away; above, capital accumulation is possible. The threshold depends on capital mobility and productivity, but more importantly on volatilities of both personal and social values. High volatility in social values increases the threshold: stocks depreciate faster than they are replaced. Moreover, shocks on goods’ social values may drive stocks above or below the threshold, either deterring or initiating capital accumulation. On the other hand, heterogeneity in personal values increases the threshold for capital accumulation for agents departing from the mainstream. Only precursor agents, i.e. agents whose personal values will become social ones in the future, may benefit from departing from present social values. Ultimately, considering a society where social values are a weighted mean of several sectors individual values, the dynamics of capital accumulation induces an eviction phenomenon. A sector will impose its values to the entire society, whereas the other sectors will experience a full depreciation of their production. The rest of this paper is organized as follows. The first section is a review of the related literature. Section 3 presents the model. In Sect. 4 we present the agent’s optimization problem. Section 5 solves the model for the cases presented above. Section 6 discusses the results and Sect. 7 concludes.",2
14.0,1.0,Journal of Economic Interaction and Coordination,23 January 2019,https://link.springer.com/article/10.1007/s11403-019-00236-7,"Inequality, mobility and the financial accumulation process: a computational economic analysis",March 2019,Yuri Biondi,Simone Righi,,Male,Female,Unknown,Mix,,
14.0,1.0,Journal of Economic Interaction and Coordination,14 March 2018,https://link.springer.com/article/10.1007/s11403-018-0218-7,Learning to save in a voluntary pension system: toward an agent-based model,March 2019,Balázs Király,András Simonovits,,Male,Male,Unknown,Male,"All over the developed world, governments operate mandatory pension systems to replace income and minimize old-age poverty. In general, the size of the mandatory system is low enough to leave room to be filled by a voluntary pension system. As a rule, participants of a voluntary system can only withdraw their voluntary savings after retirement, and as a compensation their savings enjoy tax advantages or matching. While there is a general agreement that this separation of mandatory and voluntary systems is socially advantageous, there are important debates about the qualitative as well as quantitative design. To explain our contributions in a nutshell, we introduce the following concepts. In a complex economic system, the participants can use publicly available information and can also learn from directly observing their neighborhood. Concerning lifecycle saving, we may differentiate various degrees of shortsightedness: the less shortsighted a worker, the more she saves for her retirement. In a voluntary pension system the government matches the worker’s voluntary new savings proportionally to a matching rate, at least up to a cap. To simplify the exposition we introduce a concept called relative propensity to save which is the ratio of the actual to the estimated optimal voluntary saving of shortsighted workers. Because of some myopia and weak willpower, this index is always less than or equal to 1. Creating twin-models with public and local learning, our paper sheds new light on the foregoing problems. Our analytical model with public information used by the shortsighted workers gives relatively simple results, especially for the dependence of the steady state on the matching rate and the relative propensity to save. Our agent-based model incorporates more realistic, local learning, where the more shortsighted workers also learn from less shortsighted ones.Footnote 1 The proponents of voluntary systems justify the subsidies as follows: a mandatory system does not and cannot ensure high enough pensions, and the mostly shortsighted workers must be made interested in raising their old-age incomes through a voluntary system (e.g. Poterba et al. 1996). The opponents are afraid that these subsidies are poorly targeted, mostly subsidize the well-paid savers, while worsening the burden of the others by generating tax expenditures (Engen et al. 1996; Duflo et al. 2007). Hubbard and Skinner (1996) tried to synthesize both approaches, while OECD (2005) and Hinz et al. (2013) summarized the practice of various countries. Up to now the foregoing tax expenditures have generally been quite low though nonnegligible (about 0.7% of the GDP in the US), but in the case of a possible contraction of the mandatory system they may become much higher. Since Modigliani and Brumberg (1954) and Samuelson (1958), models of life-cycle saving and of overlapping generations have been extensively studied, respectively. A new era started with Auerbach and Kotlikoff (1987) which generalized the partial equilibrium framework into a general equilibrium one: not only savings depend on the interest rates but the interest rates also depend on savings through accumulated capital. By replacing traditional life-cycle savings with mandatory and voluntary pensions, these models have become more realistic.Footnote 2 A common problem with these models, however, is that they assume that the individuals have an extraordinary sophistication to solve the corresponding parts of their optimization problem and the willpower to achieve the results. It is widely documented, however, that a large share of the population have quite limited cognitive abilities (for a survey, see Lusardi and Mitchell 2014), quite limited information (Barr and Diamond 2008, Box 4.2) and weak willpower. A class of very simple life-cycle models operate with given interest rates and wages (small open economy). In such models, various workers’ ordinary life-cycle saving processes are independent, but adding government matching via a voluntary pension system introduces interdependence. Indeed, even if somebody does not participate in the scheme, he pays taxes according to the same earmarked tax rate. This may be the reason why in voluntary systems, individual optimization is mathematically quite difficult (see Appendix B in Király and Simonovits 2016) even if in addition to heterogeneously myopic workers, only two working age periods are distinguished. Only by neglecting the difference between young and old workers was Simonovits (2011) able to obtain analytical results on the impact of the matching rate and of the cap on income redistribution in such a transfer system. A more realistic approach to life-cycle savings is based on behavioral economics [started by Thaler and Benartzi (2004) and crowned by a recent survey by Chetty (2015)] but they neglect tax expenditures. We start the discussion of learning to save in an analytical model. For simplicity, we assume a stationary population without growth, inflation and interest. To make the impact of the individual decisions on the macro state negligible, we assume that there are a continuum of workers. Following Feldstein (1985), we distinguish at least farsighted and shortsighted workers. After the steady-state analysis, we assume that the government unexpectedly introduces a matching scheme in period 0, and this initiates a new behavior for both types. The appearance of government matching and its tax financing make the shortsighted workers aware that the farsighted save for the future and even its size can be guessed. The inadequacy of the shortsighted workers’ savings is measured by the standard deviation in their life-cycle consumption, while the redistribution from the shortsighted to the farsighted workers is measured by the standard deviation of the population lifetime average consumptions. For the sake of brevity, the two standard deviations are distinguished by adjectives internal and external. To simplify the calculations, we assume that the farsighted workers try to smooth their consumption paths without any intertemporal substitutability. Our analytical results on life-cycle saving are as follows: (a) By saving in a tax-favored system, the farsighted workers simply exploit the shortsighted ones. (b) We assume a special and admittedly artificial form of learning from public information: the active shortsighted workers guess the amount of their farsighted counterparts’ saving as the ratio of the tax rate and the matching rate provided by government statistics, and due to shortsightedness (and weak willpower), they save only a given share of this estimation: the share to be called relative propensity to save [see (6) below for its definition]. Note that if the share of the farsighted workers is very low, then they only play the role of the catalyzer but without them the model ceases to work. The process converges from the old to a new steady state (at least for moderate matching rates) and the degree of exploitation is significantly reduced. (c) We can simply disaggregate the aggregate behavior of shortsighted workers according to their different relative saving propensities and obtain subtypes. In the second model, we assume that these shortsighted subtypes also learn locally from each other. By adding local learning to using public information, we study an agent-based model (for short, ABM). These models generally enhance the realism of economic modeling (see e.g. Tesfatsion 2006). The main innovation of ABMs is that by sacrificing the ability to derive analytical results, they are able to describe more realistically the behavior of interacting heterogeneous agents. This methodology has been successfully used in several fields of economics. For example, the topic of tax evasion, related to our problem, was investigated—among others—in Méder et al. (2012), Pickhard and Prinz (2013) and Bertotti and Modanese (2016). Quite recently, Varga and Vincze (2017) used an ABM to analyze a very abstract model of ordinary saving. They assumed a very long (practically infinite) horizon and excluded mandatory as well as voluntary pensions. They distinguished three types of agents: buffer-stock savers (who follow the prescriptions of the life-cycle model, smooth their consumption path by saving), permanent income savers (forward looking individuals without prudence) and myopic savers (who spend most of their disposable income on current consumption). The main message of that paper is that notwithstanding permanent learning, different types can coexist for a very long time. Applying the ABM approach to life-cycle savings, especially to voluntary pension looks promising. Already Duflo and Saez (2003) emphasized the influence of colleagues’ choices on participation in voluntary pension plans. Here we try to explain an empirically verified fact: though the share and the extent of participation in tax favored systems are increasing functions of the wages; even controlling for wages, both indicators are heterogeneous (Baily and Kirkegaard 2009, Table 8.1, p. 456). We take homogeneous wages, neglect the cap on the voluntary contributions, thereby eliminate unmatched savings above the matched savings. We highlight the following ABM-results: (i) in the basic run, some heterogeneity in savings of the shortsighted workers remains; (ii) increasing the spread between the propensities diminishes both standard deviations; (iii) the increase in the number of types diminishes the external standard deviation but increases the internal standard deviation; (iv) randomly perturbing the network may homogenize the shortsighted workers’ savings; (v) the rise in the number of acquaintances does not reduce the standard deviations; (vi) diminishing the density of the connections by a factor of 4, the convergence is much slower; (vii) even if the workers revise their strategy annually rather than per decade, the welfare is not raised; and (viii) raising the relative propensities, the savings increase. In summary: the behavior of the complex system (ABM) cannot be fully understood from its simplified version (analytical model). Further work is needed to check the robustness of these results especially the details of local learning. To place the current paper in the related literature, Table 1 compares the presence of the following properties of five selected models of voluntary pension systems: dynamic saving, tax expenditure, simple saving rules and local learning (+ means yes, – means no). The selected models are Choi et al. (2004), Fehr et al. (2008), Simonovits (2011) and the twin-models of the current paper: analytical versus ABM. [Note that Simonovits (2011) is also an analytical model with such a simple structure that has no room for learning.] The analytical model has three and the ABM has four +s, while Simonovits (2011) has only one. Table 2 shows how the core of our twin-models is related to five selected countries’ pension systems according to the strength of progressivity and the size of the mandatory (public \(+\) private) and of the voluntary systems, respectively. We see that our twin-models go even beyond the German and the Hungarian systems in eliminating any redistribution in the mandatory system, and it copies the US system’s medium size. Concerning the voluntary system, our models are similar to the US, the Dutch and the Hungarian systems having no progressivity. It is not shown in the table, but our twin-models resemble the German voluntary system in having mandatory life annuities, and approximates the Hungarian system with its very high cap on the voluntary savings. In summary, we have copied various features of various countries arbitrarily, just to make the twin-models as simple as possible, to focus on the learning dimension. Though our model family admittedly lies quite far from any real voluntary pension system, we formulate some policy suggestions. (i) Models of voluntary pension systems should take into account the tax expenditure of operating the voluntary pension system. (ii) The design of the voluntary system should be in harmony with that of the mandatory system: probably a progressive voluntary system fits a proportional mandatory one and a proportional voluntary system fits a progressive mandatory one. (iii) If the government wants to strengthen the voluntary savings of the shortsighted, it should increase the matching rate and decrease the cap (not discussed here). The structure of the remainder of the present paper is as follows: Sect. 2 discusses an analytical model of life-cycle saving, where the shortsighted workers learn only from public information. Section 3 studies the corresponding ABM. Section 4 concludes.",3
14.0,1.0,Journal of Economic Interaction and Coordination,17 March 2018,https://link.springer.com/article/10.1007/s11403-018-0219-6,Prospect Theory in the Heterogeneous Agent Model,March 2019,Jan Polach,Jiri Kukacka,,Male,Unknown,Unknown,Male,"This paper introduces features of loss aversion and gain–loss asymmetry into the popular Brock and Hommes (1998) asset pricing model. Our work is based on findings of the iconic Prospect Theory (PT) of Kahneman and Tversky (1979), which describes the way people choose between probabilistic alternatives involving risk and is inherently a critique of other, more normative decision-making economic theories. Back in 1979, Kahneman and Tversky found that the actual behavior of human beings might be very different to what major economic theories had assumed, namely in relation to risk and attitude towards losses. According to PT, instead of behaving fully rationally and using perfect cognitive calculations, people make decisions with respect to gains and losses rather than the final outcome. Losses also have a greater emotional impact than an equivalent amount of gains; they hurt more than equal gains please. The extension that we develop in this work is aimed at accounting for these empirically observed irrationalities. Over the years, PT has become one of the most influential theories, merging psychology with economics. As Belsky and Gilovich (2010, p. 52) aptly remark, “If Richard Thaler’s concept of mental accounting is one of two pillars upon which the whole of behavioral economics rests, then Prospect Theory is the other”. Kahneman and Tversky (1979) paper is the most cited work to ever appear in Econometrica (Chang et al. 2011, p. 30). In contemporary economic theory, there is little doubt that economic agents are heterogeneous to some extent. In the late 1980s and early 1990s, empirical micro studies reported heterogeneity as an empirically significant phenomenon. Frankel and Froot (1990) attribute the reason for the divergence of the US dollar interest rate from macroeconomic fundamentals at the beginning of the 1980s to the existence of speculative traders; Hansen and Heckman (1996, p. 101) indicate a “considerable interest in Heterogeneous Agent Models in the real business cycle literature research”; and Brock and Hommes (1997, 1998) theoretically prove that it may be individually ‘rational’ for agents not to follow rational expectations and to behave, instead, according to simple predictors. Evans and Honkapohja (2001) explain that agents lack the required sophistication to rationally form expectations; Mankiw et al. (2004) draw attention to statistically significant disagreement in survey data on inflation expectations even among professional economists; Branch (2004) summarizes studies documenting “failure of the rational expectations hypothesis to account for survey data on inflationary expectations”; and Vissing-Jorgensen (2004) conducts an analysis of qualitative telephone survey data on US stock markets from 1998 to 2002, in which the author concludes that there is significant disagreement among the investors regarding expected profits. As an important experimental contribution to the hypothesis of heterogeneity of market participants, Hommes (2011) provides ‘evidence from the lab’ of the presence of heterogeneous expectations in an experimental financial market. The primary objective of this paper is thus to extend the original Brock and Hommes (1998) model with features of PT and, at the same time, keep the intrinsic mechanics of the model intact in order to preserve its simple, stylized nature. The original Brock and Hommes (1998) Adaptive Belief System (ABS) is a financial market application of the evolutionary selection system proposed by Brock and Hommes (1997), in which agents switch among different forecasting strategies according to past relative profitability of these strategies. The ABS is a discounted value asset pricing model extended to heterogeneous beliefs, in which the agents have the possibility to invest in either a risk-free or a risky asset. Our analysis consists of using Monte Carlo methods to investigate the behavior and statistical properties of the extended versions of the model and assess the economic relevance of results. One of the most important stimuli to induce the development of Agent-based Models (ABMs) in economics was certainly an erosion of trust in the Efficient Market Hypothesis (EMH)—the EMH asserts, in Eugene Fama’s words, that “...security prices at any time ‘fully reflect’ all available information...” (Fama 1970, p. 383)—and in the Rational Expectations Theory in the late 1970s and early 1980s. This was largely due to increased focus on the study of several stylized empirical facts—according to Cont (2001, p. 224), “The seemingly random variations of asset prices do share some quite non-trivial statistical properties. Such properties, common across a wide range of instruments, markets and time periods are called stylized empirical facts”. The most essential difference between natural sciences and economics is arguably the fact that decisions of economic agents are determined by their expectations of the future and contingent on them; hence, the study of how these beliefs are formed plays a vital part of any economic theory. Several scholars have published papers which confront the EMH with empirical data mainly from the perspective of non-normal returns,Footnote 1 systematic deviations of asset prices from their fundamental value, and excessive stock price volatility; it has proved impossible to attribute these phenomena to the EMH or to explain them within the rational expectations framework. Offering an insightful survey on the volatility issue at that time, West (1988) summarizes and interprets the literature related to this field. The author finds that neither rational bubbles nor any standard models for expected returns adequately explain stock price volatility and emphasizes the necessity to introduce alternative models to offer a better explanation of the apparent contradiction between the EMH, the Rational Expectations Theory, and empirical findings. This paper is structured as follows: immediately after the present Introduction, Sect. 2 summarizes the main features of Prospect Theory and Sect. 3 describes the mathematical structure and underlying mechanics of the original Brock and Hommes (1998) model; Sect. 4 develops the behavioral extension based on Prospect Theory, while Sect. 5 describes the numerical simulations using Monte Carlo methods; Sect. 6 highlights the main results of the simulations and Sect. 7 concludes the paper.",9
14.0,1.0,Journal of Economic Interaction and Coordination,28 December 2017,https://link.springer.com/article/10.1007/s11403-017-0214-3,Efficient coordination in the lab,March 2019,Aurora García-Gallego,Penélope Hernández-Rojas,Amalia Rodrigo-González,Female,Female,Female,Female,"Asymmetric information is a widespread characteristic in economic relationships. Take, for instance, a duopoly where firms have asymmetric information about the market demand of their product. In situations like these, the lack of information is one of the main drawbacks to reaching an agreement. Where agents’ decisions depend on information disclosure, strategic information transmission may be crucial and, so, sharing information becomes a pivotal in allowing agents to reach more profitable agreements. Furthermore, because of strategic concerns, there is a trade-off between revealed information and profits. To paraphrase the words of Crawford and Sobel’s (1982): revealing all information to the opponent is not usually the most advantageous policy. Some decades later, Blume and Ortmann (2007) pointed out the fact that costless messages help in overcoming strategic uncertainty, equilibrium selection problems and coordination failure. As a benchmark structure, information transmission between a sender and a receiver occurs when a message in a common language is sent through a transmission channel. Specifically, the sender is an agent with private information who sends a message that reveals “some” information to the receiver, who then takes a decision affecting both agents. The present work concerns strategic information transmission under asymmetric information. Specifically, private information of the sender comes from a non-player actions, that are only observed by the sender but ignored by the receiver. In their seminal paper Crawford and Sobel (1982) introduced a one-sided communication model between an informed sender and an uninformed receiver and showed how the conflict of interest had a negative effect on the flow of information transmission. There are many applications of strategic information transmission based on this model. For example, applications to corporations (Watson 1996; Kartik 2005), to operational management (Allon and Bassamboo 2011; Allon et al. 2011) or to political sciences (Gilligan and Krehbiel 1989; Krishna and Morgan 2001). In our set-up, we consider a sender-receiver situation where interests are aligned. However, some other features of our setting are common to Crawford and Sobel’s framework. First, information is transmitted through a one-sided communication channel. Secondly, the sender, denoted as Player 1, has complete and perfect information about Nature, whereas the receiver, denoted as Player 2, is an uninformed player who knows about Nature’s existence and actions in the past. Thirdly, sharing information is costless for Player 1. Finally, decisions of Player 2 have an effect on both players’ payoffs. Some other features in our setting are, however, specific to our communication protocol and common with the set-up of Gossner et al. (2003): both players form a team with aligned interests; Nature’s actions are modelized as an i.i.d. process, and players and Nature play in a finitely repeated fashion. Moreover, players have positive payoffs when both match Nature’s move. From a theoretical perspective, Gossner et al. (2003) characterize the equilibrium payoff that the team can guarantee against any behaviour of Nature. They construct equilibrium strategies of communication between sender and receiver in an infinitely repeated set-up based on block coding. To be precise, block coding strategies refer to the way in which players communicate their subsequent sequence of actions. In their set-up, the transmission of information occurs while playing, so that the sequence of actions played by the sender is encrypted as messages, and the receiver decodes the messages according to a common team’s codebook. The aim of our paper is to test in the lab whether under asymmetric uncertainty players whose dominant strategy is to share information are able to actually implement such a kind of block coding-encrypted messages. To do so, we first provide a theoretical characterization of the optimal block strategies for Player 1, considering that players interact a finite number of periods. Secondly, we show that the majority rule with 3-length blocks is optimal for a Nature sequence of length 55. These two variables are fixed for an experiment that fits the model and tests its robustness in the lab. In our experiment, we implement a specific channel for communication between players. Before the game, a chat is activated for 3 min. In this time, players have the opportunity to write free messages designing their strategies without any explicit cost. This chat allows the players to fix the common codebook and the decoding rules that players may eventually perform when playing the game. Once the chat closes, the 55 actions of Nature are carried out and drawn following an i.i.d. process with law \((\frac{1}{2},\frac{1}{2})\). Player 1 is informed about the entire sequence played by Nature. Players then play the game for 55 rounds. In round t, Player 1 has the whole sequence of Nature on the screen; his own action, and Player 2’s actions in the past. Nevertheless, Player 2 knows the whole history of moves by Nature and by Player 1. Any verbal communication is forbidden during the game. So, before the game starts, Player 1 may signal to Player 2 specific ways to coordinate and, therefore, reach better payoffs. How much information is transmitted depends on how informative the signal of Player 1 is and also on how receptive Player 2 is to receiving the signal. All this process eventually determines the payoff of both players measured as the number of rounds in which Nature and players’ actions coincide. We analyze the effectiveness of the chat in transmitting information in the terms of the theoretical model. That is, without specifically analysing the content of the messages in the chat, we test whether players coordinate and, if so, by how much under such conditions and, therefore, whether the model predicts reasonable strategies that could be observed from real heterogeneous agents. One main finding from our data analysis is that subjects are able to design strategies at three levels of coordination. First, low coordination strategies are those in which the receiver ignores or misunderstands any signal sent by the sender so that coordination occurs by pure chance. Secondly, there are medium coordination strategies where information is successfully transmitted by following a joint coordination code. Finally, the high coordination strategies are those where coordination codes achieve payoffs close to the optimal theoretically predicted. Furthermore, from a logit estimation we find that actions by Player 2 are significantly explained by actions played by Nature and by Player 1. With respect to Player 1’s actions, their signals are precise most of the times. The paper is structured as follows. Section 2 gives a brief review of related literature on strategic information transmission. In Sect. 3 we describe the theoretical framework of the game. Section 4 describes in detail the experimental environment. In Sect. 5 we present the analysis of the experimental data and highlight the main results. Section 6 presents the conclusions.",1
14.0,1.0,Journal of Economic Interaction and Coordination,06 June 2018,https://link.springer.com/article/10.1007/s11403-018-0223-x,Coordination in a skeptical two-group population,March 2019,Juan Carlos González-Avella,Haydée Lugo,Maxi San Miguel,Male,Unknown,,Mix,,
14.0,1.0,Journal of Economic Interaction and Coordination,20 December 2017,https://link.springer.com/article/10.1007/s11403-017-0212-5,Stagnation proofness in n-agent bargaining problems,March 2019,Jaume García-Segarra,Miguel Ginés-Vilar,,Male,Male,Unknown,Male,"When the feasible set of a bargaining problem (Nash 1950) is expanded leaving the utopia point unaffected, some bargaining solutions are not responsive to potential improvements, whereas some other solutions can translate the expanded possibilities into better payoff of at least one agent. We call this lack of responsiveness the stagnation effect. This phenomenon is not an issue for 2-agent bargaining problems, since the axiom of strong Pareto optimalityFootnote 1 implies the avoidance of the stagnation effect. However, our concern is that the vast majority of bargaining solutions do not satisfy this axiom when there are 3 or more agents. In addition, with 3 or more agents, strong Pareto optimality is not compatible even with restricted versions of monotonicity (García-Segarra and Ginés-Vilar 2015). In contrast, most of the well-known bargaining solutions satisfy weak Pareto optimality for the case of n agents. Weak Pareto optimality does not guarantee the avoidance of the stagnation effect since some of the solutions satisfying it remain stagnant under all possible expansions of the bargaining set that keeps the utopia point unchanged. In view of the stagnation effect, we introduce a new axiom, stagnation proofness. Whenever a bargaining solution satisfies this property such bargaining solution does not suffer from the stagnation effect. Many bargaining solutions are defined by means of monotone paths, for instance, the egalitarian solution and the solution characterized by Kalai and Smorodinsky (1975) (hereinafter KS).Footnote 2 Two interesting results about families of solutions generated by monotone paths deserve especial attention. The first one is a characterization of a family of solutions generated by strictly increasing paths that holds for n-agent problems (Thomson and Myerson 1980). This result focuses on the environment of solutions where there is interpersonal comparability of utility across agents. The egalitarian solution, for instance, belongs to this family. The second one is a family that generalizes a particular weighted version of the KS solution (Alós-Ferrer et al. 2017), i.e., solutions generated by increasing paths ending at the utopic point (which is the point that reflects the maximum aspirations of each agent given a bargaining problem.) There are two characterizations of this family. One of them holds for 2-agent problems (Peters and Tijs 1985), while the other one holds for the case of three or more agents (Peters and Tijs 1984). Both characterizations refer to the environment of solutions in which there is no interpersonal comparability of utility. Obviously, the KS solution belongs to this environment. The solutions characterized in Thomson and Myerson (1980) do suffer from the stagnation effect. In contrast, the family of solutions characterized in Peters and Tijs (1984, 1985) satisfy the axiom of strong Pareto optimality, therefore they do not suffer from the stagnation effect. However, in order to provide a characterization for n-agent problems, a restriction on the bargaining domain is required (Peters and Tijs 1984). Relying on the axiom of stagnation proofness, we provide a characterization of the family of solutions generated by strictly increasing paths (as the ones in Thomson and Myerson 1980) ending at the utopia point (as the ones in Peters and Tijs 1984, 1985) that holds for n agents and for the whole class of canonical bargaining problems. The paper is organized in four sections. Section 1 contains definitions and notation. In Sect. 2, we introduce the stagnation effect and the concept of stagnation proofness. In Sect. 3, we state and prove our main result. Section 4 concludes.",1
14.0,2.0,Journal of Economic Interaction and Coordination,20 October 2017,https://link.springer.com/article/10.1007/s11403-017-0204-5,Emergence of anti-coordination through reinforcement learning in generalized minority games,June 2019,Anindya S. Chakrabarti,Diptesh Ghosh,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Journal of Economic Interaction and Coordination,05 November 2017,https://link.springer.com/article/10.1007/s11403-017-0205-4,Asset diversification and systemic risk in the financial system,June 2019,Yichen Zhou,Honggang Li,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Journal of Economic Interaction and Coordination,06 March 2018,https://link.springer.com/article/10.1007/s11403-018-0216-9,Understanding the consequences of diversification on financial stability,June 2019,Opeoluwa Banwo,Paul Harrald,Francesca Medda,Unknown,Male,Female,Mix,,
14.0,2.0,Journal of Economic Interaction and Coordination,10 January 2019,https://link.springer.com/article/10.1007/s11403-018-00234-1,Partnership duration and concurrent partnering: implications for models of HIV prevalence,June 2019,Alan G. Isaac,Larry Sawers,,Male,Male,Unknown,Male,"Although the AIDS epidemic peaked in the late 1990s, tens of millions of people remain infected. There are almost two million AIDS-related deaths annually, which are slightly outpaced by new HIV infections. Eastern and southern sub-Saharan Africa have been particularly hard hit, experiencing an explosive late-20th-century spread and subsequent sustained high prevalence. Despite advances in prevention and treatment, some regions continue to show startlingly high HIV prevalence (Kharsany et al. 2015). Social scientists have struggled to understand the causes of this epidemic, hoping for insights that can lead to improved health policy. Economists have been interested in the behavioral correlates and the economic consequences of the epidemic (Young 2005; Jones and Klenow 2016). Efforts to understand the spread of a disease that can be transmitted sexually naturally include investigations of the sexual practices of the affected population. In the 1990s, explanations of the HIV epidemic in sub-Saharan Africa (SSA) began to emphasize the “concurrency hypothesis,” which attributes SSA’s high prevalence of HIV infection to its high prevalence of long-term concurrent sexual partnerships. The core idea behind this attribution is that overlapping partnerships produce sexual networks that are especially effective at spreading HIV. Early discussions of concurrency include Watts and May (1992) and Hudson (1993). Advocates of the concurrency hypothesis include Epstein (2007, 2008, 2010), Epstein and Morris (2011), Halperin and Epstein (2007), Mah and Halperin (2010), Morris et al. (2010), and McCreesh et al. (2012). Critics include Lurie and Rosenthal (2010a, b), Sawers and Stillwaggon (2010), Sawers et al. (2011), Tanser et al. (2011), Sawers (2013), and Sawers and Isaac (2017). At present, policy discourse and HIV prevention strategies reflect a belief in the validity of the concurrency hypothesis. For example, Epstein (2008) asserts that “ ‘long term concurrency’ probably explains why HIV in Africa has spread so rapidly beyond typical ‘high risk groups’ such as sex workers”. This view has been driven by SSA’s unusual prevalence of HIV and the regional acceptance of polygyny, along with computational models that appear to support an important role for concurrency. However, in the available survey data, correlations between concurrency and HIV prevalence remain empirically fugitive, as most famously illustrated by the “four cities” study of Caraël et al. (2004).Footnote 1 A key strand of the concurrency research constructs agent-based simulation models. Early models demonstrated that concurrency correlates with epidemic scale. Against this, concurrency skeptics have demonstrated that empirically plausible modifications of these models diminish or eliminate the correlations. Critics argue that the concurrency hypothesis has achieved a salience in policy discussions and prevention strategies that is not justified by the empirical and theoretical research. In this article, we join with the critics to raise additional questions about the adequacy of the theoretical work that has been cited in support of the concurrency hypothesis. In order to facilitate comparability of results, our contribution draws on both the structure and the parameterization of some well-known agent-based models of HIV prevalence. Since the pioneering work of Morris and Kretzschmar (1997), agent-based modeling and simulation has come to dominate the theoretical work on the epidemiological consequences of concurrency in sub-Saharan Africa. These models are often called individual-basedpair-formation models, because they explicitly model individuals who form and dissolve sexual partnerships. These partnerships are the links across which sexually transmitted disease propagates. As Eaton (2013, p. 33) puts it, such agent-based models help us understand “how biological and behavioral factors interact in promoting the spread and control of HIV.” The Morris and Kretzschmar model—hereafter, the MK1997 model—is a classic demonstration of how greater concurrency prevalence can promote more rapid and effective propagation of sexually transmitted disease. Partly because this linkage is intuitively appealing, their work had a large influence on beliefs about the SSA HIV epidemic. The MK1997 model is iconic in the literature due both to its methods and to it results. It demonstrated that agent-based modeling can shed light on potential contributors to the SSA HIV epidemic, and it flagged concurrency as a potential culprit. Simulations generated by the MK1997 model are characterized by explosive growth in HIV prevalence, even at low concurrency prevalence. As a result, this model is widely cited in support of the concurrency hypothesis. It has been cited in over 1600 scholarly works on the subject, including subsequent simulation research. Oddly, it took a decade before the model’s parameterization began to receive serious critical appraisal (Deuchert and Brody 2007; Lurie and Rosenthal 2010a). During that decade, the concurrency hypothesis became a conventional explanation for SSA’s devastating HIV epidemics. Key aspects of the MK1997 model recur in the subsequent literature. In particular, it is a dynamic, stochastic pair-formation model: partnership formation is ongoing and existing partnerships are constantly at risk of dissolution. Much of the subsequent work on concurrency has followed this general representation of partnership formation and dissolution (Morris and Kretzschmar 2000; Eaton et al. 2011; Sawers et al. 2011; Eaton 2013). We hew very closely to this literature in order to demonstrate how key assumptions have influenced its conclusions. Specifically, we explore the role of partnership duration. Particularly relevant to the present paper is the work of Morris and Kretzschmar (2000), which extends their earlier MK1997 model. Using the terminology of McCreesh et al. (2012), we can say that this MK2000 model distinguishes between long-duration and short-duration partnerships. Based on survey evidence that long-duration partnerships last about 20 years while short-duration partnerships last about 2 years, the MK2000 model incorporates mean partnership durations that are substantially longer than in the MK1997 model. While HIV epidemics simulated with this model grow much more slowly than in the MK1997 model, prevalence still triples in 5 years (from 1.0 to 2.92%). The literature contains many modifications of the MK1997 and MK2000 models. The model of Eaton et al. (2011) particularly influences our research.Footnote 2 Most importantly, the EHG2011 model incorporates evidence-based transmission rates, which are much smaller than in MK1997.Footnote 3 Our model parameterization matches EHG2011 whenever possible, adopting their transmission rates, their vital dynamics (i.e., deaths from AIDS, and the introduction of uninfected individuals), and the substantially longer time-scale of their simulations (chosen to produce simulation results representative of the model’s stochastic steady state). However, influenced by the MK2000 model, we additionally distinguish partnership types by duration. A core modeling goal of the present paper is to maintain results comparability. Our implementation and parameterization therefore closely track existing models (particularly the EHG2011 model). We want the resulting model to be encompassing, so that previous results can be reproduced simply by means of a few parameter changes. In particular, we explore the partnership duration assumptions of the previous literature. To this end, we distinguish primary and secondary partnerships and independently vary the mean partnership duration for each partnership type.Footnote 4 It is widely recognized that stable monogamy protects susceptible individuals and traps sexually transmitted diseases within infected partnerships (Dietz 1988; Kretzschmar and Heijne 2017). The same is true of stable polygamy, but many authors have elided the distinction between concurrency and exclusivity. For example, Halperin and Epstein (2004, p. 5) argue that in contrast to a “network of concurrent relationships ... serial monogamy traps the virus within a single relationship for months or years”. Both trapping of the virus and insulation from it are determined by long-term stability and exclusivity, not by the number of partners, nor by whether the partnerships are formal or informal. While a stable and exclusive n-person group shares the trapping and protectivity features of a stable and exclusive 2-person partnership, dalliances pose a greater risk for a larger group. Suppose each partner has an independent probability p of an external sexual encounter in a given year. With n partners, the probability that the group remains completely exclusive is \((1-p)^n\), which is strictly decreasing in the number of partners. Of course the actual risk depends on HIV prevalence, and it is muted by the low per-act transmission efficiency of the virus. Partnership duration also has important effects on disease spread. We show below that even in the context of concurrent partnering, long partnership duration retards rather than promotes the spread of HIV. Since partnership duration data are scanty, our demonstration relies on examining a range of durations. Despite numerous surveys of sexual behavior in sub-Saharan Africa, survey evidence on sexual-partnership duration remains scarce. Even surveys that report duration often provide little guidance to modelers. For example, the survey of Powers et al. (2011) is restricted to patients in an STI clinic, who are not representative of the general population. Similarly, surveys restricted to youth will document partnering patterns unlikely to generalize to an older population (Harrison et al. 2008; Harrison and O’Sullivan 2010; Goodreau et al. 2012). Studies of youth also produce right censoring of partnership duration. However, recent Demographic and Health Surveys include some useful questions about primary partnership duration. In 16 countries in sub-Saharan Africa, women reported mean duration in married or cohabiting partnerships of 12.1 years, and men reported mean duration of 13.1 years. This is virtually the same as in 16 low-income countries outside Africa for which there are data (based on datasets obtained from The DHS Program, ICF International). Unfortunately, data on secondary partnerships are not reported by the DHS. There has been some attempt to address this in in the literature on formal polygyny (Reniers and Tfaily 2008, 2012; Reniers and Watkins 2010). Of particular interest are the results of a survey in Rakai, Uganda, as reported by Morris and Kretzschmar (2000, Table  1). To our knowledge, this the only survey that reports average duration of both primary and secondary partnerships for a large sample of adults (\(N = 1994\)), and it found that the average duration of secondary partnerships was about 12% of average duration of primary partnerships (28.4 months vs. 239.1 months). This difference is roughly an order of magnitude, which suggests that models of sexually transmitted diseases should explore the implications of varying partnership duration by partnership type. The available survey evidence is even less informative about the interaction of duration, concurrency, and HIV. Our model sheds light on these interactions, which may suggest directions for future data collection and empirical design. We thereby contribute to the agent-based simulation and modeling literature that elucidates how partnership characteristics influence the spread of sexually transmitted disease (Chen et al. 2008; Kim et al. 2010).",
14.0,2.0,Journal of Economic Interaction and Coordination,07 March 2019,https://link.springer.com/article/10.1007/s11403-019-00242-9,"Market efficiency, trading institutions and information mirages: evidence from a laboratory asset market",June 2019,Andrea Morone,Simone Nuzzo,,Female,Female,Unknown,Female,"As pointed out by Sunder (1995), asset markets are definitely different from other markets for at least two reasons: the informational role of prices and the duality of traders’ behaviour. The first feature suggests that prices contain information. This is generally true for any other kind of market, but there is a specific issue characterizing asset market prices. While in goods markets prices are informative in the sense that they make customers aware of their budget constraints, in asset market prices reflect the information available to each trader at any instance of time. In other words, asset markets are informative in the sense that they convey information from informed to uninformed traders. As Plott (2000) points out, asset markets could be compared to a statistician who collects and aggregates the information dispersed across the market, and the price is the form in which the findings are published. In this sense, several previous studies (see Sunder 1995) have shown that, under certain circumstances, asset markets manage to gather and spread the dispersed information through the price adjustment mechanism. The second feature refers to the fact that each trader could be a buyer as well as a seller in the same market, i.e. traders can, both, buy and sell assets in exchange for money. Our contribution is framed in the literature strand which investigates the relationship between market performance and trading institutions. The latter commonly refer to the set of exchange rules that determines how purchase and sale proposals are matched and, consequently, how trade prices are determined. Trading institutions differ from one another in several features, i.e. the richness of within period information feedback, the continuous vs. discrete time through which information is processed and the number of trading opportunities allowed in each period. It has been showed that all these variables play a prominent role in establishing different incentives and coordination strategies associated with price formation and exchange. Indeed, several studies (see Plott 1982; Holt 1995; Cason and Friedman 1999) have found, in a more general framework, that trading institutions crucially matter for market performance and convergence to the clearing outcome. In line with most of both theoretical and experimental studies on trading institutions (see Sect. 2) this paper compares the performance of a continuous double auction and a call market (with multiple orders per period). The market environment in which this comparison is performed constitutes the main novelty of our work. Indeed, while in related studies the possible presence (not necessarily the identity) of insiders in the market was common knowledge, in our setting uninformed agents cannot be sure about the presence of insiders in the market. This variation has two major implications. As a first point, it contributes to frame the relationship between market performance and trading institutions in a more realistic context, since in real world financial markets there is no certainty about the presence of informed traders. Second, it sheds light on the potential impact of trading institutions on information mirages.Footnote 1 Information mirages were first detected in a leading contribution by Camerer and Weigelt (1991). The authors showed that, in a continuous double auction market where subjects could not be sure about the presence of informed traders, the incorrect information inference during periods with no inside information led to a price path, namely an “information mirage”, in which prices departed from the efficient equilibrium price, thus undermining the overall informational efficiency of the market. In a framework which resembles that of Camerer and Weigelt (1991), we ask whether a call market institution is capable to prevent, as opposed to a continuous double auction mechanism, the occurrence of information mirages and to promote a greater level of informational efficiency when no inside information has entered the market. Second, we also compare the performance of the two trading institutions during periods whit insiders. The remainder of this work is organized as follows. In the next section we present a review of the literature and in the third section the experimental design. In Sect. 4 we discuss the theoretical background and introduce our hypotheses, then in Sect. 5 we present the results obtained. Finally, Sect. 6 concludes.",3
14.0,2.0,Journal of Economic Interaction and Coordination,27 December 2017,https://link.springer.com/article/10.1007/s11403-017-0211-6,Exponential structure of income inequality: evidence from 67 countries,June 2019,Yong Tao,Xiangjun Wu,Victor M. Yakovenko,,Unknown,Male,Mix,,
14.0,2.0,Journal of Economic Interaction and Coordination,12 April 2019,https://link.springer.com/article/10.1007/s11403-019-00250-9,Trading volume and return volatility of Bitcoin market: evidence for the sequential information arrival hypothesis,June 2019,Pengfei Wang,Wei Zhang,Dehua Shen,Unknown,,Unknown,Mix,,
14.0,2.0,Journal of Economic Interaction and Coordination,06 October 2017,https://link.springer.com/article/10.1007/s11403-017-0203-6,A note on the relationship between the total factor productivity and the network of firms,June 2019,Antonio Palestrini,Enrico Guzzini,,Male,Male,Unknown,Male,"Recently, Guzzini and Palestrini (2016) found, among other results, a positive relationship between the so-called Solow residual and the Adelman index. Since the Adelman index (Adelman 1955) is a widely used measure for vertical integration of firms, by capitalizing on the concept of production chains (Levine 2012), they interpret this index as a rough measure of connectivity among firms in the economy. In their paper they argue that idiosyncratic shocks among firms interact with the network structure of firms, thus producing an aggregate TFP-growth which has a different distribution compared to the distribution of the idiosyncratic shocks. Their basic idea relies on the fact that idiosyncratic shocks are amplified by the links among firms and the strength of these links. According to this idea, the network structure has therefore some relevance in explaining the so-called Solow residual. Besides the analytic aspects of this paper, they empirically assess the importance of links among firms in explaining the idiosyncratic shock amplification and its auto-correlation by using, as explanatory variable, a measure of industry links among firms (i.e. the complement of the Adelman index). In particular, in the empirical part of their analysis, Guzzini and Palestrini (2016) find that the relationship between the Solow residual and the measure of connectivity is non linear and positive in the relevant range of the measure of connectivity. They arrive at this result by estimating a fixed effect panel in a static setting. The aim of this note is to replicate their results, by using the same dataset, and by performing a dynamic panel analysis. The reason of our analysis is to assess the robustness of the original work, by using a dynamic specification, instead of the static one. The inclusion of the lagged value of the endogenous variable could be justified by several reasons. Firstly, the statistical relationship may itself have a dynamical nature; secondly the inclusion of lagged-endogenous variable is a way to mitigate the possibility of an omitted variable problem. In the following two sections, we carry out an empirical investigation with Italian micro level data. In particular, in Sect. 2 there is a description of the dataset and a panel data analysis of the statistical relationships. Section 3 concludes.",
14.0,2.0,Journal of Economic Interaction and Coordination,23 February 2019,https://link.springer.com/article/10.1007/s11403-019-00241-w,Franck Jovanovic and Christophe Schinckus: Econophysics and financial economics: an emerging dialogue,June 2019,Tobias Henschen,,,Male,Unknown,Unknown,Male,,
14.0,3.0,Journal of Economic Interaction and Coordination,12 September 2019,https://link.springer.com/article/10.1007/s11403-019-00263-4,Introduction to the special issue,September 2019,Tiziana Assenza,Jakob Grazzini,Domenico Massaro,Female,Male,Male,Mix,,
14.0,3.0,Journal of Economic Interaction and Coordination,11 September 2018,https://link.springer.com/article/10.1007/s11403-018-0230-y,Interbank credit and the money manufacturing process: a systemic perspective on financial stability,September 2019,Yuri Biondi,Feng Zhou,,Male,,Unknown,Mix,,
14.0,3.0,Journal of Economic Interaction and Coordination,25 August 2018,https://link.springer.com/article/10.1007/s11403-018-0227-6,Order book modeling and financial stability,September 2019,Alessio Emanuele Biondo,,,Male,Unknown,Unknown,Male,"Each financial crisis arrives unexpected. Market participants systematically find themselves beyond the frontiers of their predicting abilities, since prudential rules of conducts, risk management and sophisticated portfolio optimization techniques are not successful against instability. Financial volatility assumes the traits of a grim creature that, sometimes, suddenly awakes and causes bankruptcies, credit stress and collateral damages. Like a symptom of misunderstanding, apocalyptic portraits are frequently evoked by many observers and practitioners who ignore that financial markets—as many other macroeconomic phenomena—are, simply, complex systems. A methodological discussion about the impact of complexity on economics can be read, among others, in Delli et al. (2011), where the need of a new paradigm is clearly shown. Complexity reveals to be crucial in understanding macroeconomics, as discussed in Sornette (2009), Kirman (2011) and Ladyman et al. (2013), among many others. Such a consciousness dates back to Keynes (1936) and Majorana (1942). The interaction among elements of a system generates emergent aggregate outcomes, which qualitatively differ from the features of its constituents, as spontaneous self-organized structures at different layers of a hierarchical configuration (see Gallegati and Richiardi 2009). Thus, predictions about magnitude and timing of emergent properties in a complex context are useless (see Prigogine 1997). Should the point be just the frustration of some guru/banker/trader in seeing the (usual) failure of the best predictor at hand, financial turmoil could even be funny. Unfortunately, crises cause dramatic consequences both in financial and in real markets and, in turn, ignite poverty, unemployment and fear in society. Since predictive skills of economic models are continuously challenged by repeated confutations, this paper advances the hypothesis that something can be done, in order to reduce financial instability. In particular, while the design of policies aimed to dampen financial volatility can derive from micro-structural aspects of the market, as shown in Biondo (2018a, b), a greater attention will be given here to behavioral attitudes of traders in following informative signals. Mitchell (2009) suggests that order book dynamics is a valid example of a complex system, because it emerges as a global result of local individual interactions among traders. Thus, possibly, bubbles and crises can be prevented by fine-tuning interventions that courageously reduce the set of choices of traders and the quantity of information—i.e., the number of signals—received by agents from the market environment. The relation between information signals and trading decision has been discussed from different perspectives. Examples of such contributions are, among others: Barclay et al. (2003), who show that the quantity of information owned by the trader may play a role in the choice of the most suitable trading venue (between Electronic Communication Networks and Nasdaq market makers); Kishore and Garcia (2018), who underline the great role of informative signals by showing the different impact that they cause according to the timing of their arrival (e.g. during trading and non-trading hours); Ponta et al. (2018), who discuss how sentiments, shared among traders on sparsely connected networks, affect their decisions; Dodonova and Khoroshilov (2018), who use an experimental study to show how private information is used in price estimation; Naik et al. (2018), who explain the role of information flows in generating market volatility. A stream of studies focused on how the propagation of signals on small-world networks is responsible of financial-quakes, as in Biondo et al. (2013a, b, 2014, 2015, 2016, 2017). Further recent contributions on the relevance of traders’ networks in financial markets are those of Farmer and Foley (2009), Immonen (2017) and Ponta and Cincotti (2018), among others. Without adopting any specific opinion dynamics framework, nor relying on any topological structure among agents, this paper investigates the role of imitation as the individual propensity to obey to market signals. The presented analysis questions that policies limiting possible choices and information available to traders, are effective in dampening market fluctuations. It will be also shown that, surprisingly, aggressive traders may act as a stabilizing force because they reduce the amplification of herding price signals. 
Chakraborti et al. (2011), Slanina (2008) and Parlour and Seppi (2008) survey the vast majority of existing order-book models. Part of them can be defined trader-centric, because models of this class have been mainly based on frameworks aiming to derive fully rational optimal trading strategies, as in Chakravarty and Holden (1995), Foucault (1999), Parlour (1998), Hollifield et al. (2004, 2006) and Rosu (2009, 2016). Others can be defined facts-centric, because models of this class usually tended to study more the statistical features of the market as a dynamic process than the individual characterization of investors, as in Bak et al. (1997), Maslov (2000), Daniels et al. (2003), Farmer et al. (2005), Bouchaud et al. (2009) and Cont et al. (2010). This paper is conceived in line with a third stream of literature, founded on the computational approach of agent-based models (ABMs) in economics. Such models, developed since the Nineties, have shown several advantages in describing many aspects neglected by “orthodox” economic models, as explained in Tesfatsion (2006). Examples are, among others, Brock and Hommes (1997, 1998), Chiarella (1992), Chiarella and He (2001), Day and Huang (1990), Franke and Sethi (1998), Hommes (2001), Lux (1995, 1998) and Lux and Marchesi (1999). The determinant role played by the heterogeneity of interacting individuals is shown in many models of financial markets, as in Hommes (2006) and LeBaron (2006), and in models of order books, as in Raberto et al. (2001), Chiarella and Iori (2002), Consiglio et al. (2005), Gil-Bazo et al. (2007), Chiarella et al. (2009) and Tedeschi et al. (2012). The agent-based model here presented builds upon Biondo (2018a, b) and enriches the existing literature on the topic with regards to several aspects. First of all, traders (usually differentiated with respect to the behavioral attitude between fundamentalists and chartists) have heterogeneous informative sets and individual sensitivity thresholds to market information. Both such features ensure that each trader is different from anybody else. Secondly, orders can have variable quantity; in particular, limit orders have a time validity chosen by the trader at submission. Third, the simulated price series is entirely and exclusively generated within the model, by means of a double auction mechanism governing the order-book, which has been designed as a true contracts-driven price formation (without adding any fictitious data). In other models, as in Chiarella and Iori (2002) and in Chiarella et al. (2009), an average price is registered in their simulated price series whenever a transaction does not occur. Fourth, market orders are managed by a realistic quantity management system, used to match counterparts. Fifth, the market maker can decide to levy transaction costs. The paper is organized as follows: Sect. 2 contains the model description; Sect. 3 shows the compliance of the proposed framework with the most acknowledged stylized facts of true financial data; Sect. 4 presents simulations and discusses main results; Sect. 5 contains conclusive remarks.",11
14.0,3.0,Journal of Economic Interaction and Coordination,09 March 2019,https://link.springer.com/article/10.1007/s11403-019-00245-6,The term structure of cross-sectional dispersion of expectations in a Learning-to-Forecast Experiment,September 2019,Annarita Colasante,Simone Alfarano,Eva Camacho-Cuena,Female,Female,Female,Female,"The expectations of an economic agent regarding the future state of the economy affects his/her current individual choices. Thus, when aggregating all individual choices, expectations of agents influence the realizations of macroeconomic quantities. At the same time, the evolution of macroeconomic aggregates has an impact on how agents form and revise their expectations. The economy can therefore be modelled as an expectations feedback system. Consequently, how agents form their expectations at the individual level plays an important role in understanding the dynamics of the aggregate outcome. The rational expectations framework provides a normative indication of how expectations should be consistently formed within a given model. In the framework of rational expectations, agents share a common expectations formation mechanism. In order to introduce heterogeneity within the rational expectations framework, Mankiw et al. (2003) propose a sticky information model, where they assume that agents follow rational expectations and update their information set at different moments, taking into account acquisition and computation costs. Therefore, the heterogeneity of expectations emerges from the agents’ heterogeneous information set. We find, however, numerous examples in the theoretical, computational and experimental literature that rely on bounded rationality to introduce heterogeneity in the formation of the agents’ expectations (see Hommes 2013 and references therein). The origin of heterogeneity across individual expectations and the role that it plays in shaping aggregate outcomes is an important topic in theoretical as well as empirical research in macroeconomics. The fact that expectations are not directly observable in the same way as prices or volumes, means that there is a significant limitation when it comes to fully understanding their precise role in driving macroeconomic aggregates. Conducting forecasting surveys is one of the traditional methods used to elicit individual expectations (see Manski 2004). This methodology has been extensively used in macroeconomics, e.g. in testing the accuracy and rationality of forecasters. Only recently has disagreement about expectations, measured as the cross-sectional dispersion of forecasts, and its evolution over time, become in itself a variable of interest. It seems to contain information about, for example, the uncertainty about future development of business cycles or inflation rates (see Mankiw et al. 2003). In order to disentangle whether the origin of the observed forecast’ dispersion is a measure of the intrinsic uncertainty of macroeconomic variables or if it reflects heterogeneous priors of forecasters, Patton and Timmermann (2010) focus on how the dispersion of expectations evolves over different time horizons. They study the term structure of cross-sectional dispersion of forecasts, observing that it typically increases with the forecast horizon. Their analysis reveals that such persistent heterogeneity of expectations among forecasters in the short-run stems from different private information, whereas in the long-run it is due to the forecasters heterogeneity regarding their prior and/or prediction models. Laboratory experiments, such as surveys, allows us to directly elicit individual expectations, with the additional advantages of monitoring the information available to the subjects and using performance-based incentives. Within the experimental literature, LtFEs, introduced by Marimon et al. (1993), make it possible to study the formation of individual expectations within different expectations feedback systems, where the price depends on subjects’ predictions. By using this experimental framework, many experiments investigate the expectations of agents in financial markets (Hommes et al. 2005; Bao and Ding 2016), commodity markets (Bao et al. 2013) and aslo in a macroeconomic framework (Assenza et al. 2011; Cornand and M’baye 2016), to cite just a few contributions. In these articles, subjects have to predict prices within a 1 or 2 step-ahead horizon. One of the most pervasive results is that subjects show a certain degree of heterogeneity in their predictions, which seems to decrease over time. However, the reported level of heterogeneity is based on subjects’ short-run predictions only, which might distort the evaluation of their level of disagreement. In order to explain the disagreement among subjects’ predictions, the literature on LtFEs argued that subjects follow heterogeneous anchor-and-adjustment rules, anchoring their expectations to past prices.Footnote 1 The dynamics of prices depends crucially on the expectations feedback system, showing large oscillations around the fundamental value under a positive feedback system and a fast convergence to the fundamental value under a negative feedback system. We might think that the evolution of the level of disagreement among subjects over time and across multiple forecast horizons is directly affected by the price dynamics observed under the two feedback systems. Although there is an extensive macroeconomic literature that measures the term structure of forecasts using survey data to study the disagreement in expectations, the empirical characterization of the term structure of expectations in LtFEs is still missing. The aim of our paper is to investigate the impact of the expectations feedback system on the formation of long-run expectations as well as the empirical characterization of the term structure of subjects’ predictions. The term structure analysis allows us to have a more precise idea regarding the level of disagreement among subjects. In fact, if we do not consider a broader spectrum of expectations, we could lead us to underestimate or overestimate subjects’ disagreement. As an example, let us consider a scenario where subjects exhibit a strong coordination of their short-run forecasts together with an increasing dispersion of their long-run expectations. Measuring subjects’ disagreement as the variance of their short-run forecasts would lead us to underestimate the level of disagreement among subjects. On the contrary, we can imagine a situation where a higher dispersion of short-run forecasts, compared to the first scenario, remains constant or decreases over different forecast horizons (constant or decreasing term structure). In this case, we may overestimate subjects’ disagreement. We take the position that the term structure of expectations provides crucial information when it comes to characterizing the heterogeneity of expectations in LtFEs, and therefore long-run expectations cannot be ignored. We conduct a LtFE in which, unlike the standard settings,Footnote 2 subjects submit their predictions for periods ranging from 1 to more than 10-step-ahead time horizon. We elicit subjects’ long-run expectations at the beginning of every period, making it possible to revise their expectations as new information becomes available. In particular, we extend the novel setting introduced by Colasante et al. (2018a) and compare two different expectations feedback systems. As in Heemeijer et al. (2009), we compare two treatments: one with a positive feedback system and the other with a negative one. The positive feedback system mimics the behavior of financial markets where prices typically rise if investors expect positive changes. Conversely, the negative feedback system simulates commodity markets where, due to the delay in the production adjustment, market prices move in the opposite direction to expectations. Our results on coordination and convergence of short-run expectations, as well as on the evolution of prices, are in line with those reported in the LtFEs literature (see Hommes 2013). We propose a one-parameter term structure model to characterize the degree of disagreement of expectations in the two treatments. This characterization provides us with relevant information on how subjects form their long-run expectations. Extending the forecasting horizon reveals that subjects learn the REE in markets with negative feedback. We observe that the dispersion of long-run expectations around the fundamental price decreases over time to reach almost full convergence to the REE. By estimating the term structure of expectations, we demonstrate that mutual coordination of expectations and their convergence to the fundamental price follow the same pattern. In contrast, in the market with positive feedback, we find a persistently high level of disagreement among forecasters and it is compatible with the heterogeneous extrapolative rules employed by subjects when forming their expectations. The rest of the paper is organized as follows: Sect. 2 describes the experimental setting, while Sect. 3 introduces the theoretical framework and the working hypotheses. Section 4 presents the results of our empirical analysis, and finally, Sect. 5 summarizes the main findings and concludes.",3
14.0,3.0,Journal of Economic Interaction and Coordination,01 February 2019,https://link.springer.com/article/10.1007/s11403-019-00237-6,Exchange rate dynamics under limits of arbitrage and heterogeneous expectations,September 2019,Soumya Datta,,,Unknown,Unknown,Unknown,Unknown,,
14.0,3.0,Journal of Economic Interaction and Coordination,11 August 2018,https://link.springer.com/article/10.1007/s11403-018-0224-9,Frontier markets’ efficiency: mutual information and detrended fluctuation analyses,September 2019,Wahbeeah Mohti,Andreia Dionísio,Isabel Vieira,Unknown,Female,Female,Female,"In this study, we assess weak form efficiency in frontier countries’ stock markets using physics-based robust statistical techniques. Our econophysics analysis adds to current knowledge by focusing on markets that have been relatively disregarded by researchers. Although market efficiency has long been a major area of interest in finance, researchers have mainly examined more developed countries. Emerging and frontier markets have been assumed to be less efficient, the latter less than the former, mainly due to lower level of market depth and trade-reporting information (Gupta 2011). Nevertheless, in some of these markets, there has been improvement in the underlying market infrastructure (see Didier and Schmukler 2014) and assessments of the level of their financial efficiency provide new evidence that may be of interest for academics and for investors interested in enlarging the geographical dispersion of their portfolios. Frontier stock marketsFootnote 1 are relatively small, with few traders and low trading volumes, but the criteria to classify such markets are not fixed. Two of the more renowned frontier markets indices are produced by Standard and Poor (S&P) and by Morgan Stanley Capital International (MSCI). S&P defines these markets taking into account market turnover, number of listed stocks, attractiveness for foreign investors, and development prospects. In addition to market size and liquidity, MSCI also takes into account political stability and economic conditions. Frontier markets are often located in poor, politically mismanaged and economically fragile countries. These characteristics prevent them from attaining the status of emerging markets. However, considering solely such aspects to differentiate frontier markets may be misleading. For instance, Bangladesh and Pakistan are rapidly growing economies, and countries such as Estonia, Lithuania, Slovenia, Jordan, Mauritius, Bahrain and Croatia, have been improving their political stability and corruption levels.Footnote 2 Many frontier markets are considered as such mainly because of their modest economic dimension and of the small number of foreign owned companies listed in their stock exchanges (Quisenberry and Griffith 2010). Frontier markets are of potential interest from an international diversification perspective. Speidell and Krohne (2007) concluded that correlations between developed and frontier financial markets are low. Jayasuriya and Shambora (2009) studied the benefits of international diversification with frontier markets and suggested that investors could improve overall risk and return by adding six frontier markets to an international portfolio. Such benefits are confirmed by Quisenberry and Griffith (2010), who also found that cross correlations with developed markets are low, and that relatively high expected returns and low volatility could result from including frontier markets’ assets in a global portfolio. Berger et al. (2011) concur in highlighting the low level of international integration as an advantage for investors attempting to improve diversification benefits. Despite the potential advantages of including frontier markets in diversification considerations, few researchers have investigated their efficiency, an issue of fundamental relevance for international investment. In this study, we contribute to the literature by assessing weak form efficiency in a large set of frontier markets, using twenty three indices from different world regions. Our contribution is also methodological as we use techniques that do not require strong assumptions on the examined series’ normality, linearity or stationarity (Ferreira and Dionísio 2017). We examine dependence using Shannon’s (1948) Mutual Information Analysis (MIA) and Detrended Fluctuation Analysis (DFA), introduced by Peng et al. (1994). The former has the advantage of capturing linear and non-linear relationships, and the latter avoids spurious detection of long-range dependence in cases where the underlying data’s statistics, or dynamics, change with time. Our empirical analysis indicates that Slovenia displays the results more in line with those exhibited by more mature markets, usually classified as weak form efficient. The remainder of the study is organized as follows: Sect. 2 reviews recent empirical assessments of weak form efficiency and debates the relevance of methodological choices. Section 3 presents the study’s data and methodology. Section 4 describes the empirical analysis’ results, and Sect. 5 concludes.",12
14.0,3.0,Journal of Economic Interaction and Coordination,14 March 2019,https://link.springer.com/article/10.1007/s11403-019-00244-7,"Growth, unemployment and heterogeneity",September 2019,Piero Ferri,Annalisa Cristini,Anna Maria Variato,Male,Female,Female,Mix,,
14.0,3.0,Journal of Economic Interaction and Coordination,22 September 2018,https://link.springer.com/article/10.1007/s11403-018-0229-4,The economics of netting in financial networks,September 2019,Edoardo Gaffeo,Lucio Gobbi,Massimo Molinari,Male,Male,Male,Male,"This paper focuses on the application of on-balance-sheet (OBS) bilateral netting agreements in interbank networks. At odds with the typical offsetting of over-the-counter derivative transactions, an OBS netting scheme is a contractual arrangement that allows two financial institutions to offset the mutual credit/debt obligations that are booked on their balance sheets. This occurs through the signature of a novation agreement such that a single net amount is contractually substituted for the gross amounts they owed to each other or, more often, by stipulating a contract such that the former procedure is conditional on the default of the counterpart (close-out netting clause). Netting clauses are generally part of a so-called Master Agreement (MA), that is an arrangement reached amongst parties which defines the terms regulating one or more transactions, as well as the services characterizing the object of the contract. By suggesting the adoption within the financial community of typified schemes of MAs, sector associations seek to standardize behaviors and therefore abate uncertainty and transaction costs.Footnote 1 In addition to their potential in reducing credit and liquidity risks, close-out netting clauses tend to inhibit “cherry picking” strategies in the case of a counterpart’s default. In turn, the economic purpose of a novation netting agreement is even ampler, since it helps to shrink the management costs of maintaining multiple contracts—e.g., by diminishing the demand for costly hedging strategies—and to mitigate the operational risks at the deadline associated to inadequacies or failures of settlement procedures or technical systems. For the aforementioned reasons, practitioners and trade associations have long advocated for an extensive use of bilateral netting arrangements as a means to mitigate counterparty risks and improve the liquidity of financial markets (British Banking Association 2002; Mengle 2010). It is somehow surprising, therefore, that among the set of tools that regulators are envisaged to deploy to mitigate the cross-sectional dimension of systemic risk, the option to curb externalities by mandating some kind of netting between the cross-holdings of interbank deposits when a systemic crisis starts to unfold is virtually absent.Footnote 2 Sections 401–407 of the United States’ Federal Deposit Insurance Corporation Improvement Act of 1991, for instance, admit and recognize the benefits of netting the payment obligations that financial institutions hold on behalf of their clients, but remain silent on the admissibility and usefulness of netting the deposits that banks exchange among themselves on the Fed Funds market. In turn, the legal framework of the European Union guarantees a legal protection only to privately-signed interbank bilateral close-out netting agreements (Directive 2002/47/EC, as amended by Directive 2009/49/EC), but for specific provisions that give regulators the power to delay their enforcement in order to guarantee the well-ordered resolution of a distressed bank (Banking Recovery and Resolution Directive 2014/59/EU). The aim of this paper is to investigate whether a contingency-based mandatory enforcement of netting settlement contracts could control or even prevent default cascades in financial networks, as well as boost their systemic resilience in the run-up of a crisis. From this point of view, several research questions arise. In addition to more common tools like special resolution regimes, lender-of-last-resort facilities and debt restructuring schemes, should a regulatory framework for managing financial crises contemplate the possibility to force troubled institutions to net their interbank expositions? What pieces of information should authorities collect to maximize the likelihood of success? What kind of unwanted consequences, if any, should they take into account? In order to answer these questions, we first clarify the system-wide mechanics of OBS netting agreements in minimal-scale network topologies, in order to intuitively assess under which circumstances a policymaker mandating the offsetting of gross interbank exposures by means of OBS netting arrangements could succeed in maintaining financial stability. The variables we track in our comparative exercise are the aggregate stock of capital which is wiped out during a systemic contagion process, the number of defaults occurring through contagion, and the portion of the shock absorbed by deposits. We show that the enforcement of a comprehensive obligation to net interbank claims represents an effective crisis-management policy whenever the initial shock is big enough, while for smaller disturbances the consequences are mixed. Moreover, we find that the degree of heterogeneity of interbank loans plays a key role in driving these outcomes.
 We then offer results from a mean-field approximation and agent-based simulations performed within more complex structures, in order to provide a comparison between gross and netting settlement modes in more realistic scenarios regarding the number of banks, the network topology and the level of capitalization. Following a standard approach in the literature, we characterize different instances of the financial network by means of two key drivers—the connectedness probability and the number of nodes in the system—and we allow for the possibility that a contagion cascade could be triggered by a targeted shock wiping out a large portion of a bank’s external assets. In Erdös–Rényi networks, a mandatory OBS netting policy definitely dominates the gross settlement mode along the default and capital dimensions for different levels of leverage ratios, interbank market sizes, connectedness and degrees of heterogeneity in loans. As we move to core-periphery structures, however, we find that the magnitude of the triggering shock is a crucial variable in determining which arrangement dominates. In particular, while a strong dominance of OBS netting occurs for the default and capital metrics in almost all the cases tested whenever a big shock occurs, for smaller shocks the final output depends in a non-linear way on the network topology, the banks’ size, the depth of the interbank market and the leverage ratio. The metric relating to the portion of the shock absorbed by customer deposits displays a structurally better performance in the case of a gross settlement of interbank exposures for both the tested topologies. This is mainly due to the fact that bilateral netting, by reducing the amount of outstanding interbank debts, fosters the bank hit by the shock to re-distribute a greater share of its losses towards the creditor classes whose claims are not included in the netting agreement. Our investigation returns neat policy implications. As the risk of a systemic disruption looms large, the option to mandate a generalized novation of interbank gross exposures through OBS netting agreements should be made part of the set of crisis-management tools aimed at preserving financial stability, although its advantage depends on the magnitude and the dispersion of the shock igniting a turmoil, the topological architecture of the relationships linking banks, and their heterogeneity as regards balance-sheet sizes and equity buffers. The prompt availability of data on the scope, size and functioning of real-world financial networks is thus a key prerequisite for its deployment. Furthermore, since the offsetting of mutual interbank exposures tends to concentrate the effects of an external shock on the stakeholders of the buffeted bank, the presence of a blanket deposit insurance scheme (Laeven and Valencia 2012) is a compelled complementary policy measure that has to be deployed. The remainder of the paper is organized as follows. Section 2 presents a literature review. Section 3 illustrates with elementary examples the intuition at the basis of netting mutual exposures in interbank markets. Section 4 is devoted to a more detailed analysis by means a mean-field approximation and agent-based simulations in random networks. An extension to core-periphery topologies is offered in Sect. 5. Section 6 concludes.",5
14.0,3.0,Journal of Economic Interaction and Coordination,20 May 2019,https://link.springer.com/article/10.1007/s11403-019-00252-7,From constrained optimization to constrained dynamics: extending analogies between economics and mechanics,September 2019,Erhard Glötzl,Florentin Glötzl,Oliver Richters,Male,Male,Male,Male,"Modern economic models based on the principles of optimization under constraint and general equilibrium were inspired by the description of stationary states in mechanical models (Sect. 2). This paper argues that the dynamic theory of interacting ‘bodies’ under constraint provided by classical (Lagrangian) mechanics can be advantageous beyond general equilibrium analysis. We introduce a differential-algebraic modeling framework that extends existing analogies to mechanical systems from constrained optimization to general constrained dynamics in continuous time. The general constrained dynamics approach (Sect. 3) depicts the economy from the perspective of economic forces, economic constraint forces, and economic power. ‘Economic force’ is formalized as the desire of agents to change certain variables. The parameter ‘economic power’ corresponds to the reciprocal value of mass in the physical analogy and captures the agents’ ability to assert their interest to change the variables. Constraint forces ensure that system constraints are satisfied. We apply our framework by transforming a static Edgeworth box exchange model into a dynamic model (Sect. 4): optimization is replaced with procedural rationality described by economic forces exercised by the agents to improve their situation (gradient climbing). The price-adaptation process is changed from instantaneous to continuous. We study the convergence to the stationary state. The findings, caveats, and potential applications of the modeling approach are discussed in Sect. 5: The approach is designed to incorporate behavioral assumptions different from optimization and to relax restrictive macroscopic assumptions about aggregation of individual behavior. Economic power and the differentiation between ex-ante and ex-post dynamics are mathematically formalized. Slow price adaptation, out-of-equilibrium dynamics, and financial stocks and flows can be modeled consistently. The framework may be used to establish out-of-equilibrium foundations for general equilibrium models, whose solutions constitute fixed points of the dynamical system. Section 6 concludes.",4
14.0,3.0,Journal of Economic Interaction and Coordination,30 October 2018,https://link.springer.com/article/10.1007/s11403-018-0233-8,Fast traders and slow price adjustments: an artificial market with strategic interaction and transaction costs,September 2019,Danilo Liuzzi,Paolo Pellizzari,Marco Tolotti,Male,Male,Male,Male,"Financial markets aggregate the beliefs and trading decisions of a myriad of agents endowed with different objectives, strategies, information, and abilities. The stunning complexity of the outcomes is revealed in the nontrivial properties of financial returns that feature a set of intriguing and almost universal statistical properties known as stylized facts.Footnote 1 For a review, see Lux (2009) or the evergreen Cont (2001). In the last two decades, many models have to some extent been able to generate a handful of such regularities, suggesting possible sufficient drivers for the presence of stylized facts. In this paper, we explore a relatively new avenue and describe an artificial market where actions take place over two time scales: at the daily level, a market maker (or some entity/organization in charge of running the exchange) adjusts the price based on the excess demand for a risky asset and on some adjustable transaction cost (or tax). The closing price crucially depend also on a slowly varying market depth stochastic process, which can be thought of in terms of exogenous fluctuating (inverse) liquidity of the market. On the intra-day time scale, a large number of traders interact, crudely aiming at maximizing their short-term returns (net of transaction costs). Given the furious pace of this high-frequency trading, traders make their decisions using fast rules based on activation thresholds. These thresholds trigger sales or purchases that are contingent on a heterogenous individual signal and on an educated guess of the direction the price will take due to the decisions of all other agents. The market maker mechanically mediates fast trades and sets trading costs based on the prevailing liquidity level with no explicit or modelled objective. In contrast, the fast agents in the model strive to ideally find a local-in-time Nash equilibrium where everyone optimally buys/sells the asset given his/her noisy individual signal and actions of other traders. The result of this mechanism design is a sequence of frequent (discrete) adjustments of the intra-day price for the risky asset. In our opinion, this modeling assumption, which resembles the idea of frequent batch auctions recently discussed in Budish et al. (2015), is a simple but realistic representation of the market: high-frequency traders have some sophistication, use private signals, do not want to be outsmarted, and have to decide using fast rules in a sequence of best responses to other traders who quickly approximate a (local) pricing equilibrium. The resulting end-of-day price is finally determined by a market maker who takes into account the liquidity of the market once a day and sets the appropriate trading costs (or, in an alternative interpretation, levies a transaction tax). Our results are threefold. Firstly, the returns of our artificial market are endowed with fat tails, sizeable (excess) kurtosis, no linear predictability, and some volatility clustering. Another important result is the observation that this dual-scale mechanism, where strategic interaction happens at the intra-day level, can intensify and magnify liquidity shocks occurring over much longer (daily) time scales. Thirdly, we show that transaction costs have a non-trivial impact on the statistical properties of returns: while there is some potential to neutralize liquidity shocks, increased frictions appear to reduce volatility and activity, but generate spikes in prices and larger kurtosis in returns. Our paper takes into account the increasing awareness that (substantial) heterogeneity is needed in models of financial markets to obtain more realistic returns, see Lux (2009) and Kirman (1992) for a critical discussion of the representative agent approach. In our setup, traders’ decisions are based on heterogeneous thresholds as well as on strategic interaction with other agents, as pioneered in Granovetter (1978). Several papers in the last two decades have linked financial markets with dynamics arising from simple models where basic ‘particles’ influence each other at the individual level to produce interesting aggregates, often taking the limit for a large number of traders. While it may still be a challenge to provide a canonical microfoundation of such interactions, many results are of interest for financial economists and deserve wider recognition; see the survey provided in Lux (2016). Another stream of literature has influentially pointed out how a detailed microstructural representation of exchange may play an important role in obtaining realistic returns, or may even be responsible for the presence and intensity of several stylized facts and large jumps; see, for instance, Chiarella et al. (2009) and Maslov (2000)). The model presented in this paper differs from previous works in that many details of current intra-day markets are abstracted away to focus on the strategic search for a local pricing equilibrium. As such, we incorporate some form of herding effect and shared sentiment in the fast component of our model, as done in distinct ways in LeBaron and Yamamoto (2008) or Chiarella et al. (2017). In order to obtain our distributional results, however, there is no need to include a detailed implementation of a continuous double auction that would most likely reinforce our findings. Our model may also suggest that high-frequency trading can create lively price dynamics even if, as in our setup, returns are ultimately settled and smoothed by a conservative market maker on a daily basis; see Hasbrouck and Saar (2009, 2013) for insightful descriptions of how fast trading and technological innovations impact traditional market models. The paper is organized as follows. In Sect. 2, we define the model and its two basic components, namely how agents are involved in high-frequency trading. We also provide a description of the market maker who, on a daily basis, sets the closing price and adjusts the prevailing transaction costs, generally accounting for an exogenous liquidity measure. Section 3 is devoted to an analysis of returns: we outline the procedure used to determine the parameters of the model with the aim of matching some of the most significant stylized facts known in the literature; we then discuss the relationships between return/prices and liquidity in the market; we ultimately propose two different mechanisms for introducing a non-null transaction cost and describe the resulting impact on returns and market quality. Section 4 summarizes and draws up some closing remarks.",
14.0,3.0,Journal of Economic Interaction and Coordination,27 September 2018,https://link.springer.com/article/10.1007/s11403-018-0232-9,Degree-correlations in a bursting dynamic network model,September 2019,Fabio Vanni,Paolo Barucca,,Male,Male,Unknown,Male,"The conceptual framework of network formation can accurately describe complex systems which exhibit an intricate structure of evolving connections among the units. The common models of temporal network formation on random graphs in the literature Holme and Saramäki (2012, 2013) and Masuda and Lambiotte (2016) have focused on symmetric formation dynamics, where two random nodes have a certain chance to be connected through a particular attachment probability. These generative schemes are essentially based on the idea that a node can connect to another according to simultaneous bilateral agreement represented through a pairwise linking probability like in the Erdos–Renyi rationale and generalized random graphs known as hidden-variable (fitness models) (Caldarelli et al. 2002) or with a unit-wise connection pattern like in the cases of preferential attachment models (Albert and Barabási 2002). Among models of temporal networks, as a novelty in the literature of temporal generative models, the activity driven model (Perra et al. 2012) represents the basis for a standard model of temporal networks. In this model, link formation is driven by the activation of single nodes (according to their attributes) and a subsequent asymmetric linking function. Other research has stressed the importance of temporal models (Bardoscia et al. 2013; Ubaldi et al. 2016) for better representing dynamical structures which arise from non-static systems. A different line of theoretical research addresses models of strategic network formation, which has been receiving contributions in the economic literature. The game theoretic interaction has been formalized in terms of both bilateral and unilateral models of link formation, see Jackson (2010), Jackson and Wolinsky (1996) and Goyal (2012). In the present work we start from a dynamic network approach with a connection procedure where the network changes as individuals add and delete links over time according to a asymmetrically link formation with non-simultaneous procedure where a proposer node makes the attempt to create a link (according to its own preference) with another node which responds to the proposal (according to its own preference). In the simplest formulation, we first analyze a network formation where the proposal is immediately accepted. The model here studied is inherently dynamic and contributes to the growing body of research on temporal networks, which have been attracting considerable attention in recent years (Holme and Saramäki 2013; Masuda and Lambiotte 2016) for their ability to reproduce the complex dynamical patterns in real-world systems such as spontaneous degree correlations and bursty dynamics of the network, where the system spontaneously passes from states of high connectivity to states of low connectivity. Bursty patterns are, in fact, very commonly empirically observed in economics and financial activities (Karsai et al. 2018) and degree correlations are a fundamental feature of complex systems as the direct consequence of the interaction between heterogeneous agents (Kondor et al. 2014). We start from the model introduced by Liu et al. (2013), where the system dynamically evolves according to the preferences of each agent, the final network is a simple directed bipartite graph which, despite its simplicity, introduces a deep change in the network structure and its dynamics with emergent properties that arise for certain values of simple network parameters. In particular, the extreme introverts and extroverts model (XIE) has been shown (Bassler et al. 2015) to be characterized by a critical behavior beyond the Ehrenfest classification of phase transitions: the discontinuity displays an extreme Thouless effect. In such a condition we observe both a discontinuity in the phase transition plot of the order parameter and the appearance of anomalously large fluctuations in the link dynamics: features of first-order and second-order phase transitions respectively. Despite their recent introduction, non-simultaneous asymmetric linking mechanisms are in some cases more realistic than pairwise ones and this particular model is an attempt to represent unilateral links as alternative to pairwise random graph models, in the same way as one-sided strategic network formation has come up beside the two-sided models. For instance, in social networks like Twitter, the decision of following another user is strictly unilateral, and the ratio between followers and followed users, i.e., out-going and in-coming links, strongly characterizes the behavior and attitude of users (Grandjean 2016). Another interesting evidence for the presence of different agents’ attitudes which drive the dynamics of a system can be widely found in financial networks analysis (Barucca and Lillo 2015; Fricke 2012; Iori et al. 2007, 2008), where the various financial institutions in the interbank market can be divided into different communities related to their financial management. In recent years, financial systems have represented an important field of application for network science, which naturally captures the linkage architecture of the financial agents and their relations. Interest has increased even more as consequence of the recent financial crisis of the years 2007–2009, and a network approach is particularly crucial for assessing financial stability, since it is possible to have models and methodological tools which could describe, detect and eventually mitigate systemic risk (Battiston et al. 2016). There are many possible approaches and perspectives that one can use when studying financial systems and addressing financial network analysis, but, mainly, research has focused on the one hand on topological structures and the dynamics of network formation, and on the other hand on the dynamic processes occurring on the network. Here, we model network formation through a new mathematical description based on a master equation for the degree of the network. The solution we found provides both the transient non-equilibrium dynamics and the steady state of the degree distribution. At the same time we recover the crucial property of emergence and the behavior of a phase transition as the proportion between the two interacting groups varies (Bassler et al. 2015). We define the nature of complexity in the network model as the effect of temporal heterogeneity which arises from of the introduction of a proposer/responder rationale for the link formation (Lambert and Vanni 2018). Moreover, we investigate the limits of such approaches and shed light on a new important feature of such networks: the emergence of strong correlations among the agents when the system is at criticality, connected with the extreme Thouless effect already shown in the XIE model. We will discuss the onset of a spontaneous covariance between the two groups, which is remarkably intense at the phase transition critical point. We will study the meaning and the nature of such phenomena from the new perspective of temporal degree-degree correlations which leads us to define a temporal aggregate measure of assortativity other the common punctual one. As an additional important property of the model, we show how dynamical network interactions have the capacity to regulate and buffer unpredictable fluctuations so producing bursty events in terms of network connectivity. In conclusion, we propose a re-elaboration of the original XIE model of Bassler et al. (2015) and Liu et al. (2013), where we use a novel analytical formulation and a more detailed network analysis with a new interpretation, in particular, in terms of the degree correlations and the bursty behavior. However, those features are also present in the original model of XIE a similar analysis would have been performed. The paper is organized as follows: in Sect. 1 we introduce the network model and describe in detail its rationale and dynamics. We introduce the model dynamics in terms of a stochastic process, and analyze it through a master equation for the system according to an uncorrelated bipartite graph. In Sect. 2, then, using a mean field approximation, we write the rate equation for the network evolution so that it is possible to obtain a numerical solution of both the degree distribution and the phase transition plot. So, we outline the new coupled master equation dynamics (CMED) for the degree-distribution of generators and destroyers, explaining how it is possible to couple them. We present numerical results on the non-equilibrium dynamics, showing how CMED succeeds in predicting the full dynamical evolution of the degree-distributions in the model outside of criticality. In Sect. 3, we discuss degree correlations at criticality, estimating the intensity and the nature of such correlations. We detect the presence of strong degree correlations and we define a notion of aggregated assortativity seen as cumulative assortativity over a time window. Moreover we show the intrinsically dynamical nature of the network through the study of burstiness of the events among different states of the connectivity. In Sect. 4, we introduce a more realistic version of the agent-driven network based on the XIE model where intermittency is present in the degree trajectories. Moreover, we discuss financial and economic applications of the model. In Sect. 5, we discuss potential economic applications of the methodology developed in the paper both in terms of a financial agent-based model and in terms of an empirical study on international trade. Finally, in Sect. 6 we summarize results, stressing the need for a robust methodology to model degree correlations and their role in mixed phase transitions.",3
14.0,4.0,Journal of Economic Interaction and Coordination,24 August 2019,https://link.springer.com/article/10.1007/s11403-019-00261-6,The role of the world’s major steel markets in price spillover networks: an analysis based on complex network motifs,December 2019,Yanxin Liu,Huajiao Li,Yajie Qi,Unknown,Unknown,Unknown,Unknown,,
14.0,4.0,Journal of Economic Interaction and Coordination,13 September 2019,https://link.springer.com/article/10.1007/s11403-019-00267-0,Dissecting the myth of the house price in Chinese metropolises: allowing for behavioral heterogeneity among investors,December 2019,Ling Zhang,Wenlong Bian,Hao Zhang,,Unknown,,Mix,,
14.0,4.0,Journal of Economic Interaction and Coordination,06 March 2018,https://link.springer.com/article/10.1007/s11403-018-0215-x,Are the stock and real estate markets integrated in China?,December 2019,Chi-Wei Su,Xiao-Cui Yin,Hai-Gang Zhou,Unknown,,,Mix,,
14.0,4.0,Journal of Economic Interaction and Coordination,24 September 2019,https://link.springer.com/article/10.1007/s11403-019-00266-1,Crowdworking: working with or against the crowd?,December 2019,Georg Jäger,Laura S. Zilian,Manfred Füllsack,Male,Female,Male,Mix,,
14.0,4.0,Journal of Economic Interaction and Coordination,20 May 2019,https://link.springer.com/article/10.1007/s11403-019-00251-8,Contagion of network products in small-world networks,December 2019,Hüseyin İkizler,,,Male,Unknown,Unknown,Male,"The market offers new products every day, but most of them fail to reach mainstream consumers. According to Miller and Muir (2005), communication between consumers often determines the success or failure of a new brand. In many cases, heavy advertising campaigns are defeated by positive word of mouth. For instance, in 2000, Big Brother and Survivor were rival reality TV programs in England. While there was a massive advertising campaign for Survivor, the producers of Big Brother relied on word of mouth. After a slow start, Big Brother captured the most of audiences. The same is true for in the case of The Blair Witch Project and Godzilla. The success of the Blair Witch Project relative to Godzilla was achieved by internet rumors regarding the content of the film. A different example includes Palm Computing, whose product was able to diffuse throughout the mainstream market by focusing on a specific group of customers. On the contrary, Momenta Corporation tried to reach the mass market with a massive advertising campaign, but they failed to diffuse. Network product has a value that is contingent or partially dependent on the number of current users of the product (McIntyre and Chintakananda 2014). In our model, an agent gets value from the use of a network product contingent on the number of current users who are known by the agent. We consider a network product as a tool that supports the compatible connections between acquaintances. For instance, a file sharing application is a network product that provides fast and safe sharing between contacts. Agents choose a file sharing application that has a larger number of users known by them. The total number of users of a file sharing application is not that important. As for another motivation of this idea, a film, book, or TV program can be a network product. People want to talk about similar interests with others (Tauber 1972). Gilchrist and Sands (2016) propose that film consumption is partly driven by the preference for shared experience. Besides, agents can imitate others through social networks (Richerson and Boyd 1998; Babutsidze and Cowan 2014). So, to communicate with more friends, agents should allocate their time according to their friends’ interests. We formalize this observation in our model by assuming that if more than half of an agent’s friends read a particular book or watch a particular TV program, he will probably choose to read that book or watch that TV program.Footnote 1 It is natural to ask when a network product will diffuse with a higher probability in the market. As in the case of Palm Computing, marketers should focus on a specific group of customers at a time.Footnote 2 Besides, there is a chasm between the “early adopters” and “early majority”, and crossing this chasm brings the full diffusion of a network product with a higher probability (Moore 2002). The aim of this paper is twofold. First, we want to analyze what determines the success or failure of a new network product by focusing on the role of network structure. Second, we try to introduce a game theoretical model that can also explain the chasm between early adopters and early majority. In the theoretical part, we formulate a strategic normal form game in which the payoff of each agent depends on the actions of his neighbors. We study the equilibria of the network product adoption game and how these equilibria change with the given network. We focus on the equilibria that segregate agents into different types of adoption status: adopting (1) or non-adopting (0). Initially, the market offers a new network product. At each following period, an agent is arbitrarily chosen to consider adopting the new network product. The chosen agent adopts the new network product if more than half of his neighbors adopt the same network product; otherwise, he doesn’t change his adoption status. It is always equilibrium when all agents adopt the new network product, so we ignore such equilibria and focus on the structure of those where at least two parties have a different adoption status. We characterize the stochastically stable equilibria for complete networks and cycles. For an arbitrary network structure, we develop a novel graph decomposition method to characterize the set of recurrent communication states, which is a superset of stochastically stable equilibria of the network product adoption game. In the simulation part, we generalize the Morris (2000) Contagion model. In particular, we consider a set of agents communicating on an exogenously given undirected network. Each agent must choose one of two adoption statuses. For each agent i there exists a threshold probability \(p_{i}\) such that adopting a new network product is the best response for an agent if at least \(p_{i}\) of his neighbors adopted the same network product. To represent network structures we use Watts and Strogatz’s (1998) “small-world” model. This model allows us to analyze two extreme cases: highly cliquish networks and random networks. We calibrate the threshold probabilities for each agent using the parameters in Choi et al. (2010). The paper proceeds as follows: in Sect. 2, we review the related literature. We propose our theoretical model in Sect. 3. In Sect. 4, we discuss our theoretical results. We introduce our simulation model in Sect. 5. In Sect. 6, we discuss our simulation results. In the last section, we conclude with a discussion of the related literature.",3
14.0,4.0,Journal of Economic Interaction and Coordination,23 August 2018,https://link.springer.com/article/10.1007/s11403-018-0226-7,Agent-based modeling of systemic risk in the European banking sector,December 2019,Petr Teply,Tomas Klinger,,Male,Male,Unknown,Male,"Trends in increasing operational complexity, globalization, and organizational interdependencies have been observed in contemporary financial systems. Modern markets bring together a diverse group of stakeholders that form a rich network of interdependencies through a wide set of possible actions, such as cross-ownership. The broad use of financial products, such as collateralized debt obligations and exotic derivatives, further complicates the balance sheets and risk profiles of their users and adds to the overall organizational interconnectedness. During the past decades, international integration and changing regulations have added even more interconnectedness and have introduced concentrations of risk to the global financial system. In such an environment, systemic risk emerges as a key issue because the failure of an individual bank may impose significant costs on the entire system. As dramatically demonstrated during the global financial crisis of 2007–2009, the relevance of systemic risk was significantly underestimated. Before the crisis, financial regulators and central banks were mostly focused on ensuring the liquidity of individual banks, and the risk of contagion was, in general, considered to be low (Furfine 2003). However, the collapse of the Lehman Brothers and the need for bailout funds for American International Group (AIG) indicated that feedback elements in interconnected networks have the potential to amplify shocks to financial systems. These highly undesirable effects of systemic risk, interconnectedness, and shock propagation have drawn significant research interest from academic researchers (Bruha and Kocenda 2018; Black et al. 2016; Allen and Carletti 2013; Gai and Kapadia 2010; Haas and Horen 2012; Kvapilikova and Teply 2017; Upper 2011), international organizations and stability regulators (BCBS 2009; Borio 2011; Chui et al. 2010; Demekas 2015; Morrison et al. 2017), and policy makers (U.S. Congress 2010). The regulatory bodies responded to the crisis as well, such as by updating the Basel II recommendations; the revised version, Basel III, attempts to add robustness to the system by including mechanisms for increasing the resilience of banks to transient shocks (Basel Committee 2013). Additionally, the European Banking Authority (EBA) has performed stress tests of the EU banking sector (Basel Committee 2010). Despite the soft assumptions used in these tests, these results and the overall Basel III framework were subject to criticism (Sutorova and Teply 2014). The first network model-based research of systemic stability was performed by Allen and Gale (2000), who investigated the liquidity shock contagion. Another early study was carried out by Freixas et al. (2000), who analyzed banks with systemic importance and provided recommendations for central banks’ interventions. Cifuentes et al. (2005) and Shin (2008) added a market liquidity contagion channel to decrease the price of illiquid assets. Other studies analyzed systemic stability by simulation experiments on random networks under varying conditions, such as connectedness and exposure (Nier et al. 2007; Gai and Kapadia 2010; Georg 2013), risk diversification, innovation, and leverage (Devereux and Yetman 2010; Battiston et al. 2012; Caccioli et al. 2014; Corsi et al. 2013; van Wincop 2013). Löffler and Raupach (2016) examined the possible pitfalls in the use of return-based measures of systemic risk contributions. Regulatory requirements are investigated in Klinger and Teply (2014a) and Chan-Lau (2014), who studied the influence of capital buffers, bank solvency, and interconnectedness on system stability, as well as measures to contain the contagion during a crisis. Klinger and Teply (2016) added state aid to banks as a means of mitigating systemic crises. However, these works are theoretical rather than empirical, with exceptions such as Upper and Worms (2004), who focused on the German interbank market, or Van Lelyveld and Liedorp (2006), who analyzed the Dutch market. More recently, several realistic models of the global bank market were devised (Hale et al. 2011; Hale 2012; Montagna and Kok 2013; Gross and Kok 2013; Minoiu and Reyes 2013). Additionally, Nirei et al. (2015) calibrated the loan syndication networks model to broad market data. The limited empirical literature on systemic risk modeling is understandable because the simulation of the network structures is computationally very costly (Halaj and Kok 2013). Several studies concentrate on real-world interbank exposure modeling. For example, Boss et al. (2004), Upper and Worms (2004), Wells (2004), van Lelyveld and Liedorp (2006), and Muller (2006) analyzed the banking systems of Austria, Germany, the United Kingdom, the Netherlands, and Switzerland, respectively. Recently, Halaj and Kok (2013) attempted to approximate a network of banks that reported during the 2010 and 2011 EBA stress tests (EBA 2011). Finally, Craig and von Peter (2014) investigated the tiered structure of the real-world German banking network. However, most researchers face the problem of virtually non-existent reliable data on individual interbank exposures. This work combines theory and empirics in the model to calibrate it to the real-world data of the European banking network. Our approach builds on the probabilistic network model proposed by Gai et al. (2011) and the simulation models by Nier et al. (2007) and Klinger and Teply (2014a, b). We devise a realistic model of the European banking system based on the available data on interconnections among the banks. In this way, we would like to help bridge the aforementioned gap between theoretical insights and practical research based on current real-world data. We simulate the behavior of the European banking system when impacted by an adverse shock event, such as a bank default. Unlike in Klinger and Teply (2014a), who represented the banking system of each country, we take into account the multitude of banks in each country. We add further detail to the simulation as well by introducing the ability to model banks of various sizes. Banking networks within individual countries are modeled using real data, including market concentration, competition, and the relative power of large versus small banks, to represent the financial structures across countries as faithfully as possible. Put differently, the value-added of our research is the addition of bank heterogeneity to the modeling. The main goal of this research is to shed light on the real interconnectedness between nine Eurozone banking sectors and to estimate the levels of shock propagation in large-scale events, such as defaults of multiple banks, and in smaller events, such as the default of an individual bank. Our hope is that these findings might contribute useful information to EU policy makers, including the European Central Bank (ECB), when estimating the systemic effects of bank defaults. This remainder of this paper is organized as follows. Section 2 describes the network model, and agent-based shock modeling is covered in Sect. 3. System calibration to real data is reviewed in Sect. 4, and Sect. 5 presents the results. Section 5 concludes the paper and makes final remarks
.",12
14.0,4.0,Journal of Economic Interaction and Coordination,15 September 2018,https://link.springer.com/article/10.1007/s11403-018-0231-x,Modeling the impulse response complex network for studying the fluctuation transmission of price indices,December 2019,Qingru Sun,Xiangyun Gao,Ze Wang,Unknown,Unknown,,Mix,,
14.0,4.0,Journal of Economic Interaction and Coordination,09 September 2019,https://link.springer.com/article/10.1007/s11403-019-00264-3,Differences in the effects of seller-initiated versus buyer-initiated crowded trades in stock markets,December 2019,Liyun Zhou,Chunpeng Yang,,Unknown,Unknown,Unknown,Unknown,,
14.0,4.0,Journal of Economic Interaction and Coordination,15 July 2019,https://link.springer.com/article/10.1007/s11403-019-00257-2,Mis-measurement of inequality: a critical reflection and new insights,December 2019,Fabio Clementi,Mauro Gallegati,Joseph E. Stiglitz,Male,Male,Male,Male,"In recent times, inequality in the distribution of income and wealth has become a major issue and has spurred a renewed interest in both the political and the academic debate (Stiglitz 2012; Piketty 2014; Atkinson 2015; Stiglitz 2015). It is now a well-known fact that existing economic disparities, both between countries and individuals within a single country, tend to persist—if not worsen—with harmful effects on economic growth and increased risk of turning into a crisis (e.g. Cingano 2014; Ostry et al. 2014; Dabla-Norris et al. 2015; Berg and Ostry 2017; see also Rose 2018). These outcomes are in sharp contrast to the predictions of most economic theories, according to which there should be a tendency to convergence in average incomes between richer and poorer countries as well as a more equitable distribution of resources among the citizens of a country once full industrial development has been reached (Kuznets 1955). Concern with inequality has a long-standing tradition in economics, dating back to the early work of Pareto (1895, 1896, 1897a, b), who first observed that roughly 80% of total income/wealth is owned by the top 20% of the population. Later, the American economist Lorenz (1905) introduced the Lorenz curve, one of the most widely adopted tools for measuring the extent of income and/or wealth inequality. This curve shows how much the actual distribution of income or wealth varies from an equal distribution. If there is complete equality—people receive exactly the same amount of income or wealth—the Lorenz curve coincides with the diagonal of a unit square, whereas worsening distribution (i.e. more inequality) moves the curve away from the diagonal line (and vice-versa). Economists have also been resorting to various inequality measures for summarizing the degree of inequality in income and wealth distributions with a single number. Among them is the Gini coefficient, which has long been accepted as the workhorse measure of inequality. Named after the Italian statistician Gini, who first introduced it in 1914, this time-honored inequality metric is still widely used by scholars involved in the analysis of income and wealth distribution, as well as used in numerous technical reports from research-oriented organizations. Simplicity and ease of interpretation—thanks to its intuitive graphical relation to the Lorenz curve—as well as various extensions suggested later by several scholars, have certainly contributed to the popularity of the Gini index in economics and outside of it (e.g. Giorgi 1990, 1993, 2005, 2011; Giorgi and Gigliarano 2017; Giorgi and Gubbiotti 2017). Success and longevity of the Gini index have not, however, been without criticism. For instance, some of the arguments made over the years to abandon it are (Atkinson 1970): (i) the fact that the Gini index does not embed any functional linkage between inequality and social welfare; (ii) its habit of assigning more weight to transfer of income near the modal value of the distribution rather than at the tails; and (iii) its lack of additive decomposition by groups (within and between). However, while these criticisms were made in order to improve the Gini coefficient’s performance in the analysis of income and wealth inequality (Giorgi 1990, 1993, 2005, 2011; Giorgi and Gubbiotti 2017), the criticism will explore questions about the ability of the Gini index to be used to measure inequality at all. In particular, we will assert that the Gini only tells us about the degree of the concentration of transferable quantities,Footnote 1 it does not capture other key aspects of inequality, such as the degree of heterogeneity and asymmetry embodied in income and/or wealth distribution (Gallegati et al. 2016). On the contrary, studying the asymmetry of the Lorenz curve through an adequate measure allows us to account for all the aforementioned features. In 1914 Corrado Gini wrote: «The ratio that we are proposing in this note as the appropriate measure of concentration, can also be obtained by improving a graphical method already introduced by some authors, as Lorenz, Chatelain, Séailles, in order to evaluate inequality in the distribution of wealth. [...] The less unequal is the wealth distribution, the less accentuated is the concentration curve, that tends to a straight line (egalitarian line) in the case of equi-distribution » (Gini 1914, translation, p. 23). While referring to his famous concentration index, Gini traces the logic for its interpretation. In a non-axiomatic way, Gini’s logic moves from (the null hypothesis of) distributional inequality, that can be detected through the Lorenz curve, to conclude with the statement that “the more the distribution is unequal, the more it is concentrated”, where “unequal” means that the distribution presents an asymmetric shape revealing that few individuals hold almost all of the total amount of the transferable quantity (e.g. the “rich”) while the largest share of individuals (e.g. the “poor”) accumulates an amount that does not balance with that of the former. It is worth noting that Gini proposes his index as an appropriated measure of concentration that can be useful in measuring inequality, not the contrary. By contrast, a popular interpretation instead evaluates the degree of inequality, and can be summed up like this: “the more the distribution is concentrated, the more it is unequal”. That is precisely the contrary of the Gini’s logic. Such an alternative logic is intuitive enough but it does not come without drawbacks. It leads to a misinterpretation of the Gini index and introduces the severe methodological error of considering the Gini concentration index as the measure of inequality while, by Gini’s own admission, it appropriately measures one aspect of distributional inequality, concentration. This logical and methodological error is faulty conditioned by the fact that one often bears in mind inequality essentially as a matter of income or wealth distributions, and that the rich get richer due to income and high saving rates while the poor get poorer because of modest labor income and low saving rates. Also, this causes one to implicitly assume, without proof (i.e. axiomaticallyFootnote 2), that all the distributions that generate Gini index estimates are associated, under comparison, to Lorenz curves that exhibit concordant positive asymmetry profiles in favor of the rich side of the distribution. Finally, this often happens regardless of possible reciprocal intersections of the Lorenz curves, and nullifies the possibility of considering more unequal the distribution with higher concentration. If the asymmetry-concordance assumption were confirmed by the facts, then one might indulge in the luxury of trusting this other way of interpreting the data, but this kind of test is not always implemented and, in any case, drawing conclusions about inequality from concentration is an incorrect practice when Lorenz curves intersect, as almost always happens. As Gini told us, «This graphical approach presented two drawbacks, promptly acknowledged by Lorenz and by King: (a) it does not provide a precise measurement of concentration; (b) it does not allow to assess, not even in some circumstances, when or where concentration is stronger. In fact, if two curves cross each other (Fig. 2), it is not always possible to say if one denotes a stronger concentration than the other. This drawback, that can be deemed not relevant for the comparison of phenomena of the same nature (e.g. the concentration of incomes for two different years or countries), is particularly serious for comparing phenomena of different nature, whose distribution’s shape differs » (Gini 1914, translation, p. 24).Footnote 3 Unfortunately, and more often than not, one refers only to statistical estimates of the Gini index without examining the underlying Lorenz curve. As a consequence, implicitly (and perhaps unconsciously) one assumes that all the Lorenz curves of income distributions exhibit the same concordant positive asymmetry profiles and, even more heroically, that they, too, do not intersect. Based on this axiom, one then feels confident in measuring the intensity of inequality across income distributions by means of the Gini concentration index. Consistently with Gini’s logic, we explain that, at least in terms of statistical significance, if only one were sure that two Lorenz curves exhibit concordant asymmetry profiles (either positive or negative) then the more concentrated one may also be the more unequal, provided that the measure of inequality involved is not biased by the intersecting profiles of the curves. Said differently, if we have two income distributions with Lorenz curves that are similarly asymmetric (e.g. revealing commensurable long right-tails), then their shapes are almost similar with an asymmetry measure of the same sign (i.e. concordant). Provided that these Lorenz curves do not intersect, the one with higher concentration is also the most unequal, but if they intersect then concentration alone is not enough to state what is the curve associated to the most unequally distributed one: to this end, we need to jointly consider concentration and asymmetry in a single inequality measure that is able to account for both intensity and direction without being affected by the intersection of the Lorenz curves. In general, the axiomatic perspective of the alternative logic leads to the wrong use of the Gini index because it cannot control for the cases of intersecting Lorenz curves, nor can it account for the shape of asymmetry. To overcome these limitations, a non-axiomatic statistical point of view is needed: of course, income inequality is the most relevant but, statistically speaking, it is only a propaedeutic case. Therefore, an appropriate measure of distributional inequality must be general and suitable for the analysis of every transferable-quantity Lorenz curve. It must be able to account for discordant and concordant profiles of asymmetry of the Lorenz curves to consider the direction of inequality, and, to be sound in measuring its intensity for comparison purposes, it must not be biased by the fact that the Lorenz curves may intersect. Beyond the standard practice of using the Gini index, we prove that the Zanardi (1964, 1965) index of asymmetry of the Lorenz curveFootnote 4 is an appropriate measure of inequality as long as, among other desirable properties, it fulfils all the above-mentioned requirements. The paper makes two relevant contributions to the existing literature, again calling into question the criticality of the Gini index as a measure of inequality. First, it proves both in analytical and empirical terms that studies focusing solely on the Gini index might not consider crucial information that in turn could affect the validity of researchers’ conclusions and cause. Second, draws attention to the good properties of the Zanardi index, which are still little known among researchers. With the exception of the contributions of Tarsitano (1988) and Gallegati et al. (2016), to the best of the authors’ knowledge there are no other articles in English concerning the Zanardi index. In addition, because the Zanardi index was published in Italian, its scope had, and still has, a very limited scholarly reach. The paper is organized as follows. Section 2 provides an operational definition of inequality, discusses the asymmetry of the Lorenz curve, and sheds light on what we consider are the main drawbacks of the Gini index. Section 3 introduces the Zanardi (1964, 1965) index, which characterizes an important aspect of the shape of the Lorenz curve, namely asymmetry. Using data from the Luxembourg Income Study Database, Sect. 4 provides empirical estimates of both the Gini and the Zanardi indexes to show that the former is not enough to adequately measure inequality. Section 5 is the conclusion.",7
14.0,4.0,Journal of Economic Interaction and Coordination,17 October 2019,https://link.springer.com/article/10.1007/s11403-019-00269-y,Correction to: Mis-measurement of inequality: a critical reflection and new insights,December 2019,Fabio Clementi,Mauro Gallegati,Joseph E. Stiglitz,Male,Male,Male,Male,"In the original publication of the article, caption of Figure 3 on the third page of Sect. 4.1 was incorrectly published as (\(\circ,\;\mathcal {G}\)). It should read as (\(\delta ,\mathcal {G}\)). The original article has been corrected.",
15.0,1.0,Journal of Economic Interaction and Coordination,30 December 2019,https://link.springer.com/article/10.1007/s11403-019-00278-x,"Taming financial systemic risk: models, instruments and early warning indicators",January 2020,Gabriele Tedeschi,Fabio Caccioli,Maria Cristina Recchioni,Female,Male,Female,Mix,,
15.0,1.0,Journal of Economic Interaction and Coordination,04 October 2019,https://link.springer.com/article/10.1007/s11403-019-00268-z,Systemic financial risk indicators and securitised assets: an agent-based framework,January 2020,Andrea Mazzocchetti,Eliana Lauretta,Silvano Cincotti,Female,Female,Male,Mix,,
15.0,1.0,Journal of Economic Interaction and Coordination,21 November 2019,https://link.springer.com/article/10.1007/s11403-019-00272-3,An agent-based early warning indicator for financial market instability,January 2020,David Vidal-Tomás,Simone Alfarano,,Male,Female,Unknown,Mix,,
15.0,1.0,Journal of Economic Interaction and Coordination,24 October 2019,https://link.springer.com/article/10.1007/s11403-019-00270-5,"Financial sector bargaining power, aggregate growth and systemic risk",January 2020,Emanuele Ciola,,,Male,Unknown,Unknown,Male,"Since the seminal work of Arrow and Debreu (1954), the “growth of the financial industry has been considered a positive development by academics and regulators” (Beck et al. 2014, p. 2). In a complete market setting, economic agents can achieve a Pareto-optimal equilibrium, in which all states of the world are perfectly hedged through an efficient allocation of funds. Nevertheless, the presence of informational asymmetries and transaction costs prevents the system from reaching the Pareto optimality. Therefore, economic theory states that, by reducing the extent of those frictions, financial markets can move the economy closer to a complete market setting (see Levine 1997 for a review). For this reason, the growth of the financial industry has been welcomed by mainstream academics and policy-makers, resulting in them supporting and promoting its development.Footnote 1 Thus, it is not surprising that developed countries have seen a gradual expansion in both the size and influence of financial markets in recent decades. In advanced economies, credit to the private sector nearly doubled relative to gross domestic product (GDP) between 1980 and 2009, rising from 62% in 1980 to 118% in 2010 (Jordà et al. 2017). Moreover, most of the expansion in domestic debt occurred in the household sector. In the United States (US), credit to households grew from 48% of GDP in 1980 to 99% in 2007, most of which was through residential mortgages (Greenwood and Scharfstein 2013).Footnote 2 Finally, the distribution of firms’ income has also changed, in that there has been “(1) a slight shift in income towards capital; (2) a change in the composition of payments to capital that has increased the interest share; and (3) an increase in the financial sector’s share of total profits” (Palley 2013, p. 13). However, contrary to expectations, the rise of the financial industry was characterised by several drawbacks. First, the average growth rate of the economy has declined in developed countries during recent decades (Palley 2013; Jordà et al. 2017). In particular, the development of the financial sector coincided with a slowdown in firms’ investments in physical capital (see Davis (2017) for a review). Furthermore, recent research has provided empirical evidence of an inverted U-shaped relationship between the leverage of the economy and the growth rate of aggregate production, where it was found that, on average, an increase in private debt beyond 100% of GDP curtails economic growth instead of promoting it (Rioja and Valev 2004; Shen and Lee 2006; Cecchetti and Kharroubi 2012; Law and Singh 2014; Arcand et al. 2015; Cecchetti and Kharroubi 2015). Second, the development of financial markets has reduced the probability of economic crises, but at the cost of deeper recessions (Jordà et al. 2017). Moreover, as for the growth rate of aggregate production, scholars found an inverted U-shaped relationship between the leverage of the economy and the stability of the system. In this regard, when private debt exceeds 100% of GDP, the volatility of aggregate production rises instead of decreasing (Easterly et al. 2001; Dabla-Norris and Srivisal 2013). Similarly, Beck et al. (2014) found that the growth in the size of the financial sector, especially in non-intermediation activities, has undermined economic stability in advanced economies. Lastly, the rise of finance was also accompanied by a simultaneous increase in income and wealth inequality in developed countries (Piketty and Saez 2003; Stiglitz 2015). In particular, the shift of income towards capital depressed real wages in respect to labour productivity, with a negative impact on aggregate demand and economic equality (Orhangazi 2008; Palley 2013; Epstein 2015). Accordingly, the main aim of this article is to provide a different perspective on the rise of finance in developed countries, focusing on the impact of financial markets on aggregate growth and economic (in)stability. Specifically, we analyse the role of the bargaining power of financial intermediaries in promoting (or reducing) the entrance of new enterprises in the market. Indeed, as we will explain in Sect. 3, an increase in the bargaining power of financial institutions, raising the share of interests on firms’ income, can reduce the incentive for new companies to enter the market and indirectly decrease the development of new products and innovations, thus resulting in a negative impact on the growth of total factor productivity. Moreover, in Sect. 4, we show that only risk-prone entrepreneurs have the incentive to invest in new companies when financial institutions have strong contractual power. Consequently, a rise in the bargaining power of financial intermediaries increases firms’ leverage and, in turn, output volatility. The article is organised as follows. Section 2 provides a short review of the literature on the relationship between financial markets, aggregate growth and economic stability. In Sect. 3, we present a simple model of endogenous growth with expanding variety. Specifically, we show that the bargaining power of financial intermediates can influence the share of interests on firms’ income, thus affecting the entry of new enterprises in the market and, in turn, the growth of total factor productivity. In Sect. 4, we focus on systemic risk and show that financial institutions, affecting the weight of debt on firms’ liabilities, do not only influence the number of entrepreneurs entering the market but also their quality. Indeed, only risk-prone investors have the incentive to invest in highly leveraged enterprises. As a result, we find that the risk propensity of firms and the leverage of the system increase disproportionately with the bargaining power of banks. Finally, Sect. 5 concludes and outlines some directions for future research.",3
15.0,1.0,Journal of Economic Interaction and Coordination,06 December 2019,https://link.springer.com/article/10.1007/s11403-019-00276-z,Voluntary contributions in a system with uncertain returns: a case of systemic risk,January 2020,Annarita Colasante,Aurora García-Gallego,Andrea Morone,Female,Female,Female,Female,"Often, when we refer to an economic system, we consider a market situation with its demand and supply sides mostly inspired by the traditional view of non-cooperative interaction between sellers and buyers. In a recent paper, Barreda-Tarrazona et al. (2018) show that such a market system can also be viewed as a social dilemma in which socially better outcomes than the non-cooperative equilibrium can be achieved if buyers and sellers behave in an altruistic way. For example, selling at a low price or producing enough to fully satisfy the demand can be seen as actions of altruism. More straightforward examples can be sought in the provision of public goods with resources obtained from the collection of taxes. In such a context, the literature on voluntary contributions to public goods becomes relevant also in the general context of economic systems. In this paper, we address the emergence of systemic risk in an economic system providing a public good which entails three different but interrelated phenomena: i. uncertainty on the individual returns from the public good, ii. strategic uncertainty due to ”others’ behavior,” and iii. heterogeneity in the agents’ attitudes toward risk. Imagine for example, a population of taxpayers considering whether to evade or fully comply with their fiscal obligations. If in return to their taxes, the state provides a health service of a random or fluctuating quality, depending, say, on the quality of hospitals or doctors available, the citizens’ decisions regarding tax compliance may be negatively affected. At the same time, whether other citizens have complied or not with their obligations adds a further source of (strategic) uncertainty regarding the final quality of the health service received. Finally, both sources of uncertainty will be filtered through the taxpayers’ risk attitudes and will, eventually, create the heterogeneously perceived systemic risk, which will lead to a variety of individual decisions and, thus, to a variety of potential final outcomes at an aggregate level. Our analysis illustrates the emergence of systemic risk in a rather broad family of cases in which the interplay between uncertain returns to individual actions and uncertainty on others’ behavior is filtered through individual attitudes toward risk, leading not only to collectively worse outcomes but, in the extreme, even to the collapse of the system. After the 2007–2008 financial crisis, systemic risk has been part of a rapidly growing research agenda. As defined in Kaufman and Scott (2003), systemic risk refers to the probability of collapse of an entire system due to the correlation among most of the parts of the system itself. The majority of references in this field identify the interconnections among banks and institutions (Berardi and Tedeschi 2017; Acemoglu et al. 2015; Freixas et al. 2000; Recchioni and Tedeschi 2017) as the main systemic risk determinants. From a different perspective, Schwarcz (2008) provides an alternative definition for the same term: “systemic risk results from a type of tragedy of the commons in which market participants lack sufficient incentive, absent regulation, to limit risk-taking in order to reduce the systemic danger to others.” Always in a financial environment, Masciandaro and Passarelli (2013) describe financial systemic risk as a pollution issue, as a financial risk externality. They point out that “systemic risk is a peculiar case of externality resulting from contagion effects,” and that “any single financial portfolio produces a certain amount of systemic risk pollution, even an extremely small one.” Therefore, since free-riding leads to excess risk production, systemic risk can be defined as a negative externality. The main aspect of a public good provision is that everyone in the society benefits from the public investment, independently of whether the beneficiary contributed at all. As a matter of fact, the public investment generates returns that normally are heterogenously exploited and that are independent of the contribution level. In order to build up policies aimed at sustaining the provision of public goods, we investigate how uncertain and heterogeneous returns from public goods affect the decision of whether and how much to contribute to a common project. The returns from public investments are determined by two main factors. Namely, the total amount contributed (i.e., the taxes paid), and the allocation of those contributions. If a majority of citizens prefer not to invest in the public project, a collapse of the system may occur and, consequently, public services may not be provided. Related to exactly this aspect of tax undercontribution, several experimental investigations (Listokin and Schizer 2013; Doerrenberg 2015) have shown that the policy of introducing earmarking in public expenditure reduces the subjects’ willingness to evade taxes. Strictly speaking, the earmarking increases the transparency in the allocation of public contributions and, therefore, it reduces the uncertainty of future returns. As a result, earmarking encourages subjects to contribute more to public investments and, therefore, reduces the risk of collapse of the system. In the standard setting of a public good game (PGG, henceforth), players simultaneously decide how much to contribute to the public good and, as a result, they face the natural uncertainty due to uncertainty on others’ behavior (Messick et al. 1988) at the moment of deciding whether and how much to contribute. Furthermore, previous related research like Fischbacher and Gächter (2010) and Morone and Temerario (2018) remarks how each subject’s preferences are characterized by a different willingness to cooperate. Those papers find subjects behaving as free-riders, subjects that are altruistic and subjects that act as conditional cooperators. Players in a PGG are part of a randomly determined group in which different kinds of people are matched, and where each individual has no prior information about the preferences of others in the group. Moreover, players decide simultaneously about their individual contribution to the public good. In other words, players decide under imperfect and uncertain information. When players play in a repeated and partners setup, uncertainty may be reduced since subjects try to learn about others’ preferences and future behavior by observing the decisions taken in previous periods. Seminal works by Andreoni (1988) and Andreoni and Croson (2008) remark that the explanation behind the usual pattern of decaying contributions is a combination of learning to play the dominant strategy and strategic play by self-interested players.Footnote 1 Research conducted by Neugebauer et al. (2009) sheds light on these results. In their experiment, authors elicit beliefs about others’ contributions and compare results from the ”Info” treatment—where the individual contribution of each member is shown at the end of each period—and the ”NoInfo” treament in which subjects only receive feedback about the aggregate value of the public good. The decline in contribution is observed solely in the ”Info” treatment, causing also correlation between beliefs and individual investment to rise. This may be interpreted according to the conditional cooperation strategy. More specific to strategies used in a PGG, Fischbacher et al. (2001) find that around 50% of the subjects show conditional behavior such that the own contribution increases in the other group members’ average contribution.Footnote 2 Furthermore, when allowed, being a conditional cooperator may imply punishing members of the group that free-ride in the previous period (Fehr and Gächter 2002). It is reasonable to assume that in a PGG with uncertainty the proportion of free-riders would increase, being this fact by itself a source of systemic risk. In fact, one’s unwillingness to contribute propagates through the conditional cooperation strategy, likely ending up with the failure in the provision of the public goods, presumably leading to a collapse of the whole system. We aim at carrying out the analysis of systemic risk under the same perspective of Schwarcz (2008), as a problem of failure in the provision of public goods due to the lack of incentives provided by the uncertain marginal per capita return (MPCR, henceforth). Indeed, when uncertainty increases due to the fact that subjects are uncertain about the marginal return of their contributions, they may prefer not to invest at all in the common project. Based on this reasoning, we draw our first research question: RQ1 The number of free-riders is expected to increase with uncertainty. In order to test RQ1, we propose a public good experiment with uncertain individual returns. In our setting, subjects do not know their MPCR when deciding their contribution level in each period. Three possible realizations of the MPCR are considered in our experiment, each randomly allocated within the 3-member-group. Each MPCR realization is equally likely, which implies that ties are possible. In order to test whether different degrees of uncertainty affect subjects’ contribution levels, we run three different treatments. First, a baseline (BL) in which the MPCR takes the same value for all subjects and this is common knowledge; therefore, subjects are certain about their MPCR. Second, a treatment with high risk (HR) and, third, a low risk (LR) treatment. The main difference between the treatments with risk, HR and LR, is the variance between the MPCR values. This aspect of our design is explained in detail in Sect. 3. In this environment with induced uncertainty, the contribution level is expected to be affected by the individuals’ risk preferences. Budescu et al. (1990) as well as Sabater-Grande and Georgantzis (2002) find that risk aversion negatively affects cooperation. Based on this evidence, we formulate our second research question: RQ2 Subjects that are more risk averse contribute significantly less to the public good, and this effect is intensified with risky returns. Our results partially confirm the conjecture that uncertain future returns have a significant negative effect on individual contributions. Moreover, this negative effect is stronger in the treatment with higher risk. In general terms, compared with the non-risky treatment, we find that the uncertainty on the individual degrees of appropriation of the public good leads to lower levels of individual involvement in the social project. Furthermore, evidence is found from a source of systemic risk in this context that emerges from the interplay between the aforementioned uncertain returns and individual risk attitudes. Specifically, the individuals’ risk aversion degree enhances the system to collapse due to lower individual contributions under return uncertainty. The rest of the paper is structured as follows. In Sect. 2, we review the relevant experimental literature on PGG with uncertainty. Section 3 is dedicated to the experimental design and procedures. The theoretical framework and predictions are developed in Sect. 4. The analysis of our experimental data is detailed in Sect. 5. Section 6 concludes. “Appendix A” includes the instructions (translated from Spanish) given to the participants at the beginning of each session.",2
15.0,1.0,Journal of Economic Interaction and Coordination,07 December 2019,https://link.springer.com/article/10.1007/s11403-019-00275-0,From FDI network topology to macroeconomic instability,January 2020,Giulia De Masi,Giorgio Ricchiuti,,Female,Male,Unknown,Mix,,
15.0,1.0,Journal of Economic Interaction and Coordination,02 April 2019,https://link.springer.com/article/10.1007/s11403-019-00247-4,Networks and market-based measures of systemic risk: the European banking system in the aftermath of the financial crisis,January 2020,Gian Paolo Clemente,Rosanna Grassi,Chiara Pederzoli,Male,Female,Female,Mix,,
15.0,1.0,Journal of Economic Interaction and Coordination,21 December 2019,https://link.springer.com/article/10.1007/s11403-019-00277-y,Systemic risk governance in a dynamical model of a banking system with stochastic assets and liabilities,January 2020,Lorella Fatone,Francesca Mariani,,Female,Female,Unknown,Female,"The notion of systemic risk refers to the risk of collapse of an entire system rather than simply the failure of its individual parts. Systemic risk and systemic risk governance are therefore important research topics that have applications in many different contexts, such as physics, biology, engineering, and finance. We limit our attention to the modeling of systemic risk in banking systems. In this case, systemic risk refers to the collapse of banking systems due to the banks’ default and it depends on the banks’ interconnections. Because of the complexity of the connections between banks, systemic risk is often unpredictable and unmeasurable and the construction of a safe banking system capable of measuring and governing systemic risk plays a key role in systemic risk research. For a survey of the various aspects of systemic risk and the use of mathematical models in studying systemic risk, we refer to Fouque and Langsam (2013), Hurd (2016) and the references therein. Models found in the literature often analyze banking activities and systemic risk under static (time-independent) or, more often, discrete-time models (see e.g., Haldane and May 2011; May and Nimalan 2010; Iori et al. 2006; Grilli et al. 2014, 2017; Tedeschi et al. 2012; Lenzu and Tedeschi 2012; Caccioli et al. 2012, 2014; Berardi and Tedeschi 2017). Among others, Berardi and Tedeschi (2017), Grilli et al. (2014), Tedeschi et al. (2012), and Lenzu and Tedeschi (2012) develop agent-based interbank networks where the banks are interconnected through credit relationships. Iori et al. (2006) construct a systemic risk contagion model based on the interbank network, and Caccioli et al. (2014) model the banking system as a bipartite network where the two groups are represented by the banks and the assets. The widespread use of discrete-time models in describing the interbank dynamics is due to the fact that these models usually allow several microeconomic variables to be included that are useful for a realistic description of a banking system. Moreover, static or discrete-time models are mathematically more tractable than continuous models. As banking practices become more intense and seamless, it is natural to think of approximating banking activities using continuous-time models instead of discrete-time models, even if, from the mathematical point of view, they are usually more demanding. As can be seen in detail in the following, some tentative steps in this direction can be found in Fouque and Ichiba (2013),  Fouque and Sun (2013), Carmona et al. (2015), Mukuddem-Petersen and Petersen (2006, 2008),  Sun (2018), Biagini et al. (2019a), Fatone and Mariani (2019). In this context, it is also desirable to rigorously define the systemic risk of a banking system. In fact, especially when continuous-time models are considered, there is not a general consensus on a unique definition of systemic risk (Fouque and Langsam 2013; Hurd 2016). The banking system model we present in this paper is situated in the framework of mean field theory. This theory, initially introduced in statistical mechanics (see e.g., Gallavotti 1999), has recently been successfully applied to the study of financial models where, in contrast to the discrete-time models mentioned above, the dynamic evolution is studied by means of a system of interacting stochastic differential equations (see among the others, Fouque and Ichiba 2013; Fouque and Sun 2013; Carmona et al. 2015; Garnier et al. 2013; Sun 2018; Biagini et al. 2019a; Fatone and Mariani 2019). In particular, under suitable assumptions, the mean field theory allows a possibly high-dimensional dynamical model (e.g., a banking system model) to be governed via a control law determined using a low-dimensional dynamical model (e.g., the mean field approximation of the banking system model). The financial dynamical models proposed in this context typically consist of an initial value problem for a system of stochastic differential equations, usually diffusion equations, whose dependent variables represent, for example, monetary reserves, wealth or other more general indicators of the health of financial institutions. These diffusion equations are usually tied together through a term in the drift that implies the network structure. The banking system model studied in this paper generalizes those presented in Fouque and Sun (2013), Carmona et al. (2015), Fatone and Mariani (2019) and exploits some ideas taken from Haldane and May (2011), May and Nimalan (2010),  Mukuddem-Petersen and Petersen (2006, 2008). In Fouque and Sun (2013),  Carmona et al. (2015), the dependent variables of the stochastic differential equations that define the model are the log-monetary reserves of the banks as functions of time and there is a cooperation mechanism that regulates borrowing and lending activities among banks. Moreover, the probability of systemic risk in a bounded time interval is studied using the mean field approximation and the theory of large deviations. The model presented in Fatone and Mariani (2019) generalizes those presented in  Fouque and Sun (2013), Carmona et al. (2015). In particular, in Fatone and Mariani (2019) a model with two cooperation mechanisms is studied. The first cooperation mechanism regulates the borrowing and lending activities among banks, while the second describes the borrowing and lending activities between the banks and the monetary authority. Furthermore, a technique for governing the probability of systemic risk in a bounded time interval is introduced and studied. In Haldane and May (2011), May and Nimalan (2010), Mukuddem-Petersen and Petersen (2006, 2008), assets–liabilities models of banking systems are presented, in which each bank is modeled by its assets and liabilities. Time-independent (static) Haldane and May (2011),  May and Nimalan (2010) and time-dependent (dynamic) Mukuddem-Petersen and Petersen (2006, 2008) assets–liabilities banking system models have been studied. In Mukuddem-Petersen and Petersen (2006, 2008), banks’ assets and liabilities are further decomposed as the sum of more specific addenda; the time dynamics of each addendum is specified. Finally in Haldane and May (2011),  May and Nimalan (2010) the analogies between systemic risk in banking systems and systemic risk in several other domains of science and engineering are explored. In detail, this paper is concerned with measuring, monitoring and governing systemic risk in an assets–liabilities continuous-time dynamical model of a banking system. In other words, we consider a continuous-time dynamical model of a banking system where each bank holds assets and has liabilities that are stochastic processes in time. The assets and liabilities of each bank are defined implicitly as functions of time by an initial value problem for a system of stochastic differential equations. The net worth of a bank is defined as the difference between the bank’s assets and liabilities and a bank is solvent when its net worth is greater than or equal to zero; otherwise, the bank has failed. A political/technical authority is responsible for managing the banking system and, in particular, for governing systemic risk. For convenience we refer to this authority as the monetary authority. The model proposed describes a homogeneous population of banks where each bank interacts with the other banks and with the monetary authority. The homogeneity of the bank population implies that all banks in the model behave in the same way. From an economic point of view, the homogeneity of the bank population can be seen, for example, as “perfect” herding behavior occurring when the banks try to “perfectly” copy each other’s behavior. For theoretical explanations of the motivations behind bank herding behavior, we refer to Acharya and Yorulmazer (2008) and the references therein. Note that the assumption of bank homogeneity makes it possible to successfully apply mean field theory. The main features of the model are a cooperation mechanism among banks that regulates interbank borrowing and lending activities and the possibility of the monetary authority’s (direct) intervention in the banking system dynamics. The cooperation mechanism is based on the idea that “those who have more (assets, liabilities) give to those who have less (assets, liabilities)”. The monetary authority’s intervention in the banking system dynamics consists of the choice of two functions representing, respectively, the assets and liabilities of a kind of “ideal bank” as functions of time and the choice of the rules controlling the cooperation mechanism among the banks. It is worthwhile noting that the model proposed here is a deliberate over-simplification of interbank borrowing and lending activities. Banking systems are, in reality, very complex and diverse dynamic systems and the hypothesis of a uniform bank population initially proposed in Fouque and Sun (2013) and used here is a very strong hypothesis that is far from being true in real banking systems. As well, the assumption of banks that lend to and borrow from each other in the spirit of “those who have more (assets, liabilities) give to those who have less (assets, liabilities)” is simply a game feature introduced to model lending and borrowing among banks with the final scope of better understanding the banking system and systemic risk. This assumption was initially introduced in Fouque and Sun (2013) but has been commonly accepted and used by several authors (see for example, Fouque and Ichiba 2013; Carmona et al. 2015; Fatone and Mariani 2019; more recently, Sun 2018; Biagini et al. 2019a) in order to have a banking system model that is simple enough for mathematical analysis, yet captures how banks’ lending preferences affect possible multiple bank failures. Introducing incentives for lending and borrowing would make the model more realistic. Some attempts in this direction can be found in Carmona et al. (2015), Rogers and Veraart (2013), Eisert and Eufingerbi (2014) and may serve as a topic of future research. From an economic standpoint cooperation among banks can be seen, for example, as an “extreme” risk-sharing mechanism. This is the case when independent banks, having significant exposure to the same assets, begin cooperating to protect themselves from the decrease in assets values which could prejudice their interests and can lead to a crisis in the entire banking system. Finally, the hypothesis implied by the borrowing and lending mechanism adopted in these models, i.e., the potentially unlimited possibility of borrowing and lending money among banks and between banks and the monetary authority, is a very strong assumption. However, these simplifications are balanced by the fact that, in the simplified models considered, a pseudo mean field limit of the system can be easily computed and used to study systemic risk governance via an ad hoc optimal control problem. Moreover, understanding the simplified models can be seen as a preliminary step in the study of more realistic banking system models. As emphasized in many papers dealing with discrete-time models of banking systems (see for instance, Berardi and Tedeschi 2017; Caccioli et al. 2014; Grilli et al. 2014, 2017; Iori et al. 2006; Lenzu and Tedeschi 2012; Tedeschi et al. 2012), heterogeneity influences the resilience and stability of a banking system, thereby determining the level of systemic risk. Bank heterogeneity concerns several aspects, for example, bank size, investment opportunities (Iori et al. 2006), balance-sheet distribution (Berardi and Tedeschi 2017; Iori et al. 2006), and topology of the banking network (Berardi and Tedeschi 2017; Caccioli et al. 2012; Tedeschi et al. 2012). All these aspects affect systemic risk in a banking system in different ways. The mechanism of shock propagation among financial institutions and its relation to systemic risk is also usually studied using static or discrete-time models. We have focused our attention on the models proposed in May and Nimalan (2010), Haldane and May (2011) since the authors investigate shock propagation in a static assets–liabilities model of a banking system using a mean field approach. In these papers, the effect of exogenous shocks on the individual bank is modeled as a sudden reduction in assets value. It would be interesting to investigate how the above-mentioned properties are reflected when dealing with continuous-time dynamical models of banking systems. A first step in this direction is to analyze simple continuous-time dynamical models (like the one presented here) and then extend this study to more general, complex, diverse and realistic dynamical models. In order to do this, at least from a theoretical viewpoint, we explain the details of the banking system model studied in this paper and its potential generalizations. We consider N banks that lend to and borrow from each other. These banks form the nodes of a network representing the banking system. From a mathematical point of view, the network is a graph of order N, i.e., a collection of N nodes with links among them. The links denote the presence of a cooperative relationship among the nodes and each link has a capacity indicating the “intensity” of the cooperation mechanism between the linked nodes. Various assumptions about the structure of the financial network, and therefore about the topology of the graph, can be made. These assumptions hold important consequences for the study of systemic risk associated with the financial network. In the simplest models, such as the one considered here and in Fouque and Sun (2013), Fatone and Mariani (2019), the bank population is homogeneous and any one of the N banks is linked in the same way to all the other banks as a lender and borrower, that is, all the links have the same capacity and the graph associated with the financial network is the clique. In particular, this means that all banks are copies of a “unique bank”. More refined and realistic banking system models can be considered. For example, the banks in the model can be linked to a central bank (see e.g., Carmona et al. 2015) and they may or may not be linked among themselves. In the latter case, the graph corresponding to the financial network is a star graph. The banking system model can be generalized to consider banks of different sizes, for example, a situation characterized by the presence of big banks and small banks or big banks, medium banks and small banks. For instance, in May et al. (2008) it is shown that the topology of the USA Fedwire system, composed of some 9500 participating banks, is “highly non-random in a dissociative way”, that is, there are a few big banks and each big bank is connected to many small banks. Moreover, the small banks are connected to only a few other banks, which are mainly the big banks. More realistic models for these networks could be obtained, for example, by introducing assets and liabilities accounting, incentives for borrowing and lending, and upper bounds to the borrowing and lending activities among banks. More generally, models that are networks of networks are sometime used to study real banking systems. The study of some of these more realistic models shows that, under some suitable assumptions, it is possible to reduce the default risk for each type of bank present in the system (i.e., big banks, medium banks, small banks), but the price of this reduction is an increase in the default of the entire system (see e.g., Haldane and May 2011). As a consequence, systemic risk must be governed. In these more complex models, the original mean field approximation cannot be used. In fact, different types of banks do not have the same “average behavior”. One possibility would be to aggregate information deduced from the behavior of the mean field approximation of each type of bank present in the model according to the effective relationship between the banks. In this way, the resulting approximation could be used to govern the systemic risk associated with the original banking system. For example, the model studied in this paper could be thought of as a rough representation of the “big bank” system of a given financial network. A possible extension is a model characterized by the presence of “big banks and small banks” (and in general a model characterized by the presence of banks of different types). In this case, two mean field approximations can be considered (one for the big banks and the other for the small banks) and these two approximations can be coupled in a dynamical system that describes the qualitative interaction among the different types of banks in the model. The feasibility and effectiveness of this approach depend on the specific model under investigation, and, in particular, on the graph that expresses the interaction among the banks. For an exhaustive review on systemic risk in the context of financial networks, including the study of potential default cascades due to various contagion effects, we refer to Hurd (2016), Fouque and Langsam (2013),  Biagini et al. (2019a). Concerning the modeling of shocks, we generalize the shock propagation mechanism adopted in Haldane and May (2011), May and Nimalan (2010) to continuous-time models. In the banking system model proposed, realistic situations of banking distress due to the deterioration in the quality of banks’ assets and/or liabilities can be modeled. Shocks that hit the banking system are simulated with jumps in the volatilities of the stochastic differential equations satisfied by the banks’ assets and liabilities and with jumps in the correlation coefficients of the stochastic differentials of the diffusion terms that appear on the right-hand side of the assets and liabilities equations. We use “systemic risk” or “systemic event” in a bounded time interval to refer to the fact that in that time interval at least a given fraction of the banks in the model fails. Given a banking system model, we use statistical simulation to evaluate the probability of systemic risk in a bounded time interval. The action of the cooperation mechanism among banks reduces the default probability of the individual bank at the expense of an increase in the default probability of all or almost all the banks in the banking system. The latter case is called “extreme” systemic risk. When the number of banks in the model goes to infinity, a heuristic approximation of the banking system model called “pseudo mean field approximation” is introduced. This approximation is inspired by the mean field approximation of statistical mechanics (see e.g., Gallavotti 1999) and is based on the homogeneity of the bank population. The pseudo mean field approximation is a stochastic dynamical system with two degrees of freedom. We present a method to govern the probability of systemic risk in a bounded time interval. The goal of governance is to keep the probability of systemic risk in a bounded time interval between two given thresholds. Governance exploits the choice made by the monetary authority for the assets and liabilities of a kind of “ideal bank” as functions of time and the solution of a stochastic optimal control problem for the pseudo mean field approximation of the banking system model. In fact, in a homogeneous bank population when there are enough banks, all banks behave like a sort of “mean bank”, the behavior of which is approximated with the behavior of the pseudo mean field approximation of the banking system model. This behavior is forced to be similar to the behavior of the “ideal bank” by solving a stochastic optimal control problem for the pseudo mean field approximation of the banking system model. With a homogeneous bank population, the governance of the pseudo mean field approximation is easily translated into the governance of the entire bank population. More specifically, it is translated into the rules for the cooperation mechanism among banks. In this way, systemic risk governance induces the individual banks to behave like the ideal bank. Shocks in the banks’ assets and liabilities are simulated and numerical examples of systemic risk governance in the presence and absence of shocks are presented. The paper is organized as follows. In Sect. 2, an assets–liabilities banking system model is defined. In Sect. 3, the definition of systemic risk in a bounded time interval is given and the implications of the presence of the cooperation mechanism and homogeneity of the bank population for the systemic risk probability are investigated. In Sect. 4, the mean field and pseudo mean field approximations of the banking system model defined in Sect. 2 are discussed. In Sect. 5, an optimal control problem for the pseudo mean field approximation of the banking system model is solved and the optimal control that is found is translated into the rules that determine the behavior of the cooperation mechanism among banks. Finally in Sect. 7, a method to govern systemic risk in a bounded time interval is presented and some numerical examples of systemic risk governance of banking systems in the presence and absence of shocks are discussed.",4
15.0,1.0,Journal of Economic Interaction and Coordination,18 June 2019,https://link.springer.com/article/10.1007/s11403-019-00253-6,Identifying financial instability conditions using high frequency data,January 2020,Maria Elvira Mancino,Simona Sanfelici,,Female,Female,Unknown,Female,"Identifying, measuring and reducing systemic risk, and forecasting financial crisis are important tasks of authorities for safeguarding financial stability and central banks. To be able to take appropriate actions, policymakers should benefit from aggregate measures which can capture the potential for instability in the financial system to develop into a large economic downturn (Deghi et al. 2018). The strong connection between financial turmoil, economic growth and welfare entails that financial stability indicators should combine a large set of macro-financial indicators in order to be able to adequately capture the spillover and contagion risk. Combining both economic and financial information should enable analysts to make more accurate predictions about adverse economic outcomes. Recent studies suggest that financial condition indicators can provide valuable information about risks to future economic growth, because frictions and fragilities in the financial sector can amplify initial adverse shocks (Adrian et al. 2017). Both theoretical and empirical research has been developed to understand the causes of financial crises. Financial crisis forecasts based on the quantitative study of macro-economic data have been studied among others by Ericsson (2016), Stekler and Symington (2016). A large stream of literature on financial crisis forecasting aims at producing a comprehensive model for the genesis, dynamics and prediction of financial crises, using, for instance, the powerful tools of time-series analysis (Van den Berg et al. 2008; Sornette and Johansen 2010; Jiang et al. 2010), network theory (Celik and Karatepe 2007; Niemira and Saaty 2004), machine learning approach (Fuertes and Kalotychou 2007), multinomial logit models (Bussiere and Fratzscher 2006), operations research and decision theory (Demyanyk and Hasan 2010), chaos theory and data filtering techniques (Guégan and Ielpo 2008), agent-based models (Grilli et al. 2015). Many empirical and theoretical studies have investigated the relationship between market volatility and financial crisis. However, although there is a general consensus that unusual levels of financial market volatility imply an increased likelihood of a subsequent financial crisis, the use of high market volatility as an indicator of financial instability is not completely reliable. Indeed, while on one side high market volatility can be seen by forward-looking economic agents as a sign of the increased risk of unfavorable future outcomes, on the other side low volatility may indicate a crisis as well, as argued in Danielsson et al. (2016), Recchioni and Tedeschi (2017). The use of volatility measures and the study of their deviations from a long run trend may be criticized based on considerations related to persistency, real-time data availability and measurement issues. This can hinder our ability to identify harmful deviations from the trend, whenever financial conditions start to rapidly deteriorate in the run-up to a crisis. As long as policymakers intervene just when risks materialize, this may lead to a too little too late response. If we were able to go deeper in assessing the regularity of the latent volatility dynamics by enhancing the discontinuities characterizing its evolution, we could detect relevant differences between volatility paths that, under standard scrutiny methods, would seem similar. The contribution of our paper goes in this direction: the new indicator we propose is, therefore, designed to amplify relevant fluctuations in financial data, and thus increase our ability to predict large price variations even at short-time horizons. More precisely, instead of relying on a first-order measure (such as the volatility), we consider a second order quantity (named price-volatility feedback rate), which is supposed to describe the ease of the market in absorbing small price perturbations. The recent literature on volatility modeling has pointed out the existence of feedback effects of assets prices on volatilities and viceversa in financial markets. Volatility feedback and leverage effects are related to the same phenomena. On the one hand, the leverage effect explains why a negative return causes an increase in the volatility. On the other hand, the notion of volatility feedback effect is based on the argument that volatility is priced and an increase in the volatility requires a higher rate of return from the asset; this can only be produced by a decline in the asset price, as observed by French et al. (1987). Moreover, in Bekaert and Wu (2000) the interaction of these two effects is analyzed and used to explain the irregular behavior of volatility causing instability in financial markets such as excess volatility and volatility smile. The price-volatility feedback rate, that we propose as an indicator of financial instability, relies on the computation of the decay rate for the propagation of a given market shock. The rate of variation through time of an initial perturbation enables us to understand if such a shock will be rapidly absorbed or, on the opposite, it will be amplified by the market. To address this problem, we use the stochastic calculus of variations, which allows us to consider the effects of any perturbation of the model (e.g., perturbation on the initial datum, volatility, drift) on random variables and, therefore, is often used to compute Greeks of contingent claims, see Malliavin and Thalmaier (2006). The price-volatility feedback rate quantifies the rate of propagation of a perturbation of the price process, thus it can be interpreted as a market breakdown index. When the coefficient is positive, the perturbation propagates over a trajectory, whereas, when the coefficient is negative, the perturbation is attenuated over a trajectory. Our main results concern the variation of the stochastic process when the underlying setting is a diffusive model (i.e., a Brownian semimartingale) and the volatility is level dependent. In some sense, our mathematical construction provides a theoretical explanation to some empirical evidences already pointed out in the literature suggesting a strong correlation between volatility-related parameters and macroeconomic instability and systemic risk. A different attempt to analyze these interactions is provided by Recchioni and Tedeschi (2017). Based on an expansion of the analytical marginal conditional probability density function in powers of the volatility of volatility, they were able to build an early warning indicator for significant instabilities in the government bond yield dynamics. The result is an early warning indicator, based on the estimation of the initial volatility and the volatility of volatility, which anticipates phases of instability characterizing the time series investigated. Furthermore, in da Fonseca et al. (2013) a measure based on the Hurst parameter of the underlying fractional Brownian motion has been studied, which is intended to understand the regularity of the volatility path and used to distinguish intrinsic-systemic fluctuations and external ones. Our financial instability index combines in a non-linear way volatility, leverage and covariance between leverage and priceFootnote 1 and can be non-parametrically computed from high frequency financial data; this provides an early warning indicator of instability characterizing the financial time series under investigation. In fact, all the constituents (namely volatility, leverage and covariance between leverage and price) and, as a consequence, the price-volatility feedback rate can be estimated in real time exploiting the Fourier method developed in Malliavin and Mancino (2002b). The price-volatility feedback rate has been originally proposed in Barucci et al. (2003), where its analytical expression has been derived for a general univariate or bivariate level-dependent stochastic volatility model. However, the statistical properties of its estimator using the Fourier methodology were yet incomplete. The present paper contributes to fill this gap by providing consistent estimators for the three covariation processes appearing in the definition of the feedback rate. More precisely, consistent estimators for the volatility and the leverage using the Fourier method are proposed in Malliavin and Mancino (2009) and Curato and Sanfelici (2015), while here we prove a new consistency result for the third quantity involved, i.e., the covariance between leverage and price. In the case of the Constant Elasticity Variance model for the asset price we obtain an analytical formula for the indicator, fixing an error present in the analogous formula in Barucci et al. (2003). Furthermore, we use this explicit formula to perform a simulated analysis showing the effectiveness of our approach in estimating the feedback effect and the relation occurring between the feedback rate sign and the price variations. The potentiality of our indicator is tested on a sample of the S&P500 Futures including the following financial market crashes: 2000-03-10: NASDAQ Crash (dot-com Bubble); 2001-02-19: Turkish Crisis; 2001-09-11: Twin Tower Attacks; 2001-12-27: Argentine Default. Those crises are very different in nature to each other and had quite different impact on the S&P500 Futures series. The purpose of our study is to estimate the volatility feedback rate and to analyze its effects on the market behavior. More precisely, we provide empirical evidence that large values of the feedback rate reveal conditions in the market where perturbations in the price level may evolve in large price declines or changes in general. Our findings suggest that the volatility feedback rate can help discriminate between market conditions that are able to rapidly absorb price shocks and conditions when the price process has a potential risk of being adversely affected by an increase in the volatility. The paper is organized as follows. In Sect. 2 the instability indicator is defined and it is analytically computed under the CEV model. Section 3 contains the consistency results for the Fourier estimator of the indicator. In Sect. 4 a simulation study is conducted to analyze the role of the feedback rate in relation to price variations and to test the efficiency of the proposed feedback effect estimator. In Sect. 5 we present an empirical analysis for the S&P 500 index futures from 03-Jan-2000 to 31-Dec-2002. Sect. 6 concludes.",9
15.0,1.0,Journal of Economic Interaction and Coordination,26 August 2019,https://link.springer.com/article/10.1007/s11403-019-00260-7,A simulation analysis of systemic counterparty risk in over-the-counter derivatives markets,January 2020,Yuji Sakurai,Tetsuo Kurosaki,,Male,Male,Unknown,Male,"Although the economic and social benefits of over-the-counter (OTC) derivatives markets are heavily debated after the recent financial crisis, these markets still remain large. According to BIS statistics, the size of the global OTC derivatives markets at the end of 2018 is around 544 trillion dollars in notional terms and 10 trillion dollars in terms of gross market value.Footnote 1 To reduce systemic risks in these markets, transferring OTC derivatives contracts to central clearing counterparties (CCPs) is being implemented.Footnote 2 As a result, the majority of credit default swaps (CDSs) are now centrally cleared. Yet, ISDA argues that a significant percentage of OTC derivatives will remain non-cleared (International Swaps and Derivatives Association 2013). For example, the BIS statistics show that only 3% of FX derivatives were centrally cleared in notional terms (Bank for International Settlements 2019). It is essential for any regulator to assess the systemic risks in OTC derivatives markets. In this paper, we propose a simulation framework to assess the systemic risks in OTC derivatives markets. A key aspect of the OTC derivatives markets is that derivative dealers are actively mitigating their counterparty credit risks by making a margin call and trading CDS as a hedging instrument. Such active risk management may not be observed in other financial markets such as loan markets. To capture both margin calls and CDS hedging in a unified framework, we employ the concept of credit valuation adjustment (CVA) which is the mark-to-market value of counterparty credit risk exposure booked on the balance sheet of financial institutions. CVA is a key to understanding systemic risk in the OTC derivatives markets. First, CVA is widely used as an adjustment of counterparty credit risk in OTC derivatives markets. Banks are required to make fair value adjustments for CVA under International Financial Reporting Standards.Footnote 3 Second, CVA is one of the important topics in the discussion of new banking regulations. During the 2008–2009 crisis, roughly two-thirds of losses related to counterparty credit risks came from increased CVA losses.Footnote 4 As a result, financial institutions will be required to maintain a certain capital buffer for CVA risk in Basel III.Footnote 5 Our primary research question is whether or not and how hedging activity of CVA could destabilize CDS markets and, consequently, create systemic risk. The adverse feedback effect of CVA hedging on CDS markets is not merely a theoretical possibility. In fact, some market participants argue that CDS markets are getting less liquid because of the transfer of the OTC derivatives toward CCP.Footnote 6 According to Carver (2013), market participants are concerned that a decline in the liquidity available in CDS markets might damage the foundation of computing CVA. In other words, CVA may reflect not only the fundamental default risk of counterparties but also the illiquidity of underlying CDS markets. Another example of the adverse feedback effects is sovereign CDS markets. Bank of England (2010) reports that there is a strong demand for protection against sovereign defaults for CVA hedging. Consequently, hedging activity increases the sovereign CDS spreads. Sovereign entities do not post collateral to their counterparty banks, and thus, banks cannot find a way to reduce CVA except by buying protection in the sovereign CDS markets. In the European Systemic Risk Board, Brunnermeier et al. (2013) call the same mechanism “CDS–CVA feedback loop” and argue that it is a product of mark-to-market accounting and difficult to measure accurately. Although it is difficult to empirically quantify such a feedback effect, it is important to develop a quantitative model to capture it. Our simulation framework is the first step toward that direction. Our simulation framework has several building blocks. We implement the Merton (1974) model to describe CVA booked in a bank’s balance sheet. We then compute CDS spreads of banks following Hull et al. (2004). Specifically, a bank’s CVA is computed from its counterparty’s CDS spreads and its exposure to the counterparty. Once computed, CVA is subtracted from the bank’s asset value. In our simulation framework, a bank’s CDS spread is valued using the information about the bank’s balance sheet. We use the Merton (1974) model for three reasons. First, as we discuss later, a structural model enables us to model the illiquidity-driven volatility of a bank’s CDS spreads separately from its fundamentals-driven volatility. Our motivation behind this is to see whether mark-to-market accounting of counterparty risk creates an adverse feedback loop in the OTC derivatives markets. Second, while it is possible to utilize other credit risk models to describe CVA on a bank’s balance sheet, we use the Merton (1974) model to keep our simulation framework computationally feasible. Third, there is empirical evidence that structural models are useful for explaining bank CDS spreads (Hasan et al. 2016). Our simulation framework works in the following way. Let us assume that there are only three derivative dealer banks, A, B and C, for expository simplicity. Assume that bank B is exposed to bank A’s default risk by its derivative trades. Bank C is exposed to default risk of both bank A and bank B. Figure 1 depicts this situation. Suppose that the value of derivative trades between bank A and B changes in favor of bank B. At the same time, bank A’s default risk increases due to a loss from a non-derivatives business. Then, bank B mitigates part of the increased counterparty credit risk by making a margin call and purchasing a CDS protection on bank A. In the next time period, two different feedbacks occur. The first feedback arises due to CVA. If bank B’s risk management of CVA is imperfect,Footnote 7 bank B’s CVA for A increases. If the increase in CVA due to higher default probability exceeds the increase in the derivative value, bank B’s default risk goes up. Then, bank C needs to hedge its CVA for B. Consequently, total CVA across these three banks increases. The second feedback arises due to the lack of liquidity in the CDS market. If bank A’s CDS market is illiquid, bank B’s purchase of CDS protection increases bank A’s CDS spreads further. As a result, bank C’s CVA for bank A increases since it is evaluated based on the default probability of bank A extracted from bank A’s CDS. A simplified example of the network in the OTC derivatives markets Feedback mechanism of CVA, CDS and banks’ balance sheet The feedback mechanism discussed above is depicted in Fig. 2. Arrow 1 and Arrow 2 are straightforward: Arrow 1 indicates that leverage and asset volatility determine CDS spreads, while Arrow 2 shows that default intensity implied from CDS spreads impacts CVA. Arrow 3 and Arrow 4 are key to our simulation framework. Arrow 3 indicates that the mark-to-market (MTM) value of the derivative portfolio is adjusted by subtracting CVA. We call it the balance sheet channel. Arrow 4 means that CVA hedging impacts CDS spreads. We call it the hedging impact channel. Our simulation model captures both channels. As a measure of systemic counterparty credit risk, we employ total CVA aggregated across financial institutions (which we call “banks”). Our simulation framework enables us to generate a distribution of total CVA, which is not available without relying on some quantitative model. Using the generated distribution of total CVA, we can also discuss its tail risk. We examine how the distribution of total CVA aggregated across banks changes using a simulation of the OTC derivatives markets. Our simulation results show that both volatility and the tail risk of CVA increase dramatically as the liquidity in the CDS markets declines below a certain level. Also, CDS spread volatility increases as the market impact of CVA hedging on the CDS spreads increases. The relationship between them is nonlinear. In other words, the model successfully captures the “CDS–CVA feedback loop” discussed above. Importantly, the feedback effect arises even though (1) we model the OTC derivatives market to be mean-reverting and (2) each individual bank is optimally hedging their CVA and posting collateral to each other. As an extension, we incorporate the cost of posting time-independent collateral into bank’s utility. This extension is motivated by the fact that a large number of financial institutions will be required to have a certain amount of Initial Margin (IM) for their OTC derivatives exposure by 2020, based on the newly introduced regulation of margin requirement. IM is collateral that is posted at the start of a trade in order to cover potential counterparty risk not covered by frequent margin calls. By design, IM is not impacted by daily change of mark-to-market derivatives and thus is more time-independent. According to the new margin requirement, the amount of IM depends on the size of the notional amount. Lynch (2016) discusses that posting IM reduces counterparty credit risk while it would increase funding costs and risks. Gregory (2014) argues that counterparty risk is transformed into funding liquidity risk of posting margin. Our extended model shows that a larger amount of IM attenuates the adverse effect between CDS and CVA, because more IM reduces CVA. Hence, as IM increases, the CVA volatility is lowered and systemic counterparty credit risk is reduced. At the same time, as IM increases, the cost of posting collateral also increases. This suggests that there is a trade-off between CDS spread volatility, CVA volatility and the cost of posting collateral. The most closely related research is done by Macroeconomic Assessment Group on Derivatives (2013). They analyze the OTC derivatives exposure network with CVA to understand the impact of OTC derivatives reform on long-run GDP. As studied in this paper, they model the spiral of increased default probability, CDS-based CVA and rising leverage. However, our simulation framework differs in two ways. First, we explicitly model the market impact of CVA hedging on CDS spreads. Explicit modeling of such a market impact is important because some practitioners raise concerns about the decline in single-name CDS liquidity as mentioned above. Second, we employ a structural credit risk model to associate CVA on banks’ balance sheet with CDS spreads while they use a reduced-form credit risk model. As mentioned earlier, a structural model allows us to see how much additional volatility of CDS spreads and CVA is created due to the fact that banks are using the CDS-implied default probability of their counterparties. The rest of the paper is organized as follows. Section 2 is a literature review. Section 3 explains the terminology of counterparty credit risk management such as CVA and DVA. Section 4 gives a detailed explanation of our simulation framework. Section 5 investigates different scenarios from the perspective of financial stability by applying the simulation framework. Section 6 extends our model to capture the cost of posting collateral. Section 7 concludes.",2
15.0,1.0,Journal of Economic Interaction and Coordination,06 April 2019,https://link.springer.com/article/10.1007/s11403-019-00248-3,"Market microstructure, banks’ behaviour and interbank spreads: evidence after the crisis",January 2020,Burcu Kapar,Giulia Iori,Guido Germano,Female,Female,Male,Mix,,
15.0,2.0,Journal of Economic Interaction and Coordination,22 August 2017,https://link.springer.com/article/10.1007/s11403-017-0201-8,"Much ado about making money: the impact of disclosure, news and rumors on the formation of security market prices over time",April 2020,Yuri Biondi,Simone Righi,,Male,Female,Unknown,Mix,,
15.0,2.0,Journal of Economic Interaction and Coordination,16 November 2017,https://link.springer.com/article/10.1007/s11403-017-0207-2,Racial residential segregation in multiple neighborhood markets: a dynamic sorting study,April 2020,Sheng Li,Kuo-Liang Chang,Lanlan Wang,,Unknown,Unknown,Mix,,
15.0,2.0,Journal of Economic Interaction and Coordination,11 December 2017,https://link.springer.com/article/10.1007/s11403-017-0209-0,Combining monetary policy and prudential regulation: an agent-based modeling approach,April 2020,Michel Alexandre,Gilberto Tadeu Lima,,Male,Male,Unknown,Male,"The 2008 financial crisis promoted a revival of the debate on the interaction between the real and financial sectors. Although some arguments, such as the financial accelerator proposed by Bernanke and Gertler (1995), had already pointed out the existence of transmission channels from one sector to the other, the crisis suggested that such interlinks could be much more complex than initially thought. The recent financial crisis made it clear that small disturbances in one sector, through essentially nonlinear relationships, could be amplified and spread across both sectors. Indeed, although there is no consensus regarding the roots of the recent financial turmoil, it is widely accepted that it has crossed the border of the financial sector, bringing heavy losses to the real economy.Footnote 1 Before the crisis, the reigning view concerning macroeconomic policy was that monetary policy and prudential regulation could pursue their goals—macroeconomic and financial stability, respectively—acting independently, without any need of coordination. According to this dominant paradigm, monetary policy based on inflation targeting and flexible exchange rates and financial regulation grounded on microprudential measures would both accomplish their independent objectives (Canuto and Cavallari 2013). More recently, the balance started to be tipped in favor of a more harmonious interaction between monetary policy and prudential regulation. In fact, through linkages between real and financial variables, one policy may affect the other’s target, sometimes even in an undesirable way. For instance, by pursuing macroeconomic stabilization in a recession scenario, monetary policy may reduce the interest rate. However, low interest rates may lead to less banks’ incentive to monitor borrowers, over-leverage in banks and the bearing of higher risks by agents in order to achieve higher returns, bringing threats to financial stability (IMF 2013). Hence, the stand-alone execution of policies may lead to unsatisfactory outcomes. To explore the issue of such policy coordination, existing macroeconomic models should be endowed with an attribute which proved to be important in the recent financial crisis: the ability to deal with nonlinear interdependencies between real and financial variables. In the case of Dynamic Stochastic General Equilibrium (DSGE) models, the main workhorse of most central banks, this is done through the incorporation of financial frictions (BCBS 2012). This allows the study of optimal combinations between monetary policy and prudential regulation within the DSGE framework, as has been done recently by some researchers (e.g., Agénor et al. 2013; Beau et al. 2012; Goodhart et al. 2013; Lambertini et al. 2013). Nonetheless, DSGE models have some inherent characteristics which limit their usefulness to the assessment of policy actions. As discussed above, a key element to grasp the occurrence of financial crashes is an understanding of nonlinear feedbacks among financial and real variables. Nevertheless, the incorporation of nonlinearities in DSGE models is quite limited, as they are usually solved through log-linearization around a unique steady state. Furthermore, the embodying of some endogenous elements responsible for the propagation of the crisis, as boundedly rational behavior and inefficient markets, is a real challenge in DSGE models, due to their assumption of forward-looking fully rational behavior (Canuto and Cavallari 2013). Indeed, in these models, risk is brought in by exogenous shocks. Additionally, the hypothesis of full rationality and optimizing behavior is even more unrealistic during financial crisis, as the validity of such hypothesis relies on historical relationships which no longer hold (Bookstaber 2012). Another strategy has been the development of new approaches. One of the most promising alternative approach is the Agent Based (AB) modeling.Footnote 2 The AB modeling approach conceives of the economy as a complex system, defined by the presence of emergent properties, that is, an aggregate behavior remarkably distinct from the simple extrapolation of the individual units’ behavior (Krugman 1996). In the AB modeling framework, agents (consumers, firms etc.) are rationally bounded computational entities. They interact with each other following simple behavioral rules, giving rise to nonlinear patterns. Such rules, grounded on incentives and information, may evolve according to their fitness—the payoff they provide to the agents adopting them. In this case, the model is called evolutionary. Once reasonable initial conditions and parameters of the model are set, the modeler can observe how the system evolves over time. AB models have some advantages over DSGE models regarding the assessment of economic policy. According to Fagiolo and Roventini (2012), such advantages belong to two classes: theory and empirics. From the theoretical point of view, as they are not a priori required to be analytically solvable, they allow the relaxing of several simplifying assumptions (e.g., equilibrium and fully rational expectations) necessary for mathematical tractability. Their disengagement from analytical solvability allows them to cope with nonlinearities much better than DSGE models. This flexibility is at the root of their empirical advantage; it enables them to be much more realistic than DSGE models as regards inputs (assumptions more similar to the observed ones), as well as outputs (replication of stylized facts of interest). Policy experiments with AB models are typically done in the following way. First, the AB model is designed to reproduce relevant stylized facts of the policy target (e.g., the price level). Then, it is gauged how the implementation or changes in some policy parameter (e.g., the interest rate) impact on the behavior of such variable (Fagiolo and Roventini 2012). The possibility of implementing a wide range of policy measures turns AB modeling very attractive for performing policy exercises, as has been done extensively in recent years.Footnote 3 The aim of this paper is to study the interaction between monetary policy and prudential regulation in an AB modeling framework. In the model proposed here, firms borrow funds from the banking system in an economy regulated by a central bank. The central bank is responsible for carrying out monetary policy, by setting the interest rate, and banking prudential regulation. Among the multiple tools available for prudential regulation, we deal with a specific one in this paper, namely, the capital requirement through the setting of a cyclical buffer. Different combinations of interest rate and capital requirement rules are carefully evaluated with respect to macroeconomic and financial stability. The former depends negatively on output and price volatility and the latter is measured by the non-performing loans (hereafter, NPL)-to-credit ratio. While monetary policy employs Taylor-type interest rate rules, the capital requirement rules involve the establishment of a cyclical component, as proposed in the Basel II agreement. Our purpose is to gain qualitative insights on suitable combinations between monetary policy and prudential regulation. AB modeling has been extensively applied by researchers to analyze the impacts of monetary policy on the economy. Usually, effects of monetary policy are explored through changes in the interest rate, as in Dosi et al. (2013). In some studies (e.g., Delli Gatti et al. 2010; Raberto et al. 2008; Mandel et al. 2010; Riccetti et al. 2013c), such changes are driven by Taylor-type rules. Prudential regulation has also been the object of study of AB models. Teglio et al. (2012) and Cincotti et al. (2012), for instance, study the impact of capital requirement rules on the economy employing the EURACE model.Footnote 4
Neuberger and Rissi (2012) show that regulatory policy may be effective in homogeneous or bank-based financial systems, depending on the stability measure used. Krug et al. (2015) develop a stock–flow consistent AB model to assess the effects of the main components of Basel III on financial stability. Starting from a decentralized matching AB macroeconomic model, Riccetti et al. (2013a) find a positive effect of the implementation of a capital requirement buffer. Nonetheless, studies devoted to test different combinations of monetary policy and prudential regulation within the AB modeling framework are relatively scarce. As far as we know, there are not many AB models explicitly intended to explore this issue. Krug (2015) presents elements in favor of the Tinbergen’s principle by arguing that the “leaning against the wind” monetary policy seems to have no impact on financial stability, which can be reached through macroprudential regulation. A slightly different view is proposed by Popoyan et al. (2017), according to which the “leaning against the wind” monetary policy contributes to the stability of the economy as a whole. Furthermore, the authors point out that macroprudential tools have limited impact on inflation, but reduce unemployment and output gap. Simulations performed by van der Hoog (2015) suggest that the best scenario regarding financial stability is achieved by combining the use of a non-risk-weighted total capital ratio with the full credit rationing of all financially unsound firms. Somehow similar exercises were conducted mainly through the use of DSGE models, as referenced above. A noteworthy exception is Barnea et al. (2015), which develop an overlapping-generations model to analyze the interaction between these policies. However, we believe that AB models, due to their greater flexibility to deal with interacting agents and nonlinearities, can shed new and relevant light on this issue. In addition to this introduction, this paper has four other parts. Section 2 outlines the model, while simulation results corresponding to the baseline case are shown in Sect. 3. Meanwhile, Sect. 4 contains an assessment of various combinations of monetary policy and prudential regulation, considering their performance with respect to both macroeconomic and financial stability. Concluding remarks are presented in the last section.",12
15.0,2.0,Journal of Economic Interaction and Coordination,13 March 2018,https://link.springer.com/article/10.1007/s11403-018-0217-8,Network calibration and metamodeling of a financial accelerator agent based model,April 2020,Leonardo Bargigli,Luca Riccetti,Mauro Gallegati,Male,Male,Male,Male,"The relationship between Agent Based Models (ABMs) and empirical evidence is a widely discussed topic among scholars in the field. On the one hand, ABMs provide a more faithful representation of economic reality, introducing more realistic behavioral assumptions than mainstream models. Thus, they should potentially provide a better agreement with empirical data. Indeed numerous contributions have underlined the success of ABM in replicating “stylized facts” thanks to the introduction of agent heterogeneity, bounded rationality and learning, and decentralized out-of-equilibrium interactions.Footnote 1 On the other hand, there is still little consensus in the field on how to evaluate the agreement between models and facts. Some ABM scholars claim that calibration or validation, not to speak of estimation or forecasting, are neither possible nor desirable (Valente 2005). Most researchers underline instead that empirical evidence imposes a much needed discipline on model building, and that ABMs should accept the challenge of a stringent comparison with this evidence (Fagiolo et al. 2007). In particular, a growing number of contributions tackle the issue of econometric estimation of agent-based models, although these exercises are confined at the moment to relatively simple models of financial markets (Alfarano et al. 2005; Manzan and Westerhoff 2007). One key characteristic of ABMs is that the mathematical form of the relationship between endogenous and exogenous variables is unknown. Generally speaking, if y is a time series generated from the ABM, h is a vector function defined over y, whose components are usually called moments, and \(\theta \) is the vector of parameters of the model, y and h(y) are typically unknown, possibly non-linear, random functions of \(\theta \) with unknown likelihood. Thus maximum-likelihood estimators, or standard approximations of the likelihood function, are of no use. Instead, we can employ a class of methods defined as “simulated minimum distance” (Grazzini and Richiardi 2015), which can be stated as follows where F is a criterion function, s is a label for a fixed generator of pseudo-random numbers, \(\Theta \) is the domain of variation of parameters and x is a real time series. If the model is overidentified, a frequent choice for F is a quadratic loss function with an optimal weighting matrix, i.e. one that minimizes the uncertainty of estimation. If h results from the estimation of the same “auxiliary” statistical model over real and simulated data, this approach is usually termed “indirect inference”; if h stands for a set of moments computed over x and y, we obtain the method of simulated moments (MSM) (Gouriéroux C. and Monfort 1996). In general we cannot exclude that ABMs are unidentifiable. Indeed, even linear or linearized DSGE models exhibit a number of pathologies in estimation due to the flatness of the objective function or to the existence of multiple maxima (Canova and Sala 2009). ABMs entail additional difficulties due to the possible non linearity of moments in the parameters. On the other hand, the existence of non linearities in the model is usually assumed but not proved, so we cannot exclude either that the ABM might be characterized by linear relationships between variables. In order to address the identification issue, the parameter space should be explored systematically before any estimation exercise. In this context, it’s useful to estimate the influence of \(\theta \) on \(h(y(\theta ,s))\) by means of a metamodel, i.e. a statistical auxiliary model of the following form: where \(f(\theta )\) is a deterministic, possibly non linear, vector function of \(\theta \), \(\beta \) is a vector of coefficients, and \(u_s\) is a second-order stationary, zero mean, potentially heteroskedastic, random term with given covariance matrix. This approach is widely used for ABM metamodeling in various fields (see e.g. Salle and Yildizoglu 2014; Dancik et al. 2010 and references therein). The metamodel is estimated from a sample of points in the parameter space, which still represents a computationally costly exercise for ABMs that can be made more efficient by an appropriate choice of evaluation points, e.g. with latin hypercubes or other parsimonious sampling designs (see “Appendix A”). Furthermore, the parameter space may be eventually restricted through the calibration of at least some of them, following the suggestion of Brenner and Werker (2007). The result obtained from the estimation of a metamodel represents the analogue, for a simulated model, of the reduced form of an analytically solvable model. We can employ this reduced form, if its fitness compared to the original ABM is good enough, for a variety of purposes, like sensitivity analysis (Campolongo et al. 2000), calibration and estimation. Given the general framework outlined above, in this paper we proceed as follows: we introduce a model of financially constrained production with a credit network composed of heterogeneous firms and banks; we calibrate some parameters of the model before simulations (input calibration), in particular we calibrate the network of the model with Japanese real credit network data; we simulate the ABM using an efficient sampling scheme of \(\Theta \); we estimate different specifications of Eq. (2), i.e. different metamodels, on simulated data; we choose the best metamodel performing some goodness-of-fit analysis on its predictions \(\hat{h}\); we employ \(\hat{h}\) for sensitivity analysis, quantifying the effect of each parameter on the components of \(h(\theta ,s)\); we identify the parameters of the ABM matching \(\hat{h}\) with a set of empirical moments \(\bar{h}\); we verify that at the optimal parameter values \(\theta ^*\) the response of the ABM is consistent with the predictions of the metamodels we check if at the optimal parameter values \(\theta ^*\) the ABM is able to replicate the empirical moments \(\overline{h}\) The model we introduce is parsimonious in terms of parameters if compared to others of a similar vein (Riccetti et al. 2013), since we wish to focus on the methodological novelty of the metamodelling approach. Indeed, the simplifications we have introduced limit the scope of the economic analysis and a more complicated model should lead to more realistic results. However, we underline that the proposed metamodeling methodology can be extended to more complex frameworks with no additional qualifications, since no a priori restrictions are imposed on ABMs for its application. In particular, this methodology is especially fit for large scale models with a large number of parameters since, especially when combined with optimal sampling schemes, it allows to reduce dramatically the number of simulations required for calibration or estimation (Barde and van der Hoog 2017). The main novelty of our model is that we allow firms to have multiple credit suppliers (see e.g. Bargigli et al. (2014)). In particular we opt for a representation of credit market interactions by means of a random network model where both firms and banks can have multiple connections. We make this choice mainly because it is less expensive in terms of computational time. In order to allow for multiple credit connections, standard AB simulations should go through all the potential links, i.e. cycle over \(n \times m\) steps, where n and m are the number of firms and banks respectively, but this is a slow, inefficient, solution. Using a random network model, we perform, instead, two faster operations: firstly we compute the parameters of a set of \(n \times m\) probability distributions; secondly, we draw \(n \times m\) random variables from these distributions. The additional advantage of this choice is that we can easily calibrate the parameters of credit market interactions with real data. The main motivation for pursuing calibration is that credit markets represent a typical example of a sparse network.Footnote 2 If a network is sparse, its topological propertiesFootnote 3 become non trivial. For instance, the size of the neighborhood of a node, i.e. her degree, becomes very important. Nodes with a high number of neighbors, called hubs, are typically conducive of large systemic effects, in particular they can potentially trigger bankruptcy avalanches, if affected by external shocks, through balance-sheet effects on many other agents (Shin 2008). Since real credit markets display a high fraction of hubs, their degree distributions are typically right-skewed. We wish to replicate this property in our model. A well known solution for this task is to build a statistical ensemble of random networks for which the average degree of each node is equal to the degree of the same node in the real network (Park and Newman 2004). Random networks drawn from this ensemble trivially replicate the degree distribution of the original network. Here we follow a different route, because we wish to connect the degree distribution with the economic variables of the model, net worth in particular. At the same time, we wish to replicate the debt and loan size distributions of the real market. In order to control for topological properties and assign loan amounts at the same time, we need to follow an approach in two stages: firstly, a couple of firms and banks activate a credit relationship with a given probability; secondly, if the link is activated, the loan amount is determined. The paper is organized as follows. In Sect. 2 we introduce our model. In Sect. 3 we calibrate the parameters required for credit market interactions using Japanese data. In Sect. 4, after having specified a suitable sampling design for the remaining parameters of the model and after having defined the components of h, we turn to agent-based simulations. Then, using simulated data, we compare a number of alternative metamodels which could serve as reduced form of \(h(\theta ,s)\) and, after having selected the most fitting metamodels, we quantify the effect of each parameter on the components of h using the corresponding predictions \(\hat{h}\). Finally, in Sect. 5 we employ the same approach to identify the parameter values of the ABM compared to a set of moment conditions. Section 6 provides some conclusions along with considerations regarding the long standing issue of aggregation.",16
15.0,2.0,Journal of Economic Interaction and Coordination,17 August 2018,https://link.springer.com/article/10.1007/s11403-018-0225-8,Spatial distribution of economic activities: a network approach,April 2020,Federico Pablo-Martí,Josep-Maria Arauzo-Carod,,Male,Unknown,Unknown,Male,"It is important to accurately analyse firm location and co-location patterns in order to better adapt public policies to decisions taken by firms in terms of where to locate and what to do at those locations. Concretely, these patterns may illustrate firms’ preferences regarding the type of economic environment that they need, which may be taken into account by policy makers aiming to provide these firms with what they need at a local level. Given that location and co-colocation data may be used for policy purposes, public organisations need access to sufficient and reliable data about location strategies used by firms and industries, as clusters may play a key role in local and regional development (Porter 1998). These data requirements highlight the importance of accurate measurements, both in terms of the methodologies used to identify clusters and the spatial aggregation levels used in this identification. In terms of methodologies used for clusters measurements, there currently are two main approaches:Footnote 1 Industrial Districts and Clusters. The first ones arising mainly from the Italian tradition of districts identified by Becattini in which supramunicipal areas specialize in some activities and generate a strong interaction among all agents involved in these industries (i.e., firms, workers, local banks, public administrations, etc.), and the second ones coming from Porters’ contributions in which main interest was on competitive advantages of given areas. Empirical applications arising from them are not exactly of the same characteristics, being that whilst the former approach is more popular mainly due to the standardization of the Sforzi-ISTAT methodology, the latter is potentially easier to use because of its lower data requirements. On account of the advantages and disadvantages of both approaches, in this paper we will use the cluster one, due to both data availability and the shortcomings of the Sforzi-ISTAT methodology (Boix and Galletto 2008; Sforzi and Lorenzini 2002; ISTAT 1996). In terms of spatial aggregation levels, most analyses of spatial distribution of economic activity have been carried out using extant administrative units (e.g. counties, regions, etc.), but unfortunately, these analyses suffer from the major shortcoming that administrative units vary greatly in size and shape, do not always coincide with real economic areas and are sometimes arbitrary. To deal with these constraints, recent research has started to use ad hoc units (usually smaller), as we do in this paper. These units are created by equally dividing a space into homogeneous squared cells which therefore do not exactly match any extant administrative unit.Footnote 2 Accordingly, the methodology proposed in this paper aims to overcome previous methodological constraints, to obtain more precise results regarding firms’ location and co-location patterns and to improve public policy tools aiming to attract new firms. Concretely, in this paper we identify manufacturing and service clusters (from all sectors) in Spain and we classify these clusters according to the reasons behind clusterization processes; that is, (a) whether firms tend to locate together because they look for the same types of site (regardless of the industry to which they belong to) (i.e., joint-location), or (b) whether firms look to be located close to their supplier/customers in order to optimise commercial exchanges or, simply, for unknown reasons related with interindustry linkages (i.e., co-location). In specific terms, by dividing Spain into homogeneous cells we can check whether each industry follows a concentrated or dispersed pattern and subsequently, whether similar location exist for pairs of industries, so that clusters formed by different industries can also be identified. Finally, once we have identified these industry location patterns, we apply network theory to show the local microstructure of clusters. This paper contributes to the literature by identifying patterns of concentration of manufacturing and service industries in Spain using highly disaggregated geographical data (i.e. the space is divided into 10 km * 10 km cells) that enables it to fully account for location and co-location patterns of firms in different industries. The methodology used in this paper allows us to overcome previous technical constraints when analysing this geographical concentration of economic activity, as well as the shortcomings caused by the definitions of administrative borders. Additionally, although this is an empirical oriented paper focused in Spain, its results are easily applicable into other institutional settings.Footnote 3 To sum up, in order to properly analyse previously detailed joint-location and co-location issues, the main hypothesis to be tested in this paper is the following: “Whether industry location patterns can be explained in terms of relationships along the supply chain (i.e., co-location) or by common location determinants shared by those industries (i.e., joint-location)”. This paper is organised as follows. In the next section we review the main literature on the spatial distribution of economic activity and the spatial units used in empirical analysis. In the third section we explain the data set, we describe and analyse the spatial distribution of firms in Spain and we define the methodology. In the fourth section we present and discuss our main empirical results. In the final section we present our conclusions.",4
15.0,2.0,Journal of Economic Interaction and Coordination,01 September 2018,https://link.springer.com/article/10.1007/s11403-018-0228-5,Interbank rules during economic declines: Can banks safeguard capital base?,April 2020,Mitja Steinbacher,Timotej Jagrič,,Male,Male,Unknown,Male,"Banks are financial intermediaries in a stochastic economic environment. They use interbank liquidity to perform financial intermediation and avoid excessive risk. That is, banks adapt to the stochastic environment, among other things, by changing interbank loans and deposits. These operations usually include a strategic component in that banks try to match their liquidity needs with the liquidity available on the interbank market. Consequently, a complex network of interbank flows emerges that can have strong effects on the dynamics of the capital base in a banking system. To account jointly for the features of both the influence of interbank flows on the dynamics of the capital base of a banking system and the operations that control interbank liquidity flows, particularly during periods of economic downturn, we need to construct explicitly the way banks interact with each other and utilize information about the macrostate of the economy, as well as the strategic component of their decisions. From this perspective a network approach seems to be a natural choice for the analysis (Bargigli and Tedeschi 2014). Among other things, an agent-based approach enables the study of the ways in which rules governing individual behavior bring about macroscopic regularities and organizations (Epstein 1999; Tesfatsion 2002). In addition, insight can be gained for distinguishing strategies that may be appropriate, that is, warranting further empirical investigation, from those that are not (Robertson 2003). This conforms to the basic framework of agent-based modelling as a natural environment for the study of connectionist phenomena as elucidated by the social sciences (Farmer and Foley 2009; Gatti et al. 2010). In our case, banks are connected through interbank deposits and interbank loans. Dynamic interbank connections form (Fagiolo et al. 2007; Babus 2016), where banks are heterogenous, adaptive, non-learning, agents (Macal and North 2010) operating in an environment that is subject to perpetual change. Great efforts have been invested in the study of interbank relations by the use of a network approach, particularly following the seminal works by Allen and Gale (2000) and Freixas et al. (2000), which raised the question of contagion with respect to the topological structure of interbank networks. Initially, interbank networks were static constructions with passive banks. However, the static approach evolved into a dynamic one (Upper 2011), in which changes in interbank links by active banks were postulated. An early example of dynamic interbank link formation was provided by Babus (2016) who investigates the way banks decide on direct balance sheet linkages, as well as constructs a network formation process, and studies implications for contagion risk. A vast body of interbank network models have been designed for different purposes, using numerous types of interbank topologies, ranging from the theoretical, such as the Bernoulli random network (Nier et al. 2007), general random networks (Gai and Kapadia 2010), and an artificial small-world network (Steinbacher et al. 2014), to replications of true interbank networks that resemble core–periphery topology (Fricke and Lux 2014), a scale-free topology (Inaoka et al. 2004; Iori et al. 2008), a small-world topology (Boss et al. 2004a), etc. In a recent study Karimi and Raddant (2016), for instance, analyze the influence of interbank loan structure on stability by comparing an empirical interbank network to a rewired empirical network and a random network. A network approach to studying interbank relations has been popularized at an increasing number of academic conferences and workshops worldwide.Footnote 1 This paper belongs to the subfields of interbank models of strategy and interbank models of contagion. An early network model of strategy for studying the intermediation of banks can be found in Robertson (2003). Babus (2016), on the other hand, investigates how banks decide on direct balance sheet linkages and constructs an example of an endogeneous interbank network formation process. Models have also emerged that study interbank link formation as a bargaining game (Hałaj and Kok 2015) or as a decentralized insurance scheme (Babus 2016). An example of a choice model with memory has been provided by Iori et al. (2015), the principle notion being that a lender tends to borrow from an agent from whom he has borrowed in the past. Following agent-based simulation (Macal and North 2010) by a network approach (Schweitzer et al. 2009; Jackson and Wolinsky 1996; Jackson 2010; Wasserman and Faust 1994; Allen and Gale 2000; Allen and Babus 2009) we are explicitly focused on the way banks balance liquidity mismatch between the dynamics of loans and that of deposits of their nonbanking clients. Our focus is on liquidity flows between banks that emerge from liquidity needs in the external environment. Banks have no influence on the dynamics of these needs. They cooperate with each other by matching liquidity surpluses with liquidity shortages that arise in the economy. This uncertainty creates a demand for interbank liquidity, whereby banks use interbank loans (Freixas et al. 2000) and interbank deposits (Allen and Gale 2000). Besides interbank loans and deposits, banks in the model operate with loanable funds (nonbanking and banking), financial assets and liabilities, and deposits (nonbanking and banking). They use profits to increase capital and short-term reserve funds. Throughout this paper, liquidity preferences of real sectors are stochastic and are assumed to be seemingly uncorrelated. This is similar to Allen and Gale (2000), who assume an imperfect correlation of liquidity preference shocks (Diamond and Dybvig 1983). The network approach enables facilitation of interbank flows and the integration of rules with the need for interbank liquidity on a micro level, where these needs emerge. In particular, banks in our case perform the role of intermediation given a set of base-level rules that define their liquidity-matching choices. The novel feature of our investigation is the introduction of consistency in the implementation of base-level rules. Base-level rules are designed to promote a deposit insurance role of interbank flows, particularly in periods of increased volatility in the economy during economic declines. In our setting, banks differ in the consistency with which they follow these rules. We particularly focus on changes in the capital base of the banking system during economic downturn. We do not seek to propose an optimal rule or replicate interbank flows (or the equity base) of true banking systems. The replication of a true banking system would demand a design of an integrated artificial economy with a multitude of interdependent multilayered subenvironments. We could have designed banks with more complex decision-making features (Farmer and Foley 2009), but this would be outside the scope of this paper. However simple our experiments, the results are striking: changes in the attitude of banks towards simple rules (consistent vs. inconsistent use of rules) can have a dramatic impact on group behavior (changes in the overall capital base). These effects are pronounced particularly during periods of increased volatility in the economy. The remainder of the paper is structured as follows. Section 2 discusses the interbank liquidity model, its main assumptions and theoretical backgrounds. The section explains how risk enters the model and provides sources of interbank liquidity. Section 3 explains the simulator of interbank flows with parameterizations of simulation experiments. Section 4 presents a discussion of the results of the simulation experiments. The concluding section frames the main findings, highlights some comparisons, and comments on lessons that can be drawn.",
15.0,2.0,Journal of Economic Interaction and Coordination,04 February 2019,https://link.springer.com/article/10.1007/s11403-019-00239-4,Adhere to the rules or be discretionary? Empirical evidence from the euro area,April 2020,Zongsen Zou,Xiuling Wang,Dengtian Feng,Unknown,Unknown,Unknown,Unknown,,
15.0,2.0,Journal of Economic Interaction and Coordination,18 February 2019,https://link.springer.com/article/10.1007/s11403-019-00240-x,Aspects of complexity in citizen–bureaucrat corruption: an agent-based simulation model,April 2020,Jana Zausinová,Martin Zoričak,Vladimír Gazda,Female,Male,Male,Mix,,
15.0,3.0,Journal of Economic Interaction and Coordination,07 March 2019,https://link.springer.com/article/10.1007/s11403-019-00243-8,Convergence to Walrasian equilibrium with minimal information,July 2020,Ratul Lahkar,,,Unknown,Unknown,Unknown,Unknown,,
15.0,3.0,Journal of Economic Interaction and Coordination,20 March 2019,https://link.springer.com/article/10.1007/s11403-019-00246-5,Macroprudential regulation for a dynamic Chinese banking system with a scale-free network,July 2020,Qianqian Gao,Hong Fan,,Unknown,,Unknown,Mix,,
15.0,3.0,Journal of Economic Interaction and Coordination,17 April 2019,https://link.springer.com/article/10.1007/s11403-019-00249-2,Information versus imitation in a real-time agent-based model of financial markets,July 2020,Alessio Emanuele Biondo,,,Male,Unknown,Unknown,Male,"Both interaction and coordination among heterogeneous individuals are relevant ingredients of complexity in macroeconomics. The mainstream microeconomic approach to macro problems reveals to be inadequate to provide a realistic analysis of such economic relations, as explained in Delli Gatti et al. (2008, 2011). The interdependence of individual opinions is the core of many socio-economic problems and a vast literature has been developed about opinion dynamics. Specifically, financial markets are a valid example of strong informative connections influencing the process of decision-making of investors. Such a topic has been studied from many points of view, as in Banerjee (1992, 1993), Orléan (1995), Stauffer and Sornette (1999), Cont and Bouchaud (2000), Cooper et al. (2001), Iori (2002), Markoseetal (2004), Yamamoto and Lebaron (2010), Tedeschi et al. (2009), Tedeschi et al. (2012), and Zhao et al. (2014), among others. Networks literature, widely surveyed in Boccaletti et al. (2006, 2014), has been very often related with the interaction among agents operating in a number of contexts. In particular, many contributions dealing with contagion and imitation in economics exist. Generally speaking, the term contagion is referred to circumstances in which the instability arises from the interaction among banks and institutions, as for example in Allen and Gale (2000), Kiyotaki and Moore (1997), Lagunoff and Schreft (2001), Rochet and Tirole (1996), Leitner (2005), Elliot et al. (2014) and Chong and Küppelberg (2018). Imitation is, instead, often referred to the situation in which the behavior of a person is copied by someone else. Causes of imitation, which may also reflect an individual predisposition, are mainly related to social influence, in terms of both diversity -as discussed, for example by Lorenz et al. (2011), and dimension of groups -as explained by Kao and Couzin (2014). The emergence of behavioral avalanches is the visible effect of such a complex interaction, for people think it is convenient to follow the crowd, Bikhchandani et al. (1992), revealing the existence a common root in very different contexts, Moussaid et al. (2009). The choice of a leader is, in itself, the first and very fundamental step of any herding process, specially in financial markets, where the reputation and the reliability of market participants are inferred by looking at their performances (see Cooper et al. 2001; Hirshleifer and Hong Teoh 2003; Clement and Tse 2005; Booth et al. 2014, among many others). This paper tries to combine the analysis of the endogenous imitation among traders, spreading over networks by means of social interaction, with a realistic agent-based order book model, with the aim to obtain some specific advances in relating informative flows with market instability. In particular, the correlation between imitative processes (triggered by three different criteria) and volatility will be shown in order to discuss some implications about the quality of information for trading decisions. A section is also dedicated to show results deriving from the implementation of a real time engine, which allows replicating the time schedule of Borsa Italiana S.p.A. The real-time feature is a promising extension that will be further investigated in future research: the comparison between empirical and simulated data at different time-scales can provide a more detailed understanding of the severe fluctuations of markets. In literature, many contributions refer to real-time information, among which: some discuss the impact deriving from news and macroeconomic announcements, as in Evans (2011), Frijns et al. (2015), and Gilbert et al. (2017), with a specific focus on stocks and foreign exchange markets as in Andersen et al. (2007); others are focused on route-choice as in Wahle et al. (2002), Ben-Elia and Shiftan (2010), and Dia (2002); others deal with the demand response to prices, as in Yousefi et al. (2011). However, as far to my knowledge, an agent based model with a real-time engine, following exactly the official schedule of a real financial market, has never been released. Financial order books have been studied since long ago. Two comprehensive reviews are Parlour and Seppi (2008) and Chakraborti et al. (2011). It can be useful to group some of existing contributions in three main approaches. Some models can be defined “ trader-centric”, because they aim to derive fully-rational trading strategies, more or less directly referred to agents’ utility maximization (e.g.: the optimal choice between limit and market orders), as in Copeland and Galai (1983), Glosten and Milgrom (1985), Kyle (1985), Glosten (1994), Chakravarty and Holden (1995), Bertsimas and Lo (1998), Parlour (1998), Foucault (1999), Almgren and Chriss (2001), Hollifield et al. (2004, 2006), Rosu (2009, 2010). Other models are “ facts-centric” because they are addressed to study the statistical features of the market as a dynamic process more than to analyze the individual characterization of market participants, as in Bak et al. (1997), Maslov (2000), Daniels et al. (2003), Farmer et al. (2005), Mike and Farmer (2008), Bouchaudetal (2009), Farmer and Foley (2009), Cont et al. (2010), Cont and De Larrard (2013). Finally, agent-based models simulate a global environment where interactions among heterogeneous traders give rise to emergent phenomena at the aggregate level, as in Raberto et al. (2001), Chiarella and Iori (2002), Consiglio et al. (2005), Gil-Bazoetal (2007), Chiarella et al. (2009), Anufriev and Panchenko (2009), Tedeschi et al. (2009, 2012). The present model focuses on the approach presented by Tedeschi et al. (2009, 2012). Since imitation is a social concept, it has been designed to operate by means of information networks: agents may decide to imitate successful traders and success is defined on the basis of different criteria, namely, wealth, performance, and reputation. In particular, Tedeschi et al. (2012) show the effects of realized profits of a trader on the number of her connections with other traders, thus inducing the creation of gurus. Such influencers increase the stocks share in their portfolios and have more extreme expectations as they become more connected. The present paper, instead, proposes three different mechanisms to explain the reputational role of some traders, whose expectations and orders remain however totally unaffected by their in-degree. There are not specific reasons to imagine that notoriety induces more extreme expectations and a stronger activity on markets, nor to imagine that a trader can imitate the exact expectations of another. Therefore, in the model, the imitation is defined as to do the same thing done by another, irrespectively of the personal orientation, with a probability directly proportional to the traders propensity to trust others more than herself. Finally, no accumulation of reputation is modelled here. The microstructural part of the model draw substantially on Biondo (2018a, b, c), whereas the network, describing the consequences of imitation on market stability, is designed as a two-layer multiplex. More precisely, one layer is based on a simple star topology, dynamically changing according to negotiations, while the other one (designed by means of three different topologies) evolves because of the social interaction and the imitation. The present model proposes a truly operative order book microstructure used by traders displaced over a network. Such a design will be adopted in a forthcoming paper, which will be focused on the analysis of the self-organized behavior of the market. The fact that traders are unconsciously part of a mechanism that determines the price as an emergent phenomenon (thus disjoint from each individual action) is mostly ignored by investors. As in the famous metaphor used by Keynes in his General Theory, market participant are immersed in a ’Beauty contest’ global game, as proposed in Morris and Shin (2002). However, the model does not deal with perceptions of others opinions or higher-order expectations as in Golub and Morris (2017) and in related literature. It will be assumed that the signal of the imitated trader is clear, perfectly perceived, and equal for all those connected to him. If a trader imitates another, the possible result of the imitation is exclusively one, namely, to choose the same strategy chosen by the other. The paper is organized as follows: in Sect. 2, the model is described; in Sect. 3, after discussing the compliance of the model with relevant statistical features of empirical data, results of simulations are reported, with regards to the imitation-driven dynamics on different network topologies, the liquidity conditions of the market, and the real time machine replicating the daily time schedule of Borsa Italiana S.p.A; in Sect. 4 main findings are described, and some policy suggestions advanced.",1
15.0,3.0,Journal of Economic Interaction and Coordination,15 June 2019,https://link.springer.com/article/10.1007/s11403-019-00254-5,Does social learning promote cooperation in social dilemmas?,July 2020,Ozgur Aydogmus,Hasan Cagatay,Erkan Gürpinar,Unknown,Male,Male,Male,"The evolution of large-scale cooperation in humans has been the focal point of several disciplines in the last few decades (Axelrod 1984; Boyd and Richerson 1985; Bowles and Gintis 2011). It is argued that genetic relatedness can explain the emergence of cooperation (Hamilton 1964). However, humans cooperate on vast scales, more substantially than their genetic relatedness allows. Some explanations for this phenomenon include punishment, direct and indirect reciprocity, group competition etc. (see, for instance, Young 2015; Yu et al. 2015; Henrich and Boyd 2001). In this paper, we contribute to the literature by analyzing how social learning affects the evolution of cooperation. Social learning is the transmission of behavioral patterns by teaching and/or imitation (Boyd and Richerson 1985; Galef 1976; Rogers 1988). This concept is elaborated further by Enquist et al. (2007), Molleman et al. (2013). Cooperation and defection are examples of social behavior, where success of a particular behavior depends on the actions of others. In particular, we study how individual and social learning can affect the evolution of cooperation in social dilemmas represented by Prisoner’s Dilemma (PD) and Snowdrift (SD) games. Individual learning is basically interacting directly with the environment and learning from the feedback received, whereas social learning is observing other agents who interact with the same environment. Note that social learning, and social behavior are different (Boyd and Richerson 1985; Rogers 1988). To illustrate, an individual may learn new farming techniques from another person, even if farming is not social behavior since the success of a particular farming technique is not affected by the actions of others (see Rogers (1988, p. 10)). On the other hand, certain social norms can be learned individually even if they are examples of social behavior. Individual and social learning, to put it differently, are alternative methods of learning. Social learning matters, since the ability to learn from others allows individuals to foresee the consequences of their actions. The most common social learning strategies have been listed by Vostroknutov et al. (2018) as follows: frequency-dependent rules, such as conformity or anti-conformity to the most chosen alternative (Eriksson et al. 2007; Wakano and Aoki 2007; Morgan et al. 2011); payoff-based rules, where the level of imitation depends on the payoffs achieved by a demonstrator in the recent past (Kendal et al. 2009); confidence-based rules, when confidence of individuals and demonstrators modulate imitation (Morgan et al. 2011); and prestige-based rules, where the level of imitation depends on the status of the observed other (Lehmann et al. 2008; Lehmann and Feldman 2008). In our framework, individual learners choose to cooperate or defect by making payoff calculations, while social learners imitate the action of a random agent, which is called cultural model. Hence the proportions of cooperators and defectors in the population determine the social learners’ choice. We choose to model social learning using conformity since our game theoretical model is also a frequency-dependent model (Rogers 1988; Eriksson et al. 2007; Wakano and Aoki 2007; Morgan et al. 2011). Beware that each learning strategy possesses its own weakness: while individual learning is prone to errors due to computational limits of agents, social learning amounts to the mere imitation of the behavior of others and can lead to a population finding itself obsolete when the environment changes. It is beyond the scope of this paper to discuss how social and individual learning have evolved in the past. For the details see, among many, Bowles and Gintis (2011), Gould and Lewontin (1979), Wilson (2000), Boyd and Richerson (1985), Tomasello (1996), Rogers (1988), Sahlins (1976), Cavalli-Sforza and Feldman (1981), Richerson and Boyd (2005), Tomasello (1996). The paper is organized as follows. In Sect. 2, we discuss how we extend the game theoretic study of social dilemmas. In particular, we add social learners into the population, and we allow agents to make mistakes when choosing among alternative strategies. In Sect. 3, we develop the formal model. In Sect. 4, we provide numerical results and their implications. Section 5 concludes the paper.",5
15.0,3.0,Journal of Economic Interaction and Coordination,04 July 2019,https://link.springer.com/article/10.1007/s11403-019-00255-4,Epidemiology of inflation expectations and internet search: an analysis for India,July 2020,Saakshi,Sohini Sahu,Siddhartha Chattopadhyay,Unknown,Unknown,Unknown,Unknown,,
15.0,3.0,Journal of Economic Interaction and Coordination,01 July 2019,https://link.springer.com/article/10.1007/s11403-019-00256-3,"Margin trade, short sales and financial stability",July 2020,Hui Ying Sng,Yang Zhang,Huanhuan Zheng,,,Unknown,Mix,,
15.0,3.0,Journal of Economic Interaction and Coordination,31 July 2019,https://link.springer.com/article/10.1007/s11403-019-00258-1,"Innovation, finance, and economic growth: an agent-based approach",July 2020,Giorgio Fagiolo,Daniele Giachini,Andrea Roventini,Male,Female,Female,Mix,,
15.0,3.0,Journal of Economic Interaction and Coordination,06 August 2019,https://link.springer.com/article/10.1007/s11403-019-00259-0,"Tradability, closeness to market prices, and expected profit: their measurement for a binomial model of options pricing in a heterogeneous market",July 2020,Yossi Shvimer,Avi Herbon,,Male,Male,Unknown,Male,"Investigating the prices of options at times close to their expiration is particularly interesting. Many studies, such as Golez and Jackwerth (2012), Ni et al. (2005), Avellaneda et al. (2012), Avellaneda and Lipkin (2003), and Officer and Trennepohl (1981), have shown that the stock market is inefficient several days before expiration, meaning that as a result of the impending expiration of the options, there are atypical fluctuations in the price of the underlying asset that did not exist previously. The second focus of the current study is the effect of heterogeneity in the players’ beliefs on options pricing, a subject that has received little attention in the literature. Buraschi and Jiltsov (2006) empirically examined the effect of the “dispersion index” of players’ beliefs on options pricing. Using the S&P 500 Index between the years 1986–1996, they found that a model that takes into account the heterogeneous information about players’ expectations of a rise or fall in the stock market partly explains the overpricing of options. However, their empirical examination was restricted to time periods relatively far from expiration (at least 45 days beforehand). Research on options pricing has mainly focused on uncovering the factors that affect the spread between the prices predicted by existing models and the actual market prices, while comparatively little attention has been paid to developing option buying and selling strategies that include these factors. Among the factors affecting the prices of options are the Bid-Ask spread on the underlying asset, shocks in the price of the underlying asset, demand pressure, and heterogeneity among players regarding their beliefs about the market. Cho and Engle (1999) examined the effect of the Bid-Ask spread of the underlying assets on the Bid-Ask spread of options on the S&P 100 Index. They argued that if market makers (those who serve as intermediaries between buyers and sellers in the derivatives market) can use the underlying asset to hedge their position (due to timing differences between buying and selling, the market makers must hold options for a certain period of time), then the Bid-Ask spread in the derivatives market will be determined by the liquidity in the underlying asset market and not by liquidity in the derivatives market itself. Accordingly, Fedenia and Grammatikos (1992) found a positive correlation between the Bid-Ask spread in the option and the Bid-Ask spread in the underlying asset. This indicates an “illiquidity” premium in the stock market. A shock in the stock market (for example, as a result of a large stock-market transaction) affects the prices of options and violates the assumption of “perfect markets.” Cetin et al. (2006) assumed a stochastic supply curve of the price of the underlying asset as a function of the size of the transaction. Specifically, they assumed that the size and direction of the transaction (the buying or selling of shares) affect the share price at which the transaction will be executed, with larger transactions resulting in higher prices. They further assumed that for a given supply curve, players behave as “price-takers,” i.e., they buy at the suggested ask price. By using 5 optionable stocks, Cetin et al. (2006) showed that the prices of the options on the trading floor rise as the liquidity risk increases, which is a further example of how the underlying asset can impose an illiquidity premium on the option price. Bollen and Whaley (2004) showed that changes in the implied volatility (skewness) in actual option prices are directly affected by demand pressure (an increase in the number of option buyers). Chou et al. (2011) examined the liquidity risk using the prices of options on the Dow Jones index, where the prices of the Dow Jones index itself were used to represent the prices of the underlying assets. They found that options with a relatively small Bid-Ask spread have a higher level of implied volatility when there is a larger Bid-Ask spread in the underlying asset. Thus, options are more expensive when the demand pressure associated with the underlying asset is higher. This finding, which shows that options become more expensive in practice as the options market becomes less liquid, supports the “illiquidity premium” claim. Since speculative traders sell mostly Out-The-Money (OTM) options, as these options are cheap, the implied volatility decreases rapidly as trading turnovers increase. Kang and Luo (2016) empirically examined the impact of players’ beliefs about the market on the pricing of Call options on the S&P 500 Index. Attitudes were assessed using consumer confidence index surveys and the predictions of financial forecasters regarding the stock market. They found that in times of recession, options are priced higher than in times of stability (although lower than during booms in the stock market), due to low expectations of the players for the S&P 500 Index to rise. Hence, the researchers concluded that players’ assessments of market direction affect options pricing. Kang and Luo (2016), like others, examined options between 15 and 45 days prior to exercise, because they estimated them to be the most tradable. Models of options pricing that do not assume a “liquidity risk” can provide only an imprecise description of the options market and of the market in which the underlying asset is traded. To the best of our knowledge, there are no models in the research literature that allow an option-price estimate to be adjusted for each of two different players at any point in time, and especially shortly before the expiration day of the options. Furthermore, there do not appear to be any models that allow the price to be affected by changes in demand and supply (in our model, both tradability and price will be affected by the number of players operating on the trading floor) and by the heterogeneous attitudes of the various players. An exception is the study by Shvimer and Herbon (2019) in which a binomial model (hereafter: the “SH model”) was developed to estimate the price of an option in real time, even when there is a “liquidity risk.” The model may be applied at any point in time, including close to the expiration day. Furthermore, the model does not assume a “representative agent,” as is customary in classical models. Heterogeneity of the agents is a fundamental characteristic of the SH model and is represented by two elements. Firstly, each player is characterized by their subjective assessment of the value of the index of the underlying asset on expiration. Secondly, each player is characterized by their eagerness level, representing their willingness to accept a given price from the opponent. The price of an option is determined for each of two speculators (hereafter: the “players”) with different beliefs regarding the opposing player’s willingness to execute a transaction in a feasible price range. The goal of each player is to maximize their expected profit on the expiration date. Under the assumption of full information, Shvimer and Herbon (2019) found that if both players are too “compromising” in their willingness to execute a transaction (i.e., if they are too willing to accept an unattractive offer from their opponent), then there might be no transaction at all, even if the offered prices are within the feasible range (meaning that each player would stand to benefit from the transaction if executed). Thus, the model developed in their work suggests setting a price for the players that reflects the liquidity risk. In the present study, theoretical predictions of the SH model are obtained using real data from the S&P 500 Index as input variables. The examination of a binomial model close to expiration day is a worthwhile endeavor, since binomial models enable explicit results to be obtained for option prices in a relatively simply way. As mentioned, the binomial model of Shvimer and Herbon (2019) assumes that full information (i.e., the players’ beliefs and eagerness levels) is available to both players involved in the transaction and to all other players on the market. However, in the present study, the validity of the model is examined for an environment where the information is partial. Our contribution to the research literature is as follows: Market tradability is computed using a theoretical model in which the players acting according to the model are speculators who trade under real market prices. These prices are based on real data from the S&P 500 Index between the years 2010–2017 for the 30 days prior to the expiration of options. The expected profit of the speculator on expiration day is presented, based on empirical data. To the best of our knowledge, there are no other studies in the research literature that address the pricing of options from the perspective of a speculative player. Results are presented for the difference in option prices between the model and the market, for each point in time until expiration day. The rest of this study is organized as follows: Sect. 2 provides a short description of the SH binomial model. Section 3 presents the predicted “tradability,” based on empirical data, and examines the proximity of the model prices to market prices. Section 4 determines the expected profit of a speculator who operates according to the proposed model, while the market acts as the opponent. Section 5 presents a summary, conclusions, and additional research directions.",
15.0,4.0,Journal of Economic Interaction and Coordination,23 August 2019,https://link.springer.com/article/10.1007/s11403-019-00262-5,"Arbitrage, speculation and futures price fluctuations with boundedly rational and heterogeneous agents",October 2020,Qingbin Gong,Zhe Yang,,Unknown,,Unknown,Mix,,
15.0,4.0,Journal of Economic Interaction and Coordination,30 October 2019,https://link.springer.com/article/10.1007/s11403-019-00271-4,Does a ‘financial transaction tax’ drive out information mirages? An experimental analysis,October 2020,Andrea Morone,Pasquale Marcello Falcone,Piergiuseppe Morone,Female,Male,Male,Mix,,
15.0,4.0,Journal of Economic Interaction and Coordination,21 November 2019,https://link.springer.com/article/10.1007/s11403-019-00273-2,Quantifying the risk of price fluctuations based on weighted Granger causality networks of consumer price indices: evidence from G7 countries,October 2020,Qingru Sun,Xiangyun Gao,Yang Li,Unknown,Unknown,,Mix,,
15.0,4.0,Journal of Economic Interaction and Coordination,19 November 2019,https://link.springer.com/article/10.1007/s11403-019-00274-1,Financial contagion in inter-bank networks with overlapping portfolios,October 2020,Peilong Shen,Zhinan Li,,Unknown,Unknown,Unknown,Unknown,,
15.0,4.0,Journal of Economic Interaction and Coordination,09 January 2020,https://link.springer.com/article/10.1007/s11403-019-00279-w,Investment behaviour and “bull & bear” dynamics: modelling real and stock market interactions,October 2020,Serena Sordi,Marwil J. Dávila-Fernández,,Female,Unknown,Unknown,Female,"Over the last 30 years, the boundedly rational heterogeneous agents literature that started with Day and Huang (1990) and Chiarella (1992) has successfully shown that trading activity of heterogeneous interacting speculators accounts for a large part of the dynamics of financial markets. Ten years or so after the financial crisis, there has been a renewed interest in studying the role of financial actors and institutions in amplifying fluctuations not only in the financial side of the economy but also in the real sector. Different sources of behaviour heterogeneity have been identified such as trend extrapolation, noise trading, overconfidence, overreaction, optimistic or pessimistic traders, upward- or downward-biased traders, and so on (for a review on some recent developments, see Lux 2009; Hommes 2013; Dieci and He 2018). On the other hand, the real side of the economy has been traditionally introduced in a disequilibrium framework in which output adjusts to excess demand. Complex dynamics, close to well-known macroeconomic and financial stylised facts, are obtained as a result of the interplay of heterogeneous agents with financial and real variables.
 
Westerhoff (2012) is one of the first contributions to develop a model, formulated in discrete time, where the goods market is connected with the stock market. In his model, nonlinear interactions between aggregate demand, chartists, and fundamentalists result in complex “bull & bear” dynamics. A similar exercise was performed by Naimzada and Pireddu (2014) who introduced an extra nonlinearity in aggregate demand and assumed that the speed of adjustment in the stock market limits to infinite. Fiscal policy considerations were brought to attention by Cavalli et al. (2017) in an extension of previous models that adopts the nonlinear accelerator as a theory of investment.
 The aforementioned contributions divided agents in the financial markets between chartists and fundamentalists but did not allow for endogenous changes in their composition. Endogenous switches between alternative heuristics were formalised by Naimzada and Pireddu (2015) and Cavalli et al. (2018). A basic assumption they make is that all agents populating the stock market are fundamentalist but unable to observe the underlying fundamental so that their beliefs are biased either to optimism or pessimism. Other models, in general formulated in continuous-time, have preserved the Keynesian adjustment of aggregate demand and the chartists–fundamentalist approach. For example, making use of Lux (1995) formalisation of herd behaviour in speculative markets, Franke (2012) developed a macrodynamic model with a preliminary financial distress variable. His set-up has been extended allowing for a link between real and financial sides of the economy through Tobin’s q theory of investment (e.g. Franke and Ghonghadze 2014; Flaschel et al. 2018). A different strand of the literature, on the other hand, has preferred the Brock and Hommes (1997) approach for modelling changes in the share of agents that use different heuristics, though always maintaining the basic view on expectations under bounded rationality (for a discrete time example, see Proaño 2011, 2013). Housing market considerations and the introduction of other financial actors, such as banks, have been brought to attention by Chiarella et al. (2015) and Dieci et al. (2018). In fact, there exists a vast literature on the crucial role of credit as a factor leading both to the instability of the system and to a strengthening of real–financial linkages in the economy. This tradition à la Minsky has recently incorporated different levels of behaviour heterogeneity, enlightening different aspects of the interdependency between firm’s external financial structure and the state of the economy (see, for example, Sordi and Vercelli 2014; Lojak 2018).
 Finally, Lengnick and Wohltmann (2013, 2016) have extended the New-Keynesian model by introducing a financial market with chartist and fundamentalist agents. Combining deterministic and stochastic features, their models abandon the problematic hypothesis of rational expectations and show how market sentiments play an important role in the endogenous development of business cycles and stock market bubbles. It must be noted, however, that a full analysis of the interaction of the real economy and the financial market is still missing and we are far from a consensus. In an attempt to contribute to fill this gap, in this paper we develop a simple behavioural macrodynamic model formulated in continuous-time exploring possible channels of interaction between the real economy and the stock market. Our choice of a continuous-time approach has a twofold motivation. Although individual economic decisions are generally made in discrete time intervals, it is difficult to believe that they are coordinated in such a way as to be perfectly synchronised (Gandolfo 2009, pp. 568–573). Moreover, a specification in continuous-time is particularly useful for the formulation of dynamic adjustment processes based on excess demand and it is interesting to note that the first contributions on the topic explicitly advocated the use of continuous-time models (see, for example, Goodwin 1948). Our starting point is the model by Westerhoff (2012) who was able to generate complex dynamics of both national income and stock price providing an explanation of the irregularity of economic time-series. This is not surprising, however, given that the dynamic system of his model turns out to be a first-order \(2\times 2\) system of difference equations with a crucial—somehow ad hoc—nonlinearity. In what follows, we rewrite the model in continuous-time and show that, as expected, the dynamics it is able to generate are much simpler than in the discrete-time case and even non-persistent for economically meaningful values of the parameters. We proceed by considering a more general version of the model which we obtain by enriching the specification of its real side in various directions. We improve the specification of aggregate demand by distinguishing between consumption and investment expenditure and assuming that the latter is determined by the flexible accelerator principle. Following a long tradition in disequilibrium analysis, the introduction of a theory of investment based on the accelerator allow us to incorporate “Harrodian instability” as a source of endogenous economic fluctuations (for a recent presentation, see Franke 2018). The resulting first-order \(3\times 3\) system is capable of producing significant boom-and-bust dynamics with the amplitude of the cycles being conditional to the magnitude of the accelerator effect. As a final step, we allow for switches between chartist and fundamentalist behaviour in line with Lux’s (1995) mutual contagion mechanism in speculative markets. Among other things, we show that endogenous investment and stock market dynamics emerge, procyclical to each other. Such a feedback reflects the interaction of induced investment with alternating waves in speculators’ sentiments. The resulting nonlinear, first-order \(4\times 4\) system is shown to generate various dynamic regimes with interesting economic implications. The succession of periods of macroeconomic stability leading to more financially fragile structures and speculative booms is a common feature of the literature on real and financial market interactions with heterogeneous agents. Nonetheless, as pointed out by Nikolaidi and Stockhammer (2017), a key difference concerns the main source of instability. Does it come from one of the individual markets or from the interplay between them? We understand that most existing contributions have focused on the financial side of the story and, more specifically, on the destabilising role of chartists. It is our purpose in this article to show that a strong investment accelerator might be a crucial force generating fluctuations that, on the one hand, are transmitted and amplified by chartists and, on the other hand, are contained by fundamentalists. The reminder of the paper is organised as follows. Section 2 is concerned with the study of a continuous-time version of Westerhoff’s model. Section 3 modifies the model by introducing the flexible accelerator. We continue, in Sect. 4, by removing the ad hoc nonlinearity in the fundamentalist behavioural equation and allowing the share of chartists and fundamentalist to change endogenously. Section 5 concludes. The Appendix at the end of the paper contains the proofs and some long computations omitted from the text.",2
15.0,4.0,Journal of Economic Interaction and Coordination,14 January 2020,https://link.springer.com/article/10.1007/s11403-019-00280-3,A path integral approach to business cycle models with large number of agents,October 2020,Pierre Gosselin,Aïleen Lotz,Marc Wambst,Male,Unknown,Male,Male,"In many instances, representative agent models have proven unrealistic, lacking both the collective and emerging effects stemming from agents’ interactions. To remedy these pitfalls, various paths have been explored: complex systems, networks, agent-based systems or econophysics. However, agent-based and networks models rely on numerical simulations and may lack microeconomic foundations. Econophysics builds on statistical facts and empirical aggregate rules to derive macroeconomic laws. These laws are prone, like ad hoc macroeconomics, to the Lucas critique (see Lucas 1976). The gap between microeconomic foundations and multi-agent systems remains. The present paper attempts to fill this gap by adapting statistical physics methods to describe multiple interacting agents. It is an introduction to the method developed in Gosselin et al. (2017), illustrated by a basic economic application to a business cycle model. Our setup models individual, i.e., microeconomic interactions for a large number of agents in the context of statistical field theory which allows to recover a global, macroeconomic description of the system. In physics, the use of field theory to study dynamic models with large number of degrees of freedom in condensed matter has a long history. The field formulation provides a compact way to find the possible macroeconomic features emerging from the interactions of a large set of microeconomic structures. In economics, once adapted to account for standard microeconomic concepts such as preferences, constraints, rationality and economic interactions, the field formalism allows an analytical treatment of a broad class of models with an arbitrary number of agents. By combining statistical physics and economics, our field formalism provides an understanding of the transition from the individual to the collective scale. The microeconomic setup shapes the field model from which emerges a specific collective background or phase. The field description, in turn, describes the impact of this collective background on individual behaviors. This provides a back and forth interpretation between scales and accounts for the interplay between phases and the interactions at the microeconomic level. Additionally, field theory has the particularity to distinguish microscopic interactions that fade away at large scales from those that become predominant. Applied to economic systems, the relevance or irrelevance of some microeconomic concepts during the change of scale could indirectly shed some light on the aggregation problem of economic variables. The statistical approach of economic systems presented here is a two-step process. In a first step, the usual model of optimizing agents is replaced by a probabilistic description of the system. In an interacting system involving an arbitrary number of agents, each agent is described by an intertemporal utility function depending on an arbitrary number of variables. However each agent’s utility function is subject to unpredictable shocks. In such a setting, individual optimization problems are discarded. Each agent is described by a time-dependent probability distribution centered around this agent’s utility optimum. Unpredictable shocks deviate each agent from his optimal action, depending on each individual shock variance. When these variances are null, standard optimization results are recovered. This blurred behavior can be justified by the inherent complexity of agents: each period, their goals and behaviors can be modified by some internal, unobservable and individual shocks. This setup is a path integral formalism in the abstract space of agents’ economic variables, i.e., the state space. It is actually very similar to the statistical physics or quantum mechanics systems. This description is a good approximation of standard descriptions and allows to solve otherwise intractable problems. Compared to standard optimization techniques, such a description markedly eases the treatment of systems with a small number of agents. Working with a probability distribution is often easier than solving optimization equations. This approach is thus consistent and useful in itself. It provides an alternative to the standard modeling in the case of a small number of interacting agents. The average dynamics recovered is close and at times identical to the standard approach. It also allows to study the set of agents’ dynamics and its fluctuations under some external shocks. This formalism, useful for small sets, becomes intractable for a large number of agents. It can nonetheless be conveniently modified using methods of statistical field theory developed by Kleinert (1989), into another and more efficient description directly grounded on our initial path integral formalism. In a second step, therefore, the individual agents’ description is replaced by a model of field theory that replicates the properties of the system when N, the number of agents, is large. This modeling, although approximate, is compact enough to allow an analytical treatment of the system. A double transformation is thus performed with respect to the usual optimization models. The optimization problem is first replaced by a statistical system of N agents, that is then itself replaced by a specific field theory with a large number of degrees of freedom. This field theory does not represent an aggregation of microeconomic systems in the usual sense. It rather describes an environment of an infinite number of interacting agents, from which various phases or equilibria may be retrieved, as well as the behaviors of the agents, and the way they are influenced by, or interact, with their environment. This is the so-called phase transition of field theory: the configuration of the ground state represents an equilibrium for the whole set of agents, and shapes interactions and individual dynamics. Depending on the parameters of the system, the form of the ground state may change drastically the description at the individual level. It is thus possible to compare the particular features of the macroeconomic state of a system and those of the individual level. As such, it may confirm or invalidate some aspects of the representative agent models. To sum up, the advantages of statistical field theories are threefold. They allow, at least approximatively, to deal analytically with systems with large degrees of freedom, without reducing them to mere aggregates. They reveal features otherwise hidden in an aggregate context. Actually, they allow switching from micro- to macroeconomic description, and vice-versa, and to interpret one scale in the light of the other. Moreover, and relevantly for economic systems, these model may exhibit phase transition. Depending on the parameters of the model, the system may experience structural changes in behaviors, at the individual and collective scale. In that, they allow to consider the question of multiple equilibria in economics. Section 1 reviews the literature. Section 2 presents a probabilistic formalism for a system with N identical economic agents, interacting through mutual constraints. Section 3 introduces and discusses the associated field formalism for a large number of agents. In Sect. 4, we present an application of this formalism to a business cycle model. Section 5 concludes.",1
15.0,4.0,Journal of Economic Interaction and Coordination,17 February 2020,https://link.springer.com/article/10.1007/s11403-020-00281-7,Financial accumulation implies ever-increasing wealth inequality,October 2020,Yuri Biondi,Stefano Olla,,Male,Male,Unknown,Male,"Wealth inequality is an important matter for economic theory and policy. The recent rise in wealth inequality has been discussed in connection with the recent development of active global financial markets (Beck et al. 2007; Piketty 2013; Krugman 2013; Stiglitz 2012; Solow 2014). While some have argued for the role of financial development in reducing income inequality and benefiting the poorer, others criticise the increasing concentration of financial capital and related income as the main driver of rising inequality. Moreover, issues of wealth distribution were raised by the 99% movement in the USA following the Global Financial Crisis of 2007–2008. They have claimed that increased financialisation of the economy and society in recent decades has involved growing appropriation of wealth by the richest 1% of the population to the detriment of the remaining 99%, implying a more unequal and unfair wealth distribution. The existing literature on wealth distribution links wealth inequality to a variety of drivers (Bertola et al. 2006; Snowdon and Vane 2005). Already at the beginning of the twentieth century, pointing to wealth concentration in the economy and society, economist and sociologist V. Pareto suggested the so-called Pareto wealth distribution as an empirical regularity (Pareto 1897), while economic statistician C. Gini developed ingenious statistical techniques to represent wealth inequality through the so-called Gini Index (Gini 1912). In this context, a stream of the relevant literature draws upon Champernowne (1953) and Rutherford (1955) to develop an elegant formal modelling strategy that explains wealth concentration and the Pareto wealth distribution under conditions of financial market efficiency, involving stochastic distribution of financial returns across individuals investing in that market (Levy 2005; Levy and Levy 2003). This modelling strategy considers financial investment as a multiplicative process closely related to a Kesten process (Kesten 1973; Redner 1990). The existing literature and current debate on wealth inequality suggest some connection between wealth concentration and the financial investment process through active financial markets. Our approach involves disentangling the minimal components of this process in order to formalise its working and infer general results as to its impact on the economy and society. In particular, we identify two minimal institutions that surround individual investment strategies: compound return structure and active financial markets. Compound return structure is central to this financial investment process. It implies that individual investors keep reinvesting financial proceeds together with previous capital stock through time and circumstances. Meanwhile, active financial markets ensure that this financial investment process is played as a fair game, involving some market order over return-seeking individual strategies. In particular, under efficient financial markets, individual investors extract their realised returns from the same distribution, while individual results remain independent through space and time, preventing arbitrage opportunities (see Samuelson 1965, 1973; Fama 1995; Biondi and Righi 2017 for a further literature review). Our mathematical modelling strategy formalises the financial investment process in line with both minimal institutions, in order to study its relationship to relative wealth concentration across individuals and through time in the economy and society. In particular, we prove that, in the absence of countering forces, financial accumulation implies ever-increasing relative wealth inequality most of the time. We define financial accumulation as the peculiar financial investment process in which financial proceeds are systematically reinvested together with previous capital stock through time and circumstances. The rest of the article is organised as follows. The next section summarises our modelling strategy. Sections 3 and 4 provide mathematical proof of the relationship between wealth concentration and the financial investment process under compound and simple return structures, respectively. Section 5 develops some implications of and perspectives on our results. Section 6 concludes.",1
15.0,4.0,Journal of Economic Interaction and Coordination,04 March 2020,https://link.springer.com/article/10.1007/s11403-020-00282-6,An artificial Wicksell–Keynes economy integrating short-run business cycle and long-term cumulative trend,October 2020,Ichiro Takahashi,Isamu Okada,,Male,Male,Unknown,Male,"Macroeconomists have raised the thought-provoking question of whether increased price–wage rigidityFootnote 1 is stabilizing or destabilizing (Dutt 1986; De Vroey 2006).Footnote 2 The dominant view supported by mainstream macroeconomists (e.g., Modigliani 1944; Lucas 1978) is that large quantity adjustments, e.g., severe unemployment and its consequences, result from price–wage rigidity.Footnote 3 Conversely, wages fall so long as unemployment exists, which increases profits and thus employment “until all the unemployed are absorbed” (Lerner 1936, p. 38). In contrast, another view, the so-called Old Keynesian view, regards price–wage rigidity as a stabilizing factor, because it reduces fluctuations in aggregate nominal income and thus aggregate demand (Tobin 1975; Iwai 1981; Chiarella and Flaschel 2010).Footnote 4 This raises a question: How does the degree of price–wage rigidity affect the autonomous stability of a monetary economy? To answer this question, we must first address the question of whether a macroeconomy with a fractional-reserve banking system is inherently stable. This question of autonomous stability seems more essential regarding deflation than inflation for two reasons: (1) an expansionary process naturally comes to a halt when eventually faced with supply constraints [see, e.g., Chapter 21 of Robinson (1979)], and (2) conventional monetary policy seems ineffective once a commonplace, short-lived recession, often triggered by a bubble bursting, turns into persistent deflation. For these reasons, we address a fundamental question of a macroeconomics: Does a monetary economy have a self-correcting mechanism that can reverse a deflationary spiral? We also explore how the nominal rigidities of prices and wages are involved in the stabilizing mechanism. Hence, the main aim of this paper is to answer these questions by constructing and analyzing an agent-based macroeconomic model with demand-constrained firms that set prices and wages. Here, we explore macroeconomic instability caused by various positive feedback relationships between the determinants of aggregate demand. In particular, we focus on fluctuations in real aggregate demand that arise regardless of whether a financial system is robust.Footnote 5 Unquestionably, financial crises cause tremendous damage to real economies. It is also true, however, that a crisis is often preceded by a period of credit expansion (Kaminsky and Reinhart 2000), which is mostly an endogenous consequence of increased real demand (Chapter 9, Yoshikawa 1995). More specifically, we focus on the dynamics of real wages for the following reasons. First, real wages are a decisive determinant of consumer spending, which constitutes a significant part of GDP.Footnote 6 Second, changes in real wages influence labor demand. Third, real wages affect investment demand directly by affecting the cash flow of firms as well as the relative cost of capital stock.Footnote 7 Consequently, the speed of price–wage adjustment to supply–demand gaps, which determines how real wages move, is crucial in determining both aggregate demand and the profitability of employment as well as investment and thus the level of macroeconomic stability. Macroeconomic instability is a consequence of various positive feedback relationships. Wicksell (1936) observed the positive feedback dynamics among prices, investment, and credit creation, which is described as a cumulative process. In particular, when the credit supply is endogenously determined by a fractional-reserve banking system, the cumulative interdependence between investment and credit supply causes “inherent instability of credit” (Hawtrey 1962, pp. 166–174).Footnote 8 This cumulative process tends to be intensified by positive feedback relationships that Iwai (1981) found between prices and wages set by individual firms and their average levels. For example, deflation is difficult to stop deflation once it starts: In a deflationary period, any attempt by an individual firm to reduce its price below the average price to secure demand for its products is likely to be unsuccessful; rather, it will result in an even lower average price, because its competitors will also lower their prices. Conversely, prices (output prices, nominal wage rates, and interest rates) can counteract these feedback loops: Price adjustment is expected to stabilize an unstable economy by closing the supply–demand gaps generated by both by individual agents and by the overall economy. Old Keynesians are cautious about the positive feedback loop, whereas mainstream macroeconomists trust the stabilizing effect of prices. Accordingly, macroeconomic stability would ultimately seem to depend on which force dominates: the disequilibrating effect of positive feedback loops, or the equilibrating effect of price adjustments. We thus constructed an agent-based macroeconomic model that captures these two opposing effects in an attempt to elucidate an elementary mechanism operating in the complex interactions among agents. We consider the agent-based approachFootnote 9 to be a natural methodology choice, because the autonomous stability of an economy has a self-emerging nature (Kirman and Kirman 1992; Fagiolo and Roventini 2017).Footnote 10 Delli Gatti et al. (2011) elaborated that macroeconomic outcomes should be explained as “emerging from the continuous adaptive dispersed interactions of a multitude of autonomous, heterogeneous and bounded rational agents” (p. vii, emphasis in original). Macroeconomic research using the agent-based approach has been vibrant (e.g., Dosi et al. 2010, 2019; Dawid et al. 2012, 2018). Delli Gatti et al. (2005, 2007, 2008) explained that complex interactions between firms and the banking system give rise to financial fragility. Delli Gatti et al. (2006, 2009, 2010) demonstrated that a default by one agent can generate a bankruptcy crisis throughout a network. Lengnick and Wohltmann (2013) combined an agent-based model of financial markets and a New Keynesian macroeconomic model with learning agents to answer questions on international trade policy. Our virtual experiments show that an economy is stabilized when prices and wages are both moderately rigid. Taylor (1986), Yoshikawa (1995), and De Long and Summers (1986a, b) suggested that reduced flexibility of prices and wages should lead to improved economic performance. Our simulation results are reasonably consistent with those empirical findings. Moreover, in a stable economy, both short-run business cycles and long-run fluctuations emerge. Given the powerful positive feedback loops mentioned above, it seems difficult, without some counteractive policy intervention, to reverse a downward process once it starts. Interestingly, however, our simulated economy often times exhibits a remarkable resilience: It gets back on a growth track and achieves full or nearly full employment by reversing a persistent deflationary spiral that occurs in the initial adjustment period. Close examination of the simulated behaviors of key macro-variables reveals the following key condition for stability: A slow reduction in real wages during a demand-sufficient economy produces the above resilience. When a majority of firms follows the “price-marginal cost principle” without being constrained by a demand condition (Negishi 1979), firms tend to reduce their real wages by raising their commodity prices and simultaneously increase employment, thus increasing the aggregate income and demand. Therefore, to gain autonomous stability, an ample number of demand-sufficient periods must emerge during deflation, so that the employment expansion in each of these periods successively eliminates the unemployment accumulated over time. The simulation study demonstrated that an economy with highly flexible nominal wages fails to satisfy the above stabilizing condition and causes a destabilizing positive feedback loop between falling real wages and aggregate demand. This is because, for a given level of unemployment, a substantial reduction in wages will decrease employment, thus reducing real labor income so that the amount of employment is more likely to be demand constrained and smaller than that in the previous period. With a similar mechanism, excessive flexibility in prices—by causing a large reduction in real wages in a demand-sufficient period—also tends to induce a demand-deficient economy prematurely, i.e., before sufficiently reducing unemployment. Thus, highly flexible prices and wages destabilize an economy. These results are largely consistent with the Old Keynesian view. Conversely, extreme nominal rigidities are also destabilizing. Excessive wage rigidity prevents the downward adjustments of real wages that are required for investment to gain profits, thus keeping investment inactive. Similarly, excessive price rigidity keeps price levels from rising during a demand-sufficient period. This failure in reducing real wages prevents both labor demand and investment demand from increasing. Thus, excessive nominal rigidity in prices and wages decreases both the aggregate demand and productive capacity over a long period of time. This poor performance due to a lack of price adjustment supports the mainstream view. Our model is completely closed and absolutely free from any external shocks. It incorporates three elements: (1) firms setting prices and wages, (2) debt-financed investment, and (3) endogenous credit supply. The first and third elements give the model a Wicksellian flavor without resorting to the Taylor rule.Footnote 11 The second element, together with demand deficiency per Negishi, gives a Keynesian–Minskian flavor to the model, because firms make their investment decisions based on expected future profits (Minsky 1975; Negishi 1979). Investment in our model involves a gestation period, which naturally induces alternating seller’s markets and buyer’s markets, thereby inducing business cycles (Robertson 1915). In addition, the model assumes that firms adaptively form expectations about output prices, wages, and interest rates, as well as demand for their products.Footnote 12 The rest of this paper is structured as follows. Section 2 describes the model and our basic assumptions. Section 3 explains the simulation setting. Section  4 shows the results obtained from the simulation. Finally, Sect. 5 presents and discusses the underlying mechanism.",3
16.0,1.0,Journal of Economic Interaction and Coordination,21 February 2020,https://link.springer.com/article/10.1007/s11403-020-00283-5,A model of market making with heterogeneous speculators,January 2021,Leonardo Bargigli,,,Male,Unknown,Unknown,Male,"Models of financial markets with heterogeneous bounded rational speculators generally come in two flavors. The seminal model of Brock and Hommes (1998) makes the standard assumption of market clearing. Other models assume instead the existence of a market maker, which accumulates inventories and adjusts the price in the same direction of net demand following a simple linear rule (Day and Huang 1990; Lux 1995). The market maker hypothesis is widely considered a better description of the price adjustment mechanism of actual markets. In particular, it is the standard assumption for models of the FX market (Westerhoff 2009). On the other hand, the standard linear adjustment rule, which is generally used in these models, might lead to large inventory unbalances. This implication is inconvenient, since the empirical evidence shows that FX dealers manage actively their inventories in order to end the trading day on a balanced position (Manaster and Mann 1996; Bjønnes and Rime 2005). To overcome this limitation, Westerhoff (2003) incorporates inventory management in the price adjustment rule, showing that a more aggressive control of inventories makes the market more volatile and less stable. Equivalent results are obtained by Carraro and Ricchiuti (2015) and Zhu et al. (2009), who consider market makers that take speculative positions by adjusting their inventories toward a given exogenous target. These models don’t derive the pricing rule from an optimizing behavior of the market maker, instead they rely on ad hoc pricing mechanisms. Following a similar path, Hommes et al. (2005) extend the BH98 framework to a market maker scenario, using a linear adjustment rule, and prove that the dynamic behavior of the system is pretty similar to BH98. Anufriev and Panchenko (2009) compare through simulations different market protocols (Walrasian auctioneer, market maker, batch auction and order book), allowing for the endogenous evolution of the proportions of speculators. They show that, no matter which type of market clearing is used, two different regimes with completely different dynamical properties occur depending on the value of the intensity of choice, and that the trading protocol strongly affects the critical value of the intensity of choice. This paper presents a model which incorporates a more sophisticated representation of the behavior of the market maker than the current literature. In particular, I suppose that there are two types of speculators (fundamentalists and chartists) who submit their trades to a monopolistic market maker who is an optimizing bounded rational agent. I further suppose that the market maker knows the optimal demand of speculators, in accordance with the evidence that market makers profit from their knowledge of the market (see Sect. 2). In the first version, profits come only from market making, while in the second version the market maker might trade on her own account with bounded rational expectations regarding the future price. My model confirms the main result of Hommes et al. (2005) while, on the other hand, it shows that the market maker has conflicting effects on the stability of the market. From this perspective, my results converge with those of Zhu et al. (2009) and Carraro and Ricchiuti (2015), who also underline that the market maker destabilizes the market when she manages aggressively her inventory. On the other hand, these works provide analytical results which are restricted to the case of fixed proportions of speculators, while my results are derived using the heuristic switching mechanism of BH98 which allows these proportions to evolve endogenously. Moreover, since I derive the market clearing case of BH98 as a special case, I can prove analytically that the trading protocol affects the critical values of the intensity of choice, thus strengthening the results of Anufriev and Panchenko (2009). The remaining of this paper is organized as follows. In Sect. 2, I relate the hypotheses of this paper to the literature on the microstructure of FX markets. In Sect. 3, I present the model with a “pure” market maker, and in Sect. 4 the model with an “activist” market maker. Section 5 concludes.",
16.0,1.0,Journal of Economic Interaction and Coordination,25 February 2020,https://link.springer.com/article/10.1007/s11403-020-00284-4,Predator–prey model for stock market fluctuations,January 2021,Miquel Montero,,,Male,Unknown,Unknown,Male,"Financial models based on interacting agents possess a large tradition in the economic literature (Hommes 2006)—one of the first references in which the evolution of a market is related to the activity of individual investors dates back to 1974 (Zeeman 1974)—but they have gained relevance in the interdisciplinary literature in relatively recent years (De Martino and Marsili 2006; Samanidou et al. 2007). The complete list of such models is so extensive and their properties so diverse that we can merely sketch here the recurrent traits shared by most of the models, and address the reader to the references cited in Hommes (2006), De Martino and Marsili (2006) and Samanidou et al. (2007). The pioneering work of Zeeman (1974) already contains one of the more ubiquitous ingredients in the subsequent agent-based models: heterogeneity. Agents are assumed to be heterogeneous to some extend, and therefore, they can be aggregated into one out of a finite set of categories. Since the minimum number of different categories is two, and simplicity is often a plus, investors are usually arranged into two (competing) groups. The terms used to name them and their defining properties are uneven across the literature—chartists and fundamentalists in Zeeman (1974), trend followers and contrarians in De Martino et al. (2004), speculators and producers in Zhang (1999), imitators and optimizers in Conlisk (1980)—but the underlying ideas are similar, and can be well represented by chartists and fundamentalists. Chartists are (sometimes adaptive) agents whose investment strategy is based on the belief that past information may contain clues about the future evolution of the security and, therefore, that they can infer future prices. Fundamentalists are in essence agents who think they can deduce the present value of a firm on the basis of the information currently available, such as dividend payments or earning rates. Fundamentalists operate in a rather predictable way since they expect the market to correct any observed deviation between fundamental and market prices: They sell overpriced securities and buy underpriced ones. The picture is not so simple for chartist-like investors since at the end they deploy rule-of-thumb strategies, sometimes based on market indicators like the moving average convergence–divergence (MACD) indicator or the relative strength index (RSI), two tools of technical analysis broadly used by actual financial practitioners. Therefore, the list of available strategies in agent-based models may be so large that, in the most extreme situation, strategies may differ for any pair of investors in the market, as in some instances of the minority game market model (Challet and Zhang 1997; De Martino and Marsili 2006). In fact, any single agent may combine technical trading rules with fundamental ones, or decide among them, what makes evolve the profile of the investors. There is no doubt that this diversity adds more heterogeneity into the model. Another general trait of current models is that agents have bounded rationality (Simon 1979): They decide their actions in the next time step on the basis of a limited and possibly incomplete amount of information. They ignore the beliefs of the rest of investors and usually cannot evaluate the consequences of their own decisions. Under these circumstances, selfish agents try to maximize a payoff or utility function, a measure of their individual success. The final ingredient is the pricing mechanism. The usual paradigm when the activity of the agents does not explicitly set the price of the asset is to define a differential equation or a finite difference equation that relates the price evolution to the relevant (global) variables of the model. Since these variables are affected by the mutual interaction of the investors in a complex way, two complementary approaches are generally considered: The behavior of the system is computer simulated and/or the complexity is reduced by considering that the number of agents approaches to infinity, the thermodynamic limit. As we will shortly show, some of the previous ingredients either are not present in this agent-based model or have been introduced with a different philosophy. The model was inspired by a previous article on population dynamics by McKane and Newman (2005), where the authors reported the presence of large oscillations in the species densities due to a finite size stochastic effect. We export this idea into the financial language with the development of a model that describes general aspects of investor dynamics. The behavior of the asset price is first obtained by assuming a simple model of excess demand, and subsequently, we follow the same approach to model the interplay between limit and market orders in a stock market. The overall result exhibits similarities with prevailing agent-based financial models (see, e.g., Beja and Goldman 1980; Kirman 1993; Lux and Marchesi 1999; Cont and Bouchaud 2000; Challet et al. 2001). The paper is structured in three main sections. Section 2 deals with the agent model strictly speaking: First, we define who are the agents and the three different states in which they can be found at every moment, the mechanisms that govern the changes from one state to the other and the transition rates between states. Then we derive a master equation that characterizes the time evolution of the system, and analyze the stationary solutions of this equation in the thermodynamic limit. Finally, we find the second-order corrections and show their relevance in finite size models. In Sect. 3, we establish a first connection between the agent model and market price changes: We simulate the time evolution of the system under representative market conditions, analyze the most relevant traits and compare them with well-known empirical properties of actual financial time series. In Sect. 4, we propose a second identification for the species categories (liquidity providers and liquidity takers), and a different price formation procedure is considered. The outcome presents new properties that are still consistent with what one may find in practice. This reinforces the potentials of the model. Conclusions are drawn in Sect. 5, and some technical aspects are left to the appendices.",
16.0,1.0,Journal of Economic Interaction and Coordination,07 March 2020,https://link.springer.com/article/10.1007/s11403-020-00285-3,Firms in financial distress: evidence from inter-firm payment networks with volatility driven by ‘animal spirits’,January 2021,Rémi Stellian,Gabriel I. Penagos,Jenny P. Danna-Buitrago,Male,Male,Female,Mix,,
16.0,1.0,Journal of Economic Interaction and Coordination,23 March 2020,https://link.springer.com/article/10.1007/s11403-020-00286-2,Quantifying the importance of different contagion channels as sources of systemic risk,January 2021,Christoph Siebenbrunner,,,Male,Unknown,Unknown,Male,"The financial crisis which started in 2008, in particular the events following the collapse of the investment bank Lehman Brothers, has shown that interconnectivity creates risks for financial stability. In today’s highly interconnected financial systems, shocks can spread from single entities to the entire system. While such interconnections may serve as a technique for spreading risk, they can also lead to contagion effects that threaten the stability of the financial system. Such contagion effects may occur via different channels. In this paper, I will study the channels of direct contagion and overlapping portfolios in greater detail. The goal of this paper is to provide a quantitative framework for computing systemic contagion losses in a financial system, taking into account multiple channels of contagion. This is important since stress tests, the most prominent form of financial stability analyses, still do not account for such systemic feedback effects. Stress tests have become an important tool for central bank policy makers and supervisors when assessing the capital adequacy of major banks. Major international exercises by the Federal Reserve (DFAST 2019), the European Banking Authority (EBA 2019) and the Bank of England (BoE 2019), however, still conduct stress tests as a mainly ‘microprudential’ exercise. Under this approach, the financial health of several banks under a stress scenario is assessed by looking at the impact of a stress scenario on each bank in isolation. Microprudential stress tests have proven to be very useful tools in navigating the financial crisis. But they are not designed to identify risks which stem from interconnections within the banking system. The goal of this paper is to provide a framework that allows computing contagion losses that are consistent with a macroeconomic stress scenario. The output of the model presented in this paper can thus be used as a ‘macroprudential’ extension module for a microprudential stress test that accounts for systemic feedback effects stemming from the first-round macroeconomic shock to the financial system. Early applications of such a model with only one contagion channel in a stress test can be found in Elsinger et al. (2006) and Feldkircher et al. (2013). The model presented in this paper significantly extends this approach to include multiple contagion channels. I will further present a framework for ranking the importance of different contagion channels. This is useful for policymakers when defining regulations aimed at reducing systemic risk, such as the the capital buffer requirement for systemically important institutions (EBA 2014). Being able to quantify through which channels a given bank contributes to systemic risk allows for better targeted regulations.",4
16.0,1.0,Journal of Economic Interaction and Coordination,06 April 2020,https://link.springer.com/article/10.1007/s11403-020-00287-1,Productivity and unemployment: an ABM approach,January 2021,Carlos M. Fernández-Márquez,Matías Fuentes,Francisco J. Vázquez,Male,Male,Male,Male,"Labour productivity is a generally accepted measure of economic efficiency; therefore, it is considered a usual performance indicator for an economy (Mas-Colell et al. 1995). It measures how efficiently labour input is combined with other factors of production, and it depends not only on the personal capacities of workers or the intensity of their effort but also to a large degree on the presence and/or use of other inputs (e.g. capital, intermediate inputs, technical change and economies of scale). The impact of labour productivity on economic growth and social development has been widely studied (see Atolia et al. 2012, for a review). However, little attention has been paid to the effect that increases in productivity have on employment, presumably due to difficulties in identifying such a causal link. In fact, as shown below, a simple scatter graph of real dataFootnote 1 allows us to illustrate this uncertain influence. Complications derived from possible countervailing dynamics between employment and productivity were recognized by the International Labour Organization (ILO) as far back as its Employment Policy Convention (No. 122, 1964). This caused the ILO to aim for “full and productive employment”. Figure 1 shows the plot of the unemployment rate versus labour productivity for three countries: Australia, Germany and Japan. Although spurious correlations could be present, meaningful fluctuations clearly appear in all three cases, which might suggest that small increases in labour productivity can produce both upward and downward swings in the unemployment rate. Similar oscillating patterns are present to a greater or lesser degree in all OECD countries.Footnote 2 Furthermore, the significant relative size of the fluctuations can hardly be attributed to random events. As shown in Fig. 2, countries in the second and third quartiles (Q2 and Q3) have high residual deviations, ranging from 16 to 25%. Unemployment rate versus labour productivity Relative deviations from linear regression (the solid line represents the average, and the dashed lines show the first and third quartiles) Economic theory provides us with different explanations of how the unemployment rate may be affected by a sudden increase or fall in the productivity growth rate associated with technological developments. In the next section, we discuss some of the mainstream arguments related to an array of channels: employment search (Pissarides 2000), the creation–destruction of sectors (Aghion and Howitt 1992), technological uncertainty (Manuelli 2000), the wealth and income distributions (Atolia et al. 2012), worker effort (Leibenstein 1966, 1975, 1978a, b, 1979; Stigler 1976) and psychological damage (Darity and Goldsmith 1996; Goldsmith et al. 1996). However, none of these proposed channels seem to enlighten us properly about the observed fluctuations, probably due to the complexity of the phenomenon (where both institutions and decisions of agents are involved), which makes it difficult to model a relationship directly between employment and productivity. In this work, we address the uncertain linkage between productivity and employment in an attempt to discover new theoretical explanations within a framework of agent-based modelling (ABM). Agent-based models are useful for simulating social interactions within complex structures with a view to detecting collective behaviour. It is in these computational models that emerging properties and novelties appear, and so such models can be helpful for our purpose here, which also has practical implications. In particular, if our analysis detects nonlinear, oscillating patterns, economic policies aimed at increasing labour productivity could have either positive or negative side effects on the unemployment rate, so further exploration should be carried out before such policies are implemented to determine whether we are facing a positive or negative correlation stage. The paper is structured as follows. In Sect. 2, we summarize the main theoretical explanations for the employment–productivity linkage and justify the use of agent-based modelling. The specific agent-based framework (introduced in Riccetti et al. 2015) is described in Sect. 3. In Sect. 4, we present the results of our simulations, which show a marked fluctuating, nonlinear pattern. Section 5 focuses on providing an empirical validation of this pattern. Finally, we summarize our findings in Sect. 6.",
16.0,1.0,Journal of Economic Interaction and Coordination,08 July 2020,https://link.springer.com/article/10.1007/s11403-020-00288-0,Collective rationality and functional wisdom of the crowd in far-from-rational institutional investors,January 2021,Kevin Primicerio,Damien Challet,Stanislao Gualdi,Male,Male,Male,Male,"The collective ability of a non-expert crowd to accurately estimate an unknown quantity is known as the “Wisdom of the Crowd” (Surowiecki 2005) (WoC thereafter). In many situations, the median or average estimate of a group of unrelated individuals is surprisingly close to the true value, sometimes significantly better than those of experts (Galton 1907; Hill and Ready-Campbell 2011; Landemore 2012; Nofer and Hinz 2014). Understanding how, why and when WoC works is thus an important research topic. By far the most important condition under which WoC may emerge is diversity (Hong and Page 2004; Surowiecki 2005; Davis-Stober et al. 2014). On the other hand, social imitation is detrimental as herding may significantly bias the collective estimate (Lorenz et al. 2011; Muchnik et al. 2013), while averaging several guesses of each individual (known as the crowd within) may lead to sharper estimations (Vul and Pashler 2008; Steegen et al. 2014). In many economic systems, human beings must act according to their estimates of some quantities (such as the value of an item, of retirement plans, etc.), often each in their own way, with their own tools, experience and knowledge. Mainstream Economics takes a radical theoretical short-cut by assuming perfect individual rationality, i.e. that agents’ estimates are perfect, hence the same ones. This is the so-called the representative agent approach (Hartley and Hartley 2002). An alternative view is that the economic decisions of a population of economic agents are more complex, interdependent and prone to fail to converge to the rational choice, except in fortunate situations, such as WoC. The latter is a consistent aggregation of possibly inconsistent individual estimates (Hogarth 1978), which we call collective rationality. Determining the conditions under which the aggregation of quite diverse individual actions, especially in a dynamic context where expectations are continuously revised, is still an open problem (Kirman 1992). That said, since the mean absolute error of the average estimate is proportional to that of the estimates among a population, one should not forget about agent heterogeneity. For the sake of simplicity, wisdom of crowd is usually restricted to the collective estimation of a single, scalar quantity. This implicitly assumes that the agents, in addition to their heterogeneous way to compute estimates, is homogeneous in an essential way, i.e. that all its agents face the same choice with the same essential constraints, aims, or lack thereof. However, as soon that the agents have different essential constraints, i.e. constraints that lead to different correct choices, the presence of WoC should be judged according to these constraints. In the simplest case, there are G essentially different constraints, i.e. groups, each of trying to estimate a different value. By extension, WoC can hold in a continuous way in a given population, i.e. for functional relationships. For example, Haerdle and Kirman analyse the prices and volume of many transactions in Marseille fish market: while a scatter plot of these two quantities is rather noisy, the market self-organizes so that when more fish are sold, prices are lower, as revealed by a local average (Härdle and Kirman 1995). More generically, many simple relationships found in Economics textbooks may only hold approximately on average if the system is stationary enough, but not for each agent or each transaction and at each time Finance is an interesting candidate for WoC, because their competitive nature has two important consequences which are known to be two pre-conditions of existence of WoC (Hong and Page 2004): competition forces market participants to be heterogeneous (Arthur 1994), which ensures a large diversity amongst them, and it is a strong incentive for market participants not to behave randomly and to try to do their best; some of them even try to build optimal portfolios from incomplete information. Asset price efficiency is an obvious instance of WoC in Finance: it asserts that the current price of an asset, determined by the actions of many traders, is the best possible estimate and fully reflect all available information on average (Malkiel and Fama 1970; Malkiel 2003; Fama 1998). This is yet another example of WoC assessed by a single number. However, more complex relationships in finance may be related to rational benchmarks. Since large funds do perform portfolio optimization according to their own cost functions, we shall focus on their portfolio structure. Diversity of opinions, a crucial requirement for WoC, is also related to the fact that market participants have diverse constraints, some of them externally imposed (laws, regulations, exchange rules, etc.) and self-imposed (asset classes, fund prospectus, computational and mathematical methods, performance and risk objectives, personal biases, performance benchmarks, etc.). One of the striking aspects of WoC is that a group of non-experts is able to produce accurate average estimates. As we analyse portfolios of large investment funds, it may seem rather odd to assume that such market participants are non-experts in portfolio optimization. Most of them certainly believe to be experts, but as we shall see, the staggering amount of variability of the decisions among funds is most probably not uniquely caused by expertise. Judging how optimal the portfolios of investors are with respect to their own optimization functions or to their operational constraints is impossible. We circumvent this obstacle by measuring the relationship between the amount under management and the number of assets, which is not directly optimized by any common portfolio theory, but whose optimal relationship is given in Ref. de Lachapelle and Challet (2010). As even today only a negligible fraction of fund managers is likely to ever have come across this work since its publication, we can safely assume that the agents are not expert with respect to this aspect of portfolio optimization. There is more: this optimal relation involves two parameters: one of them depends on the particular details of risk optimization, hence is fund-specific, while the other one depends on the transaction cost structure (see below) which is way more likely to be common to all the funds: for obvious reasons, we focus on the latter. Our findings imply first that the concept of WoC holds on a more generic level, which has profound implications for the actual role of rationality in collective economics outcomes: on the one hand, they suggest that collective rationality is a useful benchmark when judging a collective outcome, on the other hand our results clearly illustrate that individual rationality is not only unneeded, but also a fairy tale when making complex decisions with imperfect tools. Finally, rationality is only a benchmark regarding the average but forgoes the tremendous variability of decisions, which is a dangerous (and naive) simplification. Another important result comes from the respective importance of two types of constraints. On the one hand, as explained above, institutional investors face many regulatory, legal, and self-inflicted constraints, whose diversity explains in part the typical large individual deviations from the rational benchmark; however, these constraints are not essential and thus have no effect on the ability of the population to approximate the rational benchmark. On the other hand, liquidity constraints are essential: as it is not possible to invest too much in assets that are not frequently traded without a sizeable impact, funds must invest into many more assets than the rational benchmark suggests. We therefore propose a minimal model of how these funds manage to alleviate this essential constraint.",
16.0,1.0,Journal of Economic Interaction and Coordination,24 June 2020,https://link.springer.com/article/10.1007/s11403-020-00289-z,Bayesian estimation and likelihood-based comparison of agent-based volatility models,January 2021,Nils Bertschinger,Iurii Mozzhorin,,Male,Male,Unknown,Male,"Financial markets exhibit some remarkable and often surprisingly stable statistical signatures, often referred to as stylized facts (Cont 2001; Lux 2009). Most notable and researched are the properties of asset price returns exhibiting fat-tailed distributions and volatility clustering. Volatility in particular has received much attention in the econophysics community for its autocorrelation decaying as a power law suggesting a long-memory process. Volatility also plays a prominent role in econometrics, risk management and finance. Correspondingly, phenomenological statistical models such as GARCH (Bollerslev 1986) and stochastic volatility models (Kim et al. 1998) are extensively studied and routinely fitted to market data. Agent-based models consider the statistical signatures of financial markets as emergent properties, i.e., arising from the collective actions of many interacting traders. They provide a complement to standard economic models, which, presuming rational actors, are often unable to explain the rapid changes in volatility between calm market phases and highly volatile episodes. Shiller has coined the term excess volatility, hinting at these shortcomings (Shiller 1980). In contrast, agent-based models allow for bounded rational actors and can often reproduce the stylized facts presuming chartist trading and/or herding behavior (Samanidou et al. 2007). Agent-based models of speculative behavior in financial markets are nowadays able to replicate many stylized facts simultaneously. They provide an alternative to standard econometric models, offering behavioral explanations of observed market statistics (Lux 2009; LeBaron 2000). Yet, estimation of such models is still challenging and has mostly resorted to simulation-based methods striving to match selected moments of the data (Franke and Westerhoff 2011; Ghonghadze and Lux 2016). More recently, direct comparisons between the probabilistic dynamics of simulated and observed time series have been proposed. To this end, transition probabilities of return time series, observed and simulated, are discretized and compared in terms of context-tree weighted Markov approximations (Barde 2016, 2017) or JS divergence (Lamperti 2018). Alternatively, predictive likelihoods are estimated on the continuous time series of returns via (Guerini and Moneta 2017) fitting a VAR model or (Kukacka and Barunik 2017) kernel density estimation and again compared and matched with model predictions. These methods go beyond moment matching and allow to generate and evaluate model predictions. They still fall short to recover latent dynamical states as estimates are not conditional to data but merely matched to their probabilistic structure. In order to recover latent dynamics, either maximum likelihood estimation or Bayesian methods are required. Estimating agent-based models in this fashion is challenging as their dynamics are often highly nonlinear. Yet recently, sequential Monte Carlo methods (Lux 2018) and the unscented Kalman filter (Majewski et al. 2018) have been successfully used in this context. Here, we follow this line of research and utilize modern software tools from machine learning and statistics to fit agent-based market models. In particular, we employ Stan (2017), a probabilistic programming language for Bayesian modeling, to fit several different agent-based models, namely from Vikram & Sinha (2011), Franke & Westerhoff (2012) and Alfarano, Lux & Wagner (2008). We believe that Bayesian estimation has many advantages as it allows to access parameter uncertainties as well as to generate model predictions. Furthermore, being based on the full model probability, including the likelihood, different models can be systematically compared, e.g., based on their predictive likelihood on held-out data. Indeed, for similar reasons Bayesian estimation is popular in macroeconomics for estimating classical DSGE (An and Schorfheide 2007) as well as agent-based models (Grazzini et al. 2017). Overall, our contribution is threefold: First, we discuss several agent-based models and the behavioral assumptions they are based on. In particular, this includes the model by Vikram & Sinha, which had not been fitted before, as well as a novel moving average specification for the model of Franke & Westerhoff. Secondly, we implement all models in Stan, a modern probabilistic programming language for Bayesian modeling. Thirdly, we provide a detailed pairwise comparison of all models based on cross-validated predictive likelihoods. In particular, we find that the best agent-based models are competitive with standard econometric models. While this has been observed previously for the Franke & Westerhoff model (Barde 2016), we provide evidence that other herding dynamics give comparable results provided that the model allows for persistent mispricing between fundamental and observed prices. Our presentation is structured as follows: In Sect. 2, we introduce Bayesian data modeling and Markov chain Monte Carlo (MCMC) algorithms. In particular, we shortly explain Hamiltonian Monte Carlo (HMC) and how it is implemented in Stan. Then, in Sect. 3 we introduce all considered models and express them as probabilistic models for return data. Our results on simulated as well as actual S&P 500 stock return data are summarized in Sect. 4. Finally, we conclude by discussing our main findings in Sect. 5.",11
16.0,1.0,Journal of Economic Interaction and Coordination,30 June 2020,https://link.springer.com/article/10.1007/s11403-020-00291-5,Risk sharing and financial stability: a welfare analysis,January 2021,Weijia Wang,Shaoan Huang,,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Journal of Economic Interaction and Coordination,26 June 2020,https://link.springer.com/article/10.1007/s11403-020-00292-4,"Hand-to-mouth consumers, rule-of-thumb savers, and optimal control",April 2021,Orlando Gomes,,,Male,Unknown,Unknown,Male,"Economic agents are decision-makers. For each decision an agent takes, there is a deliberative process that involves a series of relevant dimensions: information availability, cognitive capabilities, personality traits, degree of engagement, and so forth. Typical economic analysis assumes homogeneity along the mentioned dimensions by placing individuals at an extreme position, namely the position where everyone is fully endowed with information, cognitive skills and will to arrive to the perfect decision. This is the optimal planning approach, under which agents are fully rational and the dynamics of the economy are reduced to the deliberative process of a single representative agent.Footnote 1 The representative agent framework is convenient from the analytical standpoint, but it conducts to an incomplete and partial view about economic behavior. To enhance and widen economic research, one must recognize the persistence of heterogeneity: while the behavior of some, in some occasions, may approach the optimal planning prototype, there are individuals and entities that, for several reasons, do not conceive and implement optimal plans and choose, instead, to adopt simple heuristics or rules-of-thumb to govern their decision-making processes. As the literature review in Sect. 2 will suggest, there is today a pressing need to change the way in which aggregate economic behavior is scrutinized and studied. The shift goes in the direction of attributing a prominent role to the sources and implications of agent heterogeneity. A strand of literature going in this direction is associated with the emergent research on agent-based macroeconomics. Supporters of the agent-based perspective share the belief that macroeconomics should evolve to be a science where the interaction among heterogeneous agents becomes a pervasive trivial feature (Ballot et al. 2015; Dosi et al. 2017; Gualdi et al. 2017; Guerini et al. 2018; Dawid et al. 2019; Dosi and Roventini 2019). Furthermore, agent-based economics has been the stage for a fruitful discussion about the use of simple rules-of-thumb or heuristics in deliberative processes. Haldane and Turrell (2018, 2019) emphasize the relevance of heuristics, however recognizing that it is a difficult task to set up a general and pervasively accepted framework of analysis to replace the strict optimality benchmark. For instance, these authors list a series of consumption functions that have been employed to replace the outcome of the typical dynamic optimal choice problem; the long extension of the list is symptomatic of how hard it is to reach a consensus on which are the rules-of-thumb economists should use to best describe the actual decisions of agents. Beyond the contribution of agent-based modeling, although at a slower pace and with a less disruptive intent, mainstream standard macro-literature is also making its transition toward heterogeneity and the use of simple decision-making rules. Some notable macro-models, namely the new Keynesian model, have embarked on this path. In two pioneering studies, Gali et al. (2004, 2007) have approached the impact of consumer heterogeneity over monetary and fiscal policies. These studies have proposed models highlighting the coexistence in the economy of two classes of agents: those who make plans and those who live hand-to-mouth. The two classes have acquired the designations of Ricardian consumers (the optimal planners, for whom the Ricardian equivalence holds) and Non-Ricardian consumers (who are the hand-to-mouth and, thus, ignore intertemporal trade-offs). The findings of this research have triggered an interesting debate concerning the determinacy of interest rate rules and the effectiveness of monetary policy (Amato and Laubach 2003; Colciago 2011; Di Bartolomeo et al. 2011), and about public finance and debt sustainability (Natvik 2012; Cho and Kim 2013; Rossi 2014; Bilbiie 2017). Kaplan et al. (2018) extended further the analysis of macroeconomic new Keynesian models under agent heterogeneity, by complementing the two-agent framework with a setting that allows for the simultaneous presence in the economy of a wide range of distinct household consumption and savings behavior; these authors called their model HANK (heterogeneous agents New Keynesian model), in opposition to the TANK (two-agent New Keynesian model) and the RANK (representative-agent New Keynesian model).Footnote 2 One area of macroeconomic thought in which the implications of heterogeneity and rule-of-thumb decisions, in the terms mentioned in the above paragraphs, have not yet been consistently and systematically explored, is optimal growth theory. Therefore, the aim of the current study is to engage in a systematic evaluation of consumption-savings decisions, in the context of a typical optimal control Ramsey growth model, taking as central assumption behavioral heterogeneity. While a share of the agents eventually sticks with intertemporal planning, others follow different forms of non-optimal behavior, based in simple heuristics. Four models are explored, in which optimal planners, hand-to-mouth consumers and rule-of-thumb savers coexist. Optimal planners choose a finite degree of impatience to govern their intertemporal decisions and maximize consumption utility, hand-to-mouth consumers are the agents who adopt an infinite discount rate and, thus, only current consumption matters to them, and rule-of-thumb savers select some ad-hoc savings rate, hence behaving as in a straightforward Solow-like growth setting. The investigation of behavioral heterogeneity in standard optimal growth models has been approached previously in the economics literature, mainly through the contribution of Krusell and Smith (1996). These authors explicitly add to the typical neoclassical growth model a cost of optimization, i.e., a cost associated with processing, computing and implementing optimal plans. In such a setting, rule-of-thumb behavior endogenously emerges for those agents who exhibit a lower capacity to face the mentioned costs. Sophisticated agents who optimize and unsophisticated agents who opt for adopting a constant savings rate will coexist in the long-run steady-state, where every agent has to weigh utility losses of not optimizing against the costs of formulating optimal plans and behaving accordingly. While the confrontation between optimal planners and rule-of-thumb decision-makers is a fundamental piece of the analysis to be pursued in this study, the study has a much broader scope, approaching many issues that the Krusell–Smith growth model overlooks: in a systematic and pervasive way, and relying on a same simple growth framework, this research puts into confrontation various kinds of agents and a variety of possible interactions. The protagonists are not only Ramsey-like agents and Solow-like agents who, respectively, select and adopt endogenous and exogenous savings rates, but also different kinds of hand-to-mouth agents (including, following the terminology by Kaplan et al. (2014), the wealthy hand-to-mouth). Moreover, heterogeneity becomes pervasive as one considers the possibility of coexistence of distinct rates of time preference (in the case of optimal planners) and the possibility of coexistence of distinct savings rates (for rule-of-thumb savers). A common feature of the frameworks to be assessed is that, while agents are heterogeneous regarding their consumption-savings behavior, they are identical when it comes to their participation in the productive process. All agents contribute in the same way to production, supplying their labor force and their capital assets. They share an identical labor productivity, and therefore earn a same wage, and there is a unique capital good, that provides an identical return for anyone that holds it. Decision-making heterogeneity, though, makes agents to accumulate capital at different rates (or not to accumulate it at all) leading to long-term steady-state outcomes that differ from the benchmark case and that can only be unveiled and understood under the heterogeneity assumption. It is this variety of outcomes that one scrutinizes in the sections that follow, thus highlighting the relevance of heterogeneity and non-optimal behavior in shaping aggregate economic growth performance. Most of the analysis throughout the paper concentrates on the evaluation of long-term outcomes, comparing steady-state consumption results of optimizers and non-optimizers. Although an analysis of transitional dynamics is, in some cases, pursued, there is no systematic evaluation and comparison of intertemporal utility across the various groups of agents. This is because the intertemporal utility outcome is, by definition, known: it should be obvious that consumption trajectories of optimal planners conduct to an utility result that outperforms the utility over time of those who do not conduct such optimal plans. However, in the context of our framework, being or not being an optimizer is not necessarily a choice, what justifies exploring the long-term performance differences between those who had the chance of establishing optimal plans and those who were forced to adopt other courses of action. Although the expected outcome might appear, at a first glance, trivial and obvious, it is not, because of the interplay between optimizers and non-optimizers, who participate in the same production process and might benefit or be penalized by how others accumulate or do not accumulate capital over time. The remainder of the article is organized as follows. Section 2 reviews recent literature that attributes to consumption-savings heterogeneous behavior a relevant role in explaining various macroeconomic phenomena; this short survey allows to motivate and emphasize the pertinence of the current analysis. In Sects. 3 to 6, the four different models are discussed and the respective dynamics investigated; these models involve: (i) optimal planners and hand-to-mouth consumers; (ii) optimal planners and rule-of-thumb savers; (iii) heterogeneous optimal planners, with a share of hand-to-mouth consumers emerging endogenously; (iv) hand-to-mouth consumers and rule-of-thumb savers (and complete absence of optimal intertemporal planning). Section 7 concludes.",1
16.0,2.0,Journal of Economic Interaction and Coordination,17 July 2020,https://link.springer.com/article/10.1007/s11403-020-00293-3,Participation in and provision of public goods: Does granularity matter?,April 2021,Ricardo Arlegi,Juan M. Benito-Ostolaza,Nuria Osés-Eraso,Male,Male,Female,Mix,,
16.0,2.0,Journal of Economic Interaction and Coordination,31 July 2020,https://link.springer.com/article/10.1007/s11403-020-00294-2,The nonlinear distribution of employment across municipalities,April 2021,Faustino Prieto,José María Sarabia,Enrique Calderín-Ojeda,Male,Male,Male,Male,"The spatial distribution of employment has been analyzed in the economic literature from different angles. For instance, changes in the spatial concentration of employment at the county level in service and non-service sectors in US have been discussed in Desmet and Fafchamps (2005); the effects of the labor market size on the wage inequality in Korpi (2007); the relation between employment density, industrial specialization, and wage distribution is examined in Matano and Naticchioni (2011); also, the relation between the development of the British and the German financial sector and the spatial distribution of financial employment in those countries in Wójcik and MacDonald-Korth (2015); and the socioeconomic impact of the geographic labor force mobility in Australia (Rampellini and Veenendaal 2016); among other references. However, to the best of our knowledge, no papers have empirically discussed the spatial distribution of employment (number of workers), at the municipal level, in the whole range, from a statistical perspective—in contrast to, for example, the efforts made to model the firm size distribution, in terms of number of workers (Axtell 2001; Lyócsa and Výrost 2018). In particular, Axtell (2018) introduced an agent-based model of endogenous firm and labor dynamics and compared the output of that model with the Zipf distribution of U.S. firm sizes (Zipf distribution is also known as power-law or Pareto distribution, with a slope close to 1). In that line, other authors have introduced agent-based models to model power-law behaviors observed in different socioeconomic complex systems (see Pinto 2012 for a review). However, in many economic and social phenomena, the Pareto distribution is usually valid only in the upper tail of the distribution, and if we are interested in the whole system, a more general statistical model could be necessary for describing collective behaviors in the whole range, a model with a power-law tail to which could compare the output of the agent-based model. The aim of this study is to determine which statistical distribution is useful to model the spatial distribution of employment of Spanish municipalities, observed on a monthly basis, over the period from January 2003 to December 2017. For this purpose, we have used freely available datasets available at www.seg-social.es (2018). In order to carry out our analysis, we use the family of Generalized Power Law (GPL) probabilistic models, firstly introduced by Prieto and Sarabia (2017). The first step of our analysis is based on testing the power-law behavior of employment data in the upper tail. Then, we use the family of generalized power-law (GPL) distributions and other well-known size distributions: the Dagum, Lognormal, Lomax, Burr type XII (Singh–Maddala) and Fisk (Log-logistic) distributions, to examine the whole range of the empirical distribution. Parameter estimation is performed by the method of maximum likelihood. Next, model selection is conducted in terms of the Bayesian information criterion, graphical validation (i.e., rank-size plots), and also by bootstrap resampling. The rest of this paper is organized as follows: in Sect. 2, we study new properties of the family of Generalized Power Law (GPL) distributions, its connection with the Extreme Value Theory, and we also show that its hierarchical structure nests the hierarchy of Pareto (power-law) distributions; in Sect. 3, we analyze the nonlinear distribution of employment across municipalities in Spain, by providing empirical evidence of its power-law behavior in the upper tail, and we also discuss the efficacy of the family GPL to explain employment data at the municipal level in the whole range of the distribution; finally, conclusions are given in Sect. 4.",1
16.0,2.0,Journal of Economic Interaction and Coordination,13 August 2020,https://link.springer.com/article/10.1007/s11403-020-00295-1,Measuring income inequality based on unequally distributed income,April 2021,Joongyang Park,Youngsoon Kim,Ae-Jin Ju,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Journal of Economic Interaction and Coordination,03 September 2020,https://link.springer.com/article/10.1007/s11403-020-00297-z,Decision-facilitating information in hidden-action setups: an agent-based approach,April 2021,Stephan Leitner,Friederike Wall,,Male,Female,Unknown,Mix,,
16.0,2.0,Journal of Economic Interaction and Coordination,08 September 2020,https://link.springer.com/article/10.1007/s11403-020-00299-x,"3% rules the market: herding behavior of a group of investors, asset market volatility, and return to the group in an agent-based model",April 2021,Sunyoung Lee,Keun Lee,,Unknown,,Unknown,Mix,,
16.0,2.0,Journal of Economic Interaction and Coordination,13 February 2021,https://link.springer.com/article/10.1007/s11403-020-00307-0,Of pride and prejudice: agent learning under sticky and persistent stereotype,April 2021,Georg D. Blind,Stefania Lottanti von Mandach,,Male,Female,Unknown,Mix,,
16.0,2.0,Journal of Economic Interaction and Coordination,27 October 2020,https://link.springer.com/article/10.1007/s11403-020-00305-2,Studying the correlation structure based on market geometry,April 2021,Chun-Xiao Nie,,,,Unknown,Unknown,Mix,,
16.0,3.0,Journal of Economic Interaction and Coordination,03 January 2021,https://link.springer.com/article/10.1007/s11403-020-00308-z,Estimating the proportion of informed and speculative traders in financial markets: evidence from exchange rate,July 2021,Ping-Chen Tsai,Chi-Ming Tsai,,Unknown,,Unknown,Mix,,
16.0,3.0,Journal of Economic Interaction and Coordination,02 January 2021,https://link.springer.com/article/10.1007/s11403-020-00310-5,Information flows and crashes in dynamic social networks,July 2021,Phillip J. Monin,Richard Bookstaber,,Male,Male,Unknown,Male,"A central object of study in social networks is the evolution of opinions, particularly whether agents’ opinions converge to a consensus and, if so, what that consensus is. In models with a fixed number of agents and a fixed network topology, a typical result is that convergence to a consensus is obtained in the long run, regardless of whether agents update rationally using Bayes’s rule or naively by, for example, averaging the opinions of others (see, for example, DeGroot 1974 and Acemoglu and Ozdaglar 2011). But what if accuracy matters, in that agents with inaccurate opinions fail and exit the system with a higher probability than those with relatively more accurate opinions? Such a situation can arise, for example, in a business context. If accurate information provides more value than inaccurate information and thus leads to higher expected profits, then over time businesses with inaccurate information will fail. We develop a dynamic model of information transmission and aggregation in social networks in which continued membership in the network is contingent on the accuracy of opinions. Agents in the model have an opinion about some fixed state of the world and form links to other agents probabilistically. Information is shared across connections, and agents update their opinions according to a DeGroot learning rule, that is, by continuously averaging those of their connections. The weights agents place on others’ opinions depend on how reliable their information is judged to be. The reliability of an agent’s opinion is proportional to how long it has been in the system. After updating their opinions, agents survive to the next period with a probability depending on how far their opinions are from the state of the world. Agents that do not survive fail and exit the system, and their information is lost. A failed agent’s connections are severed, and the agent is replaced by an unconnected agent with random opinions. Thus, our model extends DeGroot learning to a dynamic setting in which the network topology dynamically depends on how accurate agents’ opinions are with respect to an exogenous state of the world. Our main result is that some parameterizations of the model produce cyclical dynamics of information sharing. That is, the system alternates between a state of high connectivity, in which agents have achieved consensus opinions that are close to the state of the world, and a state of low connectivity, in which agents’ opinions are widely dispersed. These cycles are stochastic in that they occur irregularly and last for varying amounts of time. This result is novel in the literature on DeGroot learning, directly contrasting with the asymptotic convergence of opinions to a consensus that is typical of other models (see, among others, DeGroot 1974; DeMarzo et al. 2003, and Golub and Jackson 2010). In the classical DeGroot (1974) model, agents with opinions about a state of the world are connected in a static network. Agents iteratively update their opinions by repeatedly taking weighted averages of the opinions of their connections. The main result from the literature is that, under mild assumptions on the nature of the network of connections, agents in the long run converge to a common opinion or consensus. This is an intuitive result; if opinions are ordered, then agents with relatively high (low) opinions will have neighbors with low (high) opinions, and these will average out with the repeated updating over time. DeMarzo et al. (2003) argue that DeGroot updating is a boundedly rational form of learning, as opposed to fully rational (Bayesian) forms of learning. Our motivation for using DeGroot updating rather than Bayesian learning is based on both conceptual and empirical considerations. Social networks are often large and extremely complex, which can render fully rational Bayesian approaches infeasible, as they may be too demanding on the agents (Golub and Jackson 2010 and Acemoglu and Ozdaglar 2011). Boundedly rational or naive learning methods, such as DeGroot updating, might then be viewed as more realistic behavioral rules. Moreover, recent empirical evidence supports the view that naive learning methods such as DeGroot updating more realistically reflect the actual behaviors of agents than Bayesian learning methods (Chandrasekhar et al. 2015; Grimm and Mengel 2020 and Mueller-Frank and Neri 2020). In our model, a given agent forms a directed link with another agent in a given period according to an exogenous probability that is common across agents. For each agent, at most one new link can be formed in a given period. Information is transmitted along all connections, and, at any fixed time, each agent updates its opinion by taking a weighted average of those of its connections. The weight for each connection is proportional to its age, which is the number of consecutive periods for which it has been in the system. Links are destroyed when agents exit the system, i.e., when they “die” or “fail,” which happens with a probability depending on how far their opinions are from the state of the world. There is also a random component of agent failure in our model, irrespective of proximity to the true state (see also Jackson and Watts 2002 and Staudigl 2013). This random component reflects the notion that some agents may simply leave the network regardless of their opinion for idiosyncratic reasons, such as death or moving on with their lives. Thus, the so-called interaction matrix, which in DeGroot learning contains weights that agents place on the opinions of others in the averaging process, is time-varying, as agents continually reassess the relative trustworthiness of the information provided by others. This contrasts with the standard DeGroot model in which the interaction matrix is assumed constant over time. In addition, unlike Hegselmann and Krause (2002, 2005), Weisbuch et al. (2002), and Mirtabatabaei and Bullo (2012), who model time-varying interaction matrices with weights depending on similarity of opinions, agents in our model connect and transmit information in a directed fashion according to an exogenous probability and weight information depending on how long connections have been in the system. Our paper also contributes to the literature on dynamic network formation (see, among others, Bala and Goyal 2000; Watts 2001; Jackson and Watts 2002; Jackson and Rogers 2007; Skyrms and Pemantle 2009, and Staudigl 2013). Models in this literature propose and analyze mechanisms of dynamic network formation in which agents form and sever links through time. Bala and Goyal (2000) study directed communication networks in which the link formation process is governed by agents weighing costs of maintaining links against their benefits. Watts (2001) studies convergence properties in a network with a deterministic dynamic, while Jackson and Watts (2002) consider a wider class of models under stochastic evolution. Staudigl (2013) considers a general model of an interaction game in which the interaction probabilities are time-varying functions of the players’ actions. While agents in these models form and sever links strategically, our agents have no direct control over how links are formed and severed. This is consistent with their naive DeGroot updating learning rule and has a similar motivation: in large and complex networks, it is reasonable to think that agents are limited in their ability to assess other agents and the ramifications of connecting to them prior to learning about them. Moreover, as there is no cost to connecting in our model, agents should want to connect to as many other agents as possible. It is only after the link has been made that the trustworthiness of an agent’s information is assessed based on the proxy of how long the agent has been in the system. After setting the model on a foundation for analysis by proving that it is mathematically represented as an ergodic Markov chain (see Sect. 3), we show through simulations that the dynamics of the network can be the source of cycles of information sharing. That is, under a range of parameterizations, the model produces cycles of convergence and divergence of opinions, which occur despite holding the state of the world fixed. A cycle begins with the system in a state of high interconnectivity in which agents have achieved near-consensus opinions that are close to the true state of the world. Over time, some agents die and the mortality rate increases, the more inaccurate the agents’ opinions are. The connections of dead agents are severed, and they are replaced by naive ones with relatively inaccurate opinions. If such agents are able to connect to others, thereby spreading their relatively poor information, then their poor information can propagate quickly to the rest of the agents, causing the opinions of the rest of the agents to drift away from the state of the world. This can precipitate more deaths, and the process can feed back onto itself, eventually causing a nearly complete collapse of the network. This is the process by which the state transitions to one of low connectivity in which agents’ opinions are widely dispersed. We call such a transition from a state of high to low connectivity a crash. Intuitively, the crash occurs because a very old interconnected agent with good information dies and is replaced by a relatively uninformed agent, who then spreads its poor information though the links it forms. Eventually, a period of rebuilding occurs in which connections are reformed and the distribution of opinions narrows, arriving at a high-connectivity state. The cycle then repeats. We conclude our study with a comparative statics analysis on the per-period probability of connection and the exogenous probability of failure. The results suggest that the level curves of crash frequency are approximately linear in these parameters. The paper is structured as follows. Section 2 introduces the model. Section 3 shows how the model can be mapped to an ergodic Markov chain, providing the basis for the simulation results presented in Sects. 4 and 5. Section 6 concludes.",1
16.0,3.0,Journal of Economic Interaction and Coordination,15 January 2021,https://link.springer.com/article/10.1007/s11403-020-00314-1,On optimal regimes of knowledge exchange: a model of recombinant growth and firm networks,July 2021,Ivan Savin,,,Male,Unknown,Unknown,Male,"Innovations are vital for economic growth (Romer 1990; Aghion and Howitt 1992, 1998). Hence, creation and diffusion of new knowledge is a key for economic prosperity. Knowledge generation (Zook and Rigby 2002; Uzzi and Spiro 2005) and its diffusion (Cowan 2005) take place through networks. In particular, firms have to recombine internal competences with external ones to remain competitive.Footnote 1 In doing that, companies pay particular attention to proximity with their partners (e.g., technological, organizational, cultural; see Capaldo and Messeni Petruzzelli 2014; Capaldo and Petruzzelli 2015) and appropriation of the generated knowledge (Capaldo and Messeni Petruzzelli 2011; Savin and Egbetokun 2016). But what are the most productive network structures for that purpose? Does it depend on the nature of knowledge exchange, free versus quid pro quo basis? Are these structures universally superior under different degrees of knowledge appropriation? While in the literature, there are studies separately analyzing factors of effectiveness of knowledge generation (to name just a few, Cowan et al. 2007; Cowan and Jonard 2009; Dawid and Hellmann 2014) and diffusion (see, e.g., Morone and Taylor 2004; Cowan and Jonard 2004, 2007; Carayol and Roux 2009; Mueller et al. 2017), none of them to our knowledge has addressed the two processes simultaneously. This is an important research gap since the two processes are closely interdependent: generating new knowledge is critically dependent on external knowledge sources (Cassiman and Veugelers 2006; Schmiedeberg 2008; Wang et al. 2020), while before sharing new knowledge one has to discover it in first place. The present study aims to close this gap focusing on the questions above-mentioned. To address the research problem, we suggest to represent knowledge by distinct letters (fields of technological expertise) and words (new inventions), which can be both recombined with each other to produce further words and accumulated by agents over time. The value of knowledge is measured by the frequency of words’ use in English language. This choice of knowledge representation is supported by a series of stylized facts. In particular, building a network of words as a directed graph, where each time a longer word containing a shorter one implies they are connected (the longer word ‘cites’ the shorter one), results in a graph structurally similar to the one built out of patents and their citations. Both these graphs, for example, have hierarchical modular structure and power law distributions of value and citations (see Sect. 2.1 for more details). Pursuing new inventions, agents recombine knowledge not randomly. Instead, they follow some semi-definite structures (‘ideas’) on what words can be constructed based on the words they already discovered. Uncertainty present in those ideas and captured by ‘*’ symbols, that have to be replaced by certain word–letter combination, reflects the risks and the opportunities with respect to which knowledge pieces have to be recombined. A more ‘noisy’ idea (containing more ‘*’s) is less likely to be realized simply because it may require knowledge pieces absent by a given agent to constitute a viable invention. On the other hand, it can be explored in a larger number of ways possibly serving as a basis for more than one discovery. Agents differentiate between ideas attaching more value to ideas that relate to the knowledge these agents have already accumulated in the past, i.e., agents prioritize possibilities where they have an advantage. Furthermore, agents prefer ideas involving, ceteris paribus, less uncertainty, i.e., lower chance to fail. If through recombination agents construct a word available in the vocabulary of English language, they claim its discovery and receive a payoff, which is drawn from the power law distribution of the frequency of words’ use and reflects the highly skewed distribution of innovation size observed empirically. Effectively, creation of new words based on words invented in the past captures the network property of knowledge with the ability to produce complementarity effects when certain distinct pieces are combined in a particular way. In doing so, we follow the literature started by Schumpeter (1912/1932, p. 65) defining innovations as ‘new combinations’ of the existing knowledge, and continued by theories of architectural innovation (Henderson and Clark 1990), recombinant growth (Weitzman 1998), combinatorial technology models (Arthur and Polak 2006) and works on technological capabilities (Hidalgo and Hausmann 2009) considering knowledge as a collection of heterogeneous pieces being interconnect-able with each other in one or another way. Equipped originally only with a small subset of letters, agents interact with each other in diffusing ideas on words that can be constructed and share resources (letters and words) to discover new words. This interaction is facilitated through a network of firms, where interaction between firms implies they are connected. In this case, the structure of the network is of critical importance. We proceed by considering four stylized network topologies, namely regular lattice, small world, random and scale free, that capture (dis)advantages over a set of commonly discussed network properties valuable for knowledge generation and diffusion, namely clustering, mean path length and asymmetry in degree distribution. Due to the absence of market for knowledge, agents share knowledge according to one of the two mechanisms observed empirically. The first one consists in free exchange of knowledge documented by Allen (1983) and coined as ‘collective invention.’ Its examples have been found in the past (e.g., in nineteenth century in steel and steam-engine manufacturing), while nowadays it is discussed in relation to the open source software (Bessen and Nuvolari 2015). In the present model, this mechanism is implemented by allowing agents to take use of the words and letters accumulated by their ego-network (i.e., sample of agents they are directly connected to). The alternative to free knowledge sharing is an R&D alliance, where agents share knowledge only for the sake of mutual benefit. This idea is similar to the quid pro quo principle described by von Hippel (1987). We implement this by following bilateral collaboration, and in case of success acknowledge both agents as inventors equally sharing the payoff from discovery (Mowery et al. 1996). On top of that, we consider two opposing scenarios with respect to intellectual property rights (IPR) protection. In the first one, agents who discovered a given word first can prohibit others to ‘re-invent’ it (knowledge as a private good with perfect IPR). In the second case, knowledge is a semi-public good where words can be re-invented but with each further discovery reducing its value by a fixed percentage (imperfect IPR). When considering the model’s output, apart from the aggregate outcome in terms of the number and value of words discovered in each type of network, the distribution of words and their value among agents is of critical importance. This is because the inequality in payoffs (right of inventor, profits) affects motives of agents to share knowledge freely or only in return for the share of payoffs. For example, since the actual R&D industrial network has a highly skewed degree distribution (Cowan and Jonard 2009), a higher inequality resulting from collective invention can affect agents’ motives to sustain the mode of knowledge sharing. Logically enough, the more advanced is the group of leaders (agents with most of the valuable inventions) compared to the laggards, the higher is the likelihood that those leaders have the superior production technology and will appropriate a larger share of the consumer market and profits (Dawid and Hellmann 2014). Hence, agents that loose a share of their payoffs due to the knowledge sharing regime may insist on changing the conditions under which knowledge is shared. Our results can be summarized as follows. First, the scale-free network (with highest degree asymmetry and shortest mean path length) typically is the most productive network structure on the aggregated level resulting in the largest number of words and overall largest value discovered. This holds particularly true under perfect IPR and R&D alliances. Under R&D alliances the scarcity of knowledge resources (words and letters agents have on their disposal) is higher than under collective invention, and scale-free networks enable a handful of agents to concentrate this knowledge discovering rare word–letter combinations. Perfect IPR just reinforce the effect as one needs to combine an even larger number of unique word–letter combinations instead of replicating those from one’s partners. There is a notable exception, however. In particular, when knowledge is a semi-public good and agents collaborate under collective invention, clustered networks become best in discovering more valuable inventions. This is because clustered networks tend to share ideas valuable for all members of the cliques, while imperfect IPR allows several members of the clique to profit from the same knowledge. Second, when knowledge is considered as a semi-public good, the inequality among agents in the number and value of words discovered is greater under collective invention for networks with skewed degree distribution (scale free and random), but lower for clustered ones (regular and small world).Footnote 2 In clustered networks, agents from the same clique have access to the same pool of knowledge resources and ideas, thus, discovering more or less the same set of words. In networks with skewed degree distribution, in contrast, the disparity among agents on what knowledge resources are available to them increases leading to a higher inequality in knowledge output. The latter finding brings in new insights on why agents may change their strategy with respect to knowledge sharing. In a newly formed industry consisting of several clusters of firms with no clear leader, the joint adoption of collective invention principle simultaneously increases overall output and reduces inequality among agents giving all actors similar chance for further success. If over time the network changes its structure to a more asymmetric one (e.g., due to merger of several actors), majority of agents may prefer to switch to bilateral collaboration to limit the possibilities of the ‘star’ agents to exploit their dominant network position. The contribution of the paper is threefold. First, our model combines, for the first time, the processes of knowledge generation and diffusion looking for most productive network topologies, regimes of knowledge appropriation and exchange. In doing this, we add to the vast literature, much concentrated in the management field, examining the structural properties of innovation or R&D networks. Second, by examining the different regimes of knowledge exchange and comparing our results with empirical evidence, we provide a novel explanation on why industries in the past have experienced a shift from collective invention to R&D alliances. Third, we provide an original model where knowledge is recombined and diffused not randomly but following directed search facilitated by the knowledge they accumulated before. This way we contribute to the literature of recombinant growth by adding the dimension of network interaction between agents, something that to our knowledge was missing in this field. The rest of the paper is organized as follows. Section 2 describes the model set up. In Sect. 3, we explain the simulation procedure and parameters being used. We provide results of the numerical analysis in Sect. 4, while Sect. 5 discusses their implications and concludes.",3
16.0,3.0,Journal of Economic Interaction and Coordination,09 April 2021,https://link.springer.com/article/10.1007/s11403-021-00316-7,The evolution of norms within a society of captives,July 2021,Chad W. Seagren,David Skarbek,,Male,Male,Unknown,Male,,
16.0,3.0,Journal of Economic Interaction and Coordination,08 March 2021,https://link.springer.com/article/10.1007/s11403-021-00318-5,Can loss framing improve coordination in the minimum effort game?,July 2021,Christopher Roby,,,Male,Unknown,Unknown,Male,"Coordination in groups is difficult. Issues related to coordination are the focus of a large literature in economics, management, and related fields due to the large implications that coordination has for businesses. In many instances, effective coordination depends on the worker who is putting forth the least effort. In this scenario, one of the two things usually occurs. Either the group takes longer to complete the task, while waiting on this person to finish their job, or other group members have to exert more effort to compensate for the low productivity from this individual. These coordination issues are common and frequently studied using economic experiments, which allow researchers to study behaviors which can lead to poor coordination, and mechanisms with which to improve it. It is common to observe coordination failure in a laboratory setting, even in the face of higher payoffs for more efficient actions. Much of the economic literature is dedicated to exploring coordination failure and why it occurs, establishing the regularity of coordination failure. Once coordination failure became an experimental regularity, studies began analyzing how coordination can be improvedFootnote 1. While some studies have changed the payoff structure in these types of games to achieve a higher level of coordination, the role of unavoidable losses has not been considered. In both laboratory and field settings, loss framing has been effective in encouraging more efficient behaviorsFootnote 2. This is largely due to the notion of loss aversion, which says that individuals are more sensitive to losses than equal-sized gains (Kahneman and Tversky 1979; Tversky and Kahneman 1981, 1991; Bateman et al. 1997). If loss aversion is salient here, then framing payoffs as losses should lead to higher coordination levels. Devetag and Ortmann (2007) lament that “no studies exist (yet) that investigate the role of negative payoffs in a systematic way...” This is where I intend to continue the discussion with this study. While adding some negative payoffs or penalties has been studied in the context of the minimum effort game before, no one has looked at how coordination changes in the face of unavoidable negative payoffs. Coordination issues have been studied thoroughly using a variety of different mechanisms, so this seems like a natural extension. I assess the role of unavoidable negative payoffs on coordination in the minimum effort game. Even though the role of penalizing inefficient actions has been studied in the context of the minimum effort game before, no one has systematically assessed the role of losses in coordination behavior in the minimum effort game. Sensitivity to framing payoffs as losses should lead individuals to choose riskier actions, thus moving the group effort level above the risk-dominant equilibrium, which indicates coordination failure. I use three treatments to assess the dynamics of moving to a loss frame after playing under a standard payoff table with gains, a table with losses, and a table with both losses and gains. I find that framing payoffs as losses improves coordination, though not to the payoff-dominant equilibrium. The remainder of the paper proceeds as follows: Sect. 2 reviews the literature on coordination and loss framing, Sect. 3 explains the minimum effort game and the design of the experiment, Sect. 4 explores the results of the experiment, including treatment effects and the role of risk preferences, and Sect. 5 concludes.",
16.0,3.0,Journal of Economic Interaction and Coordination,13 April 2021,https://link.springer.com/article/10.1007/s11403-021-00321-w,"Financial development, income and income inequality",July 2021,George Vachadze,,,Male,Unknown,Unknown,Male,"Two empirical regularities have been widely documented in the macroeconomic literature. First, Slottje (1987), Blank (1989), Parker (1999), Maliar et al. (2005), and among others present a strong empirical evidence pointing toward a counter-cyclical income inequality in the USA and other industrialized countries, and second, Garner (1991), Blanchard (1993), Hall (1993), Carroll et al. (1994), Howrey (2001), Barsky and Sims (2012), and among others find that the aggregate economic activity is driven in part by a consumer and business confidence. The primary objective of this paper is to present a model through which income, and income inequality can be jointly determined in a counter-cyclical manner via self-fulfilling expectations.Footnote 1 For this purpose, we propose and analyze a dynamic general equilibrium model with overlapping generations, endogenous labor supply, minimum investment requirement, and imperfections in the credit market. Ex ante homogeneous agents decide on how much labor to supply to the competitive labor market and whether to work for a firm or become a self-employed worker. To transfer the current saving into second-period consumption, young agents become either depositors or entrepreneurs. Depositors lend their entire savings to the competitive credit market, while entrepreneurs borrow funds and set up a firm by investing a certain amount of the final commodity in capital stock. Firms hire labor and pay wages according to the marginal product rule. Within this framework, we describe a mechanism through which income and income inequality can be jointly determined via self-fulfilling expectations in a counter-cyclical manner. In particular, we argue that a minimum investment requirement coupled with a limited pledgeability of a firm’s profit for debt repayment implies a backward-bending aggregate labor supply curve. This along with an inelastic labor demand curve implies the possibility of multiple equilibria and thus the possibility of self-fulfilling equilibrium wage and employment. In one equilibrium, the market wage and labor income are both low, young agents who become entrepreneurs work harder and save more than young agents who become depositors. As a result, the equilibrium is characterized by low-income and high-income inequality. In another equilibrium, the market wage and labor income are both high, young agents who become entrepreneurs supply the same amount of labor and save the same as young agents who become depositors. As a result, the equilibrium is characterized by high-income and low-income inequality. In other words, agents’ expectations about labor market conditions determine market wage, aggregate employment, income, and income inequality in such a way that the initial expectation becomes a self-fulfilling prophecy. We derive the necessary and sufficient conditions for different dynamic scenarios such as (1) Poverty trap with and without income inequality, (2) indeterminacy of income and income inequality, (3) poverty and income inequality due to a self-fulfilling prophecy, (4) endogenous fluctuations of income and income inequality, and (5) low-income and high-income inequality trap. The proposed model is capable to explain the excessive volatility of income and income inequality only through fluctuations of self-fulfilling expectations. Based on the proposed model, we can discuss some policy implications. First, in some cases, policymakers may play an active role in coordinating agents’ expectations so that agents select a “good” equilibrium (one with a high-income and low-income inequality) and eliminate a “bad” one (one with a low-income and high-income inequality). Second, improvements in financial development may not always imply an increase in long-run income and a reduction of long-run income inequality. Third, in some cases, policymakers may trigger an increase in long-run income and reduction of long-run income inequality even by a temporary improvement of financial market conditions. The organization of the paper is as follows. In Sect. 2, we review the relevant literature and discuss how the present paper fits within the existing literature. In Sect. 3, we propose a basic model. In Sects. 4 and 5, we analyze agents’ optimal behavior and demonstrate that the aggregate labor supply curve, consistent with equilibrium in the financial market, has a backward-bending property. In Sects. 6 and 7, we show the possibility of multiple labor market equilibria and discuss the equilibrium relationship between income and income inequality. In Sect. 8, we identify different dynamics scenarios describing the evolution of the economy. In Sect. 9, we discuss some policy implications of the model. We conclude the paper in Sect. 10. All remaining proofs and the robustness of the main results under alternative specifications are located in “Appendix.”",
16.0,3.0,Journal of Economic Interaction and Coordination,03 March 2021,https://link.springer.com/article/10.1007/s11403-021-00322-9,The effect of social distancing on the reach of an epidemic in social networks,July 2021,Gregory Gutin,Tomohiro Hirano,Alexis Akira Toda,Male,Male,Male,Male,"How does social distancing affect the reach of an epidemic such as COVID-19 (Coronavirus Disease 2019) in social networks? To address this question, we consider the idealised problem of the susceptible–infected–removed (SIR) epidemic model (Kermack and McKendrick 1927) in the presence of temporary social distancing constraints placed on the members of a society. Our susceptible–infected–removed with social distancing model (SIRwSD model) is easily understood. As is standard, the vehicle that an infectious disease uses to spread is a contact network: a graph, G, where vertices represent people and an edge between two vertices i and j captures the idea that person i and person j came into contact in such a way that the disease might pass between them.Footnote 1 A social distancing policy is described by a function \(\kappa \), defined on the set of vertices of G, that constrains the number of neighbours that each person may come into contact with.Footnote 2 At any moment in time, the contact network G and social distancing policy \(\kappa \) together generate an infectious subnetwork. Since the social distancing policy \(\kappa \) temporarily deletes a subset of edges from G, there are simply less avenues along which the disease may be transmitted. We run Monte Carlo simulations comparing the reach of the disease when there is no social distancing with that when social distancing measures are imposed.Footnote 3 These comparisons are performed on three types of well-studied networks: the ‘random graphs’ model of Erdős and Rényi (1959) and Gilbert (1959) (hereafter, ERG), the ‘small world’ networks of Watts and Strogatz (1998) (hereafter, WS), and the ‘scale-free’ model of Barabási and Albert (1999) (hereafter, BA).Footnote 4 We begin by focusing on two main questions for the constrained and unconstrained case: (i) What is the likelihood that an infectious disease will become endemic? (ii) What is the distribution of the peak infection rate over the lifecycle of the epidemic? The reason for focusing on (i) is that the level of herd immunity attained is an important policy tool in knowing how, when, and by how much social distancing measures can be relaxed. The reason for focusing on (ii) is that the peak infection rate corresponds to the most overloaded instance that a healthcare service encounters over the lifecycle of an epidemic. Success in both the above dimensions is not simultaneously possible because as one goes up the other goes down. Our results help to guide what the relative trade-offs are. While the spread of an epidemic is curtailed when all individuals in a society face the same constraints, the reduction varies greatly depending on the topology of the social network. For all three social network structures, ERG, WS, and BA, that we consider, strong measures of social distancing (limiting everyone’s daily interactions to 3 or fewer) stops an epidemic with high probability (see Fig. 3). However, for societies structured according to WS, the fraction of individuals who are in state R (‘removed’) after the epidemic has passed is much less over this range (see rows labelled ‘Herd Immunity’ in Table 1). Moderate social distancing (defined as 4–5 interactions per period) delays the peak of an epidemic but has little effect on the size of the peak and on the number that the disease ultimately reaches. The effect of mild social distancing (limiting individuals to 10 social interactions) differs greatly across network structures. With ERG networks, the effect is negligible. With WS networks, the outcome is the same as no intervention. With BA networks, the peak is significantly reduced. While we begin with comparing the outcomes of no social distancing with those to social distancing, our framework is flexible enough to address a host of other policy experiments. For example, we consider a network comprised of two densely connected components, interpreted as ‘countries’, that have a small number of connections between them that we interpret as international friendships. We show that the public health benefits to a country that imposes strong social distancing measures are dramatically reduced, and perhaps eradicated entirely, if that country continues to allow international connections, interpreted as maintaining open borders, with a country that is not implementing similar measures. In a hyper-connected world, this points to the need for ‘global’ cooperation to eradicate an epidemic. In particular, if the global approach is uncoordinated with each country unilaterally applying social distancing measures without taking into account the policy choices of its neighbours, then an infectious disease may cycle around for far longer than otherwise desired. We also consider what happens when a subset of individuals are deemed ‘essential workers’ who can go about their lives facing weaker social distancing constraints than the rest of the population. Our results show that even if only a small fraction of the population is deemed essential, the reach of the epidemic is similar to that wherein there are no essential workers. Finally, we consider another policy tool: time. Specifically, we consider a policy that begins with severe social distancing measures that are incrementally relaxed over time, and compare the outcome with that from a policy of mild but constant social distancing over a shorter window. We find that the public health outcome is better under gradual relaxation for ERG networks and WS networks, but worse for BA networks. We conclude the paper by discussing how it fits in the literature and by suggesting some extensions to our framework that can be implemented in future work. However, we emphasise that the framework of this paper considers the benefits to social distancing when it is the only policy tool available. Clearly, this is not realistic. In tackling real-world epidemics, policy makers have an array of tools available. For practical purposes, governments need to understand how the many different epidemic management policies complement each other. This paper is about one such policy tool in isolation, but given its tractability, we hope that it can be incorporated into richer models.",1
16.0,3.0,Journal of Economic Interaction and Coordination,20 March 2021,https://link.springer.com/article/10.1007/s11403-021-00323-8,From agent-based modeling to actor-based reactive systems in the analysis of financial networks,July 2021,Silvia Crafa,,,Female,Unknown,Unknown,Female,"The behavior of financial systems is typically studied in terms of graph-based models, where nodes represent financial institutions and links represent financial dependencies, like interbank exposures. A large body of literature applies mathematical methods to the analysis of global properties of the financial networks, and in particular systemic risk, that is the fragility of the system when exposed to the contagious effects of distressed financial institutions (Cont et al. 2013; Cont and Minca 2016; Capponi and Bo 2015; Capponi 2016; Battiston et al. 2012, 2016; Neveu 2018; Dai Pra et al. 2009). While graph modeling enables rich mathematical descriptions and sophisticated analysis techniques, modeling—thus thinking—in terms of graphs, naturally shifts the view towards a global view of the system and to its static structure rather than its dynamics. Accordingly, the literature associates the systemic risk to a number of structural properties, i.e., network-based measures of connectivity and risk, such as counterparty susceptibility, local network fragility, feedback centrality, DebtRank. On the other hand, the economic crisis in 2008 called attention to the finer-grained study of the interactions of economic agents, so to better account for the global system’s dynamics emerging form the local components’ behavior. This is the approach followed by Agent-Based Models (ABMs) of economies (Tesfatsion and Judd 2006; Farmer and Foley 2009; Trichet 2010; Seppecher 2012; Teply and Klinger 2019). These models usually represent static network structures where nodes, called agents, have a dynamic behavior expressed in terms of rules that can be executed by a system simulator, i.e., a computer program. Compared to graph models, agent-based models have a more dynamic nature, but they still encode a rather global view of the system. Indeed, even if heterogeneous behavior rules can be assigned to different (sets of) agents, the system is generally governed by a centralized logic, where the agents’ behaviors are mathematical functions that depend (also) on global variables, like the current market state or the global timing. In particular, the time flows synchronously within the system, whose evolution is given in terms of a sequence of global and synchronous iterations. At the beginning of each simulation iteration, every agent re-executes its local behavior function using the updated values of the variables that have been modified at the end of the previous iteration. Even if each iteration is assumed to last for an unspecified period, a modeling framework with this level of synchronicity and centralization cannot account for situations where different financial institutions react to the same event (e.g., a macroeconomic shock or a bank default) at very different times (e.g., a bank may quickly absorb the shock, another one might need time to liquidate part of its assets, and different countries might impose different procedures for writing off bad debts).
 Therefore, Crafa and Varacca (2018) proposed a novel conceptual framework that pushes further the ABMs with (i) a fundamentally different management of time, and (ii) a fully decentralized control logic. The new framework is borrowed from the field of computer science and ICT, in that it models financial systems as distributed and reactive systems, that is a well know class of asynchronous communicating systems, characterized by a remarkable level of decentralized control and coordinated interactions between agents located on different sites. The key point is that financial entities operate in parallel and dynamically update their state and behavior as a reaction to events issued by other entities. Economic agents are only partially aware of the current global state of the system; they asynchronously receive messages that notify than an event has occurred (e.g., a market shock or a debtor communicating its default), so that they can react to that event by modifying their behavior according to their internal policy and locally stored information (strong locality and isolation principles). We call this approach Actor-based Reactive System (ARS), both to remark its distinctive features and to allude to the specific model of information systems on which it is based, that is the so-called Actor Model (Agha 1986).
 In this article we show the feasibility of the ARS framework by putting it at work on a realistic scenario: the systemic risk analysis of the European banking network. We remark that our aim here is not to provide financial insights on the Eurosystem stability, nor to precisely estimate the systemic risk of specific local banking sectors. This would require a more experienced parameter calibration, because the system behavior is significantly influenced by the model parameters, as it is usual in the literature about systemic risk analysis. Instead, we call attention to how the ARS, compared to ABM, brings about finer-grained analyses and more flexible modeling, with a greater degree of heterogeneity and adaptivity of economic agents. In particular, we consider an European banking network closely related to that studied by Teply and Klinger (2019) using an Agent-Based Model. It is a network of 286 banks, in 9 European countries, with structured balance sheets of different sizes. Realistic interbank exposures are reconstructed using available data from the European Central Bank’s statistics website and the Bank for International Settlements’ consolidated banking statistics. We consider a nontrivial contagion procedure, that combines an initial shock on external assets with the negative feedback loop triggered by asset fire sales (as Teply and Klinger (2019) or Battiston et al. (2016)). However, the centralized logic governing the contagion cascade in agent-based and graph-based models is rephrased here in terms of a fully parallelized and decentralized logic, where actors asynchronously react to messages that notify either an interbank contagion or a funding liquidity shock. We then use the \(\textsf {FinMarkSim }\) tool developed by Crafa and Varacca (2018) to directly run the ARS model under various conditions and test the role of different parameters on the systemic risk of the European network. We study the influence of the size of the initial asset shock and the market illiquidity factor, the systemic influence of specific countries and bank groups, and the influence of the topology of the financial network. We show that, by studying the system’s evolution as an Actor-based Reactive System instead of a sequence of synchronous iterations, it is possible not only to assess the final global state of the system, but also to precisely inspect the complete and fine-grained dynamics of each agent, observing the causal chain of events-and-reactions that determined the agent’s behavior within the system execution. As a consequence, this approach allows to precisely assess and disentangle the roles of two different contagion channels, i.e., the interbank contagion and the illiquidity channel, within different countries or in specific banks. Moreover, since each complete system’s run lasts up to 3–6 seconds (most tests handle the behaviors of more than 1400 agents in 2–3 seconds), the \(\textsf {FinMarkSim }\) tool can be effectively used to design and conduct fine-grained stress tests with hypothetical scenarios in a test-oriented style. The paper is organized as follows. Section 2 discusses how to model a financial system as an ARS, providing an accessible explanation of the model’s features borrowed from computer science. Sections 3 , 4 describe the structure and the dynamics of the ARS corresponding to the European banking network that is studied in depth in this article. Section 5 reports on the stress tests, and Sect. 6 concludes the paper with final remarks.",2
16.0,3.0,Journal of Economic Interaction and Coordination,19 March 2021,https://link.springer.com/article/10.1007/s11403-021-00325-6,Dynamic bank runs: an agent-based approach,July 2021,Toni Ricardo Eugenio dos Santos,Marcio Issao Nakane,,Male,Male,Unknown,Male,"The main role of commercial banks is to serve as financial intermediaries between depositors and borrowers. One of the reasons for the existence of commercial banks is the presence of asymmetric information, which is the fact that the borrower has more knowledge about his own situation than the lender. In the presence of asymmetric information, it pays the lender to monitor the borrower. However, the monitoring cost may be high for any particular borrower. When many agents perform this monitoring activity, they may find worthwhile to delegate it to a specialized entity to save on monitoring costs. This is one possible theory explaining the origins of banks, according to Diamond (1984). In a fractional reserve bank system, banks are subject to runs. If a significant number of depositors decide to withdraw their resources from the bank, the bank will run out of its reserves, which may trigger liquidity or solvency problems. We define bank runs as withdrawals over and above the expected demand for liquidity. When this happens, bank insolvencies may arise. Such withdrawals may occur due to random shocks (Diamond and Dybvig 1983) or may be a result of depositors’ perceptions that the bank is facing some difficulties (Gorton 1985; Chari and Jagannathan 1988; Jacklin and Bhattacharya 1988; Allen and Gale 1998)Footnote 1. Apart from its historical lessons, the study of bank runs is relevant because banks with good fundamentals may go bust due to a panic crisis triggered by bank runs. The social cost of bank failures may be relevant, and policymakers may benefit from a better understanding of how bank runs work. In this paper, we simulate a bank run triggered by depositors’ strategic decisions in a coordination game based on Diamond and Dybvig (1983). In this framework, some bank depositors are subject to a negative liquidity shock that leads them to withdraw their funds from a bank. Other depositors who are not hit by the adverse shock may be compelled to withdraw their resources from the bank as well, anticipating that the bank may not be solvent if they wait to withdraw later. Such panic behavior may lead to a bank run. Our model is part of the literature on complex adaptive systems, where agents react to the environment through signals and its internal rules. They have memory and can choose which rule provides a better response, so agents adapt in such a way that they optimize their utility functions in the long term. For details about complex adaptive systems, see Holland (2014). In particular, we use an agent-based model (ABM) as our approach. ABMs are computational methods to study the overall evolution of economies as complex adaptive systems emerging from the interaction of many autonomous agents following a given set of behavioral rulesFootnote 2. The original Diamond-Dybvig economy lasts for three periods. We embed this economy in a dynamic simulation so that the three periods of this model repeat in cycles. Each agent uses data from his memory to estimate what might happen and act to maximize his expected returns. In addition, banks arise endogenously in the model; any agent can become a banker if proper conditions arise. Our paper extends Grasselli and Ismail (2013)’s model of bank failure into a context of bank runs. Grasselli and Ismail’s model has a multibank environment to study financial contagion. As in Diamond and Dybvig (1983), bank depositors face liquidity shocks that lead them to withdraw their funds from the bank. However, unlike Diamond and Dybvig, their model does not allow for bank runs due to coordination problems. Bank failures arise when the adverse liquidity shock is strong enough and banks run out of reserves to cover the withdrawals. Our paper follows the bank run literature in the Diamond and Dybvig (1983) tradition and we introduce coordination problems into the Grasselli and Ismail (2013) framework. We also consider two elements that feature prominently in the bank run literature and are absent in Grasselli and Ismail’s model. The first element is the sequential service constraint and the second is the consideration of neighborhood influence on the patient agent’s decision. Our model is therefore capable of generating bank failures due to bank runs. Our results show that the evolution of the considered economy leads to a long-term stability when there is more liquidity, and to instability in a highly concentrated market when there is less liquidity. Grasselli and Ismail (2013) also arrived at the result that there are few established banks in the long run; however, the authors do not relate liquidity to market concentration. Our most important result is that the number of bank runs decreases with the size of the banks’ reserves and with the value of the threshold that leads a patient agent to withdraw early. Temzelides (1997) reaches similar conclusions. We do not impose any deposit insurance. The only elements we have in our model are the sequential service constraint and the agent’s punishment when he does not receive the amount promised by the bank and decides not to be a customer anymore. Davison and Ramirez (2014) provide some empirical support for the relation between bank size and bank runs. They study bank panics in the USA in the 1920s and show that such episodes were less likely to occur in states with relatively larger banks. There is a long-lasting debate about possible trade-offs between financial stability and bank competition (Allen and Gale 2004). Our paper is not on bank competition, but our results indicate that possible trade-offs between financial stability and a more concentrated banking industry are indeed important, associated with enough reserves to face withdrawals above the predictions. The price to pay for a more stable banking sector may be an increase in concentration. The structure of the paper is as follows. In Sect. 2, we review the literature. Section 3 presents the model, Sect. 4 shows the results, and Sect. 5 concludes the paper.",2
16.0,4.0,Journal of Economic Interaction and Coordination,21 March 2021,https://link.springer.com/article/10.1007/s11403-021-00326-5,"Liquidity shocks and interbank market failures: the role of deposit flights, non-performing loans, and competition",October 2021,Demian Macedo,Victor Troster,,Unknown,Male,Unknown,Male,"Banks with high proportions of non-performing loans (NPLs) experienced severe complications to meet their liquidity needs in the interbank market during the subprime mortgage crisis in the USA from 2007 to 2009 (Brunnermeier 2009; Afonso et al. 2011; Purnanandam 2011). Unlike the subprime mortgage crisis in the US, European banks suffered liquidity shortfalls due to deposit flights and increases in the NPLs during the financial crisis. Deposit flights affected countries such as Greece, Italy, Ireland, Portugal, and Spain that suffered a reduction in retail and institutional deposits at an unprecedented rate (Moro 2014; Whelan 2014; Bibow 2015; Priftis and Rousakis 2017). European banks also coped with an increase in their non-performing exposures (Cocco et al. 2009; Cucinelli 2013; Makri et al. 2014; Roman and Sargu 2015; Cai and Zhang 2017; Ozili 2019). The proportion of NPLs to loans exceeded 15% in many countries with levels around 50% in Greece and Cyprus, undermining an important source of bank liquidity such as the revenue streams (Aiyar et al. 2015; Magnus et al. 2018). As a result, European and North-American banks had to raise their interest rates to attract deposits that eroded their profitability and led to weaker balance sheets (Messai and Jouini 2013; Van Rixtel and Gasperini 2013; Allen et al. 2014; Acharya and Mora 2015; Dimitrios et al. 2016; Grigorian and Manole 2017). Banks tend to hide loan losses and be reluctant to remove bad loans from their portfolio when exposed to financial distress.Footnote 1 Liquidity shortfalls may induce a moral hazard problem in the interbank market by exacerbating such behavior by banks. In this paper, we analyze the reaction of banks to liquidity shortages stemming from both sides of the balance sheet. We investigate whether banks react differently to liquidity shocks on the asset (a reduction in the revenue stream due to an increase in the NPLs) and liability (a contraction in the deposit supply) side of the balance sheet. We develop a theoretical model to examine how deposit flights or decreases in the value of the assets limit the access of banks with weak balance sheets to interbank funding. Both types of liquidity shocks weaken the balance sheet and reduce the incentives of banks to restructure their portfolio and to restore solvency. Nevertheless, our model suggests that “NPLs” shocks affect the balance sheets of banks more than “deposit shocks” because the former ones lead to more leveraged institutions. As a result, the debt capacity of banks is more affected under a liquidity shock coming from the asset side. Therefore, the source of a liquidity shortfall is the main determinant of the decision of banks to stop lending in the interbank market, rather than the extra amount of funds that banks need to cover. Our model features a three-period economy populated by two banks that are monopolists in their own deposit markets. Banks invest in loans that mature in the last period, which generate some cash in the interim stage. This cash is lower than the amount of deposits invested, giving rise to a maturity mismatch among assets and liabilities that forces banks to renew part of their deposits. Banks differ in their probability of loan maturity. The bank with a lower probability of success has a chance of increasing it by removing bad loans in the interim stage. However, improving the probability of success is costly. Then, the behavior of this bank depends on the strength of its balance sheet. This bank is also impaired by two mutually exclusive liquidity shocks, which represent relevant events that took place during the recent European financial crisis. The first shock is a “deposit flight” that arises from the liability side of the balance sheet of banks, when depositors require a higher interest rate to keep their savings in banks. The second one is a “cash-flow” shock due to an increase in the NPLs, as in Holmström and Tirole (2000). An increase in the NPLs reduces the cash flow of banks so that they need to renew a higher amount of deposits to fulfill its financial duties. A major implication of our model is that the reason for which banks need extra funding is crucial to explain a breakdown of the interbank market. If retail deposits are the unique source of funding for the impaired bank, then an increase in the NPLs cuts off the “cash-in-hand” of the bank so that it needs to offer higher rates to gather extra deposits for fulfilling its obligations. This increased amount of deposits and of interest rates expand its liabilities, reducing its equity. A deposit flight worsens the equity of the impaired bank as deposits are renewed at a higher cost, but their liabilities remain constant. Nonetheless, the increment in liabilities of the bank due to cash-flow shocks exceeds the reduction in the bank equity due to deposit-flight shocks. As a result, cash-flow shocks harm the debt capacity of banks more than deposit shocks do, since the former ones lead to more leveraged institutions. We examine the role of interbank relationships in the liquidity provision. We extend our model to a large number of banks under Cournot competition. The results of the base model are still valid under this setting. Liquidity shortfalls imply a greater cost for the impaired bank when they arise from the asset side of the balance sheet, regardless of the degree of interbank competition. We also investigate whether credit market competition increases the exposure of banks to an interbank failure. Unlike the traditional literature that focuses on the impact of competition on the risk-taking incentives of banks (Allen and Gale 2004a; Boyd and De Nicoló 2005; Carletti 2008; Berger et al. 2009; Martinez-Miera and Repullo 2010), we analyze the role of credit market competition on the ability of the interbank market to allocate liquidity during a crisis. To our knowledge, Carletti and Leonello (2019) is the only work that examines this issue. They find that credit market competition improves financial stability by reducing the opportunity cost of liquid reserves. Boyd et al. (2007), De Nicoló and Loukoianova (2007), Schaeck et al. (2009), Uhde and Heimeshoff (2009), Soedarmono et al. (2013), Anginer et al. (2014), Schaeck and Cihák (2014), Akins et al. (2016), Leroy and Lucotte (2017), and Goetz (2018) provide empirical evidence indicating that bank competition is associated with greater financial stability. In contrast, the positive relationship between bank competition and risk-taking incentives of banks is well-documented (Brewer III and Saidenberg 1996; Demsetz et al. 1996; Beck et al. 2006; Yeyati and Micco 2007; Ariss 2010; Beck et al. 2013; Jiménez et al. 2013; Kabir and Worthington 2017; Danisman and Demirel 2019). Nonetheless, Berger et al. (2009) and Martinez-Miera and Repullo (2010) accommodate both literature strands by showing that bank competition and risk-taking incentives have a U-shaped relationship. In this paper, we examine how both interbank and credit market competition affect financial stability. Our model implies that interbank competition reduces the liquidity provision cost, enhancing financial stability. On the other hand, credit market competition decreases the cash flow of banks so that they become more dependent to external funding. As a result, banks lower the supply of interbank funding, undermining financial stability. Therefore, we show that competition has a dual effect on financial stability. Several papers have pointed out the inefficient provision of liquidity by the interbank market as a consequence of information problems. Wagner (2007) argues that the interbank market may not allocate liquidity properly under aggregate liquidity shocks. Allen et al. (2009) assert that banks hoard liquidity to face aggregate liquidity shocks that withhold liquidity in the market. Bruche and Suarez (2010) show that deposit insurance may decrease the level of trade of interbank loans due to a rise in the counterparty risk. Acharya et al. (2012) demonstrate that banks with liquidity surplus have a significant degree of market power that allows them to withhold liquidity, inducing inefficient asset sales. Acharya et al. (2011) show that a change in the information available may reduce the ability to use assets as collateral, even though assets have a high fundamental value. Bolton et al. (2011) demonstrate that the adverse selection problem of the value of assets may accelerate inefficiently asset liquidation during a liquidity shortage. Baglioni (2012) establishes that the interaction between liquidity and credit risks may collapse the interbank market when the adverse selection problem is severe. Heider et al. (2015) highlight how asymmetric information can amplify the consequences of the counterparty risk. Accordingly, asymmetric information impairs the role of the interbank market as a liquidity provider. Boissay et al. (2016) applied a dynamic stochastic general equilibrium (DSGE) model to evaluate the effect of banking crises on macroeconomic variables. Moral hazard and asymmetric information in the banking sector may induce credit crunches and abrupt interbank market freezes. In contrast to previous literature, we focus on the source of a liquidity shortfall as a determinant of an interbank failure. To this purpose, we define two types of liquidity shocks that represent relevant events that took place during the recent European financial crisis, and we analyze their effects on the balance sheet of banks. Overall, our framework helps understand the mechanisms of liquidity redistribution, and it has relevant implications for policy makers when regulating capital and liquidity requirements of banks. The rest of the paper proceeds as follows. Section 2 describes our model. Section 3 explains the setup of the model. Section 4 presents a solution to the analytical problem of the agents. Section 5 defines the equilibria of the model, whereas Sect. 6 discusses their main implications. Section 7 examines the impact of interbank and credit market competition on the ability of impaired banks to withstand liquidity shocks. Finally, Sect. 8 concludes the paper.",
16.0,4.0,Journal of Economic Interaction and Coordination,15 April 2021,https://link.springer.com/article/10.1007/s11403-021-00327-4,Public cooperation statements,October 2021,Ann-Kathrin Koessler,Lionel Page,Uwe Dulleck,Unknown,Male,Male,Male,"Many situations in daily life are characterized by a social dilemma structure; the collective is better off if everyone cooperates, but each individual has an incentive to defect and to act upon their self-interest. Hence, a central question in the social sciences is how to stimulate cooperation in group settings. Beside punishment and monitoring, communication has been shown to be an effective approach to strengthen pro-social behavior and cooperation in experimental studies (Ledyard 1995; Chaudhuri 2011). Traditionally, studies which examine experimentally the impact of communication allow their participants to engage in an open discussion before play. While the messages in this communication format are typically costless and non-binding, previous findings suggest that players consider them as a serious opportunity to coordinate (Ostrom et al. 1992). Communication helps to establish a common cooperation norm and improves the understanding of the game (Kerr and Kaufman-Gilliland 1994). In consequence, pre-play communication can stimulate higher cooperation levels in the respective public good games. In reality, howsoever, open discussions between all relevant actors may be time-consuming or simply not feasible (Messick et al. 1983). To restrict communication in these settings may help to ease coordination. In this study, we examine whether actors’ public consent to a cooperation statement is sufficient to promote cooperation within groups. The cooperation statement is pre-determined and non-binding, i.e., it is cheap talk, and this is known by all involved actors. Given that a procedure like this is easier to implement than a common communication forum, and at a lower cost, we believe that it is important to understand if and how these cooperation statements may affect behavior. For example, it is context-dependent whether a voluntary or compulsory statement is more suitable, but our results suggest pro-social behavior lasts longer when all players make the statement. Previous research on communication in bilateral settings provides us with first indications as to why cooperation statements may be effective. Studies which give players the possibility to communicate before play, show that this exchange increases trust and trustworthiness. Individuals exhibit an aversion to lie (Gneezy 2005; Hurkens and Kartik 2009; Lundquist et al. 2009) and when they make a promise about future cooperation, they are likely to act consistently (Festinger 1957) and execute the promised behavior (Ellingsen and Johannesson 2004; Vanberg 2008; Charness and Dufwenberg 2006). Previous studies, however, also indicate that the engagement with the commitment may vary with the form of exchange. Conrads and Reggiani (2017), for example, show that cooperation promises are more likely in real-time communication than when communication is delayed. Social psychological studies, on the other hand, point to the importance of decision autonomy in commitments. Only when individuals feel that they have freely expressed the intention to help or to cooperate, they will feel committed to the promised behavior (Kiesler 1971; Linder et al. 1967; Schlesinger 2011). Elicited or pre-formulated promises, by contrast, have none or only a small effect on the consequent behavior (Charness and Dufwenberg 2010; Belot et al. 2010). At last, public promises have a stronger commitment effect than private pledges (Joule and Beauvois 1998); and being actively engaged in a pledge, i.e., statement-making is in some form effortful, increases the binding function (Kiesler 1971; Linder et al. 1967). Regarding honesty, the effect of pre-play pledges seems to be particularly pronounced. Studies analyzing the impact of solemn oaths or honor codes on behavior show that when vows are taken, participants are more likely to reveal their true preferences (Jacquemet et al. 2013; Carlsson et al. 2013), are less likely to lie in the experiment (Mazar et al. 2008; Jacquemet et al. 2019; Beck et al. 2020) and coordinate better due to more truthful communication (Jacquemet et al. 2018). While a first study suggests that pledging a solemn oath may also have a positive spillover on cooperation (Hergueux et al. 2016), we, on contrast, study the effect on behavior directly: we examine whether public consent with a cooperation statement stimulates pro-social behavior in social dilemmas. At last, this links our study to the literature on leader communication in social dilemmas. In these studies, one player has the opportunity to send a free message about intended contribution levels to the group. Whether these unilateral messages influence behavior depends on their credibility; Koukoumelis et al. (2012) find that contributions to the public good increase after the message, while in Dannenberg (2015) the leader message has rarely an effect. In the latter case, followers correctly anticipate that the leader will not realize her announced contribution level and hence do not get affected by the message.Footnote 1 In our experiment, we analyze a similar form of minimal communication, but the cooperation statement in our study differs in two fundamental aspects. First, the message is not endogenously chosen by one of the group members, but pre-determined by an external institution, that is by us, the experimenter. This allows us to determine what signal is offered as a coordination device. Second, we elicit a consent decision about the cooperation statement from each single player, so that everyone in the group has to take a stand. With this design, we identify ways by which non-enforceable cooperation statements are empirically associated with higher levels of cooperation in a public good setting. At first, we identify whether selection takes place that is more socially oriented people make the statement. First, we determine whether selection occurs, i.e., whether people are more socially oriented in making the statement. Subsequently, using our within-subject design, we evaluate whether in addition to a potential selection effect behavioral change occurs. Subjects which make the statement contribute more after the pledge, and contribution levels are highest when all group members make the statement. We attribute this to improved coordination among the group members and analyze with a separate treatment condition whether the behavioral effects change when the cooperation statements are compulsory rather than voluntary. In our experiment, both types of statements increase cooperation and produce, on average, similar contribution levels. The remainder of the paper is organized as follows. In Sect. 2, we describe the experimental design and list the behavioral predictions. In Sect. 3, we present the experimental results. We conclude with some discussion and potential insights for practice in Sect. 4.
",4
16.0,4.0,Journal of Economic Interaction and Coordination,24 May 2021,https://link.springer.com/article/10.1007/s11403-021-00328-3,Why is parochialism prevalent?: an evolutionary approach,October 2021,Nathan Berg,Jeong-Yoo Kim,Kyu Min Lee,Male,Unknown,,Mix,,
16.0,4.0,Journal of Economic Interaction and Coordination,03 September 2021,https://link.springer.com/article/10.1007/s11403-021-00329-2,Modelling Facebook and Outlook event attendance decisions: coordination traps and herding,October 2021,Julian Inchauspe,,,Male,Unknown,Unknown,Male,"This paper was born from own observations on how Facebook’s—and likewise Outlook’s—event attendees make decisions, which were confirmed in empirical studies. The formal interpretation given to these empirical findings in this paper leads to the main finding that an unavoidable coordination trap, induced by the very design of the tool, has to occur under commonly used settings. This is explained thoroughly with a theoretical model, importantly analysing the conditions under which it occurs and its solution—a simple added option to the event setting tool. The unique insights gained through this fresh theory are expected to impact the understanding of decision made on social networks, enhance user’s experience through software design, and contribute to more successful electronically planned events. With 1.49 billion daily active users and 2.27 active monthly users (Facebook 2018), Facebook has become a major network for social interaction. Its unique features, including media sharing and event invitations, offered from the start a comparative advantage over Friendster, MySpace and other platforms. Facebook rapidly expanded beyond US college networks to attract social users in general (Ellison et al. 2007), and commercial users later (Mangold and Faulds 2009; Bhagwat and Goutam 2013; Ballings et al. 2016). Among the currently available electronic options, Facebook remains the most widely used for organisation of events. The other popular choice, especially in work environments, is Microsoft Outlook’s calendar invitations, introduced in 2002. The social network service (SNS) literature has highlighted the importance of the network user’s attributes and the network itself for attendance of: social and cultural events (Lee 2008; Chang and Sun 2011; Michalko and Navrat 2012; Pessemier et al. 2012; Coppens et al. 2012; Aral and Walker 2014; Bogaert et al. 2016), work events (Mynatt and Tullio 2001; Horvitz et al. 2002; Tullio and Mynatt 2007; Lovett et al. 2010; Daly and Geyer 2011; Fang et al. 2013; Lee and Lee 2018), and academic gatherings (Klamma et al. 2009; Minkov et al. 2010; Zhang et al. 2013). From these empirical studies, only Michalko and Navrat (2012), Zhang et al. (2013), Bogaert et al. (2016) and Chang and Sun (2011) focus specifically on Facebook and factors that relate to this study. Michalko and Navrat (2012) investigates why arranging Facebook events is often problematic and finds the major factor affecting user’s decisions is the survey statement ‘(if) my friend goes there, I’ll go there’. This survey also finds the survey statement ‘Only a few people will go there, I won’t go there’ to be relevant. Zhang et al. (2013) comprehensively analyses the contribution of different factors to the success of Facebook event invitations. Three explanatory datasets are considered: (i) the similarity-based approach, proxied with user and event profile data (location, interests, etc.), (ii) the relationship-based approach, which includes information on whether or not user’s friends will attend the event, and (iii) the history-based approach, built on historic data on previous event participation. The conclusion reached in this study is that the best event participation predictions are obtained when the three explanatory approaches are combined in a single model. This is taken as evidence that factor (ii), which suggests decision interdependence, is an important explanatory element. Bogaert et al. (2016) studies data collected from an experiment that used a Facebook invitation to a football (i.e. soccer) event. Motivated by theories developed following Festinger (1954), McPherson et al. (2001) and Hartmann et al. (2008), a wide range of data were used to predict event attendance empirically—with models including a logistic regression, a neural network and a naive Bayes model, among others. The main finding is that adding data on Facebook user’s friends significantly improves the predictors of event attendance. More specifically, the study verifies three hypothesis: (i) homophily, i.e. a group gathers together because members share similar tastes and preferences; (ii) social influence, or the hypothesis that individuals follow the selection of the network friends; and (iii) trust, i.e. individuals trust the decisions made by their friends in the network. Interestingly, the study finds that the number of user friends attending an event is a statistically top predictor of the user event attendance decision. Once again, this evidence reinforces the notion that a user’s event participation decision is affected by the attendance decisions made by her network of friends. If this basic notion is applied to all network users, then a complex system of interdependent decisions emerges. This paper focuses exactly on that aspect, which has been relatively unexplored from the herding behaviour perspective. Similar results are confirmed in a slightly different setting. Chang and Sun (2011) analyse data for the use of the Facebook’s check-in application, which is location-based. The main finding is that, among the large set of potential predictors considered in the study, previous check-ins and check-ins by friends were the most significant predictors, enriching our collection of evidence that there is users’ interdependence in online event attendance decision-making. While this paper does not disagree or discredit any of the above findings above, it argues that the complex dynamic implications of having decision-making interdependence in the context of SNS event invitations, particularly Facebook’s, are not yet well-understood. The same claim applies to some extent to the Microsoft Outlook event invitation tool. Although there is little-to-no empirical studies for the latter, it will become clear that the same problems facing the type of Facebook events in this paper may apply to Outlook. The main contribution of this paper is the demonstration that undesirable outcomes may arise from the typical Facebook setting with a model, while identifying intrinsic factors that drive this outcome. Specifically, it is shown that the unavoidable implication of accepting the fact that network friends do influence a user’s decision on whether or not to participate in an event, is that in some cases the event will fail to attract attendees and materialise. The inheriting paradox that comes with it is that the event per se may be highly desirable—if enough friends attended it—however, the process may stagnate at a low level of event participation acceptances, causing a ‘trap’. The model will be self-contained and presented intuitively for a broader audience, and discussion on how it is robust to various settings will follow. It is worth pointing to the non-expert reader that this type of dynamic traps or coordination failures are well-known in microeconomic theory literature, with externalities and public goods being typical examples. The skeleton of these widely accepted theories builds on game theories introduced after the revolutionary work of Nash (1950, 1951) on non-cooperative outcomes in interdependent decision-making. Despite the various applications of dynamic interdependent decision-making found in the literature, no model has been developed to specifically address the coordination drawbacks to which Facebook’s and Outlook’s event tools are subject to, to the best knowledge of this author. I organise the remaining contents as follows. Section 2 presents the decision-making environment that applies to the Facebook’s and Outlook’s event invitation tools. The multi-player decision model is then introduced via cost–benefit analysis for each user. Section 3 presents the main result, via calibration and intuitive analysis first, and formalised towards the end. Section 4 performs sensitivity analysis by introducing changes to the main model setting to show that the coordination trap still occurs under varied conditions. I also offer a discussion on the implications for software design. Section 5 considers a variation of the model to show that coordination traps may also occur in the Facebook’s location-based check-in tool environment, followed by additional discussion. Section 6 concludes.",
16.0,4.0,Journal of Economic Interaction and Coordination,02 July 2021,https://link.springer.com/article/10.1007/s11403-021-00330-9,A statistical field approach to capital accumulation,October 2021,Pierre Gosselin,Aïleen Lotz,Marc Wambst,Male,Unknown,Male,Male,"Despite its predominant role in economic modelling, the representative agent paradigm dismisses the interplay between micro- and macroscales. For instance, collective effects stemming from heterogeneous agents’ interactions, and conversely the specific impact of the whole system on agents’ dynamics, are set aside. We argue that a large number of heterogeneous agents do not reduce to a global aggregate entity independent from its components. It should rather be seen as an environment whose characteristics largely depend on the interactions it emerges from, and that in turn impacts diversely the agents composing it. Various states of the environment may emerge that have diverging consequences on individual agents. Several branches of the literature study the interactions within a large number of agents. They inspect dynamics and equilibria arising at the collective level that are inaccessible to the representative context. However, these approaches usually rely on numerical, parameter-dependent simulations. As a consequence, and more importantly, they do not describe analytically the possible environments and their impact on individual dynamics. An alternative approach to large systems of heterogeneous agents can be drawn from statistical field theory. Extending Kleinert’s (1989) method to economic dynamical systems, Gosselin et al. (2017, 2020) showed that the microeconomic description of a system of agents can translate into an associated field theory. Its form, although non-standard compared to physical models, can be handled using usual theoretical physics techniques. This statistical field theory describes the environment formed by an infinite number of interacting agents, from which various phases—or collective patterns—may emerge. Agents’ behaviours, how they are influenced by and interact within their environment, may then be studied. Here, field theory is an efficient modelling tool to get new insights into the micro- and macrodescriptions of a system, while keeping the relevant economic features. Translating standard economic models into a statistical field model is a two-step process. In a first step, the usual set of optimising agents is replaced by a probabilistic description of the system. In such a setting, individual optimisation problems are discarded. Each agent is described by a time-dependent probability distribution centred around his classical optimisation path. In a second step, the individual agents’ description is replaced by a model of field theory that replicates the properties of the system when N, the number of agents, is large. Although approximate, this modelling is compact enough to allow an analytical treatment of the system. The advantages of this translation are threefold. First, it preserves the agents’ main microeconomic features, such as utility, production function, etc. Heterogeneity among agents stems from their initial position in the economic space, endowments, productions, or preferences. An action functional and its partition function constitute the associated field theory which encodes the microscopic interactions of the agents. Second, a microscopic system translated in terms of field is a good tool to study the emergence of collective states: each field minimising the action provides a background field—or ground state, or phase—that encapsulates a collective state emerging from the system. The possibility of several phases reminds of multiple equilibria dynamics that could remain undetected in the context of the representative agents. Third, once known, the phase of the system directly impacts the individuals’ dynamics. Expanding the action functional around the minimum yields the effective action of the system, which itself allows to recover the probabilistic dynamics of one or several interacting agents. Given initial conditions and interactions with others, each agent’s individual stochastic paths may be studied within a given phase. The parameters of the system are encapsulated in the form of the ground state and may drastically change the description at the individual level. Thus, the interdependence of the macro- and microlevels can be studied. The present paper applies a simplified version of this field formalism to a model of capital accumulation for a large number of interacting heterogeneous producer–consumer agents to focus on capital dynamics. Each agent is described by his production, consumption, capital stock and position in an abstract space of exchanges. Each agent produces one differentiated good whose price is fixed by market clearing conditions. Production functions are Cobb-Douglas, and capital stocks follow the standard capital accumulation dynamic equation. Interactions arise from exchanges and competition and depend on agents’ positions in the exchange space. Agents consume all goods but prefer goods produced by their closest neighbours. For the sake of simplicity, we assume ad-hoc love of variety consumption functions. Thus, not only does demand depend on prices, but also on the distance between consumers and producers in the exchange space. The closer the agents, the higher their propensity to exchange and the higher the demand. Moreover, the position of each agent is itself dynamic. Agents in the exchange space are subject both to attractive and repulsive forces. Exchanges drive agents closer, but beyond a certain proximity, closest agents crowd out more distant ones. In this context, heterogeneity among agents stems from differences in initial capital, position in the exchange space and time-dependent shocks in individual dynamics. The dynamic exchange space presented in this model allows to study the production, exchanges, market shares and capital accumulation behaviours within a large group of agents. What are the patterns of accumulation across agents? Is there a threshold effect in initial capital that induces an inequal capital accumulation? Is there a phase in which a better wealth distribution could be reached? All these questions can be addressed within our formalism. In our model, this formalism yields the probabilistic dynamics of individual agents through the computation of so-called transition functions. Given an initial capital stock and position in the exchange space, each individual stochastic path can be found and depends on parameters such as the strengths of interaction forces, the rate of capital depreciation and the uncertainty in economic variables. We show that depending on the strengths of interaction forces, two phases of the system may appear. In a first phase, the repulsion force compensates the attractive force induced by the exchange. Capital accumulation and mobility highly depend on initial capital stock. However, this phase is stable and allows capital accumulation for most agents. The second phase appears when the attractive force is dominant. In this phase, strong attractive forces enhance exchanges and competition. In turn, highly unequal patterns of accumulation arise. High initial capital agents accumulate faster than in the first phase, whereas low initial capital agents are evicted from exchanges. The first section reviews the literature. Section 3 describes a classical model of capital accumulation with N economic agents. Section 4 translates this classical model into a probabilistic framework. Section 5 presents the field formulation associated to the model. Section 6 describes the resolution of the system and presents its two phases and the results. Section 7 interprets the results. Section 8 concludes.",
16.0,4.0,Journal of Economic Interaction and Coordination,02 September 2021,https://link.springer.com/article/10.1007/s11403-021-00333-6,Poverty traps across levels of aggregation,October 2021,Dylan Fitz,Shyam Gouri Suresh,,,Male,Unknown,Mix,,
16.0,4.0,Journal of Economic Interaction and Coordination,21 August 2021,https://link.springer.com/article/10.1007/s11403-021-00334-5,"Communication, choice continuity, and player number in a continuous-time public goods experiment",October 2021,Yoshio Iida,,,Male,Unknown,Unknown,Male,"In recent years, there has been growing interest in continuous-time experimental studies. In a continuous-time experiment, players can change their choice at any moment and note their opponents’ current choices during the given game duration. A player's payoff for each moment is determined by applying group members’ decision at the moment to a payoff function or payoff matrix of the game. Throughout the duration of the game, the player continues to receive a value based of the payoff as his/her score.Footnote 1 Studies reveal that players show strikingly cooperative behavior in such a setting. For example, Friedman and Oprea (2012) showed that the degree of cooperation increases with repeated experience of the 60-s continuous-time prisoner’s dilemma (PD) game and eventually reaches a remarkably high level of cooperation. Bigoni et al. (2015) also showed these high rates of cooperation and concluded that cooperation is easier to achieve and sustain when the horizon is deterministic than when it is stochastic. Oprea et al. (2014) conducted a 600-s continuous-time public goods (PG) experiment. Although their results did not show high rates of contribution without communication, the mean of the players’ choice jumped to nearly full contribution when players were allowed to communicate with their group members. The positive effects of communication in PG experiments are well known, but the study showed that the effect was more drastic in continuous-time experiments than in discrete-time experiments. In the continuous-time PG game, players' decisions are asynchronous, and players can react immediately to others’ decisions. There are many real-world examples where relationships between individuals are that of continuous-time game. Examples related to the PG include contributions to local community activities, team work in business and sports, volunteer computing, and efforts to avoid overuse of shared resources. While these issues have been investigated by one-shot or repeated experiments in previous studies, as Oprea et al. (2014) pointed out, many PGs have a real-time aspect and provide utility that continuously flows in.Footnote 2 Though it may result in outcomes contrary to the public interest, there are also examples of continuous-time PG/PD game in the market. In e-commerce, companies can monitor rivals’ prices and instantly adjust their price. In that case, oligopolistic companies can cooperate by tacit agreement to maintain high prices.Footnote 3 The development of research using continuous-time experiments could also be meaningful in addressing these issues. There are few studies on continuous-time experimental studies, and much room for research is left. This study reveals several features of individual behavior in continuous-time experiments to contribute to future research on this topic. First, this study verifies the robustness of the continuous-time PG experiment conducted by Oprea et al. (2014). Despite differences in subject pools (US and Japanese undergraduate students) and differences in onscreen operability in the experiment, the results of this study are similar to those of Oprea et al. (2014). Second, this study clarifies the elements that are necessary for communication to bring high contribution rates in continuous-time PG experiments. Although many experimental studies reveal the effectiveness of communication for cooperative outcomes, the contribution made by the information (exchanged through communication) toward forming group coordination is not clear. Oprea et al. (2014) conducted an experiment in which players could communicate using a few fixed sentences and found that the degree of contribution was considerably lower than that with the free communication treatment. This study investigates an experiment that used fixed-form sentences based on the results of free communication and finds that the average contribution of the fixed-form treatment was close to that of the free communication treatment. Many groups attained group-level full contributions (all members contributed all their initial endowment to PG) by using only the fixed sentences of 1) a call for cooperation with other members and 2) a reply to the call.Footnote 4 This would spur future research to examine the minimum information exchange options needed for individuals to achieve coordination in a social dilemma situation. Third, the study examines why it is difficult for the continuous-time PG experiment with no communication to attain mutual cooperation compared with continuous-time PD with no communication, by conducting cross-experiments between them. There are differences in the number of players (four or two) and the choice of strategy (continuous or discrete) between the PG and the PD experiments. To verify which difference more strongly hinders cooperation, this study conducted two experiments: one in which players joined a group of two and their strategy choice was continuous, and another in which players joined a group of four and their strategy choice was discrete (no contribution or full contribution). In addition, the present study conducted a two-player, discrete-choice PG experiment that is substantially identical to the PD experiment. The results show that the number of players is much more important for realizing high contribution rates, whereas the strategy choice of discrete or continuous hardly matters. The next section, Sect. 2, reviews related literature. Section 3 explains the experimental design. Section 4 presents the results of the continuous-time PG experiments, compares the results of experiments of free communication and fixed-form sentence communication, and investigates how the number of members of the group and the difference in strategy choice affects the contribution behavior in each treatment. Section 5 presents the conclusions.",
17.0,1.0,Journal of Economic Interaction and Coordination,25 December 2021,https://link.springer.com/article/10.1007/s11403-021-00339-0,Foreword of the Special Issue: Nonlinear Economic Dynamics (2019),January 2022,Laura Gardini,Davide Radi,Fabio Tramontana,Female,Male,Male,Mix,,
17.0,1.0,Journal of Economic Interaction and Coordination,04 September 2020,https://link.springer.com/article/10.1007/s11403-020-00298-y,Evolutionary oligopoly games with cooperative and aggressive behaviors,January 2022,Gian Italo Bischi,Fabio Lamantia,,Male,Male,Unknown,Male,"In classical oligopoly models, the goal of each firm is to maximize (or at least to increase) its own profits. Typically in these models, strategic interaction only occurs indirectly because of the influence of competitors’ decisions on economic quantities regarding other competitors, such as the market price through the demand function or the production costs through cost externalities [for example, in the presence of spillover effects, see, e.g., D’Aspremont and Jacquemin (1988)]. However, even direct strategic interaction may be considered if some firms in the oligopoly market include (at least partially) competitors’ profits in their own objective function they wish to maximize (or increase). Following this idea, Cyert and DeGroot (1973) introduce partial cooperation by inserting a fraction of competitors’ profit (modulated by a coefficient of cooperation) in each objective function (see also Bischi et al. 2010, chapter 4). This may be related to the fact that firms wish to produce inside industrial districts where similar firms operate, in order to take advantage of common infrastructures, knowledge spillovers, cross-shareholdings, etc. On the contrary, different situations may lead some firms to increase their own profits while trying to decrease (at least partially) competitors’ ones. This attitude, which may be denoted as aggressive, may be justified in industrial competition contexts in which it is preferred to weaken the opponent even at the expense of obtaining a lower profit for themselves: a position in a ranking can be achieved both by increasing own gains and/or by decreasing competitors’ ones, an attitude that has been denoted as spiteful behavior in the literature [see, e.g., Vriend (2000); Vallée and Yildizoglu (2009)]. The literature on strategic delegation also fits into this context, see Fershtman and Judd (1987) and De Giovanni and Lamantia (2016). The issue of the evolution of cooperation in prisoner dilemma games has been widely discussed. An important point of view to explain possible behaviors different from those found with the maximization of expected utility concerns the distinction between material payoffs and utility payoffs, see Ahn et al. (2001) and Andreoni and Miller (2002). Ahn et al. (2001) also stress the different results in terms of evolution of cooperation when players are matched randomly or repeatedly with the same agent, thus introducing the idea of a kind of memory behind the behavior. Agents can adopt forms of “social preferences,” sacrificing part of their payoffs to increase the average payoffs of the population, as explained in Charness and Rabin (2002). Furthermore, when it comes to situations of strategic interaction even with two agents but which can be chosen within a larger population, the literature refers to the concept of “indirect reciprocity” to explain the evolution of cooperative behavior that would otherwise hardly emerge, see Leimar and Hammerstein (2001) and Nowak and Sigmund (2005) for an extensive overview on the point.
 Janssen (2008) proposes a model in which agents play one-shot prisoner’s dilemma games with the option of withdrawal. The trustworthiness of others when withdrawal is not chosen is assessed through the expected utility of cooperative and defective behavior with the logit model. Janssen (2008) also differentiates between the agents’ utility and expected payoff. The analysis is carried out with artificial agents and shows that the presence of learning about the reliability of opponents increases the general level of cooperation. In this paper, we propose an oligopoly model where firms may behave sometimes as partial cooperators and sometimes as aggressive agents. Firms can switch from a kind of behavior to another one through an evolutionary model driven by a replicator equation where fitness is measured in terms of accumulated profits (monetary payoffs) and actions are guided by extended utility functions encouraging cooperative or aggressive behavior (utility payoffs). This setup is similar to the classic Hawk–Dove game in ecology, like the one proposed by Smith (1982) as a prototypical evolutionary game, just at the very beginning of this research field. As it is well known, a Hawk–Dove one-shot game can give rise to a classical prisoner dilemma, where Hawk (or aggressive) behavior is dominant with respect to Dove (or cooperative) one. Hommes et al. (2018) is a relevant reference for the modeling of evolutionary oligopolies with different behavioral rules. Given that in contexts of industrial competition collusive behaviors are generally not allowed, we assume that cooperative behavior is not determined by an agreement between the agents but by the willingness of each company to go in the direction of a less strong competition that still helps the whole industry to make higher profits. In some sense, the various behaviors are basically implemented to introduce forms of indirect reciprocity in the market. For this reason, each firm chooses at the pre-commitment level which strategy to adopt (cooperative or aggressive) and then observes the choice of its opponent to determine the quantities to be produced and obtains the consequent profit. For the same reason, we do not introduce “punishments to defectors” that in many cases can indeed steer the system toward cooperation but that are out of the picture in the context at hand. Our contribution, moreover, unlike many other similar works, considers accumulated profit as a measure of fitness instead of current profit only. For this reason, we explore the role of memory on the dynamics of the model, or of history, both with analytical methods and with numerical explorations, usually necessary when dealing with global dynamics of nonlinear maps. We completely characterize the case here proposed, which is the linear oligopoly setup with linear inclusion of one’s opponent’s profit in the extended objective function. Here, we present the analytical results that apply to all possible levels of cooperative or aggressive behavior. We manage, in this simple context, to characterize the model for each possible value of these parameters. In this way, we add some insights to the vast literature on the evolutionary stability of the Walrasian equilibrium (or more generally to aggressive behavior) in an oligopolistic market, which originated with Vega-Redondo (1997), see Alós-Ferrer (2004), Vallée and Yildizoglu (2009), Apesteguia et al. (2010) and Radi (2017) and references therein. The evolutionary oligopoly model here proposed shows that the Walrasian equilibrium, that we retrieve when firms are “aggressive,” is an equilibrium of the model but it is not always an evolutionary stable one. In particular, we show the presence of a subspace of parameters in which no pure strategy (cooperative or aggressive behavior) dominates so that the dynamics do not converge to a boundary equilibrium with the presence of only the monomorphic configuration of the population. This, in particular, occurs when the cooperative behavior consists of incorporating only a small portion of the profit of the adversaries while the aggressive behavior consists in the heavy penalty toward the adversary. In this case, the presence of an equilibrium of coexistence of the two strategies (polymorphic configuration of the population) is demonstrated. However, this equilibrium can be unstable and gives rise to complex dynamics. We show in particular how these dynamics are influenced by the parameters related to the level of cooperation and by the amount of memory, which is stabilizing in our case. In this regard, it is interesting to note that the role of memory in the literature is not always univocal: sometimes it is locally stabilizing or destabilizing [see Hommes et al. (2012)], sometimes irrelevant for local stability properties of equilibria but influencing the global dynamics of the system (see Bischi et al. (2015) and Bischi et al. (2020)). For instance, Bischi et al. (2018) find that a short memory is destabilizing compared to the case without memory but a long memory (at the uniform limit) is highly stabilizing. All results about existence of equilibria, their stability and bifurcations are reported in the paper also in the dynamic extension with memory. Through global analysis tools, we also explore, numerically, cases of coexistence of attractors. Furthermore, we present at the end of the paper what are the possible dynamic trends for the total profit of the industry when the main parameters of the model vary. The paper is organized as follows. Section 2 presents the basic setup of the model with production strategy choices and evolutionary switches of behaviors based on exponential replicator dynamics with memory, as well as the particular formulation of the model in a market characterized by linear demand and cost functions with firms adopting Nash play to make their production choices. Section 3 collects the main analytical results concerning existence, stability and local bifurcations of the equilibrium points of the model. In Sect. 4, some numerical results are described to investigate bifurcation diagrams, coexistence of attractors each with its own basin of attraction and the dynamics of total industry profits. Finally, the last section is devoted to some concluding remarks and possible extensions of the dynamic model.",2
17.0,1.0,Journal of Economic Interaction and Coordination,30 June 2020,https://link.springer.com/article/10.1007/s11403-020-00290-6,Stability of dynamic asymmetric contests with endogenous prizes,January 2022,Akio Matsumoto,Ferenc Szidarovszky,,Male,Male,Unknown,Male,"Contest games model situations when the players invest in order to increase the probability of winning a given prize. Many studies have examined contests with exogenous prize (Pérez-Castrillo and Verdier 1992; Szidarovszky and Okuguchi 1997; Cornes and Hartley 2005; Yamazaki 2008 among others). These games are equivalent with hyperbolic oligopolies (Bischi et al. 2010), market share attraction games (Hanssens et al. 1990), rent seeking games (Tullock 1980) and innovation tournaments and patent-race games (Baye and Hoppe-Wewetzer 2003), to mention only a few. Exogenous prize values or their assessments cannot describe reality in many cases, when the value or the assessed value of the prize depends on the total effort of the players (for example, in R&D contests, wars, and armament development). Chung (1996), Leidy (1994), Baye and Hoppe-Wewetzer (2003) were the first who analyzed rent seeking games with endogenous prizes. In Chung (1996), the value of the prize was an increasing function of the total effort. For this case (Okuguchi 2005; Corchón 2007) proved the existence of a symmetric Nash equilibrium. Shaffer (2006) developed a model where increasing total effort had a decreasing effect on the value of the prize. In these studies, identical players were assumed in the assessments of the value of the prize as well as in their abilities. The cases of asymmetric players can be divided into three groups: in different value assessments (e.g., Hillman and Riley 1989), in different abilities to convert higher expenditures to higher productivity (e.g., Baik 1994) and also in different financial constraints (e.g., Che and Gale 1997). The existence of the unique pure Nash equilibrium for a special asymmetric rent-seeking game was proved in Szidarovszky and Okuguchi (1997), which result was later generalized by Hirai (2012) and Hirai and Szidarovszky (2013) for asymmetric contest games with endogenous prize assessments. The asymptotical stability of dynamic rent seeking games was first examined in Okuguchi and Szidarovszky (1999) where local linearization was used to show local asymptotical stability of the equilibrium. It is shown that the absence of a dominant player is the sufficient condition for the local asymptotical stability of the equilibrium under gradient adjustments. The same condition was found in Xu and Szidarovszky (1999) as well as in Bischi et al. (2010). Schmidt (2008) gave sufficient local stability conditions for two-player asymmetric contests under discrete time scales with endogenous prize value. In addition it was shown that asymmetric valuation has a destabilizing effect. Okuguchi and Yamazaki (2008) constructed a special Lyapunov function to find sufficient global stability conditions for aggregate games including rent seeking games. Most earlier studies assumed the availability of instantaneous information to all players about the actions of all players as well as in the available data and in the assessments and expectations of the players. All earlier studies had this simplification. Time delays were introduced in many variants of the oligopoly models, including the hyperbolic case, in Matsumoto and Szidarovszky (2018), which also discusses stability conditions if no delay is present as a special case. In this paper, the Hirai and Szidarovszky (2013) model is reconsidered by introducing its dynamic extensions in both continuous and discrete time scales. Our model is more general than the earlier studies by assuming endogenous prize value assessments. In Sect. 2, the basic model is described, Sections 3 and 4 discuss the stability properties in both continuous and discrete time scales. Conclusions and further research directions are given in Sect. 5.",
17.0,1.0,Journal of Economic Interaction and Coordination,05 November 2020,https://link.springer.com/article/10.1007/s11403-020-00302-5,On the significance of borders: the emergence of endogenous dynamics,January 2022,Ingrid Kubin,Laura Gardini,,Female,Female,Unknown,Female,"In the aftermath of the recent financial crises, economic models that consider constraints and boundaries have become increasingly popular, in particular constraints in the financial markets: liquidity constraints, borrowing constraints, credit constraints, collateral constraints, etc. These constraints primarily reflect imperfections in the financial markets. However, there are other constraints with an even longer, albeit less noted, history that do not involve any market imperfections: e.g. capacity constraints, which bind production decisions, feature prominently in industrial organization models, but also in business cycle models in the tradition of Samuelson and Hicks; or incentive and participation constraints that play a central role in contract theory. And there are even more fundamental constraints, such as budget constraints and non-negativity constraints for quantities and prices. Recently, considering inequality constraints in dynamic models, Rendahl (2017) pointed out that this asks for specific analytic methods.Footnote 1 Even less known is another analytic implication: the mere existence of boundaries creates the potential for cyclical and even chaotic dynamic time paths. This neglect is astonishing since the financial crises has powerfully shown that the stability of equilibria and fixed points cannot be taken as granted. Instead, the analysis of out of equilibrium dynamics should be of prime importance. In the following, we show dynamic phenomena that directly derive from the consideration of boundaries. For that purpose, we propose a very simple model that involves boundaries, but is linear otherwise and continuous. Thus, any complex dynamics cannot be attributed to nonlinear functional specifications that might be considered as arbitrary; it directly derives from the boundaries, crossing which the linear system changes definition. In particular, we present a very simple model of market dynamics in the Marshallian tradition. Quantity decisions—on current output and on capacity adjustments—depend on a comparison of the relevant demand and supply price. Output is sold against the market demand function at a market clearing price. On purpose, we assume all functional relationships to be linear. In addition, we explicitly take into account three borders that are intrinsic to the economic reasoning: a non-negativity constraint for prices; a downward rigidity of capacity (depreciation) and a capacity constraint for the production decision. The study of the dynamics which are involved in piecewise linear two-dimensional maps is not a new research subject. It started several years ago, and important early contributions include Gumowski and Mira (1980), Nusse and Yorke (1992), Mira et al. (1996), and Gardini (1992). However, this area is not so well known and developed. Also two influential surveys on analytic tools for economic dynamics, Barnett et al. (2015) and Grandmont (2008), present only tools for smooth functions. Many properties are still to be investigated and in the present paper we apply most recent analytic tools to a prototype economics model. There are also some contributions in economics that do already involve borders and not only with piecewise smooth functions, occasionally also with discontinuous specification of the central dynamic process. In business cycle theory, models in the tradition of Hicks and Kaldor with a “floor” and a “ceiling” (Hommes 1995; Sushko et al. 2003, 2010; Gardini et al. 2006) as well as regime switching models (Tramontana et al. 2010); in growth theory (Matsuyama 1999, 2007; Boehm and Kaas 2000; Kaas and Zink 2007; Tramontana et al. 2011a), in industrial organization theory, managerial economics and competition models (Kopel 1996; Laugesen and Mosekilde 2006; Bischi and Lamantia 2012; Bischi et al. 2012; Schmitt et al. 2017), models of the so-called New Economic Geography and industry location (Currie and Kubin 2006; Commendatore et al. 2008a, b, 2020; Agliari et al. 2011; Schmitt et al. 2018), and also in macroeconomic models with financial markets (Caballé et al. 2006; Tramontana et al. 2011b; Matsuyama 2013; Sushko et al. 2014, 2016; Kubin and Zörner 2019) borders play a crucial role. In fact, in his survey on macro-dynamics, Gomes (2006) identifies the piecewise smooth specification as one of five influential model types. Moreover, improvements in the dynamic tools which can be used to investigate one-dimensional piecewise smooth systems recently appeared (Sushko et al. 2015, 2016), and also two-dimensional piecewise smooth models in economic applications have been investigated (Sushko et al. 2017; Gardini et al. 2018). In our contribution, the focus relies on the connection that exists between the borders, which have a genuine economic rational, and the dynamic specification. As already remarked, we present a simple prototype model of the market dynamics in the Marshallian tradition which involves three borders and in which all functional relations are linear.Footnote 2 Time is discrete and structured by the production period. Decisions on output and capacity depend (linearily) on the difference between the corresponding supply and demand prices. At the end of the production period, output is sold at against a (linear) market demand, and the resulting market prices feed back upon quantity decisions. Output, capacity and market prices are subject to a non-negativity constraint, output is bounded by a capacity constraint, and capacity reductions cannot exceed depreciation. The resulting model is piecewise linear in two dimensions (output and capacity) and involves three borders. In particular we discuss three dynamic phenomena, the occurrence of which are intimately related to the existence of borders: centre bifurcations, border collision bifurcations and degenerate flip bifurcations. The emerging dynamics may be complex and in our analysis we pay special attention to the role of the borders in shaping the resulting dynamics. The plan of the paper is as follows. In Sect. 2, we present the model and define the different regions in the phase space in which the continuous map changes its definition. In Sect. 3 we describe the dynamic behaviour generated by this map. In particular, Sect. 3.1 completely characterises the unique fixed point of the map, showing that it loses stability via a centre bifurcation (see Sushko and Gardini 2008) that always (i.e. independently on the parameters of the map) occurs through a rotation of period six and leads to a stable period-6 cycle (belonging to an attracting closed invariant curve, saddle-node connection of a pair of 6-cycles). In addition, we show that this period-6 cycle loses stability through a so-called secondary centre bifurcation leading to annular chaotic areas (see Gardini 1994; Gardini et al. 2011) as well as to chaotic areas in one piece (see Mira et al. 1996; Fournier-Prunaret et al. 1997; Maistrenko et al. 1998; Sushko et al. 1999). In Sect. 3.2, we shall describe two additional dynamic phenomena, which are caused by the existence of the borders. In particular, we describe the so-called border collision bifurcations [following the terminology introduced by Nusse and Yorke (1992)], which occur if some periodic point of a cycle merges with a border line; and we describe the peculiar character of a flip bifurcation which, due to the linearity of the functions, is degenerate (Sushko and Gardini 2010). Section 4 concludes.",
17.0,1.0,Journal of Economic Interaction and Coordination,18 October 2020,https://link.springer.com/article/10.1007/s11403-020-00304-3,Competition and strategic alliance in R&D investments: a real option game approach with multiple experiments,January 2022,Giovanni Villani,Marta Biancardi,,Male,Female,Unknown,Mix,,
17.0,1.0,Journal of Economic Interaction and Coordination,13 October 2020,https://link.springer.com/article/10.1007/s11403-020-00303-4,Hybrid evolutionary oligopolies and the dynamics of corporate social responsibility,January 2022,Tomáš Tichý,Davide Radi,Fabio Lamantia,Male,Male,Male,Male,"The inclusion of social strategies in the main corporate objectives is a widely debated issue among academics and leading experts. The more traditional vision, authoritatively defended by M. Friedman [see “The social responsibility of business is to increase its profits” in Friedman (1970)], regards the maximization of profits as the only objective for all shareholders and considers other activities only as losses both in the value and in the focus of a company. On the other hand, the pursuit of social strategies in addition to those purely related to profits has been associated with the creation of important corporate values and pursues higher objectives that society as a whole expects from its economic protagonists, as more recently remarked in Porter and Kramer (2002). An example of social strategy is that of corporate philanthropy and, more generally, of corporate social responsibility (CSR, hereafter) which, as clarified in Seitanidi and Crane (2009), allows “the alignment of strategic business interests with societal expectations.” The paradigm of including CSR activities into firms’ actions is nowadays considered more appropriate for correct business management. In practice, many companies carry out CSR actions at various levels and with varying intensity. It is now common practice, in fact, to issue a social report with international standards for CSR activities; see UNI ISO 26000:2010 on Social Responsibility,Footnote 1 although activities of this kind are not a legal requirement for firms but only encouraged by, for example, the European-Union law, see directive 2014/95/EU, through a comply-or-explain principle. Corporate philanthropy and CSR can be also interpreted as strategic devices, linking these activities with the potential advantages that they bring to firms in terms of staff motivation, increase in reputation, and decrease in R&D costs; see on this point Smith (1994) and Hillman and Keim (2001). Abstracting from motivational or reputational aspects, the current work questions the endogenous adoption of CSR-like strategies in an oligopoly market, which is populated by profit-oriented firms and is represented by a dynamic model. An element of peculiarity of the model is the assumption that firms form beliefs about the composition of the oligopoly market, and they estimate the probability that a competitor is a socially responsible company according to an evolutionary scheme and trust this belief for a given window of time. On the contrary, the decision about the output to produce is revised continuously on the basis of the expected level of production of the competitors. The asynchronous decision process of the firms is represented by a hybrid evolutionary oligopoly model. A hybrid evolutionary model is an evolutionary model in which the purely evolutionary aspect of the system evolves with a different time scale than that of other system states. See, e.g., Bischi et al. (2013a, (2013b) and Lamantia and Radi (2015), for applications of hybrid evolutionary models to describe the diffusion of environmentally friendly technologies, or practices, in the fishery industry. This modeling framework generalizes, therefore, the classical evolutionary games used for modeling players’ updating process of the strategy to adopt or the behavioral rule to follow. See Droste et al. (2002), Bischi et al. (2015) and Hommes et al. (2018) for examples of evolutionary oligopoly games, of non-hybrid type, where firms select the behavioral rules to determine a response plan to the strategies of the opponents. In the scientific debate on the economic sustainability of socially responsible behaviors or practices, evolutionary models have been employed for studying “mixed” markets, where firms can maximize objective functions that include non-profit components. In particular, the best relative performances are not always obtained by profit-maximizing firms and mixed market configurations may endogenously arise, see, among others, Heifetz et al. (2007), Koçkesen et al. (2000) and Bendle and Vandenbosch (2014). Relatedly, Königstein and Müller (2001) show that under evolutionary pressure the inclusion of a share of consumer surplus in the firm’s objective function may provide strategic advantages to the firm. A specific application of evolutionary games in an oligopoly model with CSR activities is studied in Kopel et al. (2014), which is the contribution most closely related to this work. In Kopel et al. (2014), the choices related to CSR activities lead to a variation of the firm’s objective function, in which CSR activities translate to the maximization of profits plus a share of the consumer surplus, extending Kopel and Brand (2012) to an evolutionary setup. All in all, Kopel et al. (2014) shows that in some cases the social strategy can be beneficial for shareholders in order to gain a competitive advantage over opponents. Kopel and Lamantia (2018) deals with the model of Kopel et al. (2014) and explores the effect of increasing competition pressure on CSR activities. In that paper, it is shown that in some cases an inverse U-shaped relationship arises between the long-term survival of social strategy and the intensity of competition. Compared to Kopel et al. (2014) and Kopel and Lamantia (2018), the current model introduces an interpretation of CSR more focused on charity actions (“corporate philanthropy”) or socially responsible projects (such as investments devoted to reducing the carbon footprint) and therefore proposes a different objective function for CSR firms with respect to the mentioned contributions. Specifically, we propose a simple “mixed” oligopolistic model with competition between traditional firms, which maximize their profits in each period, and firms oriented towards CSR behavior, which allocate part of their profit to charities or for projects aimed to reduce environmental, social and corporate government risks, instead of maximizing their profits plus a share of the consumer surplus as in Kopel et al. (2014) and Kopel and Lamantia (2018). The “target-less approach” adopted is consistent with the recent legislation which, on the basis of the explain-or-comply principle, requires to generate a non-financial document that contains information about how much funds are devoted to certain CSR activities.Footnote 2 Based on the information on the CSR reports, a company undertakes actions of CSR disclosure. Although the final product provided by firms satisfies the same needs of consumers, thanks to the mentioned actions of CSR disclosure the CSR activity of a company increases the consumers’ willingness to pay for the purchase of its products, see Manasakis et al. (2014). In this way, the final consumer faces a dichotomous choice when buying the good: she/he can pay a lower price knowing that the company from which she/he buys will not carry out any “socially responsible” behavior or she/he can decide to pay a surcharge and see part of his expenses go to charitable activities. As in Kopel et al. (2014), the solidarity aspect introduces, therefore, an effect that is in some way analogous to vertical product differentiation, despite products are “physically” homogeneous. The product of the socially responsible firms, therefore, can satisfy a need of the consumer who can be willing to spend more to contribute to good initiatives. This choice by the consumer is free as long as s/he can also buy cheaper goods manufactured by pure profit-maximizing firms. The existence of socially responsible firms and profit-maximizing firms depends on strategic aspects. In fact, a firm is corporate socially responsible or pure-profit maximizer according to which strategy proves to be successful, i.e., it is able to win the competition of the best relative performance. The employed objective function for socially responsible firm, which is different from the one in Kopel et al. (2014) and Kopel and Lamantia (2018), is already an addition to the current literature. In fact, the concept of CSR is broader than that considered here, or in Kopel et al. (2014) and Kopel and Lamantia (2018), and adds to the profits those considerations related to society and the environment in the so-called triple bottom line, see Elkington (1994). In the proposed model, on the other hand, the CSR action is configured as devolution of profit in these activities so that in the model we abstract on the effective use of resources set aside for social activity. Compared to Kopel et al. (2014) and Kopel and Lamantia (2018), a further modeling innovative choice is the process of updating beliefs which is asynchronous with respect to the output decision process and the frequency of updating beliefs is parametrized. To complete the modeling framework, a possible cost for charity is considered, with the marginal costs of the socially responsible firms that may be larger to account for such a cost. Modeling firms as Nash players, that is, they produce the Bayesian–Cournot Nash equilibrium quantities as in Kopel et al. (2014), the stability of the Bayesian–Cournot Nash equilibria is investigated as a function of the fraction of profits donated for charity or invested in socially responsible projects by CSR firms.Footnote 3 In particular, the higher propensity of consumers to pay for products realized by socially responsible firms is considered fixed and the amount of profits devoted to charity is left to vary. Therefore, the investigation regards a situation in which consumers are willing to pay more because a firm is socially responsible and devote a certain fraction of its own profits to charity or to finance social projects but independently of how much charity activity and socially responsible activity are effectively conducted. Parameterizing the willingness to pay and the fraction of profit devoted to CSR activity offers modeling flexibility that captures the heterogeneity of targets and interests that CSR activities aim to reach and which are difficult to represent with a single objective function. The analysis of the dynamics of the model reveals that an industry homogeneously populated by socially responsible companies is a stable equilibrium when the fraction of profits earmarked for socially responsible activities is sufficiently limited. However, the extra marginal profits of a socially responsible firm (obtained as the result of the trade-off between extra marginal revenues on each quantity unit of product sold and fraction of profits devoted to charity) are reduced when the number of competitors increases, impeding the diffusion of socially responsible companies. In particular, keeping constant the fraction of profits devoted to CSR activities, an increased number of competitors impacts on the trade-off between a higher net margin on sales obtained by socially responsible firms and a lower level of individual production that reduces the profit gap among firms of different types. By increasing the size of the oligopoly this trade-off modifies in such a way that mixed oligopolies emerge and are more likely. Moreover, when imposing the hypothesis of neutrality of CSR activities, according to which the competitive advantage of being a socially responsible company is offset by the higher costs associated with social activities, see in particular Williams et al. (2006) and Piga (2002), the model reveals that being socially responsible is an evolutionarily stable strategy for firms and is convenient for customers. The benchmark model is confronted with a setup that introduces forms of bounded rationality in the decision process of firms in the sense that firms follow a so-called partial-adjustment-towards-the-best-response process, see Bischi et al. (2010), to determine the output to produce instead of coordinating to play the Bayesian–Cournot Nash equilibrium. The investigation of the dynamics of this model reveals the robustness of the results. In fact, introducing the described form of bounded rationality, analytical results show that the equilibria of the model are the same. Nevertheless, numerical experiments suggest that an heterogeneous population of firms representing an equilibrium configuration tends to lose stability when the frequency with which firms update their beliefs increases and to have the same stability property as in the benchmark model when the frequency of belief updating decreases. A plausible economic explanation for this phenomenon is related to the fact that reducing the frequency of belief updating, then the partial-adjustment-towards-the-best-response process has more time to converge toward the Nash strategy. Summarizing, the current investigation adds to the literature on CSR activities by questioning the sustainability (Darwinian-like survival) of socially responsible companies, while the majority of contributions on this field focuses on the optimal level of CSR activities when government subsidies for being socially responsible are present; see, e.g., Arya and Mittendorf (2015). Specifically, showing that socially responsible companies can survive when supported by socially responsible consumers that are willing to pay more for their products may reduce the attention on government subsidies and their sustainability for public finances. Moreover, it may spark interest in the market conditions that favor the diffusions of socially responsible practices. The paper is organized as follows. Section 2 describes the static setup. Section 3 introduces a hybrid evolutionary version of the model characterized by firms that behave as Nash players and by an updating mechanism of belief about the composition of the industry. In Sect. 4, the hypothesis of Nash players is relaxed and firms determine their output according to a partial-adjustment-towards-the-best-response process. Section 5 concludes. The analytical results are in “Appendix A.”",2
17.0,1.0,Journal of Economic Interaction and Coordination,06 October 2020,https://link.springer.com/article/10.1007/s11403-020-00301-6,Modeling maladaptation in the inequality–environment nexus,January 2022,Angelo Antoci,Paolo Russu,Elisa Ticci,Male,Male,Female,Mix,,
17.0,1.0,Journal of Economic Interaction and Coordination,07 January 2021,https://link.springer.com/article/10.1007/s11403-020-00312-3,Speculative housing markets and rent control: insights from nonlinear economic dynamics,January 2022,Noemi Schmitt,Frank Westerhoff,,Female,Male,Unknown,Mix,,
17.0,1.0,Journal of Economic Interaction and Coordination,14 February 2021,https://link.springer.com/article/10.1007/s11403-020-00313-2,Stochastic sensitivity of bull and bear states,January 2022,Jochen Jungeilges,Elena Maklakova,Tatyana Perevalova,Male,Female,Female,Mix,,
17.0,1.0,Journal of Economic Interaction and Coordination,28 August 2020,https://link.springer.com/article/10.1007/s11403-020-00296-0,Does too much liquidity generate instability?,January 2022,Giorgio Calcagnini,Laura Gardini,Edgar S. Carrera,Male,Female,Male,Mix,,
17.0,1.0,Journal of Economic Interaction and Coordination,31 October 2020,https://link.springer.com/article/10.1007/s11403-020-00306-1,"Autonomous demand, multiple equilibria and unemployment dynamics",January 2022,Piero Ferri,Fabio Tramontana,,Male,Male,Unknown,Male,"There is no doubt that the Great Recession has stimulated the reconsideration of multiple equilibria in the literature. The idea of an economy having both “good” and “bad” equilibria has become appealing in almost every paradigm. So, while Geanakoplos (2003, 2010) stresses the importance of multiple equilibria characterized by the presence of agents with different degrees of liquidity within a General Equilibrium approach, Boissay et al. (2016) pursue the same objective by focusing on the banking crisis but within a DSGE model. In the same paradigm, Basu and Bundick (2017) study the relationship between uncertainty and multiple equilibria, while Aruoba and Schorfheide (2013) consider the presence of two equilibria in an economy marked by a zero lower bound in the rate of interest. Finally, Takahashi and Okada (2020), within an agent-based approach, present a dynamic model characterized by the presence of two equilibria based upon seller’s and buyer’s markets. Also the Post-Keynesian field has been affected by the events of the economy, even though the presence of multiple equilibria has a long and persistent story (see Dutt 1997) rooted in the idea that growth is a history dependent phenomenon (see Robinson 1956). Many of the new contributions, above all those among the Kaleckian Post-Keynesians (KPK), have insisted on the role that income distribution can have in generating multiple equilibria within a growth model (see Assous and Dutt 2013). In the present paper, we try to investigate another route, not necessarily an alternative one, leading to multiple equilibria by means of a nonlinear relationship between unemployment and durable consumption. The model represents an extension of two previous papers studying the relationship between autonomous demand, endogenous supply, growth and unemployment. Fazzari et al. (2020) have inserted the role of autonomous demand into a Harrod–Minsky model, where aggregate supply accommodates to demand (on this aspect, see also Palley 2018). Unemployment remains bounded, while the reconciliation process between actual growth and the natural one takes place. Ferri et al. (2019) have enriched the model by taking into account a further feedback from unemployment to aggregate demand by considering heterogeneous consumers (see also Kaplan and Violante 2018), along with learning agents. In this environment, the dynamics between aggregate demand and supply become more interdependent and the instability process more complex (see also Ferri 2019). In the present model, the role of multiple equilibria is stressed, so that the analysis of the triptych of properties, i.e., existence, uniqueness and stability, is completed. In particular, the emphasis is put on the demand for durable consumption goods, which is characterized by a trend component cyclically tempered by the (nonlinear) presence of the rate of unemployment. The consequences of this assumption are twofold. On the one hand, both the aggregate demand and supply determine the steady-state values of growth and unemployment. On the other hand, the presence of nonlinearity can generate two equilibria, one with high unemployment and low growth, and the other with low unemployment and high growth. The explanation of the two equilibria is rather simple: growth in the demand for durable consumer goods depends, among other things, on an exogenously given trend rate of growth of demand and the level of unemployment. Thus, there is a high unemployment with low growth equilibrium because of low aggregate demand growth due to low durable consumption growth, and a low unemployment high growth equilibrium because of high aggregate demand growth. What is more, the two equilibria have different dynamic properties. The “virtuous” one is unstable, while the other one can be stable. The thesis put forward is that growth is a dynamic phenomenon drawn by disequilibrium processes that are history dependent. This state of affairs helps generating complex dynamics when considered within a global perspective and open the way to study policy measures in order to thwart their consequences. Furthermore, this conceptualization has an impact on the nature and on the dynamics of unemployment. The structure of the paper is the following. Section 2 introduces autonomous demand into a medium-run model. Section 3 shows the determinants of aggregate demand. All variables are presented in intensive form, i.e., they are deflated by last period output. Section 4 introduces the specification of an accommodating supply. Section 5 considers the steady states of the model. Section 6 simulates the dynamics. Section 7 carries out a sensitivity analysis for various parameters. Section 8 discusses the basin of attraction of the global system. Section 9 concludes.",3
17.0,1.0,Journal of Economic Interaction and Coordination,01 March 2021,https://link.springer.com/article/10.1007/s11403-021-00320-x,"A stylized macro-model with interacting real, monetary and stock markets",January 2022,F. Cavalli,A. Naimzada,N. Pecora,Unknown,Unknown,Unknown,Unknown,,
17.0,1.0,Journal of Economic Interaction and Coordination,25 June 2021,https://link.springer.com/article/10.1007/s11403-021-00331-8,Existence and implications of a pitchfork-Hopf bifurcation in a continuous-time two-sector growth model,January 2022,Giovanni Bella,Paolo Mattana,Beatrice Venturi,Male,Male,Female,Mix,,
17.0,1.0,Journal of Economic Interaction and Coordination,03 February 2022,https://link.springer.com/article/10.1007/s11403-022-00346-9,Local environmental quality and heterogeneity in an OLG agent-based model with spatial externalities,January 2022,Andrea Caravaggio,Mauro Sodini,,Female,Male,Unknown,Mix,,
17.0,1.0,Journal of Economic Interaction and Coordination,26 November 2021,https://link.springer.com/article/10.1007/s11403-021-00335-4,Large and uncertain heterogeneity of expectations: stability of equilibrium from a policy maker standpoint,January 2022,Domenico Colucci,Matteo Del Vigna,Vincenzo Valori,Male,Male,Male,Male,"In economic literature, heterogeneity in behavior or capacity to forecast was commonly overlooked. An implicit or explicit claim of equivalence between the full model with heterogeneous agents and a corresponding model with an average representative agent was often used. Over the last twenty years, however, this approach has raised a lot of criticism in many contexts: for example Colander et al. (2008); Stiglitz and Gallegati (2011) within macroeconomic modeling, Harrison and Rutström (2009) in the field of choice under risk, Fleurbaey (2009) in the context of measures of social welfare, to name only a few. In particular, from the point of view of out-of-equilibrium dynamics, it is now widely accepted that such a simplified (average) model may generate different dynamic outcomes with respect to those attained when heterogeneous agents are explicitly introduced. On the contrary, when the focus of the analysis is on local dynamics, the representative-agent assumption is still considered to be a good compromise granting analytical tractability at the cost of minor shortcomings. This point of view has been epitomized by Grandmont who, studying local stability conditions of the self-fulfilling equilibrium in an adaptive learning economy in Grandmont (1998), reckoned that “... the methods described below [regarding a representative agent economy] can nevertheless be made to bear upon the case of heterogeneous beliefs. The forecast \(_{t}x_{t+1}^{e}\) can then be reinterpreted as an average forecast, each individual forecast being weighted by its relative local contribution to the dynamics of the system ...”. This equivalence however only holds under suitable assumptions. For example, Colucci and Valori (2011) shows that local stability conditions under heterogeneity differ from those with a representative agent even in the standard cobweb model with adaptive expectations.Footnote 1 There is indeed a large body of literature showing evidence of heterogeneity in expectations, both in empirical contexts, e.g., Chavas (2000), and in the laboratory, e.g., Hommes (2011). From the point of view of modeling markets with heterogeneous agents and expectations it has been pointed out that there is a risk of falling in the “wilderness” of bounded rationality, characterized by an excess of degrees of freedom and parameters. Brock et al. (2005) address the issue of dimensionality reduction in a fairly general theoretical framework with many types and develop an analytical result, namely the notion of Large Type Limit, based on the idea of substituting the (large) array of stochastic parameters describing the populations of agents with the (small) parameter vector defining their joint probability distribution, hence reducing the complexity of the model. In particular, individual types are randomly sampled, heterogeneity is aggregated linearly in the function defining the dynamics of the state variable, and agents are allowed to switch over time among types (see Brock and Hommes (1997, 1998)): in this setup, the Large Type Limit entails replacing sample moments with population moments, which for particular distributions of characteristics yield closed-form expressions. The result in Brock et al. (2005) has been applied in several papersFootnote 2 among which Anufriev et al. (2013), Agliari et al. (2017) and Pecora and Spelta (2017), in order to analyze monetary policy issues, taking advantage of the assumption that the forecasting rules are actually constants. Building on Brock et al. (2005), Diks and van der Weide Diks and van der Weide (2005) assume the distribution of beliefs among agents is updated using a Continuous Choice Model,Footnote 3 which leads to price dynamics in which the beliefs’ distribution evolves together with realized prices. They call this setup Continuous Belief Systems, which give rise to random dynamical systems, containing deterministic dynamics as a special case (and therefore generalizing the Large Type Limit setup). The issue of reducing complex agent-based economic models to analytically tractable small-scale ones has also been addressed in Föllmer et al. (2005), and more recently in Schmitt and Westerhoff (2019), among others. The present paper studies a class of models in which there is a significant interplay between uncertainty about behavioral parameters of agents (as understood by an observer, e.g., a policy maker), and how heterogeneous such agents are. One would expect that, as more heterogeneity affects the system, the task of predicting its fate (for example computing the probability that the system eventually converges to a steady state) becomes ever more difficult. However, a mildly surprising asymptotic result contradicts such intuition in a variety of contexts: under suitable conditions, when the amount of heterogeneity goes to infinity, the probability of convergence becomes either one or zero depending on the value of crucial structural parameters of the economy. We describe this phenomenon as polarization. The amount of heterogeneity and its possible variations hence play a critical role in shaping the range of possible long-run outcomes of the model. Various implications seem to be the result of polarization. Policy-wise, knowing that such an effect is in place and that heterogeneity is relatively high may encourage policy makers to take action toward stabilizing the system. Similar types of implications have been reached in contexts close to the present one, e.g., Brock et al. (2009) and Colucci and Valori (2015). The present work rests on a number of simplifying assumptions which need to be briefly discussed in order to lay out the perimeter of this paper. One such assumption is that agents cannot switch between alternative forecasting rules, which is certainly oversimplifying. Laboratory experiments on learning to forecast have in fact shown that individuals use a variety of simple heuristics (see Hommes (2011), Assenza et al. (2014)), but the evidence in favor of evolutionary switching among rules does not appear to be overwhelming.Footnote 4 However our main reason to relinquish this mechanism here lies in the fact that doing so allows us “... to derive an analytically tractable system with a unique equilibrium, which is desirable from a policy maker view point ...”.Footnote 5 Our perspective in this paper is indeed that of an observer or a policy maker whose goal is to learn which conditions favor long-run stability of the economy. Therefore our results here are of a more limited nature with respect to, e.g., Brock et al. (2005), where large expectations heterogeneity is coupled with endogenous switching between forecast rules. At the same time however, the core of this paper, namely Sect. 3, explores a model in which the explicit dependence of expectations at time t on previous expectations is outside the range of a key assumption in Brock et al. (2005).Footnote 6 Our approach to expectations, which are assumed to have some degree of inertia, is supported by evidence that in complex environments observable behavior is, loosely speaking, adaptive (see, e.g., Haruvy et al. (2007); Hommes (2011)). The paper is organized as follows. Section 2 introduces the baseline model for a scalar state variable and in particular delineates how it is stochastic and shows the conditions for local stability and polarization (i.e., what happens to probability of stability in the long run when heterogeneity goes to infinity). The model is generalized in Subsection 2.1 to allow for vector state variables and vector stochastic parameters. Section 3 adds inertia in expectations and studies how results change (given the explosion in the dimension of the random matrix whose spectral radius governs stability) as a consequence. Various generalizations as to the types of laws of motion permitted and examples of such, are presented in Subsection 3.1. Some discussion and directions in which to extend and further develop the present work are indicated in Sect. 4. Concluding remarks are offered in Sect. 5. Appendix gathers all the proofs.",1
17.0,1.0,Journal of Economic Interaction and Coordination,14 September 2020,https://link.springer.com/article/10.1007/s11403-020-00300-7,Long-term causes of populism,January 2022,Gian Italo Bischi,Federico Favaretto,Edgar J. Sanchez Carrera,Male,Male,Male,Male,"Populism consists in political movements that share a demand for short-term protection, such as from immigrants and economic hardship, and it is characterized by three main properties (Guiso et al. 2017): (i) the claim that it protects the people from the elite, (ii) the focus on demand-driven policymaking, and (iii) the disregard for future consequences of their policies. Lately, populism has fostered new enthusiasm and diverse research contributions, spurring from the emergence of parties and political movements labeled both as left-wing (South American populism described by Dornbusch and Edwards 1991; Acemoglu et al. 2013) and right-wing (mostly European and in USA). In current times, populism is sweeping Europe’s political equilibria. A report published by the Guardian in November 2018, (by a group of leading political scientists) found that one in four European voting citizens will be casting their vote for a populist. In addition, we know that there were nine European countries where populists participated in government, for a total of 170 million people in 2018: In 1998, the countries were 2 for a total of 12.5 million. According to this report, the surge in populism had an impact even where these parties did not govern: countries such as the UK, Sweden, Denmark, and Germany saw a strong shift to the right on immigration due to extreme right populist groups. Populism seems to be associated with economic and financial crisis. Indeed if we categorize it as a close substitute to extreme parties on the right and the left, Funke et al. (2016) show that the vote for such parties spiked in elections held after systemic financial crises (in advanced economies between 1870 and 2014): support for extremist parties increased dramatically and especially for far-right parties (\(+30\%\)). Similar evidence for the period post-Great Depression is shown by De Bromhead et al. (2013) who find an increase in the share of votes for right-wing anti-system parties in elections in the 1920s and 1930s, thus confirming a link between political extremism and economic hard times. This paper takes a different stand: Why is right-wing and left-wing populism emerging now? How do stable democracies with mild levels of economic and social problems suddenly see an important spike of preferences for populist parties? We believe that the answers rely on two long-term processes: an increase in the salience of immigration and an increase in income and wealth inequality. We take these two facts as given to build an evolutionary game, in order to understand under what conditions populism will be sustained (or not) in the long-run. The first fact upon which our model is based, is that the salience of immigration almost doubled in Europe between 2010 and 2018 (Eurobarometer, see Fig. 1). This was also confirmed by Dennison and Geddes (2018), who noted that the most important issues for Europeans were immigration and unemployment in several countries. Right-wing populist parties reject immigration for a number of reasons including cultural and religious considerations. The key economic arguments against immigration claim that immigrants: (i) compete with natives in the labor market, take away their jobs and depress wages; and (ii) benefit from the welfare state and contribute little in the form of taxes. Populist parties also tend to reject the notion that migrants are refugees who leave their countries because of war and political prosecution. On the other hand, they claim that migrants are motivated by economic incentives and that many immigrants have entered the country illegally. This gives rise to calls for policy measures to reduce immigration. As a result, these parties propose radical changes to immigration policies. How radical these proposals are, differs considerably across populist parties (see EEAG 2017). Source: Standard Eurobarometer 20. For data series and technical details, see: https://moien.lu/wp-content/uploads/2018/12/Eurobarometer-Nov_2018.pdf Public opinion in the European Union.  A second important fact is that economic inequality has increased substantially since 1980. The World Economic Forum (WEF) gathered data from the World Bank, the Organisation for Economic Co-operation and Development and other sources, along with other indicators to create the Inclusive Development Index 2018, a snapshot of the gap between rich and poor. According to the WEF index, economic inequality has risen or remained stagnant in 20 of the 29 advanced economies while poverty increased in 17. Moreover, the report states, both in advanced and emerging economies wealth is significantly more unequally distributed than income, and the problem of economic inequality has improved little in recent years.Footnote 1 In this vein Han (2016) states that the average Gini coefficient of market (pre-tax, pre-transfer) income inequality in Western European countries increased by over 20% between 1980 and 2010 (the Standardized World Income Inequality Database, SWIID), and the average Gini coefficient of net (post-tax, post-transfer) income inequality increased by approximately 15% over the same period. In the past four decades, the poorest 80% Europeans’ average incomes grew from about 20 to 50% (Fig. 2). As soon as one looks at richer income groups, however, growth rates are markedly higher, exceeding 100% for the top 1% and culminating at 200% for the top 0.001% of European citizens. Between 1980 and 2017, the top 1% alone captured 17% of European-wide growth, compared to 15% for the bottom 50%.Footnote 2 Source: Blanchet et al. (2019). For data series and technical details, see: www.wid.world/europe2019 and https://voxeu.org/article/forty-years-inequality-europe Income inequality and growth in Europe: Growth incidence curve, 1980–2017.  Based on the above economic policy issues, this paper builds a political economy evolutionary game that defines a model of political choice based on demand for policies against immigration and economic inequality (populist policies looking for simplistic solutions to complex problems). We study what is called demand for populism, instead of the supply (Guiso et al. 2017; Rodrik 2018), taking for granted that there will be some suppliers of this political demand. We assume that populist parties are perceived by citizens as substantially more aggressive in pursuing these policies: populist leaders are not fully aware of institutional, law, administrative limitations and their rhetoric is grounded on changing overnight policy outcomes. For our framework, the essential part is that citizens believe populists to be more aggressive in pursuing these policies compared to mainstream center-right or center-left parties. The advantage of immigration policy is that it relies on identity politics: strong words, fierce verbal opposition to immigration, a connection to violent right-wing groups, and a reference to traditional and religious values may all be part of it. Moreover, populists usually propose simple and catchy solutions to complex problems and this probably convinces relatively ignorant citizens that action has been taken. Notice that citizens vote on perceptions of policies: despite international laws on immigration and refugees, we assume that common citizens tend to prefer the party that promises to “flex more the muscles” against this potential threat from a cultural and identity standpoint. We assume that citizens cannot distinguish a growth perspective between a populist government and a mainstream party one. A minority of citizens may have ideas about it, but by a great majority it is highly improbable that common citizens will penalize a populist party for fear of low economic growth. In actual populists agenda, there is a strong tie with demand policies such as higher government spending using debt emission. Since we want to analyze long-term evolutionary equilibria, our approach does not focus on it because these policies are implemented only for a short period of time (few years) before taxes need to be increased. Moreover, these policies can last even less for highly indebted countries such as Italy, Greece and Portugal. By contrast, we consider a longer time frame that allows us to assume a Ricardian taxation framework: we assume that expansionary policies funded by debt will eventually be repaid by higher taxes or higher interests on government bonds due to default risk. These assumptions allow us to concentrate on one key economic aspect of the vote for populists: the demand for less economic inequality. There is some evidence that high economic inequality is associated with higher support for extremist parties. For instance, Dorn et al. (2018) examine German counties and find that the poorer ones and the ones where within-county inequality was higher, had a higher share of votes for extremist parties. We consider an evolutionary game with two groups of players and two strategies to study the economic and psychological consequences of supporting populism. How do the economic consequences for voting populists work? In our model, citizens are labeled as Rich and Poor at any time t because of their pre-existing wealth. The division of citizens between Rich and Poor helps us to focus on distributional issues linked to voting. Hence, the Rich are assumed to work, while the Poor receive subsidies funded by distortionary taxation. This setup simplifies the economic inequality framework to a degree that the same group owns higher wealth and income. Hence, this allows us to focus on the effect of the redistribution policy by considering higher tax rate for the Rich.Footnote 3 Citizens evaluate whether to support populists based on two dimensions: whether the redistribution policy goes in their favor and by what amount and whether they have a non-economic benefit from supporting them. This benefit may be broadly conceived but we define it as the salience of immigration. How does the psychological benefit for voting populists work? We model a behavioral mechanism assuming that the elite (Rich) and the Poor may have different constant fears of immigrants due to, for example, self-protection against prospective offenders, higher proximity to housing competition, labor market competition or by higher neighborhood presence (Antoci et al. 2017). At the top of the constant part, the fear of immigrants increases if other citizens become supporters of populism and decreases if otherwise. This assumption is based on a typical feature of politics: the more a party has support, the more citizens perceive that it will achieve its political goals. We allow for different marginal effects of citizens’ support due to the fact that it is highly probable that the elite will value more the opinions and political choice of a fellow member than a Poor one. The idea that people refer to a group for psychological benefit comes from Passarelli and Tabellini (2017) and Favaretto and Masciandaro (2020). As opposed to other contributions, we assume that only non-economic issues are part of the psychological benefit of supporting populists. We focus on the dichotomy classical versus populist parties, simplifying the political spectrum and reducing it to two parties to study which of the two should prevail in a long-term perspective. Our approach is compatible with all types of non-mainstream parties, namely populist, extremists, anti-system: this allows us to be as general as possible in capturing the two dimensions of voting. The only assumption that we care to underline is that as long as there are strong demands for these two types of policies, one or more populist parties will emerge to represent it. In our framework, we are able to consider these populists all together, as an expression of the same political demands. Hence, this paper makes several contributions. First, it provides a rational choice analysis of populism as populism is commonly understood. Second, it shows the long-term causes and hence the stability or instability of a populist system. Third, it addresses two questions: (i) what are the economic roots of populism? and (ii) what are the factors that affect the emergence of right- versus left-wing populism? Therefore our contribution is original in two ways. Firstly, we tackle the demand for populism by a game-theoretic approach (closely related to a political choice and/or behavioral economic model). Secondly, we use the evolutionary game theory approach that allows us to see how changes in the share of citizens that support populists affect the equilibria. The structure of our paper is as follows. Section 2 offers a literature review highlighting what is our main contribution with respect to what already exists in the literature. Section 3 sets up the model, while Sect. 3.1 develops the replicator dynamics and it offers the dynamic equilibria and stability properties. Section 4 analysis the stability of the evolutionary equilibria showing the different cases and consequences for supporting or not supporting populism. Section 5 provides a conclusion.",2
17.0,1.0,Journal of Economic Interaction and Coordination,23 February 2021,https://link.springer.com/article/10.1007/s11403-020-00311-4,Preying on beauty? The complex social dynamics of overtourism,January 2022,Angelo Antoci,Paolo Russu,Giorgio Tavano Blessi,Male,Male,Male,Male,"The rapid increase of global mobility that has characterized the mature phase of the globalization process in the past couple of decades has also, as a consequence, led to the escalation of ‘overtourism’ issues in many global tourism destinations, and most notably in major art and heritage cities. Despite massive flows of tourists clearly benefit the local economy, they also pose a major threat to both the livability and, in some cases, even the sustainability of cities that are literally consumed by a level of human occupancy they weren’t designed or intended to host. In Barcelona, where the number of overnight stays escalated from 1.7 millions in 1990 to more than 8 millions in 16 years, overtourism is one of the key causes of an environmental pollution emergency (Ledsom 2019). In addition to the most renowned tourist locations, the geography of overtourism is also rapidly expanding due to the global visibility acquired by some cities for having been the shooting location of successful TV series, as in the case of Dubrovnik for Game of Thrones (Wiley 2019). However, an increasing number of critical voices are questioning this trend, locally as well as internationally (Economist, 2018). For residents, overtourism may have dramatic consequences. Housing for permanent residential use becomes increasingly scarce and expensive. Services catering to the needs of locals become rarer, more difficult to reach, and again more expensive. The constant noise and the overcrowding of streets and local transport can be a source of considerable stress for working people, families with small children and the elderly. In cities like Venice, the number of bed-and-breakfasts and flats for short-term tourist occupancy has nearly doubled in the space of just one year (Tantucci 2018). As a consequence, residents are evicted by landlords who find way more profitable to rent to tourists. In Florence, for instance, between October 1, 2017, and June 30, 2018, as many as 478 residents who couldn’t keep up with the rising rents had to leave their homes, including lifetime ones: 209 living in the historical center, 71 in the Unesco area and 198 in other areas of the city (Conte 2018). More generally, the so-called airification (Picascia et al. 2019) has been identified as a disruptive force that is literally ‘hollowing out’ cities (Hinsliff 2018). Such a state of things does not come as a complete surprise to the tourism studies literature. Although early warnings were appropriately sent, as in the seminal paper by van den Borg et al. (1996), they have not succeeded in convincing local policy makers to devise appropriate countervailing strategies and to take action. Now that the negative effects of the phenomenon are becoming indisputable, however, some cities are starting to react aggressively. Amsterdam has banned the concession of new licenses to business within the historical city core that offer goods and services targeting tourist demand (O’Sullivan 2017), as a way to curb the ‘Disneyfication’ of the city (Boztas 2017). Bruges has strictly limited the maximum number of cruise ships that may be hosted at its port’s docks on a daily basis and has limited its own tourism-related advertising in major nearby cities (Marcus 2019). Venice has implemented a very severe set of restrictions to many different kinds of tourist misbehavior, sanctioned with heavy fines (Spinks 2018). Ten major European heritage cities such as Amsterdam, Barcelona, Berlin, Bordeaux, Brussels, Krakow, Munich, Paris, Valencia and Vienna have jointly signed a letter to the new EU Commission asking for severe limitations to the further expansion of Airbnb and other holiday rental websites (Henley 2019). However, it is not easy to go against such a powerful trend, despite that the current COVID-19-related crisis that has caused a temporary collapse of the tourism industry worldwide will probably provide overcrowded tourist cities with an unexpected opportunity to prevent the eventual return to the ‘old normal’ once the pandemic is over (Higgins-Desbiolles 2020). The vested interests that rely upon the extractive logic of the mass tourism economy are a major local consensus pool and exert powerful political pressure (Benner 2019). On the other hand, the needs of tourists and residents significantly differ, and this is likely to spark conflict between different local stakeholders, depending on the extent to which they benefit from tourism (Concu and Atzeni 2012). Whether or not a city eventually gets colonized by the tourism economy or manages to find a reasonable compromise can therefore be the result of a very complex interplay of factors. It is therefore of particular importance to study under what conditions such interplay leads to different long-term scenarios, thus enabling public decision makers to better understand not only the nature of the problem in order to imagine and test possible solutions, but also the critical conditions that regulate the emergence of possible outcomes. Merely proposing ‘plausible’ or ‘just’ solutions is not enough. We also need to assess whether such solutions would work, and under what circumstances, once they are actually implemented. In principle, solutions that are more desirable in abstract terms need not be the ones that work best. As cities are very complex dynamical systems, the pursuit of the public interest, which in this case identifies to a significant extent with that of city residents, whose ‘right to the city’ (Lefebvre 2010) should be the object of special consideration and protection, needs to be supported by evidence-based policies building upon a sound understanding of the underlying economic and social dynamics. The aim of this paper is that of studying a simple dynamic model that analyzes the effects of the tension between residents and tourists in the social usage of city resources. We focus on the interplay of the essential factors behind such tension: the substitution between resident-oriented and tourist-oriented facilities and shops, the congestion of city space from overtourism, but also the experience value of cities as related to the effective presence of residents as a source of authenticity. Given that the escalating tourist flows are literally preying on the city’s resources from the residents’ viewpoint, it is natural to think of modeling such dynamics with the predator–prey framework in mind. We introduce an expanded variant of the predator–prey dynamics, which yields more complex dynamic behavior than the original one, and allows a better analytical treatment of the main factors at play. The model’s structure is easily interpretable, but the corresponding dynamics are not obvious. In particular, we show that the actual dynamic trajectories of the system may be very different for relatively small changes in the key parameters. This implies that even relatively small differences in local conditions and in policy actions may cause divergent outcomes, with substantial differences in terms of their social desirability. Our results should be read as a cautionary tale against delayed or unsystematic action in curbing the social costs from overtourism: intervening too little or too late, or not focusing on the truly critical parameters might lead to disappointing results. The remainder of the paper is organized as follows. Section 2 offers a brief review of the main issues discussed in the overtourism-related literature. Section 3 presents the model. Section 4 contains the main results. Section 5 discusses the results and concludes. A technical Appendix closes the paper.",2
17.0,2.0,Journal of Economic Interaction and Coordination,03 May 2022,https://link.springer.com/article/10.1007/s11403-022-00354-9,"Introduction to the special issue on the 24th annual Workshop on Economic science with Heterogeneous Interacting Agents, London, 2019 (WEHIA 2019)",April 2022,Fabio Caccioli,Tiziana Di Matteo,Simone Righi,Male,Female,Female,Mix,,
17.0,2.0,Journal of Economic Interaction and Coordination,03 December 2020,https://link.springer.com/article/10.1007/s11403-020-00309-y,Community structure in the World Trade Network based on communicability distances,April 2022,Paolo Bartesaghi,Gian Paolo Clemente,Rosanna Grassi,Male,Male,Female,Mix,,
17.0,2.0,Journal of Economic Interaction and Coordination,07 December 2021,https://link.springer.com/article/10.1007/s11403-021-00338-1,Systemic liquidity contagion in the European interbank market,April 2022,Valentina Macchiati,Giuseppe Brandi,Giulio Cimini,Female,Male,Male,Mix,,
17.0,2.0,Journal of Economic Interaction and Coordination,13 February 2021,https://link.springer.com/article/10.1007/s11403-021-00317-6,"Firm–bank credit network, business cycle and macroprudential policy",April 2022,Luca Riccetti,Alberto Russo,Mauro Gallegati,Male,Male,Male,Male,"Financial accelerator mechanisms are well established in the recent literature, showing how business cycle fluctuations can be enlarged by self-reinforcing forces. Bernanke and Gertler (1989, 1990, 1995) and Bernanke et al. (1999) show the presence of a positive feedback mechanism: a reduction of asset values held by the entrepreneurs generates an increase in the borrowers’ leverage and subsequently of the risk premium, with a consequent contraction of the economic activity. Indeed, firms are less prone to invest because they compare a reduced expected profit with an increased cost of funding; therefore, reduced investments lead to a lower output in a vicious circle. The recent crisis, in countries like Italy, shows that a negative shock on firms’ output makes banks less willing to loan funds, with a consequent credit crunch beside the increase in the interest rate. Riccetti et al. (2013a) couple the explained financial accelerator, called “leverage” accelerator, with the “network-based” financial accelerator proposed by Delli Gatti et al. (2010). The latter accelerator highlights that the presence of a credit network may produce an avalanche of firms’ bankruptcies: the bankruptcy of a firm may bring “bad debt”—i.e., non-performing loans—that affects the net worth of banks, which can also go bankrupt or, if they manage to survive, they will react to the deterioration of the net worth increasing the interest rate to all their borrowers (Greenwald and Stiglitz 1993, p. 145), making them incur additional difficulties in servicing debt and thus increasing the weakness of the whole non-financial sector, in another vicious circle. Riccetti et al. (2016) further enlarge these mechanisms adding the “stock market accelerator,” also representing possible financial market bubbles, in order to build a triple financial accelerator. Starting from the cited literature strand (Delli Gatti et al. 2010; Riccetti et al. 2013a, 2016), in this paper we build an agent-based macroeconomic model able to represent the firm–bank credit network under different phases of the business cycle. Differently from the cited papers, the business cycle is exogenous and can be set in order to reproduce various scenarios. Moreover, the current model performs a series of improving changes and refinements. In practice, we try both to build a model similar to the cited ones without some of the strong assumptions made in those models, and to reproduce some further stylized facts: Firstly, decisions about output production and capital structure are separated; with this mechanism, the production function does not necessarily need decreasing return to scale in order to stabilize the model’s output and we use constant return to scale. As for the capital structure, we improve the dividend mechanism including the possibility of stock repurchase by firms. The use of stock repurchase is very relevant for two reasons. First, it makes the model more realistic. Indeed, in the last decades, stock repurchases are increasingly used in place of dividends (Fama and French 2001). On these bases, Skinner (2008) suggests that repurchases are now the dominant form of payout. Moreover, “over the decade 2001–2010, the 500 corporations in the S&P 500 Index (representing about 75% of US stock-market capitalization) expended not only 40% of their profits on cash dividends—the normal mode of rewarding shareholders—but also another 54% on stock buybacks, the purpose of which is to give a manipulative boost to a company’s own stock price” (Lazonick 2013). Second, it helps us to prevent a too strong growth due to an excessive accumulation of internal resources which causes a reduction of firms and banks’ defaults and reduced business cycle fluctuations. To avoid this excessive growth, while in Riccetti et al. (2013a, 2016) we assumed decreasing return to scale, in the current model we adopt a stock repurchase mechanism, which makes the model much more realistic. Moreover, we allow banks to merge in order to avoid the presence of small “empty” banks (namely inactive banks that are “alive” but without customers). Another major change regards the structure of the credit market: firms and banks are located over a unitary space and firms may ask credit to multiple banks, therefore a complex spatial network arises. Again, this modeling choice is performed to make the model more realistic, in order to represent the stylized facts that the distribution of node degree is right-skewed with larger banks characterized by a higher number of links, and that larger banks supply credit both to large and small enterprises, while small banks supply credit only to (relatively) small and local enterprises. In addition, the interest rate mechanism is devised in order to consider the default probability of the firm asking for credit. Lastly, banks’ credit supply is constrained by Basel III rules. This feature allows us to perform a simulation analysis on banking regulation, as done in some other agent-based models, such as Neuberger and Rissi (2012), Cincotti et al. (2012), Krug et al. (2015), Da Silva and Lima (2017), Popoyan et al. (2017) and Riccetti et al. (2018).
 Agent-based models are particularly suited for such kind of investigation which involves the analysis of a multilayered network of financial stocks and flows and its macroeconomic implications due to the dispersed interaction of heterogeneous agents (Tesfatsion and Judd 2006). Last decades have been characterized by a blossoming of macroeconomic agent-based models: for example, Riccetti et al. (2015) proposed a macroeconomic model in which heterogeneous households, firms and banks interact on various markets through a decentralized matching protocol that successively has been employed in other papers in order to develop a fully fledged macroeconomic agent-based model,Footnote 1 like in Caiani et al. (2016). The model of Riccetti et al. (2015) showed that the financial leverage (especially bank exposure ) is nonlinearly related to the macroeconomic performance, given that as the leverage increases the economy tends to expand though with decreasing gains and up a certain threshold after which more leverage increases the probability of crises. The model is also able to endogenously generate business cycles as well as small and large crises. Other agent-based macroframeworks have been proposed, for instance in Dosi et al. (2010), Cincotti et al. (2010) and successive extensions of both papers. One of the main ingredients of such kind of macroframeworks, which is at the same time one of the main advantages of using the agent-based methodology, is the presence of direct interaction,Footnote 2 in particular regarding financial networks, which is a characterizing feature of the present paper. In this way, it is possible to overcome a number of limitations arising from assuming the presence of a Representative Agent: “no financial markets (who is lending to whom?); no scope accordingly for excess indebtedness (who owes money to whom?) or for deleveraging (who is reducing their indebtedness to whom?); no problem of debt restructuring; no meaningful capital structures (since the single individual is bearing all the risk, it is obvious that nothing can depend on whether finance is provided in the form of debt or equity); no role for bankruptcy”. (Stiglitz 2011, p. 598). One of the first attempts to analyze the interplay between financial inter-linkages (with a static network) and the business cycle is in Delli Gatti et al. (2006), then extended in Delli Gatti et al. (2010) with a dynamically evolving network with firm–bank credit relationships. The present paper has its roots in this modeling tradition,Footnote 3 highlighting the fundamental role of credit interconnections and their relation with the characteristics of the business cycle. As explained above, the explicit description of the network structure of credit and financial markets allows us to investigate the working of the financial accelerator in a network economy, and then to take into account phenomena like financial contagion and bankruptcy chains. The literature on financial contagion has stressed the role of agents’ interactions and the endogenous formation of networks as key ingredients of systemic risk: while the increase in the number of links in a financial network can be a stabilizing force due to risk-sharing (Allen and Gale 2000), the particular configuration of a growing network can also be destabilizing, possibly producing more non-performing loans and increasing the likelihood of default avalanches (Lux 2016; Bottazzi et al. 2020); in other words, the financial system can exhibit a “robust yet fragile” behavior according to which stability prevails but catastrophic events can happen with a nonzero probability; financial network models can prove useful to reconstruct contagion dynamics and proposing appropriate macroprudential tools (Caccioli et al. 2014). The present paper extends the analysis performed in previous macroagent-based models to incorporate the role of financial regulation, specifically macroprudential policy, to mitigate financial instability mainly through the countercyclical capital buffer. The remainder of the paper is organized as follows: in the next section, we present the model. Simulation results are reported in Sect. 3, while Sect. 4 proposes a macroprudential policy experiment on the countercyclical capital buffer. Finally, Sect. 5 concludes.",9
17.0,2.0,Journal of Economic Interaction and Coordination,01 July 2021,https://link.springer.com/article/10.1007/s11403-020-00315-0,A network approach to expertise retrieval based on path similarity and credit allocation,April 2022,Xiancheng Li,Luca Verginer,P. Panzarasa,Unknown,Male,Unknown,Male,"The increasing complexity of research problems calls for innovative solutions which combine knowledge from different scientific disciplines (Van Rijnsoever and Hessels 2011). As a result, many researchers become involved in interdisciplinary projects, and collaborate with people with a variety of expertise. When facing the task of finding collaborators, scholars need to answer two inter-related questions: (1) How to identify an expert, i.e., how to find someone who is competent in a given field; and (2) how to profile an expert, i.e., how to identify the fields in which a given scholar is an expert. In general, both questions jointly describe the objective of expertise retrieval (Balog et al. 2012). Indeed figuring out the research area associated with an individual represents a challenging research problem. Search engines such as Google Scholar or DBLP are of great help for finding documents (Hertzum and Pejtersen 2000). However, these engines only return scientific documents, not the specific expertise of people. Even in an academic environment, researchers still have to rely on their social networks to identify the expertise of others (Hofmann et al. 2010). Identifying experts is crucial for academic groups when they need to involve a collaborator with specific expertise. In organisational settings, knowing the expertise of relevant researchers facilitates the assignment of important roles and jobs. For example, conference organisers may search for moderators, session chairs and keynote speakers with the proper expertise. And universities may want to recruit researchers with expertise in a particular fast-developing area to improve their reputation. A good method for expertise retrieval is therefore fundamental to provide the necessary knowledge for such activities. Expertise retrieval is challenging for many reasons. First, expertise is a relatively abstract concept, and there is currently no consensus on how to define it. Besides, expertise is a particular kind of knowledge stored in one’s mind and thus hard to identify. The only way to access people’s expertise is through their works, e.g., documents, books, articles. Second, experts’ names are often ambiguous. A single name may belong to multiple people, and the name of the same expert can vary in different databases. Indeed name disambiguation has recently become a specific and independent area of enquiry, and many studies have been carried out in this field (Smalheiser and Torvik 2009). Finally, it is difficult to evaluate the strength of the association between an expert and the works he or she has been involved in, especially because an increasing amount of scientific production is co-authored by multiple individuals. Those challenges have made expertise retrieval a multifaceted research area. In particular, since we learn about researchers’ expertise mainly from their publications, the task of expertise retrieval has mainly been articulated into identifying the knowledge areas or topics in the text corpus and assigning them to the researchers (Silva et al. 2018). Inspired by previous approaches to dealing with credit allocation (Shen and Barabási 2014) and by recent studies on finding node similarity in heterogeneous information networks (HINs) (Shi et al. 2014), we formalise the topics and expertise extracted from a given scientific publication as credit to be assigned to the co-authors of the publication and propose a new method to allocate them to the co-authors based on their publication histories. Traditional approaches to the identification of the knowledge areas within the text corpus use topic-modelling methods such as Latent Dirichlet Allocation (LDA) based on controlled vocabulary from well-known classification systems such as the Medical Subject Headings (MeSH) in MEDLINEFootnote 1 and the topic tags in Microsoft Academic Graph (MAG).Footnote 2 Our work focuses on the process of evaluating the degree of each co-author’s contribution to a collaborative work. We propose a new method for properly assigning the expertise to each co-author according to his or her contribution. Our method differs from traditional ones where the contribution of authors is assumed to be equal or assessed simply based on the order of authors in the byline. Moreover, our method can deal with large-scale data sets and produces results that vary dynamically as the data set is updated over time. Unlike some citation-based approaches to the assessment of contributions, which require a certain time to account for the citations that accumulate over time, our method is experience-based and the update of authors’ expertise is determined once the new records are added into the data set. The rest of the article is organised as follows. In Sect. 2, we review strengths and limitations of existing literature on expertise identification and motivate our work. In Sect. 3, we introduce the data used in our study. In Sects. 4 and 5, we present our new method and different selection strategies. In Sect. 6, we provide some extensions to account for weights and time. In Sect. 7, we report results obtained using the MEDLINE corpus and various examples. Section 8 summarises the findings of this work and outlines their implications for research and practice.",4
17.0,2.0,Journal of Economic Interaction and Coordination,12 March 2021,https://link.springer.com/article/10.1007/s11403-021-00319-4,A simulation of the insurance industry: the problem of risk model homogeneity,April 2022,Torsten Heinrich,Juan Sabuco,J. Doyne Farmer,Male,Male,Unknown,Male,"The modern insurance systemFootnote 1 has its roots in the establishment of Lloyd’s of London in the 1680s, which was named for a coffee house that catered to marine insurance brokers. The first major crisis followed less than a decade later after the Battle of Lagos in 1693. During this battle a fleet of French privateers attacked an Anglo-Dutch merchant fleet causing estimated losses of around 1 million British poundsFootnote 2 (Leonard 2013a; Go 2009; Anderson 2000). Risk assessment was inadequate and underestimated several risk factors.Footnote 3 Worse, it was not only some few underwriters that took the risk of writing policies for this merchant fleet, it was a significant part of the industry. 33 underwriters went bankrupt. The English parliament considered legislation that would have resulted in a government bailout (House of Commons 1693), but the bill failed (Leonard 2013a, b). Today’s insurance–reinsurance systems build on centuries of experience. Modern insurance companies have moved beyond the coffee house and are built on a more solid institutional foundation and are hopefully more prudent and more competent in assessing risks. Nonetheless, the example serves to illustrate two points. First, catastrophic damages are difficult to anticipate with any accuracy and unanticipated high losses remain a reality. More recent examples include the Asbestos case, the Piper Alpha disaster, the 2017 Caribbean hurricane season, and the fallout of the 2020 Covid-19 pandemic. Second, a lack of diversity in risk models can lead to problems at a systemic scale. The insurance industry has made huge progress in its ability to estimate risks. But there is more to the insurance business than simply estimating individual risks. Companies need to make a variety of decisions, such as how much total risk to take, how much capital to hold in reserve, and how to set premiums. Insurance companies compete with each other and so they do not make these risks in isolation. This can lead to systemic effects that create systemic risks that are not visible to individual firms. Our goal here is to create a model that makes it possible to study systemic effects at the level of the insurance industry as a whole. To do this we simulate individual firms and the perils they ensure using an agent-based approach. We also simulate how firms set premiums, how they manage their capital, and how these actions affect each other. Our model is the first to simulate the catastrophe insurance industry at this level. Here we use the model to address a specific problem, the dangers of consolidating the entire industry under a few risk models. This is relevant as regulatory frameworks, such as the European Solvency II, may cause such consolidation as a side effect. We also explore the role of the reinsurance industry in mitigating risks. However, the possible uses of this model go beyond those we explore here. The paper is organized as follows: Section 2 provides a short description of the insurance industry for context. Section 3 gives an overview of previous work, and Sect. 4 introduces the model. The results are discussed in Sect. 5. Section 6 concludes.",4
17.0,2.0,Journal of Economic Interaction and Coordination,24 November 2021,https://link.springer.com/article/10.1007/s11403-021-00337-2,The competitions of time-varying and constant loadings in asset pricing models: empirical evidence and agent-based simulations,April 2022,Hung-Wen Lin,Jing-Bo Huang,Shu-Heng Chen,Unknown,,Unknown,Mix,,
17.0,2.0,Journal of Economic Interaction and Coordination,28 February 2022,https://link.springer.com/article/10.1007/s11403-022-00347-8,Staring at the Abyss: a neurocognitive grounded agent-based model of collective-risk social dilemma under the threat of environmental disaster,April 2022,Danilo Liuzzi,Aymeric Vié,,Male,Male,Unknown,Male,"Wildfires, flooding, hurricanes: more extreme weather events attributed to climate change seem to happen each year. Numerous treaties, initiatives, regulations, reports investigate the matter and propose various actions to limit the warming extent. However, environmental awareness as consciousness in the population and political structures of the extend of the threat appears limited, as the undertaken actions fail to match the severity of the danger (Eriksen et al. 2014). One critical obstacle to such awareness may be the unequal distribution of climate change consequences on the planet (King and Harrington 2018). Countries and populations less harmed by climate change may not internalize the extent of the threat for more impacted communities. Moreover, several difficulties arise when studying the emergence of awareness in a population facing an ecological danger. The complexity of interactions and emergence at the population level have been intensively studied, from the tragedy of the commons (Ostrom 1990) to more recent literature on cooperation reviewed in Sect. 2. We add that even within a single individual, such a decision is not straightforward. One may become aware of climate change and willing to change one’s actions for deliberative reasons, such as reading the scientific literature on the topic. Another may adopt this stance facing the emotion of a newsworthy catastrophe somewhere on the planet. Others may be sensitive to the adoption of this opinion in their social circles. In sum, there are many ways environmental awareness could arise. We propose an agent-based model that explores the dynamics of a collective-risk social dilemma. Uncooperative and myopic exploitation of the environment can trigger a detrimental regime shift, where environmental resources and services are no longer available. In this failure of the social dilemma, resources are only restored once hysteretic dynamics have unfolded. We model the environment as a (shallow) lake (Scheffer 1989), a bistable system where either a clean state or a turbid state can be reached, depending on the history of pollution discharge into the lake. Whenever the turbid state arises, agents must temporarily cease polluting activities, bowing to the restoration they cannot manage. Agent decisions, aggregated over the entire population, may push the lake toward a polluted (turbid) state, but the way back to the clean state is out of their hands. The agents are endowed with more natural cognitive abilities, building on the Agent Zero framework (Epstein 2014). Deliberative, affective and social components are behind the agent decisions, accounting for both bounded rationality and the plurality of channels by which dispositions to act and opinions are transmitted. This multifaceted model allows us to obtain a more precise understanding of the emergent process of cooperation induced by environmental awareness, and in so doing, it helps us understand how to prevent the transition to the polluted state possibly. Simulation results emphasize that environmental awareness is maximum when the socio-environmental system reaches critical pollution levels. Moreover, more resilient natural conditions, such as higher natural pollution decay, do not increase awareness, as the agents internalize these constraints and are encouraged to pollute more. These results explain why, in collective-risk social dilemmas, cooperation is the strongest when the system is at the edge of collapse and that more resistant environments (lower probability of collapse) may shelter a less environmentally concerned population. We analyze how the activation of different modules, or combinations in the Agent Zero framework, impacts how environmental awareness arise. As significant differences arise, as in other uses of this framework (Vié 2019), we suggest that this superposition of cognitive channels for decision-making is helpful for social simulation. The paper proceeds as follows. Section 2 describes the relevant literature on the topic of social cooperation and environmental dilemmas. Section 3 presents the details of the model. Section 4 presents the results, while Sect. 5 discusses the main findings of the paper. Section 6 concludes and presents avenues for future research.",1
17.0,3.0,Journal of Economic Interaction and Coordination,10 October 2021,https://link.springer.com/article/10.1007/s11403-021-00336-3,Bank demand for central bank liquidity and its impact on interbank markets,July 2022,Di Xiao,Andreas Krause,,Female,Male,Unknown,Mix,,
17.0,3.0,Journal of Economic Interaction and Coordination,16 December 2021,https://link.springer.com/article/10.1007/s11403-021-00340-7,Closing the invisible hand: a rehabilitation of tâtonnement dynamics,July 2022,Donald C. Keenan,Taewon Kim,,Male,Unknown,Unknown,Male,"Equilibrium analysis, like the study of most solution concepts, may be thought of as having three main positive components: existence, uniqueness, and stability. In addition, there is the normative issue of optimality of the solution, and then returning to positive economics, the possibility of comparative statics. General equilibrium theory has been quite successful on the fronts of existence and (Pareto) optimality, but much less so with uniqueness and stability. Yet stability, in particular, is essential if the equilibrium paradigm of the invisible hand is to rest on firm foundations.Footnote 1 One can simply ignore how one arrived at equilibrium, and just take it for granted that one has done so, whenever discussing a given, single equilibrium; however, as soon as you consider comparative statics, you are contemplating a new, different equilibrium, and so the question of if and how you get from the old to the new equilibrium seems inevitable and important, even if the issue is typically ignored by economists. Now, given that uniqueness of equilibrium seems less critical than stability in order that the invisible hand be considered in effect, one could in principle relax the requirement of global stability, where one always arrives at the same equilibrium, to one of system stability, where what equilibrium one arrives at can depend on what price one starts from. For instance, this, but not global stability, is always the case when there are but two goods. This weakened criterion, though, would complicate global comparative statics, since one could not study the change in equilibrium without doing an explicitly dynamic analysis, because, while one would know solely by static analysis which new equilibria there are, one might not know which is the relevant one. In any case, it turns out that many natural conditions sufficient to assure system stability are also sufficient to assure uniqueness of equilibrium, in which case the distinction between the two stability notions disappears.Footnote 2 In this paper, at least, we will concentrate entirely on global stability and so require that there be but a single equilibrium. One difficulty is that any disequilibrium dynamic, one operating only out of equilibrium, inevitably sits uncomfortably with the price-taking, perfectly competitive assumption, which is really only entirely satisfactory in equilibrium itself. With everyone taking the price as given, there is then no one to change the hypothetical prices. Nonetheless, it is observed that real markets do seem to find their way to equilibrium, and thus, dynamics have been suggested that mimic this apparent behavior. They fall into two general classes: tâtonnement and non-tâtonnement. However, non-tâtonnement suffers from the fact that it would now make ordinary comparative statics impossible: if prices out of equilibrium induce trade as they change, then the final equilibrium, even if achieved, will have been affected by the actual path of the dynamics. As a first pass at least, it seems more prudent to consider just tâtonnement dynamics, which only allow trade once the equilibrium is achieved, and so, maintain the legitimacy of the usual program of comparative statics. Among possible tâtonnement dynamics, the most traditional one is the classical form: where z(p) is mean aggregate excess demand at the vector of prices p. This dynamic encodes, in the simplest possible manner, the “law of supply and demand,” much emphasized in introductory economics classes, where prices rise when demand exceeds supply for a market out of equilibrium, whereas prices drop in the opposite case. It, of course, corresponds to the dynamics considered by Walras (1926), in first formulating general equilibrium theory. We would conjecture that most dissatisfaction with classical tâtonnement among economists arises not from its lack of foundation in perfectly competitive behavior, since, as argued, this is true of all disequilibrium dynamics, and yet there must surely be some disequilibrium process. The dissatisfaction arises, instead, from the fact that, as is well known, and contrary to economists’ beliefs about real economies, classical tâtonnement does not inevitably take an economy to equilibrium. Here we are thinking primarily of Scarf’s (1960) celebrated counterexample, consisting of an apparently perfectly reasonable economy with three agents and three goods, where such tâtonnement results not in equilibrium, but in a limit cycle in prices. Indeed, much effort has been spent, as a result of this, and similar examples, to find alternative tâtonnement mechanisms that more consistently arrive at equilibrium.Footnote 3 But it seems that these must be regarded more as devices for an economist to compute equilibrium than as viable descriptions of the economies we observe, since the dynamics work in ways few would seriously suggest imitate the behavior of actual markets. It seems a more fruitful strategy, in support of the invisible hand, to remain with classical tâtonnement, and seek reasons why unstable behavior, possible though it may be, is unlikely to be characteristic of most actual economies. We remark that experimental economics seems to offer some validation for the main principles of classical tâtonnement, to the point of reproducing the instability of Scarf’s famous counterexample in the appropriate circumstances (Anderson et al. (2004)). We take this, then, as a good sign, that classical tâtonnement is a reasonable description of matters, even in extreme situations, not a bad one, that limit cycles in prices can occur. We are able to regard such cycles with some equanimity because, as we argue, most actual economies are not like the hypothetical one presented in Scarf’s example. Actual economies, it seems to us, involve a substantial number of goods and consumers, and the greater the number of these, the less likely is it that one encounters an economy, like Scarf’s example, that exhibits something other than a unique, globally stable equilibrium. That is the main thrust of our contention, and we proceed to make arguments why we believe this to be so. In addition to a large number of consumers, we will assume that they are of diverse types, and that, roughly speaking, as the number of goods increases, substitution effects remain substantial, while income effects have no tendency to become too large. We note, though, that, while we present technical supporting arguments, we currently have nothing that rises to the level of an exact proof for the general case. Lacking such an encompassing analytical result, after explaining our reasoning, further support for our contentions then takes the form of simulations, the main feature being to gradually increase the number of goods and consumers permitted, and then study the consequences. It will be seen, perhaps oddly, that it is the cases of relatively few goods or consumers which seem the most problematic for global stability, though, classically, by reason of their relative simplicity, the most attention has been focused on such economies, which, nonetheless, serve to give a somewhat distorted view of the more general situation. We should note that, while our interests here are in the possibility of reviving the currently rather moribund project to fulfill the original general equilibrium vision of the invisible hand, a scheme dating back to at least to Walras, if not further back to Adam Smith, this, in no manner, is the only, or necessarily even the best, means of justifying perfectly competitive behavior and its consequences. Diverse alternative sorts of arguments—without prejudice to others, we mention the works of Vega-Redondo (1997), Alós-Ferrer (2005), Gintis (2007), and Lahkar (2020)—have been advanced, which, together, point to Walrasian behavior as the central organizing principle describing the outcome likely to arise from the individual interactions of the many, within a market environment.Footnote 4",
17.0,3.0,Journal of Economic Interaction and Coordination,26 December 2021,https://link.springer.com/article/10.1007/s11403-021-00341-6,Testing the convergence hypothesis: a longitudinal and cross-sectional analysis of the world trade web through social network and statistical analyses,July 2022,Lucio Biggiero,Roberto Urbani,,Male,Male,Unknown,Male,"The issue of inter-country convergence covers many fields, ranging from international economics to industrial economics, finance, etc., since countries may converge or diverge on any aspect of economic life—and moreover, on any aspect of social life. Indeed, since economies and societies are complex systems made up of interdependent and co-evolving networks, globalization and convergence are therefore complex phenomena, and it is not surprising that the research on both issues has found it hard to reach a shared and sound conclusion. In general, the convergence hypothesis (hereafter CH) is grounded on the effects of globalization (Williamson 1996), which is a somewhat multifaceted phenomenon on its own, because it may occur in a differentiated way in many economic and social spheres (Ritzer 2007). The idea that, whenever and wherever it happens, globalization pushes so toward a progressive inter-country convergence has stimulated a vast amount of literature. Convergence can be narrowly interpreted as approaching the same goal or value or, more broadly speaking, as reducing reciprocal distances—that is, increasing similarity, in terms of a given parameter. The parameters that are traditionally considered in economic analyses are those of technology (number of patents, innovations, etc.), per-capita income and/or growth rate. However, there are many other parameters beyond of the economic field that are used for sociological or political studies, namely, cultural convergence, the structure of education systems, mass media control, engagement in international governmental, non-governmental institutions, etc. Even when the analysis is limited to the economic sphere, it is difficult to overestimate the relevance of CH. If such a hypothesis were to be confirmed, several consequences, in terms of, for example, world development forecasts, would follow. International economic policies would thus receive a crucial indication about cyclical or anti-cyclical effects because countries with a higher convergence degree (hereafter CD) are more likely to react in a similar way to the same policies and vice versa. However, we will come back to these issues in more detail in discussion and conclusions section. The acceleration of globalization has further emphasized the issue of convergence, because it seems that its typical features—the growth of world trade, the reduction of trade barriers, decreasing transportation costs, the accentuated role of international economic institutions, to name just a few of the most important ones—affect, especially in developing countries, the income growth rate and the world trade amount and extension, which in turn increase the number of partner countries, as well as social and economic inequality. All this is clearly believed to reduce the distances between countries, and thus, increase convergence. In this vein, we refer to CH as the concept that globalization has increased the convergence of world trade, which is the specific field of our present investigation. It is possible to identify three approaches to economic convergence on which CH can rely: two are completely opposite each other, and the other one is still waiting for its specific position to be developed. CH is generally supported by modernization theorists (Barro and Sala-I-Martin 1992; Durlauf 1996; Krugman 1991; Meyer 2000; Quah 1993, 1996), who substantially agree with mainstream economists on the idea of the parallel growth of all nations, pushed by the powerful forces of economic development and technological innovation (Barro and Sala-I-Martin 2004; Sengupta 2011). Modernization theories are deeply rooted in mainstream economics, that is, in optimal equilibrium-based neoclassical economics. The reason for this is very clear: there is no unequal exchange in an optimal equilibrium-based approach, because such an approach would otherwise not be optimal. It could be sub-optimal, but only temporarily, that is, just the time necessary for equilibrium-adjustment forces to operate. Furthermore, even though differential competitive advantages would force countries to specialize in different sectors, their positions would need to be symmetric, at least at the highest aggregate level, and thus, CD would need to be perfect. Finally, the magic, invisible force would lead to development where it was delayed, because free markets, sooner or later, match any unsatisfied demand, and companies relocate to low-cost countries. Applied economists are aware of the many “frictions” on real markets, such as entry barriers, tariffs, scarcity of skilled workers (Quah 1993, 1996). However, genuine modernization theorists (Armstrong and Taylor 2000; Krugman 1995; Martin and Sunley 1996, 1998) would argue that all these hurdles are only temporary, because economic convenience and balancing mechanisms inevitably prevail. As we will see, they found confirmation of CH, because their applied studies were conducted on the most advanced countries, and they employed rather simplified methods. We can therefore call this group of authors “convergentists.” Two theoretical perspectives are in contrast with CH: the world-system theory (Babones and Chase-Dunn 2012; Chase-Dunn et al. 2000; Chase-Dunn and Grimes 1995; Wallerstein 2000, 2004) and the dependence theory (Abhijeet 2016; Manning and Gills 2013; Smith 1979). The former underlines that the world’s economy has traditionally been divided into three groups of countries: core counties, which correspond to the most developed countries, periphery ones, which refer to the least developed countries, and a semi-periphery group of intermediate-level countries. According to the world-systems theory, globalization has always been compatible with marked differences in the world’s trade structures between the three groups, and there is no reason to believe that the recent acceleration stage has shortened such distances. The dependence theory is even more radical as it argues that, as long as capitalism is the universal economic regime, there will be a marked difference between the three groups. In other words, the core countries can maintain their leading position and maintain their high living standard just because there are periphery countries. The core countries develop at the expense of the periphery ones. Therefore, for the theorists who belong to these two groups, there is no reason to support CH. We can therefore call such theorists “divergentists.” However, there is also a third research stream that lies somewhere in the middle between convergentists and their opponents: that of the evolutionary economic geography theory (Boschma and Frenken 2006; Boschma and Lambooy 1999; Boschma and Martin 2010; Coe et al. 2008a, b). This theoretical perspective departs substantially from standard economics (and standard economic geography), and it underlines the path dependency in which each country is entrapped from the time of their structuring into an institutional, social and economic realization. This specificity would prevent or hinder any inter-country uniformity to a great extent, as hypothesized by CH. Indeed, this group of economists has not yet specifically dealt with CH, but just with the concept of convergence, and only in reference to technology (Boschma and Martin 2010). However, a review of these different theoretical perspectives is beyond the scope of our paper, although such a review can be found in Austin et al. (2012) and in So (1999). Our primary goal is instead to offer a rather complete test of CH from several points of view and over a significantly long time series. The extension of our tests allows another main objective to be reached: to show the problems that underline the statistical and network analyses of world trade, especially when considering the absolute values of exchanges and dealing with patterns and not with aggregate country values. This aspect makes our paper methodologically rich and at the same time explains the unusual length of its methodological section, which introduces some network analysis methods that we have employed to measure CD. In fact, after some pioneering work in the seventies and nineties (Snyder and Kick 1979), international trade has been a frequent object of network analysis over the last 20 years: see Maoz (2011), Kick et al. (2011), De Bendictis and Tajoli (2011), De Bendictis et al. (2013), Fagiolo et al. (2010, 2014), and, more recently, Cepeda-López et al. (2018). Among the various acronyms used so far in this specialized literature, we have chosen WTW (World Trade Web), which was introduced by Serrano and Boguñá in 2003. We have not reviewed the aforementioned studies here, because they do not offer useful information to help explain or confirm/reject CH. Some explanatory models have been developed over the last 10 years, most of which are based on gravity models (Almog et al. 2017; Dueñas and Fagiolo 2013; García-Pérez et a., 2016; Serrano et al. 2010). These models could suggest an explanation of CD at the country group level, even though Dueñas and Fagiolo argued that their model did not work well for higher-order statistics, and CD belongs to this category. We will come back to this issue in Discussion and conclusion section. Previous studies have provided a great deal of descriptive knowledge, and some of them have underlined the need to run both a binary analysis (pure structural, topological) and a weighted one (involving link weights, that is, economic values), because they could give rather different results (Fagiolo et al. 2010), as also confirmed by our findings. We have followed this approach; thus, we distinguish economic globalization (weighted) from structural (binary) globalization and the corresponding effects on CD measured in economic (weighted) and structural (binary) terms. In other words, the economic dimension in our analysis also includes the structural dimension, because only the weight of the links changes, while the analysis is the same in terms of trade patterns. This is why the economic analysis run here should be more appropriately referred to as weighted—or, even better, integrated—as we have sometimes done hereafter. Therefore, the binary (purely structural) analysis allows us to distinguish that part of the results that is only due to the structural dimension and is separated from the role of trade values. Conversely, except for the work of Berry et al. (2014), who cannot be classified in either of the two aforementioned categories, the analysis conducted by convergentists and divergentists only deals with the economic dimension and overlooks not only the structural aspect, but also the true sense of the structure, that is, it overlooks the patterns of trade. In other words, it does not disaggregate world trade into a network of exchanges. In short, structural globalization should be considered as an aspect of economic (integrated) globalization. We test four (groups of) hypotheses, which are introduced in the next section together with a short literature review on CH in international trade. We then present the dataset and methodology which contribute to a great extent to the network analysis of international trade, since we introduce and discuss not only the methodological choices that are implemented therein, but also those that we experimented with but then abandoned because they resulted to be distortive, ineffective, or not explicative. In section four, we show the main results, from which a complex relationship between globalization and convergence emerges, and which is rather differentiated in terms of time segments, import and export and groups of countries. Therefore, although all these results are discussed in the concluding sections, we here summarize the most important ones: i) the pure structural CD, at the overall WTW level, is high and growing, while the hybrid (structural–economic) CD is low and declining; ii) the structural CD of the 20 most connected countries grew remarkably and constantly over the considered 37 years and reached an almost perfect convergence (0.96) in 2016; iii); iv) both the structural and economic measures of globalization are able to clearly explain both the structural and hybrid measures of CD, albeit through a negative effect. Discussion and conclusion sections insert these and other findings into the debate on CH and business cycle synchronization, with further implications on the evolutionary economic geography and world-systems theory approaches.",1
17.0,3.0,Journal of Economic Interaction and Coordination,28 December 2021,https://link.springer.com/article/10.1007/s11403-021-00342-5,"Economic policy uncertainty, investor sentiment and financial stability—an empirical study based on the time varying parameter-vector autoregression model",July 2022,Xin-Zhou Qi,Zhong Ning,Meng Qin,Unknown,,,Mix,,
17.0,3.0,Journal of Economic Interaction and Coordination,07 January 2022,https://link.springer.com/article/10.1007/s11403-021-00344-3,"Endogenous viral mutations, evolutionary selection, and containment policy design",July 2022,Patrick Mellacher,,,Male,Unknown,Unknown,Male,"In the autumn of 2021 it has become clear, that even wide-spread access to vaccines has not (yet) eliminated the threat of COVID-19, as new variants prove to be fitter and better able to escape immunity (Bernal et al. 2021; Hoffmann et al. 2021a, 2021b; Wall et al. 2021). Mutations have already started to become a concern in mid-2020 (Korber et al. 2020), whereas other early studies concluded that the evolutionary pace of SARS-CoV2 was too low to endanger vaccine efficacy (Dearlove et al. 2020). In the beginning of 2021, however, most experts believed that COVID-19 will become “endemic,” i.e., circulate perpetually in varied forms at least in certain areas (e.g., countries with low access to vaccines) (Phillips 2021). In this article, i aim to shed light on two research questions which naturally arise in this situation: a) in which direction will the SARS-CoV2 virus causing COVID-19 evolve?, and b) which aggregate dynamics can we expect from an evolving virus in terms of infections and fatalities? In order to study these topics, i develop a parsimonious epidemiological model that simultaneously captures two types of viral evolution: a) Genetic variation: Mutations can change the characteristics of a virus in a way that alter its evolutionary fitness, e.g., by increasing its transmissibility (Chen et al. 2020). b) Antigenic variation (Yuan et al. 2021): Antigenic drift is a process in which the virus changes its antigenic profile. Since antibody response is targeted at the antigenic profile, an antigenic drift allows a virus to escape (some) immunity previously acquired by the viral host. Research on the influenza virus shows that genetic change is more gradual than antigenic evolution and that immune escape depends on the antigenic distance between two variants (Smith et al. 2004). Both types of variation are subject to natural selection, which favors “useful” (Darwin 1859) variation, i.e., variation which increases the growth rate of the number of people infected by a variant. A recent study suggested that the viral evolution of SARS-CoV2 has been accelerating (McCarthy et al. 2021). Theoretical modeling of fitness evolution usually relies on the so-called fitness landscape approach first introduced by Wright (1932). In this model, mutations cause a species (e.g., virus) to move along an n-dimensional landscape. Each spot in this landscape represents a phenotype associated with a given fitness value, which may be static or change over time due to, for instance, environmental effects (e.g., Wilke et al. 2001). This landscape may be multi-peaked: Such an approach (using a one-dimensional landscape) was recently used to study viral evolution by e.g., Rüdiger et al. (2020). It may also, however, be single-peaked, a case in which the species continuously approaches the peak. Gurevich et al. (2021) study the evolutionary competition between two distinct strains and find that increased testing favors a test-evasive strain. In order to capture partial strain-dependent immunity, Roche et al. (2011) develop an agent-based model where previous infections provide partial cross-immunity depending on the evolutionary distance between the infecting variant and the variant that had caused an infection in the past (as observed empirically by Smith et al. 2004). An alternative approach is chosen by Griffin et al. (2020), who develop a parsimonious model with strain-dependent immunity that does not need to store the “antigenic history” (ibid) of each agent. This paper contributes to the literature on the theoretical modeling (the impact) of viral mutations and—more specifically—COVID-19 variants (e.g., Roche et al. 2011; Basurto et al. 2021; Buckee et al. 2007; Cao et al. 2021; Gabler et al. 2021; Gurevich et al. 2021; Gordo et al. 2009; Griffin et al. 2020; Halley et al. 2021; Marquioni and Aguiar 2021; Pageaud et al. 2021; Rella et al. 2021; Rüdiger et al. 2020; Williams et al. 2021). My contribution is to (a) introduce a parsimonious model of endogenous viral evolution capturing both genetic and antigenic variation (i.e., evolving intrinsic and extrinsic fitness, see Smith et al. 2004), as well as imperfect cross-immunity, and (b) use this framework to study aggregate dynamics and the direction of viral evolution under varying containment policies. This paper also contributes to the literature using agent-based modeling to studying the COVID-19 crisis (e.g., Basurto et al. 2021; Delli Gatti and Reissl 2020; Dignum et al. 2020; Gabler et al. 2021; Kerr et al. 2021; Lasser et al. 2020; Mellacher 2020, 2021a; Silva et al. 2020; Vermeulen et al., 2020; Wallentin et al. 2020). Finally, this paper is also related to a literature to the study of evolutionary processes in economics as pioneered by Nelson and Winter (1982). This approach has been adopted in a particularly fruitful way using agent-based models in the field of innovation economics (e.g., Ma and Nakamori, 2005;  Dosi et al. 2010), which inter alia considers the case of directed change due to evolutionary selection (Fanti, 2021; Hötte 2020; Mellacher and Scheuer 2021). Modeling the behavior of agents as an adaptive (boundedly rational) processes is another highly promising field of study where evolutionary processes are employed by economists. A recent example for such a model is developed by Lux (2021), who extends the classical SIR model to include adaptive endogenous social distancing. The rest of this paper is structured as follows: The second section presents the basic model and analyzes the direction of evolution under varying containment scenarios analytically. The third section discusses the implementation of this model as an agent-based simulation model (ABM). The fourth section shows the results of a quantitative analysis of the ABM using Monte Carlo simulations. Finally, section five concludes.",2
17.0,3.0,Journal of Economic Interaction and Coordination,16 January 2022,https://link.springer.com/article/10.1007/s11403-022-00345-w,An empirical behavioral model of household’s deposit dollarization,July 2022,Ramis Khabibullin,Alexey Ponomarenko,,Unknown,Male,Unknown,Male,"Deposit dollarization has always been an important feature of the Russian economy. The hyperinflation that occurred in the early 1990s and the currency crisis of 1998 increased the demand for foreign currency-linked deposits for holding savings. In subsequent years, periods of extensive de-dollarization in times of ruble appreciation have alternated with renewed shifts to foreign currency-linked deposits in times of ruble depreciation (e.g., during the financial crisis of 2008) presumably providing the indications of the speculative behavior of households. Notably, prior to 2015, the Bank of Russia adhered to a managed exchange rate strategy and conducted foreign exchange interventions trying to smooth exchange rate fluctuations. In 2015, the Bank of Russia transitioned to a fully flexible exchange rate regime and the volatility of ruble’s exchange rate’s fluctuations increased significantly. Interestingly (and somewhat surprisingly), in this environment, the changes in household’s deposits dollarization in 2015–2018 seem to have detached from the observed exchange rate development, implying an evidently time-varying link between the variables. This is a notable observation considering that the policymakers in the emerging markets are occasionally fearful that in an environment of deposit dollarization substantial exchange rate movements may result in extrapolative destabilizing re-denomination of the currency of deposits. Arguably, this consideration may be regarded as a reason of a ‘fear of floating’ (i.e., reluctance to adopt a flexible exchange rate regime). Therefore, it is generally important to conduct a comprehensive analysis of the aforementioned case. In order to model and predict the evolution of the relationship between exchange rate movements and deposit dollarization, we set up a behavioral model that allows the households to switch between different strategies. The alternative strategies adopt either extrapolating or mean-reversing expectations regarding the exchange rate developments. Presumably, this feature may be used to account for the fact that increased exchange rate volatility may discourage adaptive expectations and mute households’ reaction to realized exchange rate depreciation. We empirically estimate the model using the novel stochastic gradient variational Bayes with normalizing flows method. The model’s performance is evaluated via a forecasting exercise and compared with the results obtained by employing a set of contemporary nonlinear time series models. The results indicate that the behavioral elements facilitated the model’s faster adjustment to the new environment while the non-structural models required a relatively large number of observations to alter the parameters for the new regime. As far as we know, this is a pioneering example of a practical application of behavior finance concepts in this area. The paper is structured as follows. Section 2 briefly discusses the dollarization and exchange rate developments in Russia. Section 3 outlines the behavioral model. Section 4 describes the alternative time series models. Section 5 presents the results of the forecasting exercise. Section 6 concludes.",1
17.0,3.0,Journal of Economic Interaction and Coordination,05 March 2022,https://link.springer.com/article/10.1007/s11403-022-00348-7,Macroeconomic dynamics under bounded rationality: on the impact of consumers’ forecast heuristics,July 2022,Tae-Seok Jang,Stephen Sacht,,Unknown,Male,Unknown,Male,"Modern macroeconomic models often rely on the rational expectations (RE) hypothesis to avoid the complexity of multiperiod optimization problems in economic activities. Under so-called perfect rationality, the optimal paths for consumption and investment can be obtained (or at least approximated) if no further complications from unforeseen problems occur. The optimal solutions are specific to individuals and policymakers who try to navigate through business cycles, but they come at the expense of ignoring potential unexpected crises. Thus, an alternative to perfect rationality has always been needed. As the field of behavioral and experimental economics gradually matures, the notion of bounded rationality (BR) provides a benchmark against which agents attempt to forecast events. For example, heuristics, such as rules-of-thumb procedures, are applied as a convenient way to describe the reality and complexity of economic activities. This is because the economic agents who observe the structure of the economy barely understand the interactions between relevant variables, such as output and inflation (Munier et al. 1999). Given the lack of full information, such bounded rational agents rely on habits, imitation, and/or procedural optimization to predict changes in the economy (Day and Pingle 1991). Perhaps the most prominent type of heuristics found in the recent macroeconomic analysis literature is based on discrete choice models.Footnote 1 In some cases, heuristic expectation formation might be governed by a switching process under the consideration of heterogeneous forecasting rules (Jang and Sacht (2016, 2021)). In particular, agents sort themselves into different groups, and each group is populated by individuals who have strong beliefs in a certain expectation formation process. As a result, endogenous waves of economic beliefs, such as optimism and pessimism, are generated from period to period. This leads to fluctuations in the economic variables driven by reversals in the emotional state, which holds even in the absence of autocorrelated exogenous shocks. The theoretical underpinnings of heuristics have also received support from empirical and experimental studies. For example, the experimental results of the work by Cornand and Heinemann (2018) suggest that the simplest form of adaptive (i.e., extrapolative) heuristics provides a better description of reality. Grazzini and G. R. and Tsionas, M. (2017) explicitly consider the heuristics of fundamentalists and chartists applied to the output gap expectation when estimating a DSGE model via Bayesian techniques. Meanwhile, Assenza et al. (2013) conduct a laboratory experiment on a purely forward-looking new Keynesian model (NKM) where the discrete choice approach is applied. The results support the adaptive process in forecast heuristics and a slow convergence to the steady-state output level. A comprehensive review of expectation formations in macroeconomics can be found in Assenza et al. (2013) and Franke and Westerhoff (2018). In this paper, we contribute to the empirical literature on behavioral economics by examining the effects of bounded rationality on macroeconomic dynamics. As a continuation of the work by Jang and Sacht (2021), we consider various types of forecast heuristics that connect consumer confidence and private household expenditure. In particular, the authors investigated the role of consumer confidence in the determination of household expenditure and its influence on the business cycle. As a key feature of the BR model framework, agents are allowed to switch to the group with the best performing forecast strategy as evaluated based on discrete choice theory. Their results suggest that expectations in the US economy are grounded on the consumers’ emotional state, whereas in the Euro Area, they are purely technical. In the latter case, the fundamentalists’ and chartists’ heuristics model under consideration exhibits the best possible description of the data. The study shows that consumer confidence plays a crucial role in the determination of household expenditure and the pass-through to GDP fluctuations. The importance of confidence in decision-making processes motivates the research question regarding the effects of the occurrence of certain shocks to macroeconomic dynamics via confidence. In this paper, as an investigation of forecast heuristics, we go beyond Jang and Sacht (2021) and analyze the impulse response functions (IRFs) to various shocks to economic variables. To observe this, we select the specification in the heuristics that leads to the best description of different versions of a standard DSGE model from the empirical data. In doing so, we compare the IRFs stemming from the BR model with the ones obtained from the RE model. Both types of IRFs are confronted with the outcome of a vector autoregressive (VAR) model. We note that such analysis is of high interest, especially in the case of a potential output/inflation trade-off that the central bank will face when conducting an (optimal) monetary policy intervention. It is also worthwhile to examine the way persistence is introduced into the model. This is crucial to capture the degree of inertia being observed for the empirical time series induced by the statistical VAR model. Both theoretical models handle the issue differently. While under RE, intrinsic persistence results from the assumption of habit formation in consumption and price indexation, backward-looking elements are directly incorporated into the heuristics being applied under BR. As a novel feature, we show that the macroeconomic dynamics driven by heuristics can provide a good fit to the data that is equivalent to a RE model with a lead and lag structure. Indeed, our results address the challenges that policymakers face, especially when stimulating the economy via fiscal and/or monetary policy in the presence of ‘animal spirits’. For example, few studies have investigated the (optimal) monetary and fiscal policy under BR (cf. Caprioli 2015; De Grauwe and Macchiarelli 2015; Hollmayr and Matthes 2015). Cornea-Madeira et al. (2019) state that valid empirical evidence for behavioral heterogeneity is questioning the formulation of the optimal policy design under the RE paradigm because of the existence of multiple equilibria in a complex system under BR. They show that heterogeneity varies over time and conclude that inflation dynamics can be dominated by either forward-looking or backward-looking behavior. Also, Lengnick (2016) discuss the design and implementation of optimal simple rules in the baseline NKM with an additional financial sector included under the discrete choice switching mechanism. They show that considering a simple Taylor rule under BR (i.e., limited to specific types of fundamentalists and chartists) causes a strong increase in the central bank’s objective function. The result is explained by the fact that monetary policy rules must be more forward-looking as the model becomes more backward-looking (cf. Leitemo 2008). The remainder of this paper is structured as follows: Section 2 discusses the general representation of the model frameworks under RE and BR. The latter includes the description of the forecast heuristics applied while considering the discrete choice mechanism. Section 3 presents the methodology and data used in this study. Section 4 presents a simulation study in which we compare the IRFs obtained from both theoretical models with a VAR model in the case of demand, cost-push, and monetary policy shocks. Finally, Sect. 5 concludes. The technical details and additional results are relegated to “Appendix”.",
17.0,3.0,Journal of Economic Interaction and Coordination,26 May 2022,https://link.springer.com/article/10.1007/s11403-022-00349-6,Agent-based modeling of the word-of-mouth effect on promoting brand-name agricultural products,July 2022,Qiuyi Huang,Xiaoping Zheng,Xiaoshuan Zhang,Unknown,Unknown,Unknown,Unknown,,
17.0,4.0,Journal of Economic Interaction and Coordination,07 May 2022,https://link.springer.com/article/10.1007/s11403-022-00351-y,A bottom-up simulation on competition of online interpersonal communication platforms,October 2022,Tao Fu,Liling Zou,,,Unknown,Unknown,Mix,,
17.0,4.0,Journal of Economic Interaction and Coordination,12 May 2022,https://link.springer.com/article/10.1007/s11403-022-00352-x,Intelligence promotes cooperation in long-term interaction: experimental evidence in infinitely repeated public goods games,October 2022,Tetsuya Kawamura,Tiffany Tsz Kwan Tse,,Male,Female,Unknown,Mix,,
17.0,4.0,Journal of Economic Interaction and Coordination,08 June 2022,https://link.springer.com/article/10.1007/s11403-022-00355-8,Agent-based model generating stylized facts of fixed income markets,October 2022,Antoine Kopp,Rebecca Westphal,Didier Sornette,Male,Female,Male,Mix,,
17.0,4.0,Journal of Economic Interaction and Coordination,31 May 2022,https://link.springer.com/article/10.1007/s11403-022-00356-7,"Do macroeconomic and financial governance matter? Evidence from Germany, 1950–2019",October 2022,Taner Akan,Tim Solle,,Male,Male,Unknown,Male,"There were mainly three assumptions in neoclassical economics regarding the governance of macroeconomic and financial systems that underlay economic and financial inefficiency before the Great Recession of 2008–2009. First was that inflation targeting per se will ensure macro-financial stability, that price stability will ensure full employment and robust growth, and that there was no need for fiscal tools for short-run stabilisation because the fiscal tools such as broad-based income tax cuts, increased transfers, and expansionary government purchases cannot stimulate the macroeconomy even when the economy was at underemployment conditions. The second was that financial markets were efficient, which means that the prices of financial assets are unpredictable, fairly valued, and reflect all available information. As a corollary, third was that macroeconomic fluctuations and financial crises were obsolete under symmetric information held by rational agents having “a complete understanding of the economic mechanisms governing the world” with perfect foresight (Colander et al., 2009: 256; Acemoglu, 2009; Romer, 2011; Stiglitz, 2012). However, first, economic growth and productivity performances declined not only in developed economies such as USA and Germany but also in developing economies like Latin American countries over the last four decades compared to those in the 1950s and 1960s (World Bank, 2021; PENN World Tables, 2021). Second, financial markets turned out to be neither stable nor efficient in particular in terms of fundamental valuation efficiency (Tobin, 1981), as unveiled by the Housing Bubble when irrational exuberance triggered a speculative bubble due to the psychological contagion and amplifying stories among the investors who assumed that the prices of financial assets looked like a permanently high plateau (Shiller, 2015).Footnote 1 Among the coordinational factors accommodating irrational exuberance were, among other things, (i) excess liquidity due to overly low-interest rates during the years leading up to the crisis, (ii) over-reliance of FED on the deregulation of the finance sector, and the self-regulation by financial institutions, (iii) the aggressive homeownership policy of extending credit to families previously denied access to the financial markets by American governments, and (iv) the expanding public-led housing finance excessively sponsored by government enterprises by purchasing and guaranteeing risky mortgages, particularly, in 2005 and 2006 (Financial Crisis Inquiry Commission, 2011). Third, the world economy has incurred one of the two biggest overseas crises of the last one century, the Great Recession of 2008–2009, the stagnating impact of which has still been underway. Against this backdrop, this paper aims to illustrate two things with specific reference to macroeconomic and financial governance in Germany. First is that (i) systemic (synergy-creating) and fragmented (atomistic) modes of macroeconomic and financial governance may yield to achieving comparatively higher and lower economic efficiency in the long-run, respectively, and that (ii) the path-dependencies of the synergetic mode of governance can forestall the outbreak of a catastrophic financial and macroeconomic crisis like the Great Recession.Footnote 2 Second is that, despite the rising pessimistic assumptions about free economic systems and financial markets with the outbreak of the Great Recession, macroeconomic and financial systems can be stable, efficient, and positively intercomplementary when synergy-creating institutional mechanisms are utilised to achieve the efficient allocation of productive and financial resources by preventing or mitigating the coordinational problems to arise out of information asymmetries among economic actors. The paper exemplifies economic governance with the relationships between macro-financial governance (FGV), macro-non-financial governance (NFGV), and micro-financial development (FND) in Germany in the period 1950–2019. The FND comprises financial organisational development (FOD) and financial market development (FMD). The paper uses an institutionalist approach introducing two modes of economic governance based on institutional complementarities and tests its hypotheses using both an exhaustive structuralist analysis and a time-series quantitative technique based on the Autoregressive Distributed Lag (ARDL) cointegration model and the Vector Error Correction Mechanism (VECM). The study’s second section explains (i) the interconnections between FGV, NFGV, and FND and (ii) the role of a systemic and fragmented mode of wider economic governance in managing these interconnections from a complementary theoretic approach. The third section introduces the data and the quantitative techniques to be used for illustrating how to measure the impact of economic governance on economic performance. The fourth section examines, first, whether there has been a German model aligned with the synergetic relationship between FGV, NFGV, and FND, and, second, whether the systemic and fragmented relationship between the three variables resulted in economic efficiency and inefficiency, respectively. A holistic summary of the main trends in the German economy to form testable hypotheses and then estimate these hypotheses using econometric models are provided in the fifth section. A conclusion is provided in the sixth section.",1
17.0,4.0,Journal of Economic Interaction and Coordination,09 July 2022,https://link.springer.com/article/10.1007/s11403-022-00360-x,The universal pathway to commodity structure upgrading in global trade evolution,October 2022,Xiaomeng Li,Hongbo Cai,Qinghua Chen,Unknown,Unknown,Unknown,Unknown,,
17.0,4.0,Journal of Economic Interaction and Coordination,18 August 2022,https://link.springer.com/article/10.1007/s11403-022-00361-w,Credit allocation and the financial crisis: evidence from Spanish companies,October 2022,Marko Petrović,Andrea Teglio,Simone Alfarano,Male,Female,Female,Mix,,
17.0,4.0,Journal of Economic Interaction and Coordination,08 August 2022,https://link.springer.com/article/10.1007/s11403-022-00362-9,A collaborative evolutionary model: the self-organizing evolutionary process of urban–rural digital sharing system of social public resources,October 2022,Shengzhu Li,Fan Jiang,,Unknown,,Unknown,Mix,,
17.0,4.0,Journal of Economic Interaction and Coordination,25 July 2022,https://link.springer.com/article/10.1007/s11403-022-00363-8,Carrots and sticks: new evidence in public goods games with heterogeneous groups,October 2022,Jie Chen,,,,Unknown,Unknown,Mix,,
18.0,1.0,Journal of Economic Interaction and Coordination,02 November 2022,https://link.springer.com/article/10.1007/s11403-022-00375-4,Introduction to the special issue on agent-based models in urban economics,January 2023,Jason Barr,Jiaqi Ge,,Male,Unknown,Unknown,Male,,1
18.0,1.0,Journal of Economic Interaction and Coordination,22 March 2021,https://link.springer.com/article/10.1007/s11403-021-00324-7,The impact of social influence in Australian real estate: market forecasting with a spatial agent-based model,January 2023,Benjamin Patrick Evans,Kirill Glavatskiy,Mikhail Prokopenko,Male,Male,Male,Male,"Within economic markets, housing markets are unique for a variety of reasons. The combination of durability, heterogeneity, and spatial fixity amplifies the role of the dwellings’ perceived value and the buyers and sellers’ expectations (Alhashimi and Dwyer 2004). The extremely high cost of entry and exit into the market (with moving fees, agent fees, etc.) further complicates the decision-making of participating households (Huang and Ge 2009). There are long time delays in the market response as houses cannot be erected instantaneously to accommodate an increase in demand (Bahadir and Mykhaylova 2014). The fact that real estate can be seen as both an investment asset and a consumption good (Piazzesi et al. 2007) [and even a status good, Wei et al. (2012)] magnifies the impact of social influence on both decision-making and resultant market dynamics and structure. Consequently, housing markets are notoriously difficult to model as the ensuing market dynamics generates volatility, with nonlinear responses, and “boom–bust” cycles (Sinai 2012; Miles 2008; Burnside et al. 2016), making traditional time series analysis insufficient. Nonlinear dynamics of housing markets are ubiquitous, being observed throughout the world, from Tokyo (Shimizu et al. 2010) to Los Angeles (Cheng et al. 2014). Traditional economic modelling methods, such as dynamic stochastic general equilibrium models (DSGE), typically use representative aggregated agents while making strong assumptions about the behaviour of the markets (rational and perfect competition). Such representative agents may be limiting for economic models (Gallegati and Kirman 1999). Furthermore, these assumptions (and many other traditional economic assumptions) are known to be inadequate in housing markets, motivating a well-recognised need for change in housing market modelling (McMaster and Watkins 1999). In addressing this need, a specific type of models, called agent-based models (ABM) has been applied. ABMs aim to capture markets from the “bottom up” (Tesfatsion 2002), i.e. by focusing on the decision-making of individual agents in the market, possibly influenced by non-economic factors. In this sense, ABMs are capable of modelling macroeconomies from micro (i.e. agent-specific) behaviour (LeBaron and Tesfatsion 2008) and analysing the economic decision-making in counterfactual settings. While ABMs have shown promise in housing market modelling (Geanakoplos et al. 2012) [and wider economic modelling, Poledna et al. (2019)], current ABMs themselves are not exempt from some limitations. Many of the existing housing ABMs tend to introduce at least one of the following constraints: the spatial structure of markets is neglected (meaning submarkets are not considered), perfect information is still assumed, and/or the impact of social influence on decision-making of individual agents is underestimated. A fine-resolution model of spatiotemporal patterns within such markets is desirable: it would give an understanding of how market dynamics shape within local areas, explaining how the pricing structure directly affects the agents’ mobility over time (i.e. by forcing households out of certain regions due to gentrification and higher cost of living). Furthermore, within such housing markets [and in fact, many economic markets, Conlisk (1996)], it is also known that agents do not act perfectly rational (Wang et al. 2018, instead following bounded rationality Simon 1955, 1957). Firstly, humans are often influenced by social pressure (e.g. herd mentality), with the decisions being made purely based on social pressure rather than a perfectly rational choice. Secondly, it is difficult to process all the relevant information in the market (i.e. it is impractical for an agent to be able to view every dwelling listing within the housing market). Thus, it is unreasonable to assume that agents act in a perfectly rational manner, yet this is what many current housing market models assume (despite ABMs not intrinsically requiring these assumptions to be made). To address these limitations, we introduce a spatial agent-based model, in which the constraints imposed by various search and mobility costs create effective spatial submarkets. These submarkets are modelled graph-theoretically, with a graph-based component used in both representing imperfect information and modulating social influences. The spatial ABM is then capable of capturing the “boom–bust” cycles observed in Australia (in particular, Greater Sydney) over the last 15 years as arising from individual agent decisions. That is, we show complex nonlinear market dynamics as arising through individual buy and sell decisions in the market, with introduced factors which affect and explain the decision-making behaviour. The model succeeds in forecasting nonlinear pricing and mobility trends within specific submarkets and local areas. In exploring the pricing dynamics, we focus on the influence of imperfect spatial information and the role of social influence on agent decision-making. In doing so, we identify the salient parameters which drive the overall dynamics and decision-making, and pinpoint the parameter thresholds, beyond which the resultant dynamics exhibit strong nonlinear responses. These thresholds allow us to distinguish between different configurations of the market (e.g. markets with supply or demand dominating) and differences in agent behaviour. In addition, we identify and trace specific interactions of parameters, in particular the interplay of social influences, such as the fear of missing out and the trend-following aptitude, in the presence of imperfect spatial information. The remainder of the paper is organised as follows. In Sect. 2, we provide an overview of agent-based models of housing markets. In Sect. 3, we outline the baseline model, while in Sect. 4 we outline the proposed spatial extensions and new parameters. In Sect. 5, we analyse the sensitivity and parameters of the model, before presenting the results and discussion in Sect. 6. In Sect. 7, we provide conclusions and highlight future work.",10
18.0,1.0,Journal of Economic Interaction and Coordination,02 October 2022,https://link.springer.com/article/10.1007/s11403-022-00370-9,Dissimilarity effects on house prices: what is the value of similar neighbours?,January 2023,Said Benjamin Bonakdar,Michael Roos,,Male,Male,Unknown,Male,"Households seem to have a preference for being among neighbours similar to them. Such a preference was already postulated by Schelling (1971, 1978) and more recently shown empirically (e.g. Clark 1992; Luttmer 2005). Of course, the composition of the population in the neighbourhood is not the only factor that determines residential choice. Where households buy a home also depends on the properties of the house itself, amenities and disamenities in the neighbourhood and last but not least on affordability (Sirmans et al. 2006, Baker et al. 2015). House prices are not independent of the other factors. In fact, in a competitive market equilibrium, house prices capitalise the attributes of the house and the neighbourhood (see Rosen 1974). A large literature on hedonic pricing in the housing market estimates how the value of homes depends on these attributes (see Sirmans et al. 2005). Empirically, it is not easy to disentangle the effect of the neighbourhood on house prices from the effects of the dwellings attributes. It is particularly difficult to control for endogeneity effects that arise because the composition of the population in the neighbourhood is endogenous and depends on amenities and prices. Ioannides (2011) provides an overview how the literature attempts to identify and estimate these social interaction effects. In this paper, we present an agent-based model of an urban housing market that allows us to analyse the interaction between residential choice, the composition of the population in a neighbourhood and house prices. In particular, we ask the following research questions: (1) How do house prices that different population groups have to pay depend on neighbourhood preferences? (2) What are the effects of policy measures aimed at making housing affordable to less well-off households? Understanding how house prices depend on preferences for similar neighbours is important for hedonic estimations of house prices and their interpretations. The correct specification of empirical models that deal with the above-mentioned endogeneity problem depends on the assumptions about preferences and behaviour of the agents, which are difficult to observe. With our model, we can analyse how sensitively house prices respond to variations in unobservable preference parameters. We consider several dimensions of similarity, namely income, education and ethnicity, which are often treated either separately or implicitly in empirical studies (e.g. Green and Lee 2016; Leung and Tsang 2012). We also distinguish a preference for similarity from a preference for status. Preference for similarity means that households dislike living among different neighbours, no matter whether they are privileged or underprivileged relative to their neighbours. Preference for status, in contrast, means that households dislike being underprivileged in terms of income or education, but gain satisfaction from having higher income or better education that their neighbours. The resulting moving decisions and house price dynamics might be quite different in the two cases. Relatively rich or educated households have an incentive to leave deprived neighbourhoods if they have a preference for similarity, but with a preference for status they have an incentive to stay there. The effect of diversity and inequality on house prices is of particular relevance for policy makers. In many countries, the ethnic and cultural diversity of the population increases due to international migration (Bove and Elia 2017). At the same time, globalisation and technological change have increased and might further increase income inequality (Jaumotte et al. 2013). If households prefer similar neighbours, increased ethnic diversity and income inequality might exacerbate house price differentials leading to even stronger disparity in material living conditions. A related topic is policies to change the social mix of urban neighbourhoods (Graham et al. 2009). According to Galster and Friedrichs (2015, p. 175), “‘Social Mix’ is currently one of the ‘hottest’ (to say nothing of controversial) topics in urban policy-making and scholarly circles across the First World”. Proponents of socially diverse neighbourhoods argue that disadvantaged households can benefit from living among richer and more educated neighbours. Furthermore, socially mixed neighbourhoods could facilitate the social integration of minority groups and might foster creativity and productivity (Florida 2002). However, planning attempts to enhance the social mix might backfire, if households prefer to live in rather homogeneous neighbourhoods and discount the value of their homes if they are forced to live among dissimilar neighbours. While house price differentials are surely important from a policy perspective, the recent literature of social well-being argues that well-being is a multidimensional concept (Linton et al. 2016). Well-being does not only have a material dimension, but also an immaterial one, which is in the centre of the large literature on subjective well-being or happiness (Diener et al. 2018). We hence also analyse whether the satisfaction with dwelling conditions differs systematically among population groups in our model. If poor households are locked in a neighbourhood because they cannot afford to move to a preferred location, their life satisfaction might be depressed not only due to their deprived socio-economic conditions, but also due to dissatisfaction with their neighbours. In order to avoid such a lock-in, urban planners pursue policies aimed at the affordability of housing. Many OECD countries use a variety of policy instruments such as grants and financial assistance for home-buyers to make good-quality housing affordable to poor households (Del Pero et al. 2016). How affordability policies affect house prices and satisfaction levels is not clear a priori. It might lead to a more efficient sorting that raises social welfare, because households have more choice. On the other hand, it might also happen that grants and financial assistance only drive up house prices without any major effect on where households reside. Our model allows us to investigate how these policy instruments affect both house prices and satisfaction levels. The use of agent-based models in this context is very common, since it enables the user to capture the heterogeneity of agents and land-market representation within an urban area. Huang et al. (2014) provide a review of fifty-one relevant contributions to offer a retrospective on the developments in agent-based models of urban residential choice. The contributions vary in their focuses, since, e.g. Jordan et al. (2012) focus on individual preference rankings with budget constraints and endogenous relocations, whereas Feitosa et al. (2011) consider agents’ own attributes, budget constraints, environmental characteristics and population composition for the moving criterion. Fossett and Waren (2005) consider budget constraints, competitive bidding and endogenous relocation by letting agents consider the ethnic mix, demographic changes and random opening vacancies. Agent-based models do not impose equilibrium conditions on market processes and make it possible to analyse behaviour and price dynamics out of equilibrium. This is an advantage of agent-based models over analytical models, because existence, uniqueness and stability of a general equilibrium in the housing market can only be guaranteed under specific and quite restrictive assumptions (Li 2014). Furthermore, even if there is a long-run general equilibrium, we do not know how long it takes to attain it from an out-of-equilibrium state, e.g. after some external shock or policy intervention. In fact, it might be the case that housing markets are most of the time far away from any equilibrium, because the transaction costs of moving might be high and adjustments by moving might be slow. In this model, the main social interaction effect is a pure preference for similarity or preference for status. We do not model spillover effects or indirect social effects, such as having better labour market chances if one resides in an educated or rich neighbourhood (e.g. Wilson 2012). However, we can incorporate such effects by assuming that households prefer living among neighbours with a higher socio-economic status. Our model is meant to be a theoretical investigation of mechanisms that play a role in any housing market. Therefore, the model is not calibrated to fit any particular city or validated against empirical data. The application to housing markets in specific cities could be done as a next step after the mechanisms are well understood. In Sect. 2, we describe the model in detail. Section 3 explains how the model is parameterised and implemented in computer software. Our main results are presented and discussed in Sect. 4. Finally, we conclude in Sect. 5.",
18.0,1.0,Journal of Economic Interaction and Coordination,09 May 2022,https://link.springer.com/article/10.1007/s11403-022-00350-z,Agent models of customer journeys on retail high streets,January 2023,Paul M. Torrens,,,Male,Unknown,Unknown,Male,,2
18.0,1.0,Journal of Economic Interaction and Coordination,18 July 2021,https://link.springer.com/article/10.1007/s11403-021-00332-7,Should retail stores locate close to a rival?,January 2023,James Fain,,,Male,Unknown,Unknown,Male,"Hotelling (1929) was the first economist to examine the optimal location of a retail firm. He posited that consumers consider a good’s full price to be equal to its retail price plus the amount of travel time required to buy the item. In developing this model, Hotelling anticipates that firms will exploit a location monopoly to charge higher prices. In this situation, retail firms should space themselves out so that each one has a geographic area within which it has a degree of market power. However, it is not clear that firms behave in this way. In the USA, many retail stores and restaurants are part of a chain; these firms frequently charge prices that are (1) determined by the chain and (2) uniform across most firms in the chain. Presumably the advantages of name recognition, chain-based advertising and concentrated input buying power outweigh the profit advantage the firm might derive by possibly charging higher prices in places where it has some degree of monopoly power. A chain store that has a location advantage typically does not exploit that advantage by charging higher prices, but instead enjoys a greater volume of sales. In certain instances competing chains even seem to locate stores very close to their rival deliberately; an example of this behavior in the USA is CVS and Walgreens. Geographers have developed models of optimal firm location as well, with the main model of firm location in geography derived from Huff (1963). Huff developed what is commonly called a gravity model, where a store exerts a stronger attraction to the potential customers who are closest to it. This attractive power diminishes with distance from the firm. The Huff model posits that consumers care about the distance traveled to a store, but that is where the similarities to Hotelling’s model ends. Hotelling advanced the idea that consumers employ a knife-edge decision process: if the price at Firm A plus the time required to travel to firm A is evenly slightly lower than the same computation for Firm B, then Firm A wins all of the consumer’s business. Geographic modelers employ a probabilistic approach instead. They assume that each store will obtain a share of a neighborhood’s total business that is inversely proportional to its distance from that neighborhood. In this setting, a neighborhood that has one store three miles away and a rival store that is five miles away will give five eighths of its business to the closer store and three eighths of its business to the more distant store. This implicitly assumes that stores will be charging identical prices. This approach stands in sharp contrast to Hotelling’s assumption that consumers always choose the location with the lowest full price. Modern variations of the Huff model are incorporated into to GIS systems and are widely used by geographers to select sites for malls and other shopping areas. In this paper, I investigate a firm’s decision to locate close to a rival firm. To accomplish this, I first establish a workable agent-based model of firm competition in an urban setting. This model blends the two approaches discussed above, with the firms free to set their own prices, but with consumers making decisions about which store to select in a probabilistic manner. The firms in the model below can be considered low information firms, for they have very little knowledge regarding their local market, and they have no information about the urban area they inhabit. While the firms have very little information, they are capable of learning. The process of finding an equilibrium price is effectively a tatonnement process as described by Walras—by trial and error firms learn about their market and arrive at an equilibrium price of sorts. Each firm gradually learns its own equilibrium price, and this price is not uniform throughout the urban area. The model also forces firms that can’t make a profit to go out of business. Having fewer total firms reduces the overall level of competition in the urban area, with the firms closest to the newly expired firms likely benefiting the most from their demise. In this dynamic context, I address several location-based questions, including whether or not firms should locate close to a rival. Consider an urban area where the residential neighborhoods are segregated by income level. A number of retail stores are scattered throughout the urban area. In this segregated urban landscape, some stores are located close to wealthy neighborhoods, some are close to poor neighborhoods, and some are close to middle-class neighborhoods. These stores have the same cost structure and sell an item that the households buy on a regular basis. Grocery stores and gas stations would be two good examples. Consumers view the products sold by these stores as perfect substitutes, but due to transportation costs they care about the location of the various stores. All else being equal consumers prefer to do business with a store that is closer to their residence. Consumers have a well-specified demand curve and respond to changes in prices; in some cases that response includes buying from a different store. There is no entry in this market, but stores with persistent losses may go out of business. The stores face a very complex profit maximization problem. Since the consumers have transportation costs, the stores may more or less “own” the consumers who live very close to them. However, each store competes with other relatively nearby stores for some customers who reside between the two stores. Raising one’s price to extract a location rent from the customers who live close by may cause the store to lose other customers who are further away. Even if one had extraordinary knowledge regarding income levels and consumer demand curves, the exact demand curve each store confronts would be very difficult to describe because at various unknown price points some consumers would switch to a different store. On the supply side, even correctly identifying one’s competing stores would be a herculean task. Store A may compete with Store B to its east for a certain set of customers and compete with Store C to its west for a different set of customers. Furthermore, Store C’s actions might be influenced by Store D, which is north of Store C. Store D is too far from Store A to be considered a direct competitor, but to the degree that Store D’s actions influence Store C, which is a direct competitor to Store A, Store D’s actions may indirectly have an impact on Store A. The complicated set of direct and indirect interactions among numerous stores are arguably too complicated to fully specify in coherent form. In the model of firm competition in an urban setting that I develop below both the consumers and stores have limited information sets. Consumers are only aware of the four stores that are closest to their residence; they know both the location of these stores and the prices they charge. Consumers do not know how many total stores are in the market, nor do they know the location of most of these stores or the prices they charge. In effect this says that consumers know their own neighborhood very well but have no knowledge of stores outside of their neighborhood. Since consumers buy these products on a regular basis, they care about the distance they must travel to make the purchase. The stores are low information agents: each individual store initially knows only its own cost curve. A store does not know (and never learns or discovers) how many total stores exist in the urban area, how many stores are relatively close by, how many consumers live nearby, or the income levels of those consumers. While the stores have very little information about their market, they can experiment with different prices, remember the results and utilize this information in the future. The setting I have described is somewhat similar to a two-dimensional version Hotelling’s classic work (1929) on optimal store location. While this paper shares some common ground with Hotelling’s work (and those who have continued this line of research), I am not attempting to determine optimal store locations. To drive this point home, in this paper each store’s location is selected randomly. By random chance, some stores are located close to a rival and other are not. I perform many independent runs and am able to examine the probability that a firm survives both when it is located close a rival and when it is not. To add a degree of robustness to the outcomes, I vary the degree to which consumers care about a store’s location. If the cost of traveling to a store is relatively low, then consumers care relatively little about store locations and are willing to travel further to obtain a low price. I show below that in this case the price of the good is only about ten percent above the price predicted by perfect competition. I also consider a case in which the cost of traveling to a store is relatively high. In this scenario, consumers care a great deal about store locations. Since consumers are more inclined to buy from the closest store, the stores gain a measure of market power and can raise their prices sharply. What emerges here more closely resembles monopolistic competition, with a greater number of stores, higher prices, higher profits and substantial excess capacity on the production side. In both instances, all consumers have the same cost of travelling to a store, regardless of income. In the latter setting one is tempted to conclude that the stores are explicitly colluding, but in fact they are not. The stores have no capacity to communicate with each other, so they can’t collude. The prices are high in this setting solely because of consumer preferences that allow each firm a limited degree of monopoly power based on its location; with different consumer preferences the prices are much lower. The results in this setting raise substantial questions regarding the nature and meaning of collusion among stores, for there clearly appears to be collusion, even though collusion literally can’t exist. The outline of this paper is as follows. I review the relevant literature in Sect. 2, and in Sect. 3 I provide further details regarding the model. Section 4 contains details regarding how I run the simulations. In Sect. 5, I examine the simulation results pertaining to the number of firms that survive. In Sect. 6, I provide the results of an econometric analysis of the simulations results. Here I perform a Probit analysis of firm survival probabilities and a regression analysis in which I seek to explain the variation in the stores’ price, profit and quantity sold. I do so for both the low and high transportation cost cases. The econometric analysis shows that the model produces results that are both reasonable and consistent. In Sect. 7, I answer the question of whether or not firms should locate close to a rival firm, and Sect. 8 contains the conclusions.",
18.0,1.0,Journal of Economic Interaction and Coordination,08 September 2022,https://link.springer.com/article/10.1007/s11403-022-00367-4,"It’s worth a shot: urban density, endogenous vaccination decisions, and dynamics of infectious disease",January 2023,Andrew Souther,Myong-Hun Chang,Troy Tassier,Male,Unknown,Male,Male,"Rates of vaccination differ greatly between urban and rural areas in many parts of the world. Many factors contribute to the differences such as lack of access to health care, poverty, and socio-economic status (Hinman and McKinlay 2015). In this paper we consider an additional contributing factor: urban density. Higher levels of density in urban areas may lead to a higher risk of contracting an infectious disease. If people in dense urban areas have more contacts with other individuals, then potentially, they are at greater risk of exposure to infection and thus may be more likely to choose to be vaccinated. We use differences in the number of contacts with other agents as a proxy for differences between urban and rural density. We then model the vaccine decision of agents in terms of a rational cost benefit calculation which includes an agent-estimated probability of infection within a local area, a private cost of vaccination, and a private cost of infection. The benefits of a vaccine are the avoidance of infection when a vaccine is chosen. Our modeling choice corresponds to a growing economic epidemiology literature incorporating endogenous rational choice into models of vaccine decision-making (Brito et al. 1991, Kremer 1996, Geoffard and Philipson 1996 and 1997, Bauch and Earn 2004, Boulier et al. 2007, Goyal and Vigier 2014, Galeotti and Rogers 2013, Tassier et al. 2015). We discuss later in the paper how our model differs in significant ways from these previous models. Influenza causes high rates of morbidity and mortality throughout the world. In the U.S. alone, between the years of 2010 and 2019, influenza has created an estimated 12,000 to 52,000 deaths each year (CDC 2020a). In addition to mortality, influenza causes 140,000–710,000 hospitalizations in a typical year (Rolfes et al. 2018). The influenza vaccine is the most common public health tool used to fight influenza. Despite its recommendation, in the U.S., only about one-half of individuals received an influenza vaccine during the 2019–2020 influenza season leaving the population short of levels needed to achieve herd immunity (CDC 2020b; Plans-Rubió 2012). Further, rates of influenza vaccination are not uniform across the country. According to the U.S. Center for Disease Control (CDC) weekly flu vaccine dashboard, “Coverage among states and DC as of January 8, 2022 ranges from 28.0 to 72.1%; national coverage is 50.3%” (CDC 2022). There is even greater variation at the county level (data to be discussed below). Most empirical investigations of vaccine behavior concentrate on factors that affect access to healthcare, demographic variables, or the growing anti-vaccine movement (Ołpiński 2012). In general, researchers find that higher socio-economic status results in higher levels of influenza vaccination (Lucyk et al. 2019). Differences exist across race and ethnicity as well (CDC 2020b and Hebert et al. 2005). Much of the empirical vaccination rate literature concentrates on the vaccination status of children and particularly the measles, mumps, and rubella (MMR) vaccine. Some empirical models identify “hot spots” where vaccination rates are particularly low (Olive et al. 2018; Wallinga et al. 2005; Carrel and Bitterman, 2015; Lieu et al. 2015; Omer et al. 2008; Salmon et al. 2005). Others examine the effect of income, religion, race, and other demographic factors on vaccine coverage. Smith et al. (2004) find a distinction between under-vaccinated children and unvaccinated children. Under-vaccinated children have some vaccines but not all or have started a vaccine schedule but not completed the schedule on time. In contrast to under-vaccinated children, unvaccinated children have parents who reject vaccines. As an example, an under-vaccinated child may have received the first dose of a MMR vaccine but not a second dose, and the child is significantly past the recommended age for the second dose. An unvaccinated child simply has never received a MMR vaccine and is significantly past the recommended age. Smith et al. (2004) find that under-vaccinated children tend to come from families that are black, lack college degrees, and are in poverty. In contrast, unvaccinated children are more commonly from families that are white, have college degrees, high incomes, and high degrees of concern for vaccine safety. These families also tend to be highly clustered geographically. Other models consider the effect of varying rates of vaccine coverage on the spread of infectious disease (Atwell et al. 2013; Feikin et al. 2000). Vaccine safety is a commonly mentioned reason by parents for lack of vaccine use in their children (Allred et al. 2005; Freed et al. 2010; Gust et al. 2004; Salmon et al. 2005). There exists a smaller empirical literature that specifically examines differences in rural versus urban vaccination rates. Urban areas have been found to have higher rates of influenza vaccinations than rural areas, differing by 10 to 20 percent (O’Leary et al. 2015; Zhai et al. 2020). One reason cited for this difference is lack of easy access to healthcare and poor vaccine distribution in rural areas compared to more urban areas (Bennett et al. 2011). To the best of our knowledge there is no published research with formal modeling of urban versus rural vaccination choice. This is a primary goal of this research. There is an established literature of vaccine behavior within economics and related disciplines. Some of this research is conducted using agent-based computational models with some similarity to the model that we develop (Bansal et al. 2006; Barrat et al. 2010; Del Valle et al. 2013; Vardavas and Marcum 2013; Tassier et al. 2017; Chang and Tassier 2021). Strands of the agent-based and analytical modeling literature can be divided into several subsets of modeling frameworks. One subset consists of analyzing society level optimal vaccination rates in the presence of vaccine externalities. This research is conducted with populations assumed to be well-mixed (randomly mixed) in standard susceptible-infected-recovered (SIR) or susceptible-infected-susceptible (SIS) models. This subset does not consider heterogenous network structure. Examples of this type of research include Geoffard and Philipson (1996), Gersovitz and Hammer (2003), and Boulier et al. (2007). Other models within economics consider analytical network-based vaccine choice but in a different context than we study. For instance, Galeotti and Rogers (2013) consider individual vaccine choice when there are two groups with the possibility of overlapping network connections between the groups. However, their model incorporates an SIS setting. SIS models typically result in steady state levels of infections making them more tractable analytically. However, many of the most common vaccine preventable diseases (such as influenza or measles) are most commonly modeled within a SIR or SEIR framework as opposed to a SIS framework. SIS models are commonly used for infectious diseases which do not confer immunity upon recovery such as gonorrhea. Other models incorporate an SIR or SEIR setting and a network structure, but often have a simplified decision-making structure. For instance, Vardavas and Marcum (2013) incorporate a heterogeneous network structure and endogenous decision-making but agents respond to population level epidemic size and individual experience. Each agent has an exogenously given threshold parameter. If the agent was vaccinated in the previous year and the global population has an epidemic size above the pre-assigned threshold, the agent chooses to be vaccinated again; if the global population threshold is not met, the agent forgoes vaccination. If unvaccinated in the previous year, the agent uses her experience of having been infected or not when making a decision. In comparison, the model that we develop below allows agents to respond to infection risk in a local region (as opposed to infection risk in the overall population) and the networks that we consider vary in ways that can be interpreted in a geographic context. Other models in the agent-based paradigm change network structure variables or policy variables to investigate the effect of these changes on epidemic spread. However, these models do not incorporate endogenous decision-making by the agents. Instead, vaccination rates, parameters that determine network structure, and other relevant policy variables are assigned as parameters of the model. Examples of this literature include Bansal et al. (2006), Del Valle et al. (2013), Tassier et al. (2017), and Chang and Tassier (2021). Finally, Goyal and Vigier (2014) allow an agent to choose the population level network structure within a game theoretic setting before choosing a method of defense to contagion (which could be interpreted as a vaccine). This does not fit our research questions as no one agent in our population has the ability to design a top-down global network structure. In this paper we combine several features of these economic agent-based models and examine how vaccine behavior may differ across urban and rural areas. First, we use an explicit network model implemented in an agent-based framework. Agents exist in either a rural (less-dense) or urban (more-dense) environment which we call a region. There does not exist an explicit geography in the model but one could interpret the model as being based on geographic density. Second, as we have seen during the Covid-19 pandemic, often individuals do not act with only global risk in mind. Instead, agents consider local conditions in a state or city, for example, in making judgements of infection risk. In addition, because infection risk can vary by location, it is important to incorporate local differences in risk into an economic model of vaccine decision-making. In our model, agents make rational choices given their estimation of the risk of infection in their local region and compare this risk to private costs of infection and vaccination, which we allow to vary across agents. The agents choose to vaccinate when vaccination maximizes expected utility and forgo a vaccine when it does not. By varying costs and network structure we view the incidence of an infectious disease across time and the corresponding endogenous choices to vaccinate made by agents in the model. We find that incorporating endogenous choice of vaccine behavior is an important element of understanding epidemic outcomes. For instance, we find that changes in network structure that create larger epidemics in exogenous vaccination environments, create smaller epidemics in our endogenous vaccination environment. This occurs because agents offset the increased network risk with higher levels of vaccine usage. Thus, endogenous behavior is an important component of theoretical analysis and, importantly, needs to be incorporated into policy advice and empirical investigation. In the following sections of the paper, we demonstrate how county level vaccinations differ across the United States as a function of population density using data from the CDC as well as other publicly available sources. We then discuss a basic strategic model of vaccine choice which we embed into an agent-based epidemiological model. The epidemiological model is a commonly used Susceptible Exposed Infectious Recovered (SEIR) model (Anderson and May 1992), which will be discussed in full later in the paper. This model can be interpreted as similar to a common infectious disease such as influenza, measles, or the current novel coronavirus pandemic. After describing the experimental design of the agent-based model we run instances of the model with varying costs and network structures and discuss the results.",
18.0,1.0,Journal of Economic Interaction and Coordination,17 May 2022,https://link.springer.com/article/10.1007/s11403-022-00353-w,Direct and indirect transmission of avian influenza: results from a calibrated agent-based model,January 2023,Amanda Beaudoin,Alan G. Isaac,,Female,Male,Unknown,Mix,,
18.0,2.0,Journal of Economic Interaction and Coordination,20 August 2022,https://link.springer.com/article/10.1007/s11403-022-00364-7,The weighted cross-shareholding complex network: a copula approach to concentration and control in financial markets,April 2023,Roy Cerqueti,Giulia Rotundo,,Male,Female,Unknown,Mix,,
18.0,2.0,Journal of Economic Interaction and Coordination,03 September 2022,https://link.springer.com/article/10.1007/s11403-022-00365-6,“Less is more” or “more is better”? The effect of asymmetric information distribution on market efficiency and wealth inequality,April 2023,Rocco Caferra,Simone Nuzzo,Andrea Morone,Male,Female,Female,Mix,,
18.0,2.0,Journal of Economic Interaction and Coordination,26 August 2022,https://link.springer.com/article/10.1007/s11403-022-00366-5,Agents interaction and price dynamics: evidence from the laboratory,April 2023,Rocco Caferra,Gabriele Tedeschi,Andrea Morone,Male,Female,Female,Mix,,
18.0,2.0,Journal of Economic Interaction and Coordination,30 August 2022,https://link.springer.com/article/10.1007/s11403-022-00368-3,Gradual financial integration and macroeconomic fluctuations in emerging market economies: evidence from China,April 2023,Yong Ma,Yiqing Jiang,,,Unknown,Unknown,Mix,,
18.0,2.0,Journal of Economic Interaction and Coordination,28 October 2022,https://link.springer.com/article/10.1007/s11403-022-00369-2,How to design virus containment policies? A joint analysis of economic and epidemic dynamics under the COVID-19 pandemic,April 2023,Alessandro Basurto,Herbert Dawid,Dirk Kohlweyer,Male,Male,Male,Male,"The ongoing COVID-19 pandemic has caused a global health crisis resulting in more than 180 million reported cases and 3.800.000 casualties, as of the end of June 2021. Policymakers in many countries have responded to the pandemic by introducing a large variety of containment measures (see Cheng et al. 2020; Haug et al. 2020). Many of these measures have substantial implications for economic activity confronting policymakers with a trade-off between a rapid containment of the pandemic and the prevention of severe economic disruptions. Finding a balanced policy mix to resolve this trade-off is a major political challenge, for which it is crucial to develop a thorough understanding of the joint epidemic (number of infected, mortality) and economic (GDP loss, sectoral unemployment) effects of different measures (Murray 2020). Whereas well-established epidemiological models can be employed to address the first of these issues (e.g., Kissler et al. 2020; Giordano et al. 2020; Ferretti et al. 2020; Britton et al. 2020), rigorous approaches for studying both dynamics simultaneously are still sparse. Considering these two aspects in an integrated framework is important not only because many containment measures have direct economic effects, but also because several main infection channels are directly related to economic activity (Chang et al. 2021). The growing economic literature investigating the COVID-19 pandemic on a theoretical level mainly builds upon the standard equation-based SIR model to model the infectious disease ( Kermack and McKendrick 1927; Hethcote 2000) and introduces some links to economic activity. Measures taken to contain the pandemic thereby typically reduce production potential or consumption and hence induce an economic shock. The interplay between containment measures and economic costs is then studied as an optimization problem from a social planner’s point of view (e.g., Alvarez et al. 2020; Miclo et al. 2020; Acemoglu et al. 2020), or embedded in a simple macroeconomic framework, where agents individually optimize their decisions (e.g., Eichenbaum et al. 2021; Krueger et al. 2020; Jones et al. 2020). Such abstract models rely on deterministic representations of the virus dynamics and do not capture the local and complex social interactions associated with economic activities (Epstein 2009), which play an important role in the propagation of the coronavirus (see, e.g., Wu et al. 2020; Prather et al. 2020). Hence, these models neither take the interplay between economic structure (e.g., size and sectoral distribution of firms) and the transmission dynamics into account nor capture the stochastic variation of economic and epidemic dynamics. Although there exists a wide range of stochastic SIRD-type epidemic models, these approaches have not been incorporated into economic models so far.Footnote 1 The main contribution of this paper is to examine the economic and epidemic effects of lockdown measures using a calibrated micro-founded stochastic macroeconomic model, which explicitly captures the role different economic activities play with respect to the spread of the coronavirus. In particular, our model captures virus transmissions at the workplace, transmissions caused by interactions between consumers and producers, and transmissions via private contacts. Furthermore, we consider an age-structured population, allowing us to capture age-specific differences with respect to economic activities (e.g., working vs. retired population) and the case fatality rate of COVID-19. In addition to capturing relevant transmission channels, the detailed representation of socioeconomic interaction structures allows us to implement a wide range of specific containment measures in our framework. The model is calibrated based on German micro- and macro-data and is capable of matching to empirical time series, both for economic and for epidemiological indicators, under a policy scenario resembling measures implemented in Germany. Based on this, we investigate different lockdown scenarios by systematically varying key parameters governing the intensity of measures during a lockdown, the degree of relaxation after the lockdown, and the incidence thresholds used to end/reintroduce the lockdown measures. We show that a policy combining strict lockdown measures with a full opening-up of the economy between lockdowns and a high incidence thresholdFootnote 2 for (re)entering lockdowns is strictly dominated by alternative policies, which either combine a substantially smaller incidence threshold with weaker lockdown measures or implement a continuous light lockdown with only minor restrictions.Footnote 3 The reason that policies alternating between strict lockdowns and full opening perform worse not only with respect to the expected number of casualties, but also with respect to economic losses, is that they induce a higher degree of volatility into the economy. In light of frictions on the labor and product market, this generates higher economic losses compared to scenarios where weaker restrictions induce only smaller, although more persistent, downturns. Similar to others (Acemoglu et al. 2020; Alvarez et al. 2020; Atkeson 2020), we find that there exists a trade-off between economic losses and infection numbers when varying lockdown intensity given a fixed incidence threshold. We also demonstrate that the policies differ substantially with respect to the ex ante uncertainty about the induced economic loss. In particular, the policies which are at the efficiency frontier also tend to give rise to substantially lower variation. Understanding the implications of different policy choices for the variance of policy results seems particularly important in an area like virus containment, where the effectiveness of chosen measures also depends on the policies’ acceptance by the general public. In such a setting, bad initial outcomes might have a detrimental effect on public acceptance of the policy, deteriorating its future effectiveness (Bargain and Aminjonov 2020; Altig et al. 2020). To our knowledge, this paper is the first economic analysis of lockdown policies, explicitly addressing the relationship between policy properties and variance of the resulting economic and epidemiological dynamics. Since the value of the infection probability of the virus on the one hand is crucial for the epidemic dynamics and on the other hand entails a considerable uncertainty around its exact value, we conduct a robustness check of our results concerning this parameter. The main setting of our analysis is based on the assumption that at a certain point in time, the virus mutates to a more infectious variant, which then spreads in the population simultaneously with the original version. This pattern resembles the situation in many countries during the COVID-19 pandemic. We show that most of our qualitative insights, in particular the appeal of a policy combining a strict lockdown with a weak opening, still apply in scenarios without a mutation (reducing the infection probability) and with a higher infection probability. Clearly, a higher (lower) infectiousness of the virus leads to higher (lower) mortality rates and in consequence also to higher (lower) economic losses. By design, only the continuous weak lockdown is characterized by a stable loss of GDP and becomes more attractive, the higher the infection probability of the virus or its mutation. In the final part of our analysis, we assess the optimal ending point of the policies under a dynamic vaccination rollout. Depending on the lockdown policy, we find that after a vaccination rate of \(25-40\%\) no significant gains in terms of mortality are achieved when prolonging the containment measures. We conclude that in our setting, lockdown policies can be lifted relatively early. In light of the mechanisms underlying our insights, our qualitative results can be transferred to countries with a health system and economic structure comparable to Germany. In addition, the flexibility of the framework allows the modeler to adjust the parameters related to COVID-19 to analyze potential future pandemics. In fact, the model can easily be re-calibrated to data from other countries or from different pandemics in order to analyze appropriate policies under alternative structural conditions. Methodologically, our approach combines a SIR-type simulation model with an agent-based macroeconomic model. The design of the economic part of the model, in particular with respect to the structure of the individual decision rules as well as the market interaction protocols, builds strongly on a well-established agent-based macroeconomic framework, namely the Eurace@Unibi model, which has been already used for the analysis of a wide range of economic policy issues (e.g., Dawid et al. (2014, 2018, 2019)). Nevertheless, the model employed here is not an extension of the Eurace@Unibi model, but a separate agent-based model designed for the analysis of the interplay of economic activities and virus transmission, which has also been implemented independently from the Eurace@Unibi model.Footnote 4 Agent-based models have been used to assess the effectiveness of containment policies in purely epidemiological studies (e.g., Adam 2020; Ferguson et al. 2020; LeBaron 2021) and the approach has been applied to address a large variety of macroeconomic research questions and policy analyses in recent years (see, e.g., Foley and Farmer 2009; Dawid and Delli Gatti 2018; Dosi and Roventini 2019, for discussions). By explicitly linking economic activities and transactions to contacts between agents, agent-based economic models are particularly suited for studying the dynamics of virus transmissions in an economy. Only a few other studies have used a unified agent-based model, combining an economic framework with an epidemiological structure in the context of COVID-19. Delli Gatti and Reissl (2020) opt for a relatively smaller model in terms of the number of agents with 2800 households and 300 firms calibrated on an Italian region, Lombardy. Our paper differs from theirs in terms of the main research question. While their focus is on the consequences of different fiscal measures, our main goal is to understand the impacts of several lockdown policies on economic loss and mortality while keeping in place the fiscal measures introduced by the German government. Moreover, we also introduce the occurrence of a virus mutation in a macro-epidemic agent-based model. Sharma et al. (2021) introduce the COVID-19 crisis as exogenous shocks of demand and supply, dropping firm productivity and household consumption propensity, into the Mark-0 agent-based model. Shocks amplitude and duration are the main factors describing the crisis but epidemiological dynamics is not considered. Mandel and Veetil (2020) use a multi-sector disequilibrium model with an agent-based flavor to study the impact of COVID-19 related lockdowns, taking into account input–output data and supply chain effects. However, this paper also does not consider epidemiological dynamics. Mellacher (2020) has a very rich network-based model that enables a detailed treatment of contact spaces such as households, hospitals, or retirement homes. On the other hand, it does not feature shopping contacts which we consider crucial to assess the interconnection of economic and pandemic effects of closures. Silva et al. (2020) present a model to study the effects of various counterfactual containment scenarios on a virtual economy representing Brazil. However, our paper is the first to use such a unified framework for the evaluation of average economic and pandemic effects as well as the associated uncertainty about outcomes under different policy responses to the outbreak of the COVID-19 pandemic. The paper is organized as follows. In Sect. 2, we provide a short description of the model (a detailed description is given in Appendix A), and in Sect. 3 we describe the set of containment policies considered in our analysis. In Sect. 4, we discuss the calibration of the model and demonstrate the good fit of the model output with time series data from Germany. The main insights from our policy analysis are discussed in Sect. 5, and we end with conclusions and an outlook on potential extensions of the analysis in Sect. 6. In addition to the detailed description of the model, the Appendix contains some results with respect to additional policy variations, statistical test results underlying our findings, and lists of model variables and of parameter values.",2
18.0,2.0,Journal of Economic Interaction and Coordination,27 September 2022,https://link.springer.com/article/10.1007/s11403-022-00371-8,The persistence of economic sentiment: a trip down memory lane,April 2023,Petar Sorić,Ivana Lolić,Marina Matošec,Male,Female,Female,Mix,,
18.0,2.0,Journal of Economic Interaction and Coordination,16 October 2022,https://link.springer.com/article/10.1007/s11403-022-00372-7,"The distribution of wealth: an agent-based approach to examine the effect of estate taxation, skill inheritance, and the Carnegie Effect",April 2023,Christopher W. Kulp,Michael Kurtz,Matthew Velardi,Male,Male,Male,Male,"Wealth inequality in the USA has been on the rise since the 1970s (Saez and Zucman 2016, 2020) with similar trends elsewhere. Calls to address this growing inequality have also been on the rise as have debates on the seriousness and severity of this reality. Economic inequality has been linked to a number of negative outcomes such as decreased economic growth (Galor and Zeira 1993; Temple 1999) and poorer health (Pickett and Wilkinson 2015). Further, the ratio of wealth to income has been rising in wealthy countries (Piketty and Zucman 2014), suggesting that income taxation may be increasingly less effective at curbing economic inequality (a finding supported by Kulp et al. (2019)). For the same reason and owing to the compounding nature of wealth accumulation, inheritances are a growing portion of national income, suggesting that intergenerational wealth transfers are themselves an important factor in furthering the growing economic inequality (Piketty 2013). Combined, the above observations suggest estate taxation may provide an effective avenue to curb growing economic inequality. Piketty and Saez (2013) note that existing economic theory is far from conclusive on optimal estate taxation as measured solely by efficiency standards. However, as noted above, there may be good reason to also value equity, and indeed, the public discussion on the topic has focused increasingly on this equity–efficiency trade-off. When this trade-off is examined using a calibration model based on data from France and the USA, Piketty and Saez (2013) find the optimal estate tax rate could be over 50%. Even on efficiency alone, Pavoni and Yazici (2017) suggest the optimal taxation of intergenerational transfers may be positive. However, Kopczuk (2013) suggests that empirical and theoretical evidence on the taxation of intergenerational transfers is incomplete and the motivations and effects of which should be better studied. Indeed, Cremer and Pestieau (2011) find that the optimal wealth transfer tax depends greatly on the motivation for the bequest. For a detailed review of the theoretical and empirical literature on the taxation of intergenerational wealth transfers, see Cremer and Pestieau (2006), Cremer and Pestieau (2011), and Kopczuk (2013). In particular, the behavioral labor response to large bequests (and thus also the taxation of estates)—often called the “Carnegie Effect” or “Carnegie Conjecture”—has been shown to be difficult to measure and to vary widely in different situations. The Carnegie Effect is the hypothesized and often-observed reduction in labor market participation or work effort as a result of large inheritance (Bø et al. 2019). As such, taxation of inherited wealth can lead to an increase in labor and thus also an increase in labor income tax revenue (Kindermann et al. 2020) (an effect we explore in this paper). A number of studies have attempted to quantify the Carnegie Effect and have found a range of results that can be sensitive to factors such as whether the inheritance was expected or not and the age of the individual when the inheritance was received (Holtz-Eakin et al. 1993; Joulfaian 2006; Brown et al. 2010; Bø et al. 2019; Kindermann et al. 2020). Further, the size of the effect appears to be nonlinear relative to the size of the inheritance (Holtz-Eakin et al. 1993; Bø et al. 2019). In this paper, we simulate an economy using an agent-based model where agents are born, work, die, and leave their accumulated wealth to their next-generation agent (offspring). We examine how different estate tax rates, structures, and redistribution methods affect measures of equity and efficiency in the simulated economy as measured by three common social welfare functions (SWF)—Utilitarian, Sen, and Rawlsian—and the wealth Gini coefficient in a simple simulated economy; and then extend the models to include two key labor market features: inheritability of skill and the Carnegie Effect. Our findings are consistent with Piketty and Saez (2013), finding a flat estate tax of around 40% which exempts the lowest 87th percentile of wealth optimizes wealth equity as measured by the Sen SWF and the Gini coefficient. A lower tax rate maximizes the Rawlsian SWF (the wealth of the poorest agent in the economy) if no wealth is exempt from taxation. With exemptions, the Rawlsian increases with tax rate. When incorporating the Carnegie Effect, we see an absolute reduction in the Utilitarian SWF (the total wealth in the economy). However, as tax rates rise in the inherited-skill Carnegie model, the Utilitarian SWF also rises because inherited income falls diminishing the Carnegie Effect and increasing labor income. This suggests estate taxation has the potential to improve measures of equity and efficiency. The remainder of the paper is structured as follows: Sect. 2 describes the economic metrics used to compare different model variations, Sect. 3 describes the main model and several variations examined in this paper, Sect. 4 describes the estate taxes collected in our model, Sect. 5 provides the results, and Sect. 6 provides a discussion of the results.",
18.0,2.0,Journal of Economic Interaction and Coordination,30 June 2022,https://link.springer.com/article/10.1007/s11403-022-00359-4,"Correction to: Do macroeconomic and financial governance matter? Evidence from Germany, 1950–2019",April 2023,Taner Akan,Tim Solle,,Male,Male,Unknown,Male,"In this article the affiliation details for Author Taner Akan were incorrectly given as ‘Department of Economics, Tubingen University, Tubingen, Germany’. This has been corrected in the original article.",
18.0,3.0,Journal of Economic Interaction and Coordination,23 May 2023,https://link.springer.com/article/10.1007/s11403-023-00385-w,Emergence in complex networks of simple agents,July 2023,David G. Green,,,Male,Unknown,Unknown,Male,"Almost everywhere we look, we find examples of large-scale patterns and organisation that emerge in collections of simple entities. On the one hand, there are natural wonders, from growing crystals (Bisoyi and Kumar 2011) and flocks of birds to the beauty of growing plants (Reuter et al. 2010). On the other hand, there are devastating events, such as market crashes (Shi et al. 2022), bushfires (Zinck et al. 2011), epidemics (Jenkins et al. 2020) and accidents (Coze 2015). There are also unanticipated trends in society (Green 2014) and in the business world (Arthur 2021). Understanding emergent phenomena, and in some cases controlling them, poses a huge challenge. In every field of activity, society needs to understand how emergence occurs and what its implications are. The idea of emergence is intimately bound up with the study of complex systems. In the past few decades, research on complexity has exploded into a vast literature (Haynes and Alemna 2022), and has revealed many insights about ways in which complexity fosters emergence. In this perspective survey, I begin by defining emergence and briefly overview examples in economics and society. This includes a discussion of how intelligence emerges in groups of agents. I then survey common mechanisms that lead to emergence. Finally, I survey some modelling paradigms and tools that have been widely used to study emergence. My account focuses on emergence in networks of simple agents. The reason for this is that many, if not most, cases of emergence can be explained in terms of interactions within networks. The network model of complexity is based on the observation that all complex systems can be represented as networks (Green 1994, 2000). This has two important implications. First, properties emerge out of interactions between the nodes of the network, rather than the properties of individual nodes. Second, patterns and processes in networks underlie many properties and behaviours that we see in real systems. Although humans are certainly not “simple” agents in the general sense, simple rules often govern human behaviour, rather than considered decisions. I therefore include examples of emergence, such as crowds and markets, where networks of humans often interact and behave in simple ways. This account puts a strong emphasis on emergent phenomena in economics and related fields. However, it also draws examples from across a broad spectrum of fields, as these often throw a different light on emergence. Markets, for instance, are often compared to ecosystems. For this reason, I include ecological examples that may hold useful lessons for economics. One of the issues impeding complexity theory as a field of research is that it has been studied in so many different disciplines, often with relatively little dialogue between them. By presenting a wide range of examples, my aim is to give readers a wider perspective for understanding emergence.",
18.0,3.0,Journal of Economic Interaction and Coordination,15 October 2022,https://link.springer.com/article/10.1007/s11403-022-00373-6,Group contest in a coopetitive setup: experimental evidence,July 2023,Hubert János Kiss,Alfonso Rosa-Garcia,Vita Zhukova,Male,Male,Female,Mix,,
18.0,3.0,Journal of Economic Interaction and Coordination,25 October 2022,https://link.springer.com/article/10.1007/s11403-022-00374-5,Interaction between price and expectations in the jar-guessing experimental market,July 2023,Toshiaki Akinaga,Takanori Kudo,Kenju Akai,Male,Male,Unknown,Male,"In the market, traders guess the value of an asset and take a behavior based on their expectations about the asset value. The market disseminates information in the form of, for instance, prices that reflect traders’ expectations. Information, such as prices, affects traders’ expectations. As George Soros (1987, p. 2) says, cognition of participants and their participation in the market interfere with each other. Soros, a legendary investor, calls such interference reflexivity. Soros (1987, p. 16) complains that economic theories strangely neglect reflexivity. However, modern economic studies have attempted to unveil these reflexive aspects by introducing interactive agents explicitly in their models. Learning-to-forecast experiments (LtFEs) were introduced by Marimon et al. (1993) and spawned a series of works: Heemeijer et al. (2009), Bao et al. (2012, 2017), and Colasante et al. (2019) investigated interaction between prices and expectations in the laboratory. In these experiments, subjects forecast a price and their forecasts were aggregated to determine the price; then, they forecast again considering the realized prices. These studies focused on expectation formation rather than price formation, requiring subjects to only forecast prices. LtFEs, based on price adjustment rules, relied on a computer to calculate prices from subjects’ forecasts. In contrast, experimental economics has a long-standing tradition of studying price formation. Double-auction (DA) experiments pioneered by Smith (1962) reveal that simple laboratory markets can discover the price that is efficient by a certain criterion. In these classic experiments, values of tradable objects are given by an experimenter as experimental parameters; therefore, there is no need for subjects to form an expectation about the value of objects. Further, Smith et al. (1988) and Haruvy et al. (2007) investigated expectation formation in DA markets.Footnote 1 In these studies, assets brought about stochastic dividends. The probability distribution of dividends, given by an experimenter, determined the asset value. The present study also investigates expectation formation in DA markets. However, the value of our asset is deterministic and set as an experimental parameter. We used a glass jar filled with small balls to define the asset value. We asked our subjects to guess the number of balls in the jar and to trade an asset whose value is equal to that number. Treynor (1987) conducted a similar jar-guessing experiment in his classroom. The mean estimate of the number of beans in a jar, as estimated by his students, was close to the true value, but when they were cautioned, the mean of their estimates became worse. Treynor (1987) conjectured that his warnings about a jar caused a shared error to creep into individual estimates. This study suggests that the heterogeneity among individual expectations is essential for the accuracy of the average expectation, which determines the price in the market. Our study aimed to investigate whether the effect of the market on expectations, if any, deteriorates or improves the accuracy of expectations. We also wanted to examine how subjects’ expectations about the asset value determine its price in the market. Our experiment was conducted under the environment of the constant fundamental value, which Noussair et al. (2001) reported as a factor making bubbles smaller and less frequent in experimental markets, although it was not sufficient to eliminate the possibility of a bubble. Such a simple environment would provide a benchmark for future studies. We found that prices converged to the median expectation of the asset value. This convergence seemed to be a necessary consequence of the subjects’ behavioral tendency in trading, wherein the subjects with higher expectations tended to buy and those with lower expectations were apt to sell. Our subjects tended to revise their expectations, that is, their estimates of the asset value, in accordance with the market prices. Despite this, our experimental markets showed no definitive effect on the accuracy of the subjects’ estimations. This is because their revision process was consistent with the partial adjustment formula that targets the median estimate without any significant disturbances. We explain the design of our experiment in Sect. 2, and analyze experimental outcomes in Sect. 3, where an analysis on trading behavior is in Sect. 3.2.2. To examine the process of individual subjects to revise their expectations, we use the partial adjustment model in Sect. 4, where Sect. 4.2 presents our analysis of the disturbance in this model. The disturbance has a potential to improve or deteriorate the estimation of subjects as a cohort. Section 5 discusses the notion of the wisdom of crowds, the comparison to previous studies, and the directions for future research. We provide concluding remarks in Sect. 6.",
18.0,3.0,Journal of Economic Interaction and Coordination,13 January 2023,https://link.springer.com/article/10.1007/s11403-023-00377-w,The financial network channel of monetary policy transmission: an agent-based model,July 2023,Michel Alexandre,Gilberto Tadeu Lima,Alberto Russo,Male,Male,Male,Male,"The intrinsically ‘robust-yet-fragile’ nature of a financial network (FN) has been long recognized as a key feature requiring due and careful consideration when analyzing the dynamic stability of the economy. In fact, complex market interconnections among heterogeneous and uncoordinated agents acting in a decentralized way, by their very nature, inevitably perform as both shock-absorbers and shock-amplifiers (Chinazzi and Fagiolo 2015). Throughout this paper, by the notion of financial interconnection we mean the existence of direct contractual financial obligations between agents, such as loans extended by banks to firms and debt obligations in the interbank market. Our focus, in this paper, is on the impact of shocks to the base interest rate that propagate through direct and indirect network linkages between economic agents—here, firms and banks.Footnote 1 In an influential paper, Allen and Gale (2000) argue that the benefits of risk diversification generated by a more interconnected FN more than offset the perils of risk propagation. However, this view has been subject to various criticisms (Freixas et al. 2000; Brusco and Castiglionesi 2007). The coup de grâce, so to speak, came with the 2007-2008 financial crisis, which has clearly (and costly) shown that a localized shock may spread to a large part of the economy through complex financial linkages. As a result, policymakers and academic researchers have become considerably more aware that the role played by network interconnectedness in the propagation of a localized shock should not be neglected. Nevertheless, the ways in (and more precisely yet the mechanisms through) which network interconnectedness affect the ‘resilience-cum-fragility’ of the economy as a complex adaptive system is understandably still an open issue.
 Direct interconnections in a FN can lead to shock propagation through two main risk channels. The first one—the so-called counterparty risk—refers to the risk of creditors not recovering at least part of their investment in agents affected by a negative shock. This channel is present in many network-based models of financial contagion (Delli Gatti et al. 2010; Upper 2011; Battiston et al. 2012; Bech and Garratt 2012). The second one is the so-called funding risk. This channel affects debtors and can operate in two ways when agents hit by, say, a negative shock either refuse to roll-over short-term sources of funding, such as loans and repo lending (López-Espinosa et al. 2012), or anticipate the liquidation of assets (Allen and Gale 2000). Notice the inherent asymmetry involved in the operation of these channels: in the case of the counterparty risk, a creditor will not recover more than what was specified in the respective financial contract if the agent she had invested in is hit by a same-size but now positive shock; in the case of the funding risk, a debtor hit by a same-size but instead positive shock will not necessarily be willing or capable to either roll-over (or contract new) short-term sources of funding or anticipate the acquisition of assets. An important question raised by many studies is what topological features of the financial network are mostly related to its resilience to shock propagation and how such a relation works. Interconnectedness is often pointed as playing the most important role in determining the financial network resilience. However, there is a relative consensus on some points about this relationship: (i) there are some topological features which are important predictors of the financial network resilience, such as the degree distribution and the assortativity, (ii) it is not linear (e.g., it seems that shock transmission rather than shock absorption dominates at low levels of connectivity), and (iii) it depends on other elements, as the size of the shock (Acemoglu et al. 2015) (more on this in Sect. 4.1). Network models have been largely used both in the measurement of systemic risk and the assessment of supervision and regulation policies intended to mitigate it. In effect, macroprudential policy tools have been usually employed for such purpose. Although monetary policy is primarily concerned with achieving and maintaining macroeconomic stability, its impact on financial stability has been extensively studied, especially after the 2007–2008 crisis (Stein 2012; Alexandre and Lima 2020; Riccetti et al. 2013). But only a few financial network models have incorporated monetary policy issues to their analytical framework. In Georg (2013) and Bluhm et al. (2014), for instance, the central bank provides liquidity in the interbank market. Meanwhile, Silva et al. (2020) estimate both direct and indirect impacts of monetary policy shocks (as measured by changes in the policy interest rate) on the economy by applying a multi-layer network model to a unique Brazilian data set. Yet policy-oriented network models of financial contagion typically assume that shocks propagate through an exogenously given network. In effect, little is known about how shock propagation affects the topology of a FN, although there is robust evidence that the topology of a FN is sensitive to economic or regulation policy. Halaj and Kok (2015) calibrated a model for a sample of eighty European banks and showed that the topology of the interbank network generated by the model can be affected by different types of regulation. For instance, when high exposure limits are lowered as a regulation policy, banks reduce the size of their exposure and increase the number of connections in the interbank market. Bluhm et al. (2014) found that liquidity provision increases banks’ resilience to shocks, although it is detrimental to financial stability through two different but considerably interrelated channels: it encourages risk-taking behavior and increases interconnectedness, thus facilitating the propagation of shocks. Given that the topology of a FN plays a key role in the propagation of shocks through the financial and the real sides of the economy, which are themselves interrelated in a complex way, a proper assessment of how the manipulation of a policy instrument such as the interest rate impacts on systemic risk requires that the intermediating effects operating by means of topological features of the FN are duly considered. These intermediating effects constitute what we dub here the financial network channel of monetary policy transmission. Using an analogy with the Lucas critique (Lucas 1976) originally applied to macroeconomic stabilization policy, it seems reasonable to conjecture that the topology of a FN, by virtue of it being to a great extent affected in a complex manner by the decentralized and uncoordinated decisions and actions of its heterogeneous participants, is unlikely to be invariant to monetary policy shocks. The purpose of this paper is in contributing to a further understanding of the working of the financial network channel of monetary policy. To this aim, we develop an agent-based model (ABM) in which banks extend loans to consumption-good firms. The bank–firm credit network is topologically structured as determined by plausible and realistic behavioral assumptions, with both firms and banks being always willing (but not always able) to close a credit deal with the network partner perceived to be less risky. Our ABM succeeds in reproducing a handful of key stylized facts of bipartite bank–firm credit networks documented in the related literature. We then assess through simulation how exogenous shocks to the policy interest rate affect some key topological measures of the bank–firm credit network (density, assortativity, size of largest component, and degree distribution). In particular, we will focus our novel simulation analyses on the timely issue of how positive and negative shocks to the base interest rate that vary in terms of magnitude and duration operate through the financial network channel of monetary policy to affect the above-mentioned topological features of the bank–firm credit network. Our main results can be summarized as follows: (i) a positive (negative) interest rate shock decreases (increases) the density of the network. Therefore, when the flow of credit between banks and firms increases as a result of a negative interest rate shock, there is the creation of new links rather than a more intense flow through the existing ones. On the other hand, a decrease in the flow of credit leads to the destruction of links. That is, the flow of credit increases/decreases mainly along the extensive margin (i.e., creation/destruction of links) rather than the intensive one (i.e., more/less intense flow in the existing links); (ii) negative shocks make the financial network more disassortative, as the links created in this case are mostly between highly connected agents and poorly connected agents of the oppositive type. Similarly, the impact of positive shocks to the base interest rate is mostly to destroy the links between highly connected agents and those with less connections of the opposite type, decreasing the disassortativity of the financial network; and iii) interest rate shocks have a long-term impact in the kurtosis of the degree distribution of both banks and firms. Temporary shocks lead to a decrease (in most of the cases) in the kurtosis of the degree distribution, suggesting an asymmetry in the impact of negative and positive shocks. On the other hand, permanent negative (positive) shocks decrease (increase) the kurtosis of banks’ degree distribution. In the case of the firms, this relationship is the opposite. A possible explanation for these results is that a higher supply of credit, caused by a negative interest rate shock, takes the form of more banks supplying credit to the more credit-demanding firms. Our paper contributes to the literature on the modeling of bank–firm credit networks. Some studies (Lux 2016; Poledna et al. 2018; Chakraborty et al. 2019; Ramadiah et al. 2020; Luu 2022) employ empirically based approaches to reconstruct and analyze bank–firm credit networks. Our paper is particularly close to Banwo et al. (2019). The approach used by the authors, which extended the CRISIS Mark 1 ABM, leads to the emergence of a constantly evolving interbank and bank–firm networks. Our contribution to the literature is twofold. First, we develop an ABM able to reproduce the main stylized facts about bank–firm credit network. Second, we assess how shocks—specifically, interest rate shocks—affect the topology of the bank–firm credit network. To the best of our knowledge, our paper is the first to address this important issue bearing relevant implications for the conduct of monetary policy. As pointed out earlier, some studies have addressed the opposite question—that is, how the topology of the FN drives the propagation of shocks—assuming an exogenously given network. Besides this introduction, the paper is organized as follows. Section 2 presents the structure of the model, while Sect. 3 reports and discusses several simulation results. In particular, this section shows that our model is able to reproduce some key stylized facts of the financial network studied in this paper. After discussing the relationship between network topology and resilience, Sect. 4 shows the impact of monetary policy shocks in key topological features of the bank–firm credit network. Finally, Sect. 5 offers concluding remarks.",1
18.0,3.0,Journal of Economic Interaction and Coordination,03 February 2023,https://link.springer.com/article/10.1007/s11403-023-00378-9,Wage claim detracts reciprocity in labor relations: experimental study of gift exchange games,July 2023,Tetsuo Yamamori,Kazuyuki Iwata,,Male,Male,Unknown,Male,"Because of changes in worker-management relations over the past few decades, workers’ requests for wage increases or contract wages are common in the workplace. These include frequent job changes (Neale and Bazerman 1991), the decentralization of wage bargaining, declining union densities (e.g., Dahl et al. 2013), and the increasing use of voice (or grievance) systems for employees in nonunion workplaces (e.g., Feuille and Delaney 1992). Thus, workers can easily communicate their complaints about wages directly to managers. However, the effectiveness of mechanisms provided to workers to file complaints about wages or work environments (e.g., grievance procedures and suggestion boxes) in solving labor-management disputes has hitherto been controversial. Freeman and Medoff (1984) argued that it makes sense for both employers and employees to have a grievance mechanism based on the exit voice model (Hirschman 1970). However, field research into employees’ grievance activities has found that post-grievance settlement, grievance filers have lower promotion, attendance, and performance ratings and higher turnover rates than non-filers (e.g., Lewin and Peterson 1988). It is noteworthy that these studies relied on a subjective performance rating (hence the findings imply that employees are punished for filing grievances to some extent). Olson-Buchanan’s (1996) experiment revealed that grievance filers have lower objective job performance after grievance filing than non-filers, although they are involved in the same wage-related disputes. However, the author could not conclude that employees’ low performance was due to grievance systems since filing a grievance was at the subject’s discretion (i.e., grievance filing was not manipulated). Thus, these studies could not explicitly confirm a causal nexus between the grievance system and the outcomes. This study experimentally examines how workers’ wage claims affect the reciprocal relationships between workers and managers. As noted by Akerlof (1982) and Akerlof and Yellen (1990), labor relations can be characterized as a “gift exchange” between those who may respect social norms such as fairness and reciprocity. Such norms induce workers to choose to supply a higher effort (i.e., increase their productivity) in response to a firm’s generous fixed wage offer, even without explicit performance incentives. An examination of the impact of workers’ wage claims on the gift exchange (reciprocal) relationships between workers and managers is important to understand their impact on labor-management disputes. Will firms comply with workers’ wage claims? How do workers’ claims affect their fairness and reciprocity? What impact do these wage claims have on economic welfare? This study used a laboratory experiment to answer these questions. Our experimental design comprises three treatments to perform a detailed investigation into the content of workers’ claims. The first and baseline is represented by the bilateral gift exchange game (GE). The others are determined by a costless and non-binding voice (i.e., cheap talk) about contract wages. In GE, a firm makes a wage offer to its worker, and the worker has the option to either reject it or accept it. If the worker rejects the offer, both the firm and the worker earn nothing. If the worker accepts the wage offer, they are then free to choose their effort level. A higher effort level imposes higher costs on the worker and yields higher total welfare by increasing the firm’s payoff. In the intention treatment (IT), before the firm makes a wage offer, the worker communicates the wage they want to receive and the effort level they will provide if they receive it (“voicing the intention”). The request treatment (RT) is identical to the IT treatment, except that the worker only communicates the wage they want to receive (“voicing the desirable wage”). Regardless of the treatment, we adopt a strategy method to elicit the worker’s minimum acceptable wage (i.e., reservation wage) and effort schedule depending upon the firm’s wage offer. This will reflect their preference for the outcomes in a one-shot gift exchange game. Each treatment consists of 10 periods, in which the probability of meeting the same subject more than once was zero (i.e., one-shot game setting), and all trades were anonymous. Thus, there is no room for repeated game effects. Our data prove that workers’ voices undermine the reciprocal relationships between workers and managers. First, the contract rates in IT and RT were lower than those in GE. This may be because the firm’s offer in RT is lower than in GE, whereas the worker’s reservation wage in IT is higher than in GE. Second, the slopes of the effort schedules in IT and RT are lower than in GE, although the effort schedules are upward-sloping in all treatments, as suggested by the “gift exchange hypothesis.” As a result, actual workers’ efforts in agreed contracts in IT and RT are lower than those in GE. Workers’ changing reciprocity is also confirmed by estimating a utility function based on the inequality aversion model (Fehr and Schmidt 1999); both the voicing intention and desirable wage increase (decrease) subjects’ disutility from disadvantageous (advantageous) inequality. These results indicate that workers’ social preferences change in a self-serving direction in the sense that they conflate what is fair with their self-interests rather than the self-interested direction in the sense that they maximize their monetary payoffs.Footnote 1 Decreasing contract rates and undermining reciprocity explains the deteriorating efficiency of gift exchange games. Workers’ voices reduce total welfare in the gift exchange game (there is a slight lack of significance in this result in IT). Owing to the reciprocity of workers, the coordination aspect may be inherent in gift exchange games. Cheap talk can be used as a coordination device to address this (e.g., Cooper et al. 1992). Indeed, the firm’s wage offer depends on a worker’s cheap talk: the firm’s wage offers in RT increase as workers’ wage claims increase to a certain threshold and then decrease. Unlike RT, a positive linear relationship was found in IT. However, in RT and IT, firms were unwilling to offer wages higher than the average wage offered in GE. This result may support the possibility that deceptively using a message may make a firm skeptical. Firms may ignore workers’ messages because workers have a strong temptation to misrepresent their intentions through cheap talk. They never matched with the same partners twice in the exercise. In real life, interactions are long-term. Consequently, long-term interaction also needs to be examined. Further experiments were conducted to verify whether long-term interactions allow workers’ voices to work as coordination devices. We compared the three treatments in a repeated situation, in which the same pair interacted in all periods. Additional data indicate that workers’ cheap talk (both voicing intention and desirable wages) undermines reciprocal behavior even in repeated situations. However, voicing intention leads to a higher wage offer, which induces higher actual effort levels. The remainder of this paper is organized as follows. Section 2 reviews similar research in a literature review. Section 3 describes the experimental design and procedure. Section 4 presents experimental results. Section 5 outlines the main results of the additional experiments. Finally, concluding remarks are presented in Sect. 6.",
18.0,3.0,Journal of Economic Interaction and Coordination,31 March 2023,https://link.springer.com/article/10.1007/s11403-023-00379-8,Microfounding GARCH models and beyond: a Kyle-inspired model with adaptive agents,July 2023,Michele Vodret,Iacopo Mastromatteo,Michael Benzaquen,Female,Male,Male,Mix,,
18.0,3.0,Journal of Economic Interaction and Coordination,23 May 2023,https://link.springer.com/article/10.1007/s11403-023-00381-0,"Buy, sell or rent the farm: succession planning and the future of farming on the Great Plains",July 2023,Chi Su,Richard A. Schoney,James F. Nolan,,Male,Male,Mix,,
18.0,3.0,Journal of Economic Interaction and Coordination,04 April 2023,https://link.springer.com/article/10.1007/s11403-023-00382-z,Dynamic effects of social influence on asset prices,July 2023,Jia-Ping Huang,Yang Zhang,Juanxi Wang,,,Unknown,Mix,,
